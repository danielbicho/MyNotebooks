{"url": "https://en.wikipedia.org/wiki?curid=7681", "text": "ClearType\n\nClearType is Microsoft's implementation of subpixel rendering technology in rendering text in a font system. ClearType attempts to improve the appearance of text on certain types of computer display screens by sacrificing color fidelity for additional intensity variation. This trade-off is asserted to work well on LCD flat panel monitors.\n\nClearType was first announced at the November 1998 COMDEX exhibition. The technology was first introduced in software in January 2000 as an always-on feature of Microsoft Reader, which was released to the public in August 2000.\n\nClearType was significantly changed with the introduction of DirectWrite in Windows 7.\n\nWord 2013 stopped using ClearType, because \"There is a problem with ClearType: it depends critically on the color of the background pixels.\"\n\nComputer displays where the positions of individual pixels are permanently fixed such as most modern flat panel displays can show saw-tooth edges when displaying small, high-contrast graphic elements, such as text. ClearType uses spatial anti-aliasing at the subpixel level to reduce visible artifacts on such displays when text is rendered, making the text appear \"smoother\" and less jagged. ClearType also uses very heavy font hinting to force the font to fit into the pixel grid. This increases edge contrast and readability of small fonts at the expense of font rendering fidelity and has been criticized by graphic designers for making different fonts look similar.\n\nLike most other types of subpixel rendering, ClearType involves a compromise, sacrificing one aspect of image quality (color or \"chrominance\" detail) for another (light and dark or \"luminance\" detail). The compromise can improve text appearance when luminance detail is more important than chrominance.\n\nOnly user and system applications render application of ClearType. ClearType does not alter other graphic display elements (including text already in bitmaps). For example, ClearType enhancement renders text on the screen in Microsoft Word, but text placed in a bitmapped image in a program such as Adobe Photoshop is not. In theory, the method (called \"RGB Decimation\" internally) can enhance the anti-aliasing of any digital image.\n\nClearType is not used when printing text. Most printers already use such small pixels that aliasing is rarely a problem, and they don't have the addressable fixed subpixels ClearType requires. Nor does ClearType affect text stored in files. ClearType only applies any processing to the text while it is being rendered onto the screen.\n\nClearType was invented in the Microsoft e-Books team by Bert Keely and Greg Hitchcock. It was then analyzed by researchers in the company, and signal processing expert John Platt designed an improved version of the algorithm. Dick Brass, a Vice President at Microsoft from 1997 to 2004, complained that the company was slow in moving ClearType to market in the portable computing field.\n\nNormally, the software in a computer treats the computer’s display screen as a rectangular array of square, indivisible pixels, each of which has an intensity and color that are determined by the blending of three primary colors: red, green, and blue. However, actual display hardware usually implements each pixel as a group of three adjacent, independent \"subpixels,\" each of which displays a different primary color. Thus, on a real computer display, each pixel is actually composed of separate red, green, and blue subpixels. For example, if a flat-panel display is examined under a magnifying glass, the pixels may appear as follows:\n\nIn the illustration above, there are nine pixels but 27 subpixels.\n\nIf the computer controlling the display knows the exact position and color of all the subpixels on the screen, it can take advantage of this to improve the apparent resolution in certain situations. If each pixel on the display actually contains three rectangular subpixels of red, green, and blue, in that fixed order, then things on the screen that are smaller than one full pixel in size can be rendered by lighting only one or two of the subpixels. For example, if a diagonal line with a width smaller than a full pixel must be rendered, then this can be done by lighting only the subpixels that the line actually touches. If the line passes through the leftmost portion of the pixel, only the red subpixel is lit; if it passes through the rightmost portion of the pixel, only the blue subpixel is lit. This effectively triples the horizontal resolution of the image at normal viewing distances; the drawback is that the line thus drawn will show color fringes (at some points it might look green, at other points it might look red or blue).\n\nClearType uses this method to improve the smoothness of text. When the elements of a type character are smaller than a full pixel, ClearType lights only the appropriate subpixels of each full pixel in order to more closely follow the outlines of that character. Text rendered with ClearType looks “smoother” than text rendered without it, provided that the pixel layout of the display screen exactly matches what ClearType expects.\n\nThe following picture shows a 4× enlargement of the word \"Wikipedia\" rendered using ClearType. The word was originally rendered using a Times New Roman 12 pt font.\n\nIn this magnified view, it becomes clear that, while the overall smoothness of the text seems to improve, there is also color fringing of the text.\n\nAn extreme close-up of a color display shows (a) text rendered without ClearType and (b) text rendered with ClearType. Note the changes in subpixel intensity that are used to increase effective resolution when ClearType is enabled without ClearType, all sub-pixels of a given pixel have the same intensity.\n\nIn the above lines of text, when the orange circle is shown, all the text in the frame is rendered using ClearType (RGB subpixel rendering); when the orange circle is absent all the text is rendered using normal (full pixel greyscale) anti-aliasing.\n\nClearType and similar technologies work on the theory that variations in intensity are more noticeable than variations in color.\n\nIn a MSDN article, Microsoft acknowledges that \"[te]xt that is rendered with ClearType can also appear significantly different when viewed by individuals with varying levels of color sensitivity. Some individuals can detect slight differences in color better than others.\" This opinion is shared by font designer Thomas Phinney (Vice President of FontLab and formerly with Adobe Systems): \"There is also considerable variation between individuals in their sensitivity to color fringing. Some people just notice it and are bothered by it a lot more than others.\" Software developer Melissa Elliot has written about finding ClearType rendering uncomfortable to read, saying that \"instead of seeing black text, I see blue text, and rendered over it but offset by a pixel or two, I see orange text, and someone reached into a bag of purple pixel glitter and just tossed it on...I’m not the only person in the world with this problem, and yet, every time it comes up, people are quick to assure me it works for them as if that’s supposed to make me feel better.\"\n\nHinting expert Beat Stamm, who worked on ClearType at Microsoft, agrees that ClearType may look blurry at 96 dpi, which was a typical resolution for LCDs in 2008, but adds that higher resolution displays improve on this aspect: \"WPF [Windows Presentation Foundation] uses method C [ClearType with fractional pixel positioning], but few display devices have a sufficiently high resolution to make the potential blur a moot point for everybody. . . . Some people are ok with the blur in Method C, some aren’t. Anecdotal evidence suggests that some people are fine with Method C when reading continuous text at 96 dpi (e.g. Times Reader, etc.) but not in UI scenarios. Many people are fine with the colors of ClearType, even at 96 dpi, but a few aren’t… To my eyes and at 96 dpi, Method C doesn’t read as well as Method A. It reads “blurrily” to me. Conversely, at 144 dpi, I don’t see a problem with Method C. It looks and reads just fine to me.\" One illustration of the potential problem is the following image:\n\nIn the above block of text, the same portion of text is shown in the upper half without and in the lower half with ClearType rendering (as opposed to Standard and ClearType in the previous image). This and the previous example with the orange circle demonstrate the blurring introduced.\n\nA 2001 study, conducted by researchers from Clemson University and The University of Pennsylvania on \"18 users who spent 60 minutes reading fiction from each of three different displays\" found that \"When reading from an LCD display, users preferred text rendered with ClearType™. ClearType also yielded higher readability judgments and lower ratings of mental fatigue.\" A 2002 study on 24 users conducted by the same researchers from Clemson University also found that \"Participants were significantly more accurate at identifying words with ClearType™ than without ClearType™.\"\n\nAccording to a 2006 study, at the University of Texas at Austin by Dillon et al., ClearType \"may not be universally beneficial\". The study notes that maximum benefit may be seen when the information worker is spending large proportions of their time reading text (which is not necessarily the case for the majority of computer users today). Additionally, over one third of the study participants experienced some disadvantage when using ClearType. Whether ClearType, or other rendering, should be used is very subjective and it must be the choice of the individual, with the report recommending \"to allow users to disable [ClearType] if they find it produces effects other than improved performance\".\n\nAnother 2007 empirical study, found that \"while ClearType rendering does not improve text legibility, reading speed or comfort compared to perceptually-tuned grayscale rendering, subjects prefer text with moderate ClearType rendering to text with grayscale or higher-level ClearType contrast.\"\n\nA 2007 survey, of the literature by Microsoft researcher Kevin Larson presented a different picture: \"Peer-reviewed studies have consistently found that using ClearType boosts reading performance compared with other text-rendering systems. In a 2004 study, for instance, Lee Gugerty, a psychology professor at Clemson University, in South Carolina, measured a 17 percent improvement in word recognition accuracy with ClearType. Gugerty’s group also showed, in a sentence comprehension study, that ClearType boosted reading speed by 5 percent and comprehension by 2 percent. Those results were unusual because, typically, any gain in reading speed decreases comprehension. Similarly, in a study published last year, psychologist Andrew Dillon at the University of Texas at Austin found that when subjects were asked to scan a spreadsheet and pick out certain information, they did those tasks 7 percent faster with ClearType.\"\n\nClearType and allied technologies require display hardware with fixed pixels and subpixels. More precisely, the positions of the pixels and subpixels on the screen must be exactly known to the computer to which it is connected. This is the case for flat-panel displays, on which the positions of the pixels are permanently fixed by the design of the screen itself. Almost all flat panels have a perfectly rectangular array of square pixels, each of which contains three rectangular subpixels in the three primary colors, with the normal ordering being red, green, and blue, arranged in vertical bands. ClearType assumes this arrangement of pixels when rendering text.\n\nClearType does not work properly with flat-panel displays that are operated at resolutions other than their “native” resolutions, since only the native resolution corresponds exactly to the actual positions of pixels on the screen of the display.\n\nIf a display does not have the type of fixed pixels that ClearType expects, text rendered with ClearType enabled actually looks worse than type rendered without it. Some flat panels have unusual pixel arrangements, with the colors in a different order, or with the subpixels positioned differently (in three horizontal bands, or in other ways). ClearType needs to be manually tuned for use with such displays (see below).\n\nClearType will not work as intended on displays that have no fixed pixel positions, such as CRT displays, however it will still have some antialiasing effect and may be preferable to some users as compared to non-anti-aliased type.\n\nBecause ClearType utilizes the physical layout of the red, green and blue pigments of the LCD screen, it is sensitive to the orientation of the display.\n\nClearType in Windows XP currently supports the RGB and BGR sub pixel structures. Rotated displays, in which the subpixels are arranged vertically rather than horizontally, are \"not\" currently supported. Using ClearType on these display configurations will actually reduce the display quality. The best option for users of Windows XP having rotated LCD displays (Tablet PCs or swivel-stand LCD displays) is using regular anti-aliasing, or switching off font-smoothing altogether.\n\nThe software developer documentation for Windows CE states that ClearType for rotated screens is supported on that platform.\n\nVertical sub pixel structures are not supported in Windows XP.\n\n\nClearType is also an integrated component of the Windows Presentation Foundation text-rendering engine.\n\nClearType can be globally enabled or disabled for GDI applications. A control panel applet is available to let the users tune the GDI ClearType settings. The GDI implementation of ClearType does not support sub-pixel positioning.\n\nSome versions of Microsoft Windows, as supplied, allow ClearType to be turned on or off, with no adjustment; other versions allow tuning of the ClearType parameters. A Microsoft ClearType tuner utility is available for free download for Windows versions lacking this facility. If ClearType is disabled in the operating system, applications with their own ClearType controls can still support it. Microsoft Reader (for e-books) has its own ClearType tuner.\n\nAll text in Windows Presentation Foundation is anti-aliased and rendered using ClearType. There are separate ClearType registry settings for GDI and WPF applications, but by default the WPF entries are absent, and the GDI values are used in their absence. WPF registry entries can be tuned using the instructions from the MSDN WPF Text Blog.\n\nClearType in WPF supports sub-pixel positioning, natural advance widths, Y-direction anti-aliasing and hardware acceleration. WPF supports aggressive caching of pre-rendered ClearType text in video memory. The extent to which this is supported is dependent on the video card. DirectX 10 cards will be able to cache the font glyphs in video memory, then perform the composition (assembling of character glyphs in the correct order, with the correct spacing), alpha blending (application of anti-aliasing), and RGB blending (ClearType's sub-pixel color calculations), entirely in hardware. This means that only the original glyphs need to be stored in video memory once per font (Microsoft estimates that this would require 2 MB of video memory per font), and other operations such as the display of anti-aliased text on top of other graphics including video can also be done with no computation effort on the part of the CPU. DirectX 9 cards will only be able to cache the alpha-blended glyphs in memory, thus requiring the CPU to handle glyph composition and alpha-blending before passing this to the video card. Caching these partially rendered glyphs requires significantly more memory (Microsoft estimates 5 MB per process). Cards that don't support DirectX 9 have no hardware-accelerated text rendering capabilities.\n\nThe font rendering engine in DirectWrite supports an improved version of ClearType, as demonstrated at PDC 2008. The improved version is sometimes called \"Natural ClearType\". The improvements have been confirmed by independent sources, such as Firefox developers; they were particularly noticeable for OpenType fonts in Compact Font Format (CFF).\n\nDespite these improvements, Word 2013 stopped using ClearType. The reasons invoked are, in the words of Murray Sargent: \"There is a problem with ClearType: it depends critically on the color of the background pixels. This isn’t a problem if you know a priori that those pixels are white, which is usually the case for text. But the general case involves calculating what the colors should be for an arbitrary background and that takes time. Meanwhile, Word 2013 enjoys cool animations and smooth zooming. Nothing jumps any more. Even the caret (the blinking vertical line at the text insertion point) glides from one position to the next as you type. Jerking movement just isn’t considered cool any more. Well animations and zooms have to be faster than human response times in order to appear smooth. And that rules out ClearType in animated scenarios at least with present generation hardware. And in future scenarios, screens will have sufficiently high resolution that gray-scale anti-aliasing should suffice.\"\n\nFor the same reasons related to animation performance, the color-ware version of ClearType was abandoned in Metro and the Windows 8 (and 10) start menus.\n\nClearType is a registered trademark and Microsoft claims protection under the following U.S. patents:\n\n\nThe ClearType name was also used to refer to the screens of Microsoft Surface tablets. ClearType HD Display includes a 1366×768 screen, while ClearType Full HD Display includes a 1920×1080 screen.\n\n\n", "id": "7681", "title": "ClearType"}
{"url": "https://en.wikipedia.org/wiki?curid=7682", "text": "Centriole\n\nIn cell biology a centriole is a cylindrical cell structure composed mainly of a protein called tubulin that is found in most eukaryotic cells. An associated pair of centrioles, surrounded by a shapeless mass of dense material, called the pericentriolar material, or PCM, makes up a compound structure called a centrosome.\n\nCentrioles are present in the cells of most eukaryotes, for example those of animals. However, they are absent from conifers (pinophyta), flowering plants (angiosperms) and most fungi, and are only present in the male gametes of charophytes, bryophytes, seedless vascular plants, cycads, and ginkgo.\n\nMost centrioles are made up of nine sets of microtubule triplets, arranged in a cylinder.\nDeviations from this structure include crabs and \"Drosophila melanogaster\" embryos, with nine doublets, and \"Caenorhabditis elegans\" sperm cells and early embryos, with nine singlets.\nEdouard van Beneden and Theodor Boveri made the first observation and identification of centrioles in 1883 and 1888 respectively, while the pattern of centriole duplication was first worked out independently by Etienne de Harven and Joseph G. Gall c. 1950 \nThe main function of centrioles is to produce aster and spindle during cell division.\n\nCentrioles are involved in the organization of the mitotic spindle and in the completion of cytokinesis. Centrioles were previously thought to be required for the formation of a mitotic spindle in animal cells. However, more recent experiments have demonstrated that cells whose centrioles have been removed via laser ablation can still progress through the G stage of interphase before centrioles can be synthesized later in a de novo fashion. Additionally, mutant flies lacking centrioles develop normally, although the adult flies' cells lack flagella and cilia and as a result, they die shortly after birth.\nThe centrioles can self replicate during cell division.\n\nCentrioles are a very important part of centrosomes, which are involved in organizing microtubules in the cytoplasm. The position of the centriole determines the position of the nucleus and plays a crucial role in the spatial arrangement of the cell.\n\nIn organisms with flagella and cilia, the position of these organelles is determined by the mother centriole, which becomes the basal body. An inability of cells to use centrioles to make functional cilia and flagella has been linked to a number of genetic and developmental diseases. In particular, the inability of centrioles to properly migrate prior to ciliary assembly has recently been linked to Meckel-Gruber syndrome.\n\nProper orientation of cilia via centriole positioning toward the posterior of embryonic node cells is critical for establishing left–right asymmetry during mammalian development.\n\nBefore DNA replication, cells contain two centrioles. The older of the two centrioles is termed the \"mother centriole\", the other the \"daughter\". During the cell division cycle, a new centriole grows from each side of the mother centriole. After duplication, the two centriole pairs will remain attached to each other orthogonally until mitosis. At that point the mother and daughter centrioles separate dependently on an enzyme called separase.\n\nThe two centrioles in the centrosome are tied to one another. The mother centriole has radiating appendages at the distal end of its long axis and is attached to its daughter at the proximal end. Each daughter cell formed after cell division will inherit one of these pairs. Centrioles start duplicating when DNA replicates.\n\nThe last common ancestor of all eukaryotes was a ciliated cell with centrioles. Some lineages of eukaryotes, such as land plants, do not have centrioles except in their motile male gametes. Centrioles are completely absent from all cells of conifers and flowering plants, which do not have ciliate or flagellate gametes.\nIt is unclear if the last common ancestor had one or two cilia. Important genes required for centriole growth, like centrins, are only found in eukaryotes and not in bacteria or archaeans.\n\nThe word \"centriole\" () uses combining forms of \"centri-\" and \"-ole\", yielding \"little central part\", which describes a centriole's typical location near the center of the cell.\n", "id": "7682", "title": "Centriole"}
{"url": "https://en.wikipedia.org/wiki?curid=7683", "text": "Creation science\n\nCreation science or scientific creationism is a branch of creationism that claims to provide scientific support for the Genesis creation narrative in the Book of Genesis and disprove or reexplain the scientific facts, theories and scientific paradigms about geology, cosmology, biological evolution, archeology, history, and linguistics.\n\nThe overwhelming consensus of the scientific community is that creation science is a religious, not a scientific view. It fails to qualify as a science because it lacks empirical support, supplies no tentative hypotheses, and resolves to describe natural history in terms of scientifically untestable supernatural causes. Creation science is a pseudoscientific attempt to map the Bible into scientific facts. It is viewed by professional scientists as unscholarly, and even as a dishonest and misguided sham, with extremely harmful educational consequences.\n\nCreation science began in the 1960s, as a fundamentalist Christian effort in the United States to prove Biblical inerrancy and nullify the scientific evidence for evolution. It has since developed a sizable religious following in the United States, with creation science ministries branching worldwide. The main ideas in creation science are: the belief in \"creation \"ex nihilo\"\" (Latin: out of nothing); the conviction that the Earth was created within the last 6,000–10,000 years; the belief that mankind and other life on Earth were created as distinct fixed \"baraminological\" \"kinds\"; and the idea that fossils found in geological strata were deposited during a cataclysmic flood which completely covered the entire Earth. As a result, creation science also challenges the commonly accepted geologic and astrophysical theories for the age and origins of the Earth and Universe, which creationists acknowledge are irreconcilable to the account in the Book of Genesis. Creation science proponents often refer to the theory of evolution as \"Darwinism\" or as \"Darwinian evolution.\"\n\nThe creation science texts and curricula that first emerged in the 1960s focused upon concepts derived from a literal interpretation of the Bible and were overtly religious in nature, most notably linking Noah's flood in the Biblical Genesis account to the geological and fossil record in a system termed flood geology. These works attracted little notice beyond the schools and congregations of conservative fundamental and Evangelical Christians until the 1970s when its followers challenged the teaching of evolution in the public schools and other venues in the United States, bringing it to the attention of the public-at-large and the scientific community. Many school boards and lawmakers were persuaded to include the teaching of creation science alongside evolution in the science curriculum. Creation science texts and curricula used in churches and Christian schools were revised to eliminate their Biblical and theological references, and less explicitly sectarian versions of creation science education were introduced in public schools in Louisiana, Arkansas, and other regions in the United States.\n\nThe 1982 ruling in \"McLean v. Arkansas\" found that creation science fails to meet the essential characteristics of science and that its chief intent is to advance a particular religious view. The teaching of creation science in public schools in the United States effectively ended in 1987 following the United States Supreme Court decision in \"Edwards v. Aguillard\". The court affirmed that a statute requiring the teaching of creation science alongside evolution when evolution is taught in Louisiana public schools was unconstitutional because its sole true purpose was to advance a particular religious belief. In response to this ruling, drafts of the creation science school textbook \"Of Pandas and People\" were edited to change references of creation to intelligent design before its publication in 1989. The intelligent design movement promoted this version, then teaching intelligent design in public school science classes was found to be unconstitutional in the 2005 \"Kitzmiller v. Dover Area School District\" federal court case.\n\nCreation science is based largely upon chapters 1–11 of the Book of Genesis. These describe how God calls the world into existence through the power of speech (\"And God said, Let there be light,\" etc.) in six days, calls all the animals and plants into existence, and molds the first man from clay and the first woman from a rib taken from the man's side; a world-wide flood destroys all life except for Noah and his family and representatives of the animals, and Noah becomes the ancestor of the 70 \"nations\" of the world; the nations live together until the incident of the Tower of Babel, when God disperses them and gives them their different languages. Creation science rarely goes beyond biblical stories in its study, and attempts to explain history and science within the span of Biblical chronology, which places the initial act of creation some six thousand years ago.\n\nMost creation science proponents hold fundamentalist or Evangelical Christian beliefs in Biblical literalism or Biblical inerrancy, as opposed to the higher criticism supported by Liberal Christianity in the Fundamentalist–Modernist Controversy. However, there are also examples of Islamic and Jewish scientific creationism that conform to the accounts of creation as recorded in their religious doctrines.\n\nThe Seventh-day Adventist Church has a history of support for creation science. This dates back to George McCready Price, an active Seventh-day Adventist who developed views of flood geology, which formed the basis of creation science. This work was continued by the Geoscience Research Institute, an official institute of the Seventh-day Adventist Church, located on its Loma Linda University campus in California.\n\nCreation science is generally rejected by the Church of England as well as the Roman Catholic Church. The Pontifical Gregorian University has officially discussed intelligent design as a \"cultural phenomenon\" without scientific elements. The Church of England's official website cites Charles Darwin's local work assisting people in his religious parish.\n\nCreation science rejects evolution's theory of the common descent of all living things on the Earth. Instead, it asserts that the field of evolutionary biology is itself pseudoscientific or even a religion. Creationists argue instead for a system called baraminology, which considers the living world to be descended from uniquely created kinds or \"baramins.\"\n\nCreation science incorporates the concept of catastrophism to reconcile current landforms and fossil distributions with Biblical interpretations, proposing the remains resulted from successive cataclysmic events, such as a world-wide flood and subsequent ice age. It rejects one of the fundamental principles of modern geology (and of modern science generally), uniformitarianism, which applies the same physical and geological laws observed on the Earth today to interpret the Earth's geological history.\n\nSometimes creationists attack other scientific concepts, like the Big Bang cosmological model or methods of scientific dating based upon radioactive decay. Young Earth creationists also reject current estimates of the age of the universe and the age of the Earth, arguing for creationist cosmologies with timescales much shorter than those determined by modern physical cosmology and geological science, typically less than 10,000 years.\n\nThe scientific community has overwhelmingly rejected the ideas put forth in creation science as lying outside the boundaries of a legitimate science. The foundational premises underlying scientific creationism disqualify it as a science because the answers to all inquiry therein are preordained to conform to Bible doctrine, and because that inquiry is constructed upon theories which are not empirically testable in nature. Scientists also deem creation science's attacks against biological evolution to be without scientific merit. Those views of the scientific community were accepted in two significant court decisions in the 1980s which found the field of creation science to be a religious mode of inquiry, not a scientific one.\n\nThe teaching of evolution was gradually introduced into more and more public high school textbooks in the United States after 1900, but in the aftermath of the First World War the growth of fundamentalist Christianity gave rise to a creationist opposition to such teaching. Legislation prohibiting the teaching of evolution was passed in certain regions, most notably Tennessee's Butler Act of 1925. The Soviet Union's successful launch of \"Sputnik 1\" in 1957 sparked national concern that the science education in public schools was outdated. In 1958, the United States passed National Defense Education Act which introduced new education guidelines for science instruction. With federal grant funding, the Biological Sciences Curriculum Study (BSCS) drafted new standards for the public schools' science textbooks which included the teaching of evolution. Almost half the nation's high schools were using textbooks based on the guidelines of the BSCS soon after they were published in 1963. The Tennessee legislature did not repeal the Butler Act until 1967.\n\nCreation science (dubbed \"scientific creationism\" at the time) emerged as an organized movement during the 1960s. It was strongly influenced by the earlier work of armchair geologist George McCready Price who wrote works such as \"The New Geology\" (1923) to advance what he termed \"new catastrophism\" and dispute the current geological time frames and explanations of geologic history. Price's work was cited at the Scopes Trial of 1925, yet although he frequently solicited feedback from geologists and other scientists, they consistently disparaged his work. Price's \"new catastrophism\" also went largely unnoticed by other creationists until its revival with the 1961 publication of \"\" by John C. Whitcomb and Henry M. Morris, a work which quickly became an important text on the issue to fundamentalist Christians and expanded the field of creation science beyond critiques of geology into biology and cosmology as well. Soon after its publication, a movement was underway to have the subject taught in United States' public schools.\n\nThe various state laws prohibiting teaching of evolution were overturned in 1968 when the United States Supreme Court ruled in \"Epperson v. Arkansas\" such laws violated the Establishment Clause of the First Amendment to the United States Constitution. This ruling inspired a new creationist movement to promote laws requiring that schools give balanced treatment to creation science when evolution is taught. The 1981 Arkansas Act 590 was one such law that carefully detailed the principles of creation science that were to receive equal time in public schools alongside evolutionary principles. The act defined creation science as follows:\n\n\"'Creation-science' means the scientific evidences for creation and inferences from those evidences. Creation-science includes the scientific evidences and related inferences that indicate:\n\nThis legislation was examined in \"McLean v. Arkansas\", and the ruling handed down on January 5, 1982, concluded that creation-science as defined in the act \"is simply not science\". The judgement defined the following as essential characteristics of science:\n\nThe court ruled that creation science failed to meet these essential characteristics and identified specific reasons. After examining the key concepts from creation science, the court found:\n\nThe court further noted that no recognized scientific journal had published any article espousing the creation science theory as described in the Arkansas law, and stated that the testimony presented by defense attributing the absence to censorship was not credible.\n\nIn its ruling, the court wrote that for any theory to qualify as scientific, the theory must be tentative, and open to revision or abandonment as new facts come to light. It wrote that any methodology which begins with an immutable conclusion which cannot be revised or rejected, regardless of the evidence, is not a scientific theory. The court found that creation science does not culminate in conclusions formed from scientific inquiry, but instead begins with the conclusion, one taken from a literal wording of the Book of Genesis, and seeks only scientific evidence to support it.\n\nThe law in Arkansas adopted the same two-model approach as that put forward by the Institute for Creation Research, one allowing only two possible explanations for the origins of life and existence of man, plants and animals: it was either the work of a creator or it was not. Scientific evidence that failed to support the theory of evolution was posed as necessarily scientific evidence in support of creationism, but in its judgment the court ruled this approach to be no more than a \"contrived dualism which has not scientific factual basis or legitimate educational purpose.\"\n\nThe judge concluded that \"Act 590 is a religious crusade, coupled with a desire to conceal this fact,\" and that it violated the First Amendment's Establishment Clause.\n\nThe decision was not appealed to a higher court, but had a powerful influence on subsequent rulings. Louisiana's 1982 Balanced Treatment for Creation-Science and Evolution-Science Act, authored by State Senator Bill P. Keith, judged in the 1987 United States Supreme Court case \"Edwards v. Aguillard\", and was handed a similar ruling. It found the law to require the balanced teaching of creation science with evolution had a particular religious purpose and was therefore unconstitutional.\n\nIn 1984, \"The Mystery of Life's Origin\" was first published. It was co-authored by chemist and creationist Charles B. Thaxton with Walter L. Bradley and Roger L. Olsen, the foreword written by Dean H. Kenyon, and sponsored by the Christian-based Foundation for Thought and Ethics (FTE). The work presented scientific arguments against current theories of abiogenesis and offered an hypothesis of special creation instead. While the focus of creation science had until that time centered primarily on the criticism of the fossil evidence for evolution and validation of the creation myth of the Bible, this new work posed the question whether science reveals that even the simplest living systems were far too complex to have developed by natural, unguided processes.\n\nKenyon later co-wrote with creationist Percival Davis a book intended as a \"scientific brief for creationism\" to use as a supplement to public high school biology textbooks. Thaxton was enlisted as the book's editor, and the book received publishing support from the FTE. Prior to its release, the 1987 Supreme Court ruling in \"Edwards v. Aguillard\" barred the teaching of creation science and creationism in public school classrooms. The book, originally titled \"Biology and Creation\" but renamed \"Of Pandas and People\", was released in 1989 and became the first published work to promote the anti-evolutionist design argument under the name intelligent design. The contents of the book later became a focus of evidence in the federal court case, \"Kitzmiller v. Dover Area School District\", when a group of parents filed suit to halt the teaching of intelligent design in Dover, Pennsylvania, public schools. School board officials there had attempted to include \"Of Pandas and People\" in their biology classrooms and testimony given during the trial revealed the book was originally written as a creationist text but following the adverse decision in the Supreme Court it underwent simple cosmetic editing to remove the explicit allusions to \"creation\" or \"creator,\" and replace them instead with references to \"design\" or \"designer.\"\n\nBy the mid-1990s, intelligent design had become a separate movement. The creation science movement is distinguished from the intelligent design movement, or neo-creationism, because most advocates of creation science accept scripture as a literal and inerrant historical account, and their primary goal is to corroborate the scriptural account through the use of science. In contrast, as a matter of principle, neo-creationism eschews references to scripture altogether in its polemics and stated goals (see Wedge strategy). By so doing, intelligent design proponents have attempted to succeed where creation science has failed in securing a place in public school science curricula. Carefully avoiding any reference to the identity of the intelligent designer as God in their public arguments, intelligent design proponents sought to reintroduce the creationist ideas into science classrooms while sidestepping the First Amendment's prohibition against religious infringement. However, the intelligent design curriculum was struck down as a violation of the Establishment Clause in \"Kitzmiller v. Dover Area School District\", the judge in the case ruling \"that ID is nothing less than the progeny of creationism.\"\n\nToday, creation science as an organized movement is primarily centered within the United States. Creation science organizations are also known in other countries, most notably Creation Ministries International which was founded (under the name Creation Science Foundation) in Australia.\n\nProponents are usually aligned with a Christian denomination, primarily with those characterized as evangelical, conservative, or fundamentalist. While creationist movements also exist in Islam and Judaism, these movements do not use the phrase \"creation science\" to describe their beliefs.\n\nCreation science has its roots in the work of young Earth creationist George McCready Price disputing modern science's account of natural history, focusing particularly on geology and its concept of uniformitarianism, and his efforts instead to furnish an alternative empirical explanation of observable phenomena which was compatible with strict Biblical literalism. Price's work was later discovered by civil engineer Henry M. Morris, who is now considered to be the father of creation science. Morris and later creationists expanded the scope with attacks against the broad spectrum scientific findings that point to the antiquity of the Universe and common ancestry among species, including growing body of evidence from the fossil record, absolute dating techniques, and cosmogony.\n\nThe proponents of creation science often say that they are concerned with religious and moral questions as well as natural observations and predictive hypotheses. Many state that their opposition to scientific evolution is primarily based on religion.\n\nThe overwhelming majority of scientists are in agreement that the claims of science are necessarily limited to those that develop from natural observations and experiments which can be replicated and substantiated by other scientists, and that claims made by creation science do not meet those criteria. Duane Gish, a prominent creation science proponent, has similarly claimed, \"We do not know how the creator created, what processes He used, \"for He used processes which are not now operating anywhere in the natural universe.\" This is why we refer to creation as special creation. We cannot discover by scientific investigation anything about the creative processes used by the Creator.\" But he also makes the same claim against science's evolutionary theory, maintaining that on the subject of origins, scientific evolution is a religious theory which cannot be validated by science.\n\nCreation science makes the \"a priori\" metaphysical assumption that there exists a creator of the life whose origin is being examined. Christian creation science holds that the description of creation is given in the Bible, that the Bible is inerrant in this description (and elsewhere), and therefore empirical scientific evidence must correspond with that description. Creationists also view the preclusion of all supernatural explanations within the sciences as a doctrinaire commitment to exclude the supreme being and miracles. They claim this to be the motivating factor in science's acceptance of Darwinism, a term used in creation science to refer to evolutionary biology which is also often used as a disparagement. Critics argue that creation science is religious rather than scientific because it stems from faith in a religious text rather than by the application of the scientific method. The United States National Academy of Sciences (NAS) has stated unequivocally, \"Evolution pervades all biological phenomena. To ignore that it occurred or to classify it as a form of dogma is to deprive the student of the most fundamental organizational concept in the biological sciences. No other biological concept has been more extensively tested and more thoroughly corroborated than the evolutionary history of organisms.\" Anthropologist Eugenie Scott has noted further, \"Religious opposition to evolution propels antievolutionism. Although antievolutionists pay lip service to supposed scientific problems with evolution, what motivates them to battle its teaching is apprehension over the implications of evolution for religion.\"\n\nCreation science advocates argue that scientific theories of the origins of the Universe, Earth, and life are rooted in \"a priori\" presumptions of methodological naturalism and uniformitarianism, each of which is disputed. In some areas of science such as chemistry, meteorology or medicine, creation science proponents do not challenge the application of naturalistic or uniformitarian assumptions. Traditionally, creation science advocates have singled out those scientific theories judged to be in conflict with held religious beliefs, and it is against those theories that they concentrate their efforts.\n\nSome mainstream Christian churches criticize creation science on theological grounds, asserting either that religious faith alone should be a sufficient basis for belief in the truth of creation, or that efforts to prove the Genesis account of creation on scientific grounds are inherently futile because reason is subordinate to faith and cannot thus be used to prove it.\n\nMany Christian theologies, including Liberal Christianity, consider the Genesis creation narrative to be a poetic and allegorical work rather than a literal history, and many Christian churches—including the Roman Catholic, Anglican and the more liberal denominations of the Lutheran, Methodist, Congregationalist and Presbyterian faiths—have either rejected creation science outright or are ambivalent to it. Belief in non-literal interpretations of Genesis is often cited as going back to Saint Augustine.\n\nTheistic evolution and evolutionary creationism are theologies that reconcile belief in a creator with biological evolution. Each holds the view that there is a creator but that this creator has employed the natural force of evolution to unfold a divine plan. Religious representatives from faiths compatible with theistic evolution and evolutionary creationism have challenged the growing perception that belief in a creator is inconsistent with the acceptance of evolutionary theory. Spokespersons from the Catholic Church have specifically criticized biblical creationism for relying upon literal interpretations of biblical scripture as the basis for determining scientific fact.\n\nThe National Academy of Sciences states that \"the claims of creation science lack empirical support and cannot be meaningfully tested\" and that \"creation science is in fact not science and should not be presented as such in science classes.\" According to Joyce Arthur writing for \"Skeptic\" magazine, the \"creation 'science' movement gains much of its strength through the use of distortion and scientifically unethical tactics\" and \"seriously misrepresents the theory of evolution.\"\n\nScientists have considered the hypotheses proposed by creation science and have rejected them because of a lack of evidence. Furthermore, the claims of creation science do not refer to natural causes and cannot be subject to meaningful tests, so they do not qualify as scientific hypotheses. In 1987, the United States Supreme Court ruled that creationism is religion, not science, and cannot be advocated in public school classrooms. Most mainline Christian denominations have concluded that the concept of evolution is not at odds with their descriptions of creation and human origins.\n\nA summary of the objections to creation science by scientists follows:\n\nBy invoking claims of \"abrupt appearance\" of species as a miraculous act, creation science is unsuited for the tools and methods demanded by science, and it cannot be considered scientific in the way that the term \"science\" is currently defined. Scientists and science writers commonly characterize creation science as a pseudoscience.\n\nHistorically, the debate of whether creationism is compatible with science can be traced back to 1874, the year science historian John William Draper published his \"History of the Conflict between Religion and Science\". In it Draper portrayed the entire history of scientific development as a war against religion. This presentation of history was propagated further by followers such as Andrew Dickson White in his two-volume \"A History of the Warfare of Science with Theology in Christendom\" (1896). Their conclusions have been disputed.\n\nIn the United States, the principal focus of creation science advocates is on the government-supported public school systems, which are prohibited by the Establishment Clause from promoting specific religions. Historical communities have argued that Biblical translations contain many translation errors and errata, and therefore that the use of biblical literalism in creation science is self-contradictory.\n\nSubjects within creation science correspond to the scientific disciplines of biology, earth sciences and astronomy.\n\nCreationist biology centers on an idea derived from Genesis that states that life was created by God, in a finite number of \"created kinds,\" rather than through biological evolution from a common ancestor. Creationists consider that any observable speciation descends from these distinctly created kinds through inbreeding, deleterious mutations and other genetic mechanisms. Whereas evolutionary biologists and creationists share similar views of microevolution, creationists disagree that the process of macroevolution can explain common ancestry among organisms far beyond the level of common species. Creationists contend that there is no empirical evidence for new plant or animal species, and deny fossil evidence has ever been found documenting the process.\n\nPopular arguments against evolution have changed since the publishing of Henry M. Morris' first book on the subject, \"Scientific Creationism\" (1974), but some consistent themes remain: that missing links or gaps in the fossil record are proof against evolution; that the increased complexity of organisms over time through evolution is not possible due to the law of increasing entropy; that it is impossible that the mechanism of natural selection could account for common ancestry; and that evolutionary theory is untestable. The origin of the human species is particularly hotly contested; the fossil remains of purported hominid ancestors are not considered by advocates of creation biology to be evidence for a speciation event involving \"Homo sapiens\". Creationists also assert that early hominids, are either apes, or humans.\n\nRichard Dawkins has explained evolution as \"a theory of gradual, incremental change over millions of years, which starts with something very simple and works up along slow, gradual gradients to greater complexity,\" and described the existing fossil record as entirely consistent with that process. Biologists emphasize that transitional gaps between those fossils recovered are to be expected, that the existence of any such gaps cannot be invoked to disprove evolution, and that instead the fossil evidence that could be used to disprove the theory would be those fossils which are found and which are entirely inconsistent with what can be predicted or anticipated by the evolutionary model. One example given by Dawkins was, \"If there were a single hippo or rabbit in the Precambrian, that would completely blow evolution out of the water. None have ever been found.\"\n\nFlood geology is a concept based on the belief that most of Earth's geological record was formed by the Great Flood described in the story of Noah's Ark. Fossils and fossil fuels are believed to have formed from animal and plant matter which was buried rapidly during this flood, while submarine canyons are explained as having formed during a rapid runoff from the continents at the end of the flood. Sedimentary strata are also claimed to have been predominantly laid down during or after Noah's flood and orogeny. Flood geology is a variant of catastrophism and is contrasted with geological science in that it rejects standard geological principles such as uniformitarianism and radiometric dating. For example, the Creation Research Society argues that \"uniformitarianism is wishful thinking.\"\n\nGeologists conclude that no evidence for such a flood is observed in the preserved rock layers and moreover that such a flood is physically impossible, given the current layout of land masses. For instance, since Mount Everest currently is approximately 8.8 kilometres in elevation and the Earth's surface area is 510,065,600 km, the volume of water required to cover Mount Everest to a depth of 15 cubits (6.8 m), as indicated by Genesis 7:20, would be 4.6 billion cubic kilometres. Measurements of the amount of precipitable water vapor in the atmosphere have yielded results indicating that condensing all water vapor in a column of atmosphere would produce liquid water with a depth ranging between zero and approximately 70mm, depending on the date and the location of the column. Nevertheless, there continue to be many adherents to flood geology, and in recent years new theories have been introduced such as catastrophic plate tectonics and catastrophic orogeny.\n\nCreationists point to experiments they have performed, which they claim demonstrate that 1.5 billion years of nuclear decay took place over a short period of time, from which they infer that \"billion-fold speed-ups of nuclear decay\" have occurred, a massive violation of the principle that radioisotope decay rates are constant, a core principle underlying nuclear physics generally, and radiometric dating in particular.\n\nThe scientific community points to numerous flaws in the creationists' experiments, to the fact that their results have not been accepted for publication by any peer-reviewed scientific journal, and to the fact that the creationist scientists conducting them were untrained in experimental geochronology. They have also been criticised for widely publicising the results of their research as successful despite their own admission of insurmountable problems with their hypothesis.\n\nThe constancy of the decay rates of isotopes is well supported in science. Evidence for this constancy includes the correspondences of date estimates taken from different radioactive isotopes as well as correspondences with non-radiometric dating techniques such as dendrochronology, ice core dating, and historical records. Although scientists have noted slight increases in the decay rate for isotopes subject to extreme pressures, those differences were too small to significantly impact date estimates. The constancy of the decay rates is also governed by first principles in quantum mechanics, wherein any deviation in the rate would require a change in the fundamental constants. According to these principles, a change in the fundamental constants could not influence different elements uniformly, and a comparison between each of the elements' resulting unique chronological timescales would then give inconsistent time estimates.\n\nIn refutation of young Earth claims of inconstant decay rates affecting the reliability of radiometric dating, Roger C. Wiens, a physicist specializing in isotope dating states:\n\n\nIn the 1970s, young Earth creationist Robert V. Gentry proposed that radiohaloes in certain granites represented evidence for the Earth being created instantaneously rather than gradually. This idea has been criticized by physicists and geologists on many grounds including that the rocks Gentry studied were not primordial and that the radionuclides in question need not have been in the rocks initially.\n\nThomas A. Baillieul, a geologist and retired senior environmental scientist with the United States Department of Energy, disputed Gentry's claims in an article entitled, \"'Polonium Haloes' Refuted: A Review of 'Radioactive Halos in a Radio-Chronological and Cosmological Perspective' by Robert V. Gentry.\" Baillieul noted that Gentry was a physicist with no background in geology and given the absence of this background, Gentry had misrepresented the geological context from which the specimens were collected. Additionally, he noted that Gentry relied on research from the beginning of the 20th century, long before radioisotopes were thoroughly understood; that his assumption that a polonium isotope caused the rings was speculative; and that Gentry falsely argued that the half-life of radioactive elements varies with time. Gentry claimed that Baillieul could not publish his criticisms in a reputable scientific journal, although some of Baillieul's criticisms rested on work previously published in reputable scientific journals.\n\nSeveral attempts have been made by creationists to construct a cosmology consistent with a young Universe rather than the standard cosmological age of the universe, based on the belief that Genesis describes the creation of the Universe as well as the Earth. The primary challenge for young-universe cosmologies is that the accepted distances in the Universe require millions or billions of years for light to travel to Earth (the \"starlight problem\"). An older creationist idea, proposed by creationist astronomer Barry Setterfield, is that the speed of light has decayed in the history of the Universe. More recently, creationist physicist Russell Humphreys has proposed a hypothesis called \"white hole cosmology\" which suggests that the Universe expanded out of a white hole less than 10,000 years ago; the apparent age of the universe results from relativistic effects. Humphreys' theory is advocated by creationist organisations such as Answers in Genesis; however because the predictions of Humphreys' cosmology conflict with current observations, it is not accepted by the scientific community.\n\nVarious claims are made by creationists concerning alleged evidence that the age of the Solar System is of the order of thousands of years, in contrast to the scientifically accepted age of 4.6 billion years. It is commonly argued that the number of comets in the Solar System is much higher than would be expected given its supposed age. Creationist astronomers express scepticism about the existence of the Kuiper belt and Oort cloud. Creationists also argue that the recession of the Moon from the Earth is incompatible with either the Moon or the Earth being billions of years old. These claims have been refuted by planetologists.\n\nIn response to increasing evidence suggesting that Mars once possessed a wetter climate, some creationists have proposed that the global flood affected not only the Earth but also Mars and other planets. People who support this claim include creationist astronomer Wayne Spencer and Russell Humphreys.\n\nAn ongoing problem for creationists is the presence of impact craters on nearly all Solar System objects, which is consistent with scientific explanations of solar system origins but creates insuperable problems for young Earth claims. Creationists Harold Slusher and Richard Mandock, along with Glenn Morton (who later repudiated this claim) asserted that impact craters on the Moon are subject to rock flow, and so cannot be more than a few thousand years old. While some creationist astronomers assert that different phases of meteoritic bombardment of the Solar System occurred during creation week and during the subsequent Great Flood, others regard this as unsupported by the evidence and call for further research.\n\n\n\n\n\nNotable creationist museums in the United States:\n", "id": "7683", "title": "Creation science"}
{"url": "https://en.wikipedia.org/wiki?curid=7685", "text": "List of cartographers\n\nCartography is the study of map making and cartographers are map makers.\n\n\n\n\n\n\n\n\n\n", "id": "7685", "title": "List of cartographers"}
{"url": "https://en.wikipedia.org/wiki?curid=7689", "text": "Cirth\n\nThe Cirth (; plural of certh , in Sindarin meaning runes) are a semi-artificial script, with letters shaped on those of actual runic alphabets, invented by J. R. R. Tolkien for the constructed languages he devised and used in his works. \"Cirth\" is written with a capital letter when referring to the writing system; the runes themselves can be called \"cirth\".\n\nIn the fictional history of Middle-earth, the original Certhas Daeron was created by the elf Daeron, and was later expanded into what was known as the Angerthas Daeron. Although the Cirth were later largely replaced by the Tengwar, they were adopted by Dwarves to write down their Khuzdul language (Angerthas Moria and Angerthas Erebor) because their straight lines were better suited to carving than the curved strokes of the Tengwar. Cirth was also adapted, in its oldest and simplest form, by various peoples including Men and even Orcs.\n\nDuring the Chaining of Melkor, the Sindar of Beleriand began developing an alphabet for their language. Its letters were entirely made for carving on wood, stone or metal, hence their angular forms and straight lines. These letters were named \"cirth\" (sing. \"certh\"). The corresponding Quenya words are \"certar\" () and \"certa\" (). The assignment of values was unsystematic. The form of a certh consisted of a stem and a branch. The attachment of the branch was, if on one side only, usually made on the right side. The reverse was not infrequent, but had no phonetic significance.<br>Two basic principles were followed:\nThe original display of Cirth should have been this:\n\n\nThe known ancient cirth don't cover all the sounds of Sindarin, since we are missing \"rh\", \"lh\", \"mh\", \"y\", \"œ\". Perhaps they were used for the Old Sindarin tongue, and many of the above-mentioned sounds indeed didn't exist in that language. However still frequent sounds \"w\" and \"a\" are missing. This indicates that some ancient, unknown cirth could have existed, but didn't make it to the later systems; a fuller table therefore can't be reconstructed.<br>As for the vowel usage, perhaps the certh for \"u\" possibly was used for \"w\" (like in early Latin orthography). The certh for \"a\" can't be guessed, so maybe this sound was meant (like in some Tengwar Modes for Quenya). More possibly it was one of some other cirth that did not survive.<br>Long vowels were evidently indicated by doubling.\n\nThe elf Daeron, minstrel of king Thingol of Doriath reorganised the cirth and added new ones, being somehow inspired by Fëanor's Tengwar (therefore this mustn't have occurred before the return of the Noldor) and made the extension of the cirth known as Certhas Daeron (where \"Certhas\" means \"runic alphabet\"), used for inscribing names in Menegroth. The Dwarves working for Thingol liked them and adopted them, making them known also in the East.<br>Unlike the previous system, the reversal of the certh had a phonemic significance: reversed cirth were softer versions of their originals. This also gives us another information: perhaps lenited consonants must have started to occur in Sindarin around that time.<br>We know that at one time a sign for the sound \"mh\" was needed and the most appropriate solution was to revert the certh for \"m\" to indicate its softening, but it could not be reverted (); therefore \"m\" was given to the reversible (which until then had a value unknown to us), \"mh\" to , and the certh got the value of \"hw\".\n\nDaeron's alphabet was originally used by the Grey Elves (Sindar) in Beleriand. Later the Noldor in Eregion adopted the Cirth, added several more runes to the system and created the Angerthas Daeron (where \"Angerthas\" means \"long rune-rows\") sometimes also referred to as Angerthas Eregion. These additional letters were used to represent sounds not found in Sindarin, but present in the tongues of other peoples. The Angerthas Daeron was used primarily for carved inscriptions. For most other forms of written communication the Tengwar were used.<br>Here the Cirth are grouped according to their phonetic features:\n\n\nFor the transliteration of this alphabet, meant to be used for more than one language (for Quenya and Sindarin, at least) and needing a bigger set of sounds, Tolkien thought up to a kind of \"general Middle-earth languages phonetic transcription\", here used.\n\nDwarves first came to know the runes of the Noldor during the beginning of the Second Age. They modified them to suit the specific needs of their language, Khuzdul. The Dwarves spread their revised alphabet to Moria, where it came to be known as Angerthas Moria. The Dwarves developed both carved and pen-written forms of the runes. Travelling for trading, they spread their alphabet throughout Middle-earth: as a result, variations of Angerthas Moria were employed by other races for their languages.\n\nMany cirth here stand for sounds not occurring in Khuzdul (at least in published words of Khuzdul: of course, our corpus is very limited to judge the necessity or not, of these sounds). Here they are marked with a black star ().\n\n\n\nAccording to Tolkien's legendarium, after the Second Age, the Cirth were obsoleted by the Tengwar among the western races and remained in use only by Dwarves and Men. The Dwarves developed even pen-written cursive forms, since they used them exclusively in any form of writing communication, even in paper. At the beginning of the Third Age, the Dwarves were driven out of Moria. Some migrated to the Grey Mountains, some to the Iron Hills and Thráin I came to Erebor. There he founded his Dwarf-kingdom. There the \"Angerthas Moria\" was modified further and some new cirth were added, but some reverted to their Elvish usage, thus creating the Angerthas Erebor variation. This mode was used in Westron by Dwarves.\n\nMany cirth here stand for sounds not occurring in Khuzdul (at least in published words of Khuzdul: of course, our corpus is very limited to judge the necessity or not, of these sounds). Here they are marked with a black star ().\n\nCombining diacritics occur in Angerthas Erebor as well: a circumflex accent used to denote long consonants, a macron below to indicate a long vowel sound, and an underdot to mark cirth used as numerals.\n\n\nTolkien used Angerthas Erebor mode to write English with Cirth at least twice in \"The Lord of the Rings\":\n\nThe Book of Mazarbul shows that additional cirth were introduced in this mode (for a double \"l\" ligature, for the definite article and for the representation of six diphthongs). These were probably only to be used with English language:\n\nAdditionally, is used for English and for .\n\nThe Cirth is not the only runic writing system devised by Tolkien for Middle Earth. In fact, he invented a great number of runic alphabets, of which only a few others have been published. Many of them were included in the \"Appendix on Runes\" in The History of Middle-Earth, vol. VII, The Treason of Isengard, edited by Christopher Tolkien.\n\nAccording to Tolkien, the earliest Cirth became known to many peoples of Middle-earth like Men, Dwarves or Orcs. The people of Dale and the Rohirrim maintained a simple form of these characters — hence the \"branching runes\" that appear on the stone floor of the Meduseld in Rohan.\n\nIt is speculated that the runes used in \"The Hobbit\" are indeed the form of Cirth used by the Men of Dale, although Tolkien himself wrote that the letters used for Thror's Map are a form of our ancient runes used to transliterate actual Dwarf-runes.\n\nThese runes – used only to write in English – are indeed nearly identical to those of Fuþorc but their sound may change according to their position, just as the Latin letters do. And, in fact, Tolkien's writing mode for these runes is mainly orthographic.<br>It has one rune for each letter, regardless of pronunciation (for example the rune \"C\" can sound in the word \"\"c\"at\" or in the word \"\"c\"ellar\" or even in the word \"deli\"c\"ious\" and in the digraph \"CH\").<br>A few sounds are instead written with the same rune regardless of the letter (e.g. the sound is always written with the rune \"O\" either if in English it is written \"o\" as in \"n\"o\"rth\", \"a\" as in \"f\"a\"ll\", or \"oo\" as in \"d\"oo\"r\"). The letters that are subject to this phonemic spelling are \"a\" and \"o\".<br>In addition, there are also some runes which stand for a particular English digraph or diphthong.\n\nHere the runes used in \"The Hobbit\" are represented along with their English transliteration and Fuþorc equivalent:\n\nTwo other runes, not attested in \"The Hobbit\", were added by Tolkien in order to represent additional English sounds:\n\n<nowiki>*</nowiki> The runes marked with an asterisk are not attested in real-life fuþorc. These are completely new characters invented by Tolkien himself. These were as well included in Unicode 9.0 (see below).\n\nIt must be noticed that Tolkien always wrote the English digraph \"wh\" (representing the sound , or , like in \"\"wh\"ain\") in runes as \"HW\".<br>There is no rune to transliterate \"q\": the digraph \"qu\" (representing the sound , like in \"\"qu\"estion\") is always rendered in runes as \"CW\".\n\nThis table could be helpful for the transcription of \"a\" and \"o\" in runes:\n\nNot all the runes mentioned in \"The Hobbit\" are Dwarf-runes. The swords found in the Trolls' cave (which were from the ancient kingdom of Gondolin) bore runes that Gandalf could not read. In fact, the swords Glamdring and Orcrist, forged in Gondolin, bore a type of letters known as Gondolinic runes. They seem to have been obsoleted and forgotten by the Third Age, and this is supported by the fact that only Elrond could read the inscriptions of the swords.<br>\nTolkien devised this runic alphabet in a very early stage of his shaping of Middle-earth, but they are known to us only from a slip of paper written by J.R.R. Tolkien, a photocopy of which Christopher Tolkien sent to Paul Nolan Hyde in February 1992, who published it, together with an extensive analysis, in the 1992 Summer issue of Mythlore, no. 69.\nThe system provides sounds not found in the known Elven languages of the First Age, but perhaps it was designed for notating a variety of languages. However, the consonants seem to be, more or less, the same found in Welsh phonology, a theory supported by the fact that Tolkien was heavily influenced by Welsh when creating Elven languages.\n\nMany letters have shapes also found in the historical runic alphabets, but their sound values are only similar in a few of the vowels. Rather, the system of assignment of sound values is much more systematic in the Cirth than in the historical runes (e.g., voiced variants of a voiceless sound are expressed by an additional stroke). A similar system has been proposed for a few historical runes but is in any case much more obscure.\n\nThe division between the older Cirth of Daeron and their adaptation by Dwarves and Men has been interpreted as a parallel drawn by Tolkien to the development of the Fuþorc to the Younger Fuþark. The original Elvish Cirth \"as supposed products of a superior culture\" are focused on logical arrangement and a close connection between form and value whereas the adaptations by mortal races introduced irregularities. Similar to the Germanic tribes who had no written literature and used only simple runes before their conversion to Christianity, the Sindar Elves of Beleriand with their Cirth were introduced to the more elaborate Tengwar of Fëanor when the Noldor Elves returned to Middle-earth from the lands of the divine Valar.\n\nEquivalents for most but not all cirth can be found in the Runic block of Unicode.\n\nThree J. R. R. Tolkien-specific letters were added in June, 2014 with the release of Unicode 7.0:\nA formal Unicode proposal to encode Cirth as a separate script was made in September 1997 by Michael Everson.\nNo action was taken by the Unicode Technical Committee (UTC) but Cirth appears in the Roadmap to the SMP.\n\nUnicode Private Use Area layouts for Cirth are defined at the ConScript Unicode Registry (CSUR) and the Under-ConScript Unicode Registry (UCSUR).\n\nTwo different layouts are defined by the CSUR/UCSUR:\n\nWithout proper rendering support, you may see question marks, boxes, or other symbols below instead of Cirth.\n", "id": "7689", "title": "Cirth"}
{"url": "https://en.wikipedia.org/wiki?curid=7697", "text": "Lockheed C-130 Hercules\n\nThe Lockheed C-130 Hercules is a four–engine turboprop military transport aircraft designed and built originally by Lockheed (now Lockheed Martin). Capable of using unprepared runways for takeoffs and landings, the C-130 was originally designed as a troop, medevac, and cargo transport aircraft. The versatile airframe has found uses in a variety of other roles, including as a gunship (AC-130), for airborne assault, search and rescue, scientific research support, weather reconnaissance, aerial refueling, maritime patrol, and aerial firefighting. It is now the main tactical airlifter for many military forces worldwide. Over forty variants and versions of the Hercules, including a civilian one marketed as the Lockheed L-100, operate in more than 60 nations.\n\nThe C-130 entered service with the U.S. in the 1950s, followed by Australia and others. During its years of service, the Hercules family has participated in numerous military, civilian and humanitarian aid operations. In 2007, the C-130 became the fifth aircraft—after the English Electric Canberra, B-52 Stratofortress, Tu-95 Bear, and KC-135 Stratotanker—to mark 50 years of continuous service with its original primary customer, in this case, the United States Air Force. The C-130 Hercules is the longest continuously produced military aircraft at over 60 years, with the updated Lockheed Martin C-130J Super Hercules being produced today.\n\nThe Korean War, which began in June 1950, showed that World War II-era piston-engine transports—Fairchild C-119 Flying Boxcars, Douglas C-47 Skytrains and Curtiss C-46 Commandos—were inadequate for modern warfare. Thus, on 2 February 1951, the United States Air Force issued a General Operating Requirement (GOR) for a new transport to Boeing, Douglas, Fairchild, Lockheed, Martin, Chase Aircraft, North American, Northrop, and Airlifts Inc. The new transport would have a capacity of 92 passengers, 72 combat troops or 64 paratroopers in a cargo compartment that was approximately long, high, and wide. Unlike transports derived from passenger airliners, it was to be designed from the ground-up as a combat transport with loading from a hinged loading ramp at the rear of the fuselage.\n\nA key feature was the introduction of the Allison T56 turboprop powerplant, first developed specifically for the C-130. At the time, the turboprop was a new application of turbine engines that used exhaust gases to turn a propeller, which offered greater range at propeller-driven speeds compared to pure turbojets, which were faster but consumed more fuel. As was the case on helicopters of that era, such as the UH-1 Huey, turboshafts produced much more power for their weight than piston engines. Lockheed would subsequently use the same engines and technology in the Lockheed L-188 Electra. That aircraft failed financially in its civilian configuration but was successfully adapted into the Lockheed P-3 Orion maritime patrol and submarine attack aircraft where the efficiency and endurance of turboprops excelled.\n\nThe Hercules resembled a larger four-engine brother to the C-123 Provider with a similar wing and cargo ramp layout that evolved from the Chase XCG-20 Avitruc, which in turn, was first designed and flown as a cargo glider in 1947. The Boeing C-97 Stratofreighter also had a rear ramp, which made it possible to drive vehicles onto the plane (also possible with forward ramp on a C-124). The ramp on the Hercules was also used to airdrop cargo, which included low-altitude extraction for Sheridan tanks and even dropping large improvised \"daisy cutter\" bombs.\n\nThe new Lockheed cargo plane design possessed a range of , takeoff capability from short and unprepared strips, and the ability to fly with one engine shut down. Fairchild, North American, Martin, and Northrop declined to participate. The remaining five companies tendered a total of ten designs: Lockheed two, Boeing one, Chase three, Douglas three, and Airlifts Inc. one. The contest was a close affair between the lighter of the two Lockheed (preliminary project designation L-206) proposals and a four-turboprop Douglas design.\n\nThe Lockheed design team was led by Willis Hawkins, starting with a 130-page proposal for the \"Lockheed L-206\". Hall Hibbard, Lockheed vice president and chief engineer, saw the proposal and directed it to Kelly Johnson, who did not care for the low-speed, unarmed aircraft, and remarked, \"If you sign that letter, you will destroy the Lockheed Company.\" Both Hibbard and Johnson signed the proposal and the company won the contract for the now-designated Model 82 on 2 July 1951.\n\nThe first flight of the \"YC-130\" prototype was made on 23 August 1954 from the Lockheed plant in Burbank, California. The aircraft, serial number \"53-3397\", was the second prototype, but the first of the two to fly. The YC-130 was piloted by Stanley Beltz and Roy Wimmer on its 61-minute flight to Edwards Air Force Base; Jack Real and Dick Stanton served as flight engineers. Kelly Johnson flew chase in a Lockheed P2V Neptune.\n\nAfter the two prototypes were completed, production began in Marietta, Georgia, where over 2,300 C-130s have been built through 2009.\n\nThe initial production model, the \"C-130A\", was powered by Allison T56-A-9 turboprops with three-blade propellers and originally equipped with the blunt nose of the prototypes. Deliveries began in December 1956, continuing until the introduction of the \"C-130B\" model in 1959. Some A-models were equipped with skis and re-designated \"C-130D\". As the C-130A became operational with Tactical Air Command (TAC), the C-130's lack of range became apparent and additional fuel capacity was added in the form of external pylon-mounted tanks at the end of the wings.\n\nThe C-130B model was developed to complement the A-models that had previously been delivered, and incorporated new features, particularly increased fuel capacity in the form of auxiliary tanks built into the center wing section and an AC electrical system. Four-bladed Hamilton Standard propellers replaced the Aeroproducts three-blade propellers that distinguished the earlier A-models. The C-130B had ailerons with boost increased from to , as well as uprated engines and four-blade propellers that were standard until the J-model's introduction.\n\nAn electronic reconnaissance variant of the C-130B was designated C-130B-II. A total of 13 aircraft were converted. The C-130B-II was distinguished by its false external wing fuel tanks, which were disguised signals intelligence (SIGINT) receiver antennas. These pods were slightly larger than the standard wing tanks found on other C-130Bs. Most aircraft featured a swept blade antenna on the upper fuselage, as well as extra wire antennas between the vertical fin and upper fuselage not found on other C-130s. Radio call numbers on the tail of these aircraft were regularly changed so as to confuse observers and disguise their true mission.\n\nThe extended-range \"C-130E\" model entered service in 1962 after it was developed as an interim long-range transport for the Military Air Transport Service. Essentially a B-model, the new designation was the result of the installation of 1,360 US gal (5,150 L) \"Sargent Fletcher\" external fuel tanks under each wing's midsection and more powerful Allison T56-A-7A turboprops. The hydraulic boost pressure to the ailerons was reduced back to as a consequence of the external tanks' weight in the middle of the wingspan. The E model also featured structural improvements, avionics upgrades and a higher gross weight. Australia took delivery of 12 C130E Hercules during 1966–67 to supplement the 12 C-130A models already in service with the RAAF. Sweden and Spain fly the TP-84T version of the C-130E fitted for aerial refueling capability.\n\nThe \"KC-130\" tankers, originally \"C-130F\" procured for the US Marine Corps (USMC) in 1958 (under the designation \"GV-1\") are equipped with a removable 3,600 US gal (13,626 L) stainless steel fuel tank carried inside the cargo compartment. The two wing-mounted hose and drogue aerial refueling pods each transfer up to 300 US gal per minute (19 L per second) to two aircraft simultaneously, allowing for rapid cycle times of multiple-receiver aircraft formations, (a typical tanker formation of four aircraft in less than 30 minutes). The US Navy's \"C-130G\" has increased structural strength allowing higher gross weight operation.\n\nThe \"C-130H\" model has updated Allison T56-A-15 turboprops, a redesigned outer wing, updated avionics and other minor improvements. Later \"H\" models had a new, fatigue-life-improved, center wing that was retrofitted to many earlier H-models. For structural reasons, some models are required to land with certain amounts of fuel when carrying heavy cargo, reducing usable range. The H model remains in widespread use with the United States Air Force (USAF) and many foreign air forces. Initial deliveries began in 1964 (to the RNZAF), remaining in production until 1996. An improved C-130H was introduced in 1974, with Australia purchasing 12 of type in 1978 to replace the original 12 C-130A models, which had first entered RAAF Service in 1958. The U.S. Coast Guard employs the HC-130H for long-range search and rescue, drug interdiction, illegal migrant patrols, homeland security, and logistics.\nC-130H models produced from 1992 to 1996 were designated as C-130H3 by the USAF. The \"3\" denoting the third variation in design for the H series. Improvements included ring laser gyros for the INUs, GPS receivers, a partial glass cockpit (ADI and HSI instruments), a more capable APN-241 color radar, night vision device compatible instrument lighting, and an integrated radar and missile warning system. The electrical system upgrade included Generator Control Units (GCU) and Bus Switching units (BSU) to provide stable power to the more sensitive upgraded components.\nThe equivalent model for export to the UK is the \"C-130K\", known by the Royal Air Force (RAF) as the \"Hercules C.1\". The \"C-130H-30\" (\"Hercules C.3\" in RAF service) is a stretched version of the original Hercules, achieved by inserting a 100 in (2.54 m) plug aft of the cockpit and an 80 in (2.03 m) plug at the rear of the fuselage. A single C-130K was purchased by the Met Office for use by its Meteorological Research Flight, where it was classified as the \"Hercules W.2\". This aircraft was heavily modified (with its most prominent feature being the long red and white striped atmospheric probe on the nose and the move of the weather radar into a pod above the forward fuselage). This aircraft, named \"Snoopy\", was withdrawn in 2001 and was then modified by Marshall of Cambridge Aerospace as flight-testbed for the A400M turbine engine, the TP400. The C-130K is used by the RAF Falcons for parachute drops. Three C-130K (Hercules C Mk.1P) were upgraded and sold to the Austrian Air Force in 2002.\n\nThe \"MC-130E Combat Talon\" was developed for the USAF during the Vietnam War to support special operations missions in Southeast Asia, and led to both the \"MC-130H Combat Talon II\" as well as a family of other special missions aircraft. 37 of the earliest models currently operating with the Air Force Special Operations Command (AFSOC) are scheduled to be replaced by new-production MC-130J versions. The EC-130 Commando Solo is another special missions variant within AFSOC, albeit operated solely by an AFSOC-gained wing in the Pennsylvania Air National Guard, and is a psychological operations/information operations (PSYOP/IO) platform equipped as an aerial radio station and television stations able to transmit messaging over commercial frequencies. Other versions of the EC-130, most notably the EC-130H Compass Call, are also special variants, but are assigned to the Air Combat Command (ACC). The AC-130 gunship was first developed during the Vietnam War to provide close air support and other ground-attack duties.\nThe \"HC-130\" is a family of long-range search and rescue variants used by the USAF and the U.S. Coast Guard. Equipped for deep deployment of Pararescuemen (PJs), survival equipment, and (in the case of USAF versions) aerial refueling of combat rescue helicopters, HC-130s are usually the on-scene command aircraft for combat SAR missions (USAF only) and non-combat SAR (USAF and USCG). Early USAF versions were also equipped with the Fulton surface-to-air recovery system, designed to pull a person off the ground using a wire strung from a helium balloon. The John Wayne movie \"The Green Berets\" features its use. The Fulton system was later removed when aerial refueling of helicopters proved safer and more versatile. The movie \"The Perfect Storm\" depicts a real life SAR mission involving aerial refueling of a New York Air National Guard HH-60G by a New York Air National Guard HC-130P.\n\nThe \"C-130R\" and \"C-130T\" are U.S. Navy and USMC models, both equipped with underwing external fuel tanks. The USN C-130T is similar, but has additional avionics improvements. In both models, aircraft are equipped with Allison T56-A-16 engines. The USMC versions are designated \"KC-130R\" or \"KC-130T\" when equipped with underwing refueling pods and pylons and are fully night vision system compatible.\n\nThe RC-130 is a reconnaissance version. A single example is used by the Islamic Republic of Iran Air Force, the aircraft having originally been sold to the former Imperial Iranian Air Force.\n\nThe \"Lockheed L-100 (L-382)\" is a civilian variant, equivalent to a C-130E model without military equipment. The L-100 also has two stretched versions.\n\nIn the 1970s, Lockheed proposed a C-130 variant with turbofan engines rather than turboprops, but the U.S. Air Force preferred the takeoff performance of the existing aircraft. In the 1980s, the C-130 was intended to be replaced by the Advanced Medium STOL Transport project. The project was canceled and the C-130 has remained in production.\n\nBuilding on lessons learned, Lockheed Martin modified a commercial variant of the C-130 into a High Technology Test Bed (HTTB). This test aircraft set numerous short takeoff and landing performance records and significantly expanded the database for future derivatives of the C-130. Modifications made to the HTTB included extended chord ailerons, a long chord rudder, fast-acting double-slotted trailing edge flaps, a high-camber wing leading edge extension, a larger dorsal fin and dorsal fins, the addition of three spoiler panels to each wing upper surface, a long-stroke main and nose landing gear system, and changes to the flight controls and a change from direct mechanical linkages assisted by hydraulic boost, to fully powered controls, in which the mechanical linkages from the flight station controls operated only the hydraulic control valves of the appropriate boost unit. The HTTB first flew on 19 June 1984, with civil registration of N130X. After demonstrating many new technologies, some of which were applied to the C-130J, the HTTB was lost in a fatal accident on 3 February 1993, at Dobbins Air Reserve Base, in Marietta, Georgia. The crash was attributed to disengagement of the rudder fly-by-wire flight control system, resulting in a total loss of rudder control capability while conducting ground minimum control speed tests (Vmcg). The disengagement was a result of the inadequate design of the rudder's integrated actuator package by its manufacturer; the operator's insufficient system safety review failed to consider the consequences of the inadequate design to all operating regimes. A factor which contributed to the accident was the flight crew's lack of engineering flight test training.\n\nIn the 1990s, the improved C-130J Super Hercules was developed by Lockheed (later Lockheed Martin). This model is the newest version and the only model in production. Externally similar to the classic Hercules in general appearance, the J model has new turboprop engines, six-bladed propellers, digital avionics, and other new systems.\n\nIn 2000, Boeing was awarded a contract to develop an Avionics Modernization Program kit for the C-130. The program was beset with delays and cost overruns until project restructuring in 2007. On 2 September 2009, Bloomberg news reported that the planned Avionics Modernization Program (AMP) upgrade to the older C-130s would be dropped to provide more funds for the F-35, CV-22 and airborne tanker replacement programs. However, in June 2010, Department of Defense approved funding for the initial production of the AMP upgrade kits. Under the terms of this agreement, the USAF has cleared Boeing to begin low-rate initial production (LRIP) for the C-130 AMP. A total of 198 aircraft are expected to feature the AMP upgrade. The current cost per aircraft is although Boeing expects that this price will drop to US$7 million for the 69th aircraft.\n\nIn the 2000s, Lockheed Martin and the U.S. Air Force began outfitting and retrofitting C-130s with the eight-blade NP2000 propellers.\n\nAn engine enhancement program saving fuel and providing lower temperatures in the T56 engine has been approved, and the US Air Force expects to save $2 billion and extend the fleet life.\n\nIn October 2010, the Air Force released a capabilities request for information (CRFI) for the development of a new airlifter to replace the C-130. The new aircraft is to carry a 190 percent greater payload and assume the mission of mounted vertical maneuver (MVM). The greater payload and mission would enable it to carry medium-weight armored vehicles and drop them off at locations without long runways. Various options are being considered, including new or upgraded fixed-wing designs, rotorcraft, tiltrotors, or even an airship. Development could start in 2014, and become operational by 2024. The C-130 fleet of around 450 planes would be replaced by only 250 aircraft. The Air Force had attempted to replace the C-130 in the 1970s through the Advanced Medium STOL Transport project, which resulted in the C-17 Globemaster III that instead replaced the C-141 Starlifter. The Air Force Research Laboratory funded Lockheed and Boeing demonstrators for the Speed Agile concept, which had the goal of making a STOL aircraft that can take off and land at speeds as low as on airfields less than 2,000 ft (610 m) long and cruise at Mach 0.8-plus. Boeing's design used upper-surface blowing from embedded engines on the inboard wing and blown flaps for circulation control on the outboard wing. Lockheed's design also used blown flaps outboard, but inboard used patented reversing ejector nozzles. Boeing's design completed over 2,000 hours of windtunnel tests in late 2009. It was a 5 percent-scale model of a narrowbody design with a payload. When the AFRL increased the payload requirement to , they tested a 5% scale model of a widebody design with a take-off gross weight and an \"A400M-size\" wide cargo box. It would be powered by four IAE V2533 turbofans. In August 2011, the AFRL released pictures of the Lockheed Speed Agile concept demonstrator. A 23% scale model went through wind tunnel tests to demonstrate its hybrid powered lift, which combines a low drag airframe with simple mechanical assembly to reduce weight and better aerodynamics. The model had four engines, including two Williams FJ44 turbofans. On 26 March 2013, Boeing was granted a patent for its swept-wing powered lift aircraft.\n\nAs of January 2014, Air Mobility Command, Air Force Materiel Command and the Air Force Research Lab are in the early stages of defining requirements for the C-X next generation airlifter program to replace both the C-130 and C-17. An aircraft would be produced from the early 2030s to the 2040s. If requirements are decided for operating in contested airspace, Air Force procurement of C-130s would end by the end of the decade to not have them serviceable by the 2030s and operated when they can't perform in that environment. Development of the airlifter depends heavily on the Army's \"tactical and operational maneuver\" plans. Two different cargo planes could still be created to separately perform tactical and strategic missions, but which course to pursue is to be decided before C-17s need to be retired.\n\nThe first production aircraft, C-130As were first delivered beginning in 1956 to the 463d Troop Carrier Wing at Ardmore AFB, Oklahoma and the 314th Troop Carrier Wing at Sewart AFB, Tennessee. Six additional squadrons were assigned to the 322d Air Division in Europe and the 315th Air Division in the Far East. Additional aircraft were modified for electronics intelligence work and assigned to Rhein-Main Air Base, Germany while modified RC-130As were assigned to the Military Air Transport Service (MATS) photo-mapping division.\n\nIn 1958, a U.S. reconnaissance C-130A-II of the 7406th Support Squadron was shot down over Armenia by MiG-17s.\n\nAustralia became the first non-American force to operate the C-130A Hercules with 12 examples being delivered from late 1958. These aircraft were fitted with AeroProducts three-blade, 15-foot diameter propellers. The Royal Canadian Air Force became another early user with the delivery of four B-models (Canadian designation C-130 Mk I) in October / November 1960.\n\nIn 1963, a Hercules achieved and still holds the record for the largest and heaviest aircraft to land on an aircraft carrier. During October and November that year, a USMC KC-130F (BuNo \"149798\"), loaned to the U.S. Naval Air Test Center, made 29 touch-and-go landings, 21 unarrested full-stop landings and 21 unassisted take-offs on at a number of different weights. The pilot, LT (later RADM) James H. Flatley III, USN, was awarded the Distinguished Flying Cross for his role in this test series. The tests were highly successful, but the idea was considered too risky for routine \"Carrier Onboard Delivery\" (COD) operations. Instead, the Grumman C-2 Greyhound was developed as a dedicated COD aircraft. The Hercules used in the test, most recently in service with Marine Aerial Refueler Squadron 352 (VMGR-352) until 2005, is now part of the collection of the National Museum of Naval Aviation at NAS Pensacola, Florida.\n\nIn 1964, C-130 crews from the 6315th Operations Group at Naha Air Base, Okinawa commenced forward air control (FAC; \"Flare\") missions over the Ho Chi Minh Trail in Laos supporting USAF strike aircraft. In April 1965 the mission was expanded to North Vietnam where C-130 crews led formations of B-57 bombers on night reconnaissance/strike missions against communist supply routes leading to South Vietnam. In early 1966 Project Blind Bat/Lamplighter was established at Ubon RTAFB, Thailand. After the move to Ubon the mission became a four-engine FAC mission with the C-130 crew searching for targets then calling in strike aircraft. Another little-known C-130 mission flown by Naha-based crews was Operation \"Commando Scarf\", which involved the delivery of chemicals onto sections of the Ho Chi Minh Trail in Laos that were designed to produce mud and landslides in hopes of making the truck routes impassable.\n\nIn November 1964, on the other side of the globe, C-130Es from the 464th Troop Carrier Wing but loaned to 322d Air Division in France, flew one of the most dramatic missions in history in the former Belgian Congo. After communist Simba rebels took white residents of the city of Stanleyville hostage, the U.S. and Belgium developed a joint rescue mission that used the C-130s to airlift and then drop and air-land a force of Belgian paratroopers to rescue the hostages. Two missions were flown, one over Stanleyville and another over Paulis during Thanksgiving weeks. The headline-making mission resulted in the first award of the prestigious MacKay Trophy to C-130 crews.\n\nIn the Indo-Pakistani War of 1965, as a desperate measure the transport No. 6 Squadron of the Pakistan Air Force modified its entire small fleet of C-130Bs for use as heavy bombers, capable of carrying up to 20,000 lb (9,072 kg) of bombs on pallets. These improvised bombers were used to hit Indian targets such as bridges, heavy artillery positions, tank formations and troop concentrations. Some C-130s even flew with anti-aircraft guns fitted on their ramp, apparently shooting down some 17 aircraft and damaging 16 others.\nIn October 1968, a C-130Bs from the 463rd Tactical Airlift Wing dropped a pair of M-121 10,000 pound bombs that had been developed for the massive B-36 bomber but had never been used. The U.S. Army and U.S. Air Force resurrected the huge weapons as a means of clearing landing zones for helicopters and in early 1969 the 463rd commenced Commando Vault missions. Although the stated purpose of COMMANDO VAULT was to clear LZs, they were also used on enemy base camps and other targets.\n\nDuring the late 1960s, the U.S. was eager to get information on Chinese nuclear capabilities. After the failure of the Black Cat Squadron to plant operating sensor pods near the Lop Nur Nuclear Weapons Test Base using a Lockheed U-2, the CIA developed a plan, named \"Heavy Tea\", to deploy two battery-powered sensor pallets near the base. To deploy the pallets, a Black Bat Squadron crew was trained in the U.S. to fly the C-130 Hercules. The crew of 12, led by Col Sun Pei Zhen, took off from Takhli Royal Thai Air Force Base in an unmarked U.S. Air Force C-130E on 17 May 1969. Flying for six and a half hours at low altitude in the dark, they arrived over the target and the sensor pallets were dropped by parachute near Anxi in Gansu province. After another six and a half hours of low altitude flight, they arrived back at Takhli. The sensors worked and uploaded data to a U.S. intelligence satellite for six months, before their batteries wore out. The Chinese conducted two nuclear tests, on 22 September 1969 and 29 September 1969, during the operating life of the sensor pallets. Another mission to the area was planned as Operation \"Golden Whip\", but was called off in 1970. It is most likely that the aircraft used on this mission was either C-130E serial number 64-0506 or 64-0507 (cn 382-3990 and 382-3991). These two aircraft were delivered to Air America in 1964. After being returned to the U.S. Air Force sometime between 1966 and 1970, they were assigned the serial numbers of C-130s that had been destroyed in accidents. 64-0506 is now flying as 62-1843, a C-130E that crashed in Vietnam on 20 December 1965 and 64-0507 is now flying as 63-7785, a C-130E that had crashed in Vietnam on 17 June 1966.\n\nThe A-model continued in service through the Vietnam War, where the aircraft assigned to the four squadrons at Naha AB, Okinawa and one at Tachikawa Air Base, Japan performed yeoman's service, including operating highly classified special operations missions such as the BLIND BAT FAC/Flare mission and FACT SHEET leaflet mission over Laos and North Vietnam. The A-model was also provided to the South Vietnamese Air Force as part of the Vietnamization program at the end of the war, and equipped three squadrons based at Tan Son Nhut AFB. The last operator in the world is the Honduran Air Force, which is still flying one of five A model Hercules (FAH \"558\", c/n 3042) as of October 2009. As the Vietnam War wound down, the 463rd Troop Carrier/Tactical Airlift Wing B-models and A-models of the 374th Tactical Airlift Wing were transferred back to the United States where most were assigned to Air Force Reserve and Air National Guard units.\nAnother prominent role for the B model was with the United States Marine Corps, where Hercules initially designated as GV-1s replaced C-119s. After Air Force C-130Ds proved the type's usefulness in Antarctica, the U.S. Navy purchased a number of B-models equipped with skis that were designated as LC-130s. C-130B-II electronic reconnaissance aircraft were operated under the SUN VALLEY program name primarily from Yokota Air Base, Japan. All reverted to standard C-130B cargo aircraft after their replacement in the reconnaissance role by other aircraft.\n\nThe C-130 was also used in the 1976 Entebbe raid in which Israeli commando forces carried a surprise assault to rescue 103 passengers of an airliner hijacked by Palestinian and German terrorists at Entebbe Airport, Uganda. The rescue force — 200 soldiers, jeeps, and a black Mercedes-Benz (intended to resemble Ugandan Dictator Idi Amin's vehicle of state) — was flown over almost entirely at an altitude of less than from Israel to Entebbe by four Israeli Air Force (IAF) Hercules aircraft without mid-air refueling (on the way back, the planes refueled in Nairobi, Kenya).\n\nDuring the Falklands War () of 1982, Argentine Air Force C-130s undertook highly dangerous, daily re-supply night flights as blockade runners to the Argentine garrison on the Falkland Islands. They also performed daylight maritime survey flights. One was lost during the war. Argentina also operated two KC-130 tankers during the war, and these refueled both the Douglas A-4 Skyhawks and Navy Dassault-Breguet Super Étendards; some C-130s were modified to operate as bombers with bomb-racks under their wings. The British also used RAF C-130s to support their logistical operations.\nDuring the Gulf War of 1991 (Operation \"Desert Storm\"), the C-130 Hercules was used operationally by the U.S. Air Force, U.S. Navy and U.S. Marine Corps, along with the air forces of Australia, New Zealand, Saudi Arabia, South Korea and the UK. The MC-130 Combat Talon variant also made the first attacks using the largest conventional bombs in the world, the BLU-82 \"Daisy Cutter\" and GBU-43/B \"Massive Ordnance Air Blast\" bomb, (MOAB). Daisy Cutters were used to clear landing zones and to eliminate mine fields. The weight and size of the weapons make it impossible or impractical to load them on conventional bombers. The GBU-43/B MOAB is a successor to the BLU-82 and can perform the same function, as well as perform strike functions against hardened targets in a low air threat environment.\n\nSince 1992, two successive C-130 aircraft named \"Fat Albert\" have served as the support aircraft for the U.S. Navy Blue Angels flight demonstration team. \"Fat Albert I\" was a TC-130G (\"151891\"), while \"Fat Albert II\" is a C-130T (\"164763\"). Although \"Fat Albert\" supports a Navy squadron, it is operated by the U.S. Marine Corps (USMC) and its crew consists solely of USMC personnel. At some air shows featuring the team, \"Fat Albert\" takes part, performing flyovers. Until 2009, it also demonstrated its rocket-assisted takeoff (RATO) capabilities; these ended due to dwindling supplies of rockets.\n\nThe AC-130 also holds the record for the longest sustained flight by a C-130. From 22 to 24 October 1997, two AC-130U gunships flew 36 hours nonstop from Hurlburt Field Florida to Taegu (Daegu), South Korea while being refueled seven times by KC-135 tanker aircraft. This record flight shattered the previous record longest flight by over 10 hours while the two gunships took on of fuel. The gunship has been used in every major U.S. combat operation since Vietnam, except for Operation \"El Dorado Canyon\", the 1986 attack on Libya.\nDuring the invasion of Afghanistan in 2001 and the ongoing support of the International Security Assistance Force (Operation \"Enduring Freedom\"), the C-130 Hercules has been used operationally by Australia, Belgium, Canada, Denmark, France, Italy, the Netherlands, New Zealand, Norway, Portugal, South Korea, Spain, the UK and the United States.\n\nDuring the 2003 invasion of Iraq (Operation \"Iraqi Freedom\"), the C-130 Hercules was used operationally by Australia, the UK and the United States. After the initial invasion, C-130 operators as part of the Multinational force in Iraq used their C-130s to support their forces in Iraq.\n\nSince 2004, the Pakistan Air Force has employed C-130s in the War in North-West Pakistan. Some variants had forward looking infrared (FLIR Systems Star Safire III EO/IR) sensor balls, to enable close tracking of Islamist militants.\n\nThe U.S. Forest Service developed the Modular Airborne FireFighting System for the C-130 in the 1970s, which allows regular aircraft to be temporarily converted to an airtanker for fighting wildfires. In the late 1980s, 22 retired USAF C-130As were removed from storage at Davis-Monthan Air Force Base and transferred to the U.S. Forest Service who then sold them to six private companies to be converted into air tankers (see U.S. Forest Service airtanker scandal). After one of these aircraft crashed due to wing separation in flight as a result of fatigue stress cracking, the entire fleet of C-130A air tankers was permanently grounded in 2004 (see 2002 airtanker crashes). C-130s have been used to spread chemical dispersants onto the massive oil slick in the Gulf Coast in 2010.\n\nA recent development of a C-130–based airtanker is the Retardant Aerial Delivery System developed by Coulson Aviation USA. The system consists of a C-130H/Q retrofitted with an in-floor discharge system, combined with a removable 3,500- or 4,000-gallon water tank. The combined system is FAA certified.\n\nSignificant military variants of the C-130 include:\n\n\nThe C-130 Hercules has had a low accident rate in general. The Royal Air Force recorded an accident rate of about one aircraft loss per 250,000 flying hours over the last 40 years, placing it behind Vickers VC10s and Lockheed TriStars with no flying losses. USAF C-130A/B/E-models had an overall attrition rate of 5% as of 1989 as compared to 1-2% for commercial airliners in the U.S., according to the NTSB, 10% for B-52 bombers, and 20% for fighters (F-4, F-111), trainers (T-37, T-38), and helicopters (H-3).\n\nA total of 70 aircraft were lost by the U.S. Air Force and the U.S. Marine Corps during combat operations in the Vietnam War in Southeast Asia. By the nature of the Hercules' worldwide service, the pattern of losses provides an interesting barometer of the global hot spots over the past 50 years.\n\n\n\n\n\n\n\n\n\n\nNotes\nCitations\nBibliography\n\n\n", "id": "7697", "title": "Lockheed C-130 Hercules"}
{"url": "https://en.wikipedia.org/wiki?curid=7699", "text": "Commodore 1570\n\nThe Commodore 1570 is a 5¼\" floppy disk drive for the Commodore 128 home/personal computer. It is a single-sided, 170 kB version of the Commodore 1571, released as a stopgap measure when Commodore International was unable to provide large enough quantities of 1571s due to a shortage of double-sided drive mechanisms (supplied from an outside manufacturer). Like the 1571, it can read and write both GCR and MFM disk formats.\nThe 1570 utilizes a 1571 logic board in a cream-colored original-1541-like case with a drive mechanism similar to the 1541's except that it was equipped with track-zero detection. Like the 1571, its built-in DOS provides a data burst mode for transferring data to the C128 computer at a faster speed than a 1541 can. Its ROM also contains some DOS bug fixes that didn't appear in the 1571 until much later. The 1570 can read and write all single-sided CP/M-format disks that the 1571 can access.\n\nAlthough the 1570 is compatible with the Commodore 64, the C64 isn't capable of taking advantage of the drive's higher-speed operation, and when used with the C64 it's little more than a pricier 1541. Also, many early buyers of the C128 chose to temporarily make do with a 1541 drive, perhaps owned as part of a previous C64 setup, until the 1571 became more widely available.\n\nThe drive uses the CPU MOS 6502, floppy controller WD1770 or WD1772, I/O controllers 2x MOS Technology 6522 and 1x MOS Technology 6526.\n", "id": "7699", "title": "Commodore 1570"}
{"url": "https://en.wikipedia.org/wiki?curid=7700", "text": "Commodore 1571\n\nThe Commodore 1571 is Commodore's high-end 5¼\" floppy disk drive. With its double-sided drive mechanism, it has the ability to use double-sided, double-density (DS/DD) floppy disks natively. This is in contrast to its predecessors, the 1541 and 1570, which can fully read and write such disks only if the user manually flipped them over to access the second side. Because flipping the disk also reverses the direction of rotation, the two methods are not interchangeable; disks which had their back side created in a 1541 by flipping them over would have to be flipped in the 1571 too, and the back side of disks written in a 1571 using the native support for two-sided operation could not be read in a 1541.\n\nThe 1571 was released to match the Commodore 128, both design-wise and feature-wise. It was announced in the summer of 1985, at the same time as the C128, and became available in quantity later that year. The later C128\"D\" had a 1571-compatible drive integrated in the system unit. A double-sided disk on the 1571 would have a capacity of 340 kB (70 tracks, 1,360 disk blocks of 256 bytes each); as 8 kB are reserved for system use (directory and block availability information) and, under of each block serve as pointers to the next logical block, = 337,312 B or about were available for user data. (However, with a program organizing disk storage on its own, all space could be used, e.g. for data disks.)\n\nThe 1571 was designed to accommodate the C128's \"burst\" mode for 2x faster disk access, however the drive cannot use it if connected to older Commodore machines. This mode replaced the slow bit-banging serial routines of the 1541 with a true serial shift register implemented in hardware, thus dramatically increasing the drive speed. Although this originally had been planned when Commodore first switched from the parallel IEEE-488 interface to a custom serial interface (CBM-488), hardware bugs in the VIC-20's 6522 VIA shift register prevented it from working properly.\n\nWhen connected to a C128, the 1571 would default to double-sided mode, which allowed the drive to read its own 340k disks as well as single-sided 170 kB 1541 disks. If the C128 was switched into C64 mode by typing GO 64 from BASIC, the 1571 will stay in double-sided mode. If C64 mode was activated by holding down the C= key on power-up, the drive would automatically switch to single-sided mode, in which case it is unable to read 340 kB disks (also the default if a 1571 is used with a C64, Plus/4, VIC-20, or PET). A manual command can also be issued from BASIC to switch the 1571 between single and double sided mode. There is also an undocumented command which allows the user to independently control either of the read/write heads of the 1571, making it possible to format both sides of a diskette separate from each other, however the resultant disk cannot be read in a 1541 as it would be spinning in reverse direction when flipped upside down. In the same vein, \"flippy\" disks created with a 1541 cannot be read on a 1571 with this feature; they must be inserted upside down.\n\nThe 1571 is not 100% low-level compatible with the 1541, however this isn't a problem except in some software that uses advanced copy protections such as the RapidLok system found on Microprose and Accolade games.\n\nThe 1571 was noticeably quieter than its predecessor and tended to run cooler as well, even though, like the 1541, it had an internal power supply (later Commodore drives, like the 1541-II and the 3½\" 1581, came with external power supplies). The 1541-II/1581 power supply makes mention of a 1571-II, hinting that Commodore may have intended to release a version of the 1571 with an external power supply. However, no 1571-IIs are known to exist. The embedded OS in the 1571 was an improvement over the \n\nEarly 1571s had a bug in the ROM-based disk operating system that caused relative files to corrupt if they occupied both sides of the disk. A version 2 ROM was released, but though it cured the initial bug, it introduced some minor quirks of its own - particularly with the 1541 emulation. Curiously, it was also identified as V3.0.\n\nAs with the 1541, Commodore initially could not meet demand for the 1571, and that lack of availability and the drive's relatively high price (about US$300) presented an opportunity for cloners. Two 1571 clones appeared, one from Oceanic and one from Blue Chip, but legal action from Commodore quickly drove them from the market.\n\nCommodore announced at the 1985 Consumer Electronics Show a dual-drive version of the 1571, to be called the Commodore 1572, but quickly canceled it, reportedly due to technical difficulties with the 1572 DOS. It would have had four times as much RAM as the 1571 (8 kB), and twice as much ROM (64 kB). The 1572 would have allowed for fast disk backups of non-copy-protected media, much like the old 4040, 8050, and 8250 dual drives.\n\nThe 1571 built into the European plastic-case C128 D computer is electronically identical to the stand-alone version, but 1571 version integrated into the later metal-case C128 D (often called C128 DCR, for D Cost-Reduced) differs a lot from the stand-alone 1571. It includes a newer DOS, version 3.1, replaces the MOS Technology CIA interface chip, of which only a few features were used by the 1571 DOS, with a very much simplified chip called 5710, and has some compatibility issues with the stand-alone drive. Because this internal 1571 does not have an unused 8-bit input/output port on any chip, unlike most other Commodore drives, it is not possible to install a parallel cable in this drive, such as that used by SpeedDOS, Dolphin DOS and some other fast third-party Commodore DOS replacements.\n\nThe drive detects the motor speed and generates an internal data sampling clock signal that matches with the motor speed.\n\nThe 1571 uses a saddle canceler when reading the data stream. A correction signal is generated when the raw data pattern on the disk consists of two consecutive zeros. With the GCR recording format a problem occurs in the read signal waveform. The worst case pattern 1001 may cause a saddle condition where a false data bit may occur. The original 1541 drives uses a one-shot to correct the condition. The 1571 uses a gate array to corrected this digitally.\n\nThe drive uses the CPU MOS 6502, floppy controller WD1770 or WD1772, I/O controllers 2x MOS Technology 6522 and 1x MOS Technology 6526.\n\nUnlike the 1541, which was limited to GCR formatting, the 1571 could read both GCR and MFM disk formats. The version of CP/M included with the C128 supported the following formats:\n\n\nIf the CP/M BIOS is modified, it is possible to read any soft sector 40-track MFM format. Single density (FM) formats are not supported because the density selector pin on the MFM controller chip in the drive is disabled (wired to ground).\n\nA 1571 cannot boot from MFM disks; the user must boot CP/M from a GCR disk and then switch to MFM disks.\n\nWith additional software, it was possible to read and write to MS-DOS-formatted floppies as well. Numerous commercial and public-domain programs for this purpose became available, the best-known being SOGWAP's \"Big Blue Reader\". Although the C128 could not run any DOS-based software, this capability allowed data files to be exchanged with PC users. Reading or disks was possible as well with special software, but the standard format, which used FM rather than MFM encoding, could not be handled by the 1571 hardware without modifying the drive circuitry as the control line that determines if FM or MFM encoding is used by the disc controller chip was permanently wired to ground (MFM mode) rather than being under software control.\n\nIn the 1541 format, while 40 tracks are possible for a drive like the 154x/157x, only are used. Commodore chose not to use the upper five tracks by default (or at least to use more than 35) due to the bad quality of some of the drive mechanisms, which did not always work reliably on those tracks. \n\nFor compatibility and ease of implementation, the 1571's double-sided format of one logical disk side with was created by putting together the lower 35 physical tracks on each of the physical sides of the disk rather than using two times even though there were no more quality problems with the mechanisms of the 1571 drives.\n\n\n", "id": "7700", "title": "Commodore 1571"}
{"url": "https://en.wikipedia.org/wiki?curid=7701", "text": "Cocaine\n\nCocaine, also known as coke, is a strong stimulant mostly used as a recreational drug. It is commonly snorted, inhaled, or injected into the veins. Mental effects may include loss of contact with reality, an intense feeling of happiness, or agitation. Physical symptoms may include a fast heart rate, sweating, and large pupils. High doses can result in very high blood pressure or body temperature. Effects begin within seconds to minutes of use and last between five and ninety minutes. Cocaine has a small number of accepted medical uses such as numbing and decreasing bleeding during nasal surgery.\nCocaine is addictive due to its effect on the reward pathway in the brain. After a short period of use, there is a high risk that dependence will occur. Its use also increases the risk of stroke, myocardial infarction, lung problems in those who smoke it, blood infections, and sudden cardiac death. Cocaine sold on the street is commonly mixed with local anesthetics, cornstarch, quinine, or sugar which can result in additional toxicity. Following repeated doses a person may have decreased ability to feel pleasure and be very physically tired.\nCocaine acts by inhibiting the reuptake of serotonin, norepinephrine, and dopamine. This results in greater concentrations of these three neurotransmitters in the brain. It can easily cross the blood–brain barrier and may lead to the breakdown of the barrier. Cocaine is made from the leaves of the \"coca\" plant which are mostly grown in South America. In 2013, 419 kilograms were produced legally. It is estimated that the illegal market for cocaine is 100 to 500 billion USD each year. With further processing crack cocaine can be produced from cocaine.\nAfter cannabis, cocaine is the most frequently used illegal drug globally. Between 14 and 21 million people use the drug each year. Use is highest in North America followed by Europe and South America. Between one and three percent of people in the developed world have used cocaine at some point in their life. In 2013 cocaine use directly resulted in 4,300 deaths, up from 2,400 in 1990. The leaves of the \"coca\" plant have been used by Peruvians since ancient times. Cocaine was first isolated from the leaves in 1860. Since 1961 the international Single Convention on Narcotic Drugs has required countries to make recreational use of cocaine a crime.\n\nTopical cocaine can be used as a local numbing agent to help with painful procedures in the mouth or nose. TAC is one such formulation used for pediatrics. \n\nCocaine was historically useful as a topical anesthetic in eye and nasal surgery, although it is now predominantly used for nasal and lacrimal duct surgery. The major disadvantages of this use are cocaine's intense vasoconstrictor activity and the potential for cardiovascular toxicity. Cocaine has since been largely replaced in Western medicine by synthetic local anesthetics such as benzocaine, proparacaine, lidocaine, and tetracaine though it remains available for use if specified. If vasoconstriction is desired for a procedure (as it reduces bleeding), the anesthetic is combined with a vasoconstrictor such as phenylephrine or epinephrine. In Australia it is currently prescribed for use as a local anesthetic for conditions such as mouth and lung ulcers. Some ENT specialists occasionally use cocaine within the practice when performing procedures such as nasal cauterization. In this scenario dissolved cocaine is soaked into a ball of cotton wool, which is placed in the nostril for the 10–15 minutes immediately before the procedure, thus performing the dual role of both numbing the area to be cauterized, and vasoconstriction. Even when used this way, some of the used cocaine may be absorbed through oral or nasal mucosa and give systemic effects. An alternative method of administration for ENT surgery is mixed with adrenaline and sodium bicarbonate, as Moffett's Solution.\n\nCocaine is a powerful nervous system stimulant. Its effects can last from fifteen or thirty minutes to an hour. The duration of cocaine's effects depends on the amount taken and the route of administration. Cocaine can be in the form of fine white powder, bitter to the taste. When inhaled or injected, it causes a numbing effect. Crack cocaine is a smokeable form of cocaine made into small \"rocks\" by processing cocaine with sodium bicarbonate (baking soda) and water.\n\nCocaine increases alertness, feelings of well-being and euphoria, energy and motor activity, feelings of competence and sexuality. Cocaine's stimulant effects are similar to that of amphetamine, however, these effects tend to be much shorter lasting and more prominent.\n\nMany users rub the powder along the gum line, or onto a cigarette filter which is then smoked, which numbs the gums and teeth – hence the colloquial names of \"numbies\", \"gummers\", or \"cocoa puffs\" for this type of administration. This is mostly done with the small amounts of cocaine remaining on a surface after insufflation (snorting). Another oral method is to wrap up some cocaine in rolling paper and swallow (parachute) it. This is sometimes called a \"snow bomb.\"\n\nCoca leaves are typically mixed with an alkaline substance (such as lime) and chewed into a wad that is retained in the mouth between gum and cheek (much in the same as chewing tobacco is chewed) and sucked of its juices. The juices are absorbed slowly by the mucous membrane of the inner cheek and by the gastrointestinal tract when swallowed. Alternatively, coca leaves can be infused in liquid and consumed like tea. Ingesting coca leaves generally is an inefficient means of administering cocaine. Advocates of the consumption of the coca leaf state that coca leaf consumption should not be criminalized as it is not actual cocaine, and consequently it is not properly the illicit drug.\n\nBecause cocaine is hydrolyzed and rendered inactive in the acidic stomach, it is not readily absorbed when ingested alone. Only when mixed with a highly alkaline substance (such as lime) can it be absorbed into the bloodstream through the stomach. The efficiency of absorption of orally administered cocaine is limited by two additional factors. First, the drug is partly catabolized by the liver. Second, capillaries in the mouth and esophagus constrict after contact with the drug, reducing the surface area over which the drug can be absorbed. Nevertheless, cocaine metabolites can be detected in the urine of subjects that have sipped even one cup of coca leaf infusion. Therefore, this is an actual additional form of administration of cocaine, albeit an inefficient one.\n\nOrally administered cocaine takes approximately 30 minutes to enter the bloodstream. Typically, only a third of an oral dose is absorbed, although absorption has been shown to reach 60% in controlled settings. Given the slow rate of absorption, maximum physiological and psychotropic effects are attained approximately 60 minutes after cocaine is administered by ingestion. While the onset of these effects is slow, the effects are sustained for approximately 60 minutes after their peak is attained.\n\nContrary to popular belief, both ingestion and insufflation result in approximately the same proportion of the drug being absorbed: 30 to 60%. Compared to ingestion, the faster absorption of insufflated cocaine results in quicker attainment of maximum drug effects. Snorting cocaine produces maximum physiological effects within 40 minutes and maximum psychotropic effects within 20 minutes, however, a more realistic activation period is closer to 5 to 10 minutes, which is similar to ingestion of cocaine. Physiological and psychotropic effects from nasally insufflated cocaine are sustained for approximately 40–60 minutes after the peak effects are attained.\n\nCoca tea, an infusion of coca leaves, is also a traditional method of consumption. The tea has often been recommended for travelers in the Andes to prevent altitude sickness. However, its actual effectiveness has never been systematically studied. This method of consumption has been practised for many centuries by the indigenous tribes of South America. One specific purpose of ancient coca leaf consumption was to increase energy and reduce fatigue in messengers who made multi-day quests to other settlements.\n\nIn 1986 an article in the \"Journal of the American Medical Association\" revealed that U.S. health food stores were selling dried coca leaves to be prepared as an infusion as \"Health Inca Tea.\" While the packaging claimed it had been \"decocainized,\" no such process had actually taken place. The article stated that drinking two cups of the tea per day gave a mild stimulation, increased heart rate, and mood elevation, and the tea was essentially harmless. Despite this, the DEA seized several shipments in Hawaii, Chicago, Georgia, and several locations on the East Coast of the United States, and the product was removed from the shelves.\n\nNasal insufflation (known colloquially as \"snorting,\" \"sniffing,\" or \"blowing\") is a common method of ingestion of recreational powdered cocaine. The drug coats and is absorbed through the mucous membranes lining the nasal passages. When insufflating cocaine, absorption through the nasal membranes is approximately 30–60%, with higher doses leading to increased absorption efficiency. Any material not directly absorbed through the mucous membranes is collected in mucus and swallowed (this \"drip\" is considered pleasant by some and unpleasant by others). In a study of cocaine users, the average time taken to reach peak subjective effects was 14.6 minutes. Any damage to the inside of the nose is because cocaine highly constricts blood vesselsand therefore blood and oxygen/nutrient flowto that area. Nosebleeds after cocaine insufflation are due to irritation and damage of mucus membranes by foreign particles and adulterants and not the cocaine itself; as a vasoconstrictor, cocaine acts to reduce bleeding.\n\nRolled up banknotes, hollowed-out pens, cut straws, pointed ends of keys, specialized spoons, long fingernails, and (clean) tampon applicators are often used to insufflate cocaine. Such devices are often called \"tooters\" by users. The cocaine typically is poured onto a flat, hard surface (such as a mirror, CD case or book) and divided into \"bumps\", \"lines\" or \"rails\", and then insufflated. The amount of cocaine in a line varies widely from person to person and occasion to occasion (the purity of the cocaine is also a factor), but one line is generally considered to be a single dose and is typically 35 mg (a \"bump\") to 100 mg (a \"rail\"). As tolerance builds rapidly in the short-term (hours), many lines are often snorted to produce greater effects.\n\nA 2001 study reported that the sharing of straws used to \"snort\" cocaine can spread blood diseases such as hepatitis C.\n\nDrug injection provides the highest blood levels of drug in the shortest amount of time. Subjective effects not commonly shared with other methods of administration include a ringing in the ears moments after injection (usually when in excess of 120 milligrams) lasting 2 to 5 minutes including tinnitus and audio distortion. This is colloquially referred to as a \"bell ringer\". In a study of cocaine users, the average time taken to reach peak subjective effects was 3.1 minutes. The euphoria passes quickly. Aside from the toxic effects of cocaine, there is also danger of circulatory emboli from the insoluble substances that may be used to cut the drug. As with all injected illicit substances, there is a risk of the user contracting blood-borne infections if sterile injecting equipment is not available or used. Additionally, because cocaine is a vasoconstrictor, and usage often entails multiple injections within several hours or less, subsequent injections are progressively more difficult to administer, which in turn may lead to more injection attempts and more consequences from improperly performed injection.\n\nAn injected mixture of cocaine and heroin, known as \"speedball\" is a particularly dangerous combination, as the converse effects of the drugs actually complement each other, but may also mask the symptoms of an overdose. It has been responsible for numerous deaths, including celebrities such as John Belushi, Chris Farley, Mitch Hedberg, River Phoenix, Layne Staley and Philip Seymour Hoffman.\n\nExperimentally, cocaine injections can be delivered to animals such as fruit flies to study the mechanisms of cocaine addiction.\n\nInhalation or smoking is one of the several means cocaine is administered. Cocaine is smoked by inhaling the vapor by sublimating solid cocaine by heating. In a 2000 Brookhaven National Laboratory medical department study, based on self reports of 32 abusers who participated in the study,\"peak high\" was found at mean of 1.4min +/- 0.5 minutes. Pyrolysis products of cocaine that occur only when heated/smoked have been shown to change the effect profile, \"i.e.\" anhydroecgonine methyl ester when co-administered with cocaine increases the dopamine in CPu and NAc brain regions, and has M- and M- receptor affinity.\n\nSmoking freebase or crack cocaine is most often accomplished using a pipe made from a small glass tube, often taken from \"love roses,\" small glass tubes with a paper rose that are promoted as romantic gifts. These are sometimes called \"stems\", \"horns\", \"blasters\" and \"straight shooters\". A small piece of clean heavy copper or occasionally stainless steel scouring padoften called a \"brillo\" (actual Brillo Pads contain soap, and are not used) or \"chore\" (named for Chore Boy brand copper scouring pads)serves as a reduction base and flow modulator in which the \"rock\" can be melted and boiled to vapor. Crack smokers also sometimes smoke through a soda can with small holes in the bottom.\n\nCrack is smoked by placing it at the end of the pipe; a flame held close to it produces vapor, which is then inhaled by the smoker. The effects, felt almost immediately after smoking, are very intense and do not last long usually 5 to 15 minutes.\n\nWhen smoked, cocaine is sometimes combined with other drugs, such as cannabis, often rolled into a joint or blunt. Powdered cocaine is also sometimes smoked, though heat destroys much of the chemical; smokers often sprinkle it on cannabis.\n\nThe language referring to paraphernalia and practices of smoking cocaine vary, as do the packaging methods in the street level sale.\n\nLittle research has been focused on the suppository (anal or vaginal insertion) method of administration, also known as \"plugging\". This method of administration is commonly administered using an oral syringe. Cocaine can be dissolved in water and withdrawn into an oral syringe which may then be lubricated and inserted into the anus or vagina before the plunger is pushed. Anecdotal evidence of its effects is infrequently discussed, possibly due to social taboos in many cultures. The rectum and the vaginal canal is where the majority of the drug would be taken up through the membranes lining its walls.\n\nWith excessive or prolonged use, the drug can cause itching, fast heart rate, hallucinations, and paranoid delusions. Overdoses cause hyperthermia and a marked elevation of blood pressure, which can be life-threatening, arrhythmias, and death.\n\nAnxiety, paranoia, and restlessness can also occur, especially during the comedown. With excessive dosage, tremors, convulsions and increased body temperature are observed. Severe cardiac adverse events, particularly sudden cardiac death, become a serious risk at high doses due to cocaine's blocking effect on cardiac sodium channels.\n\nChronic cocaine intake causes strong imbalances of transmitter levels in order to compensate extremes. Thus, receptors disappear from the cell surface or reappear on it, resulting more or less in an \"off\" or \"working mode\" respectively, or they change their susceptibility for binding partners (ligands)mechanisms called downregulation and upregulation. However, studies suggest cocaine abusers do not show normal age-related loss of striatal dopamine transporter (DAT) sites, suggesting cocaine has neuroprotective properties for dopamine neurons. Possible side effects include insatiable hunger, aches, insomnia/oversleeping, lethargy, and persistent runny nose. Depression with suicidal ideation may develop in very heavy users. Finally, a loss of vesicular monoamine transporters, neurofilament proteins, and other morphological changes appear to indicate a long term damage of dopamine neurons. All these effects contribute a rise in tolerance thus requiring a larger dosage to achieve the same effect.\n\nThe lack of normal amounts of serotonin and dopamine in the brain is the cause of the dysphoria and depression felt after the initial high. Physical withdrawal is not dangerous. Physiological changes caused by cocaine withdrawal include vivid and unpleasant dreams, insomnia or hypersomnia, increased appetite and psychomotor retardation or agitation.\n\nPhysical side effects from chronic smoking of cocaine include coughing up blood, bronchospasm, itching, fever, diffuse alveolar infiltrates without effusions, pulmonary and systemic eosinophilia, chest pain, lung trauma, sore throat, asthma, hoarse voice, dyspnea (shortness of breath), and an aching, flu-like syndrome. Cocaine constricts blood vessels, dilates pupils, and increases body temperature, heart rate, and blood pressure. It can also cause headaches and gastrointestinal complications such as abdominal pain and nausea. A common but untrue belief is that the smoking of cocaine chemically breaks down tooth enamel and causes tooth decay. However, cocaine does often cause involuntary tooth grinding, known as bruxism, which can deteriorate tooth enamel and lead to gingivitis. Additionally, stimulants like cocaine, methamphetamine, and even caffeine cause dehydration and dry mouth. Since saliva is an important mechanism in maintaining one's oral pH level, chronic stimulant abusers who do not hydrate sufficiently may experience demineralization of their teeth due to the pH of the tooth surface dropping too low (below 5.5).\n\nChronic intranasal usage can degrade the cartilage separating the nostrils (the septum nasi), leading eventually to its complete disappearance. Due to the absorption of the cocaine from cocaine hydrochloride, the remaining hydrochloride forms a dilute hydrochloric acid.\n\nCocaine may also greatly increase this risk of developing rare autoimmune or connective tissue diseases such as lupus, Goodpasture syndrome, vasculitis, glomerulonephritis, Stevens–Johnson syndrome, and other diseases. It can also cause a wide array of kidney diseases and kidney failure.\n\nCocaine misuse doubles both the risks of hemorrhagic and ischemic strokes, as well as increases the risk of other infarctions, such as myocardial infarction.\n\nCocaine addiction occurs through accumbal ΔFosB overexpression, which arises through transcriptional regulation and epigenetic remodeling of the nucleus accumbens.\n\nCocaine dependence is a form of psychological dependence that develops from regular cocaine use and produces a withdrawal state with emotional-motivational deficits upon cessation of cocaine use.\n\nAccording to a 2005 review, \"it is unclear whether prenatal cocaine exposure is associated with SIDS.\"\n\nThe pharmacodynamics of cocaine involve the complex relationships of neurotransmitters (inhibiting monoamine uptake in rats with ratios of about: serotonin:dopamine = 2:3, serotonin:norepinephrine = 2:5) The most extensively studied effect of cocaine on the central nervous system is the blockade of the dopamine transporter protein. Dopamine transmitter released during neural signaling is normally recycled via the transporter; i.e., the transporter binds the transmitter and pumps it out of the synaptic cleft back into the presynaptic neuron, where it is taken up into storage vesicles. Cocaine binds tightly at the dopamine transporter forming a complex that blocks the transporter's function. The dopamine transporter can no longer perform its reuptake function, and thus dopamine accumulates in the synaptic cleft.\n\nCocaine's affects certain serotonin (5-HT) receptors; in particular, it has been shown to antagonize the 5-HT3 receptor, which is a ligand-gated ion channel. The overabundance of 5-HT3 receptors in cocaine conditioned rats display this trait, however the exact effect of 5-HT3 in this process is unclear. The 5-HT2 receptor (particularly the subtypes 5-HT2AR, 5-HT2BR and 5-HT2CR) are involved in the locomotor-activating effects of cocaine.\n\nCocaine has been demonstrated to bind as to directly stabilize the DAT transporter on the open outward-facing conformation. Further, cocaine binds in such a way as to inhibit a hydrogen bond innate to DAT. Cocaine's binding properties are such that it attaches so this hydrogen bond will not form and is blocked from formation due to the tightly locked orientation of the cocaine molecule. Research studies have suggested that the affinity for the transporter is not what is involved in habituation of the substance so much as the conformation and binding properties to where and how on the transporter the molecule binds.\n\nSigma receptors are affected by cocaine, as cocaine functions as a sigma ligand agonist. Further specific receptors it has been demonstrated to function on are NMDA and the D1 dopamine receptor.\n\nCocaine also blocks sodium channels, thereby interfering with the propagation of action potentials; thus, like lignocaine and novocaine, it acts as a local anesthetic. It also functions on the binding sites to the dopamine and serotonin sodium dependent transport area as targets as separate mechanisms from its reuptake of those transporters; unique to its local anesthetic value which makes it in a class of functionality different from both its own derived phenyltropanes analogues which have that removed. In addition to this cocaine has some target binding to the site of the Kappa-opioid receptor as well. Cocaine also causes vasoconstriction, thus reducing bleeding during minor surgical procedures. The locomotor enhancing properties of cocaine may be attributable to its enhancement of dopaminergic transmission from the substantia nigra. Recent research points to an important role of circadian mechanisms and clock genes in behavioral actions of cocaine.\n\nCocaine can often cause reduced food intake, many chronic users lose their appetite and can experience severe malnutrition and significant weight loss. Cocaine effects, further, are shown to be potentiated for the user when used in conjunction with new surroundings and stimuli, and otherwise novel environs.\n\nCocaine is extensively metabolized, primarily in the liver, with only about 1% excreted unchanged in the urine. The metabolism is dominated by hydrolytic ester cleavage, so the eliminated metabolites consist mostly of benzoylecgonine (BE), the major metabolite, and other significant metabolites in lesser amounts such as ecgonine methyl ester (EME) and ecgonine. Further minor metabolites of cocaine include norcocaine, p-hydroxycocaine, m-hydroxycocaine, p-hydroxybenzoylecgonine (pOHBE), and m-hydroxybenzoylecgonine. If consumed with alcohol, cocaine combines with alcohol in the liver to form cocaethylene. Studies have suggested cocaethylene is both more euphoric, and has a higher cardiovascular toxicity than cocaine by itself.\n\nDepending on liver and kidney function, cocaine metabolites are detectable in urine. Benzoylecgonine can be detected in urine within four hours after cocaine intake and remains detectable in concentrations greater than 150 ng/mL typically for up to eight days after cocaine is used. Detection of accumulation of cocaine metabolites in hair is possible in regular users until the sections of hair grown during use are cut or fall out.\n\nCocaine in its purest form is a white, pearly product. Cocaine appearing in powder form is a salt, typically cocaine hydrochloride. Street cocaine is often adulterated or \"cut\" with talc, lactose, sucrose, glucose, mannitol, inositol, caffeine, procaine, phencyclidine, phenytoin, lignocaine, strychnine, amphetamine, or heroin.\n\nThe color of \"crack\" cocaine depends upon several factors including the origin of the cocaine used, the method of preparation – with ammonia or baking soda – and the presence of impurities, but will generally range from white to a yellowish cream to a light brown. Its texture will also depend on the adulterants, origin and processing of the powdered cocaine, and the method of converting the base. It ranges from a crumbly texture, sometimes extremely oily, to a hard, almost crystalline nature.\n\nCocaine - a tropane alkaloid - is a weakly alkaline compound, and can therefore combine with acidic compounds to form various salts. The hydrochloride (HCl) salt of cocaine is by far the most commonly encountered, although the sulfate (-SO) and the nitrate (-NO) are occasionally seen. Different salts dissolve to a greater or lesser extent in various solvents – the hydrochloride salt is polar in character and is quite soluble in water.\n\nAs the name implies, \"freebase\" is the base form of cocaine, as opposed to the salt form. It is practically insoluble in water whereas hydrochloride salt is water-soluble.\n\nSmoking freebase cocaine has the additional effect of releasing methylecgonidine into the user's system due to the pyrolysis of the substance (a side effect which insufflating or injecting powder cocaine does not create). Some research suggests that smoking freebase cocaine can be even more cardiotoxic than other routes of administration because of methylecgonidine's effects on lung tissue and liver tissue.\n\nPure cocaine is prepared by neutralizing its compounding salt with an alkaline solution which will precipitate to non-polar basic cocaine. It is further refined through aqueous-solvent liquid-liquid extraction.\n\nCrack is a lower purity form of free-base cocaine that is usually produced by neutralization of cocaine hydrochloride with a solution of baking soda (sodium bicarbonate, NaHCO) and water, producing a very hard/brittle, off-white-to-brown colored, amorphous material that contains sodium carbonate, entrapped water, and other by-products as the main impurities.\n\nThe \"freebase\" and \"crack\" forms of cocaine are usually administered by vaporization of the powdered substance into smoke, which is then inhaled.\n\nThe origin of the name \"crack\" comes from the \"crackling\" sound (and hence the onomatopoeic moniker \"crack\") that is produced when the cocaine and its impurities (i.e. water, sodium bicarbonate) are heated past the point of vaporization.\n\nPure cocaine base/crack can be smoked because it vaporizes smoothly, with little or no decomposition at , which is below the boiling point of water.\n\nIn contrast, cocaine hydrochloride does not vaporize until heated to a much higher temperature (about 197 °C), and considerable decomposition/burning occurs at these high temperatures. This effectively destroys some of the cocaine and yields a sharp, acrid, and foul-tasting smoke.\n\nSmoking or vaporizing cocaine and inhaling it into the lungs produces an almost immediate \"high\" that can be very powerful (and addicting) quite rapidly – this initial crescendo of stimulation is known as a \"rush\". While the stimulating effects may last for hours, the euphoric sensation is very brief, prompting the user to smoke more immediately.\n\nCoca herbal infusion (also referred to as coca tea) is used in coca-leaf producing countries much as any herbal medicinal infusion would elsewhere in the world. The free and legal commercialization of dried coca leaves under the form of filtration bags to be used as \"coca tea\" has been actively promoted by the governments of Peru and Bolivia for many years as a drink having medicinal powers. Visitors to the city of Cuzco in Peru, and La Paz in Bolivia are greeted with the offering of coca leaf infusions (prepared in teapots with whole coca leaves) purportedly to help the newly arrived traveler overcome the malaise of high altitude sickness. The effects of drinking coca tea are a mild stimulation and mood lift. It does not produce any significant numbing of the mouth nor does it give a rush like snorting cocaine. In order to prevent the demonization of this product, its promoters publicize the unproven concept that much of the effect of the ingestion of coca leaf infusion would come from the secondary alkaloids, as being not only quantitatively different from pure cocaine but also qualitatively different.\n\nIt has been promoted as an adjuvant for the treatment of cocaine dependence. In one controversial study, coca leaf infusion was used—in addition to counseling—to treat 23 addicted coca-paste smokers in Lima, Peru. Relapses fell from an average of four times per month before treatment with coca tea to one during the treatment. The duration of abstinence increased from an average of 32 days prior to treatment to 217 days during treatment. These results suggest that the administration of coca leaf infusion plus counseling would be an effective method for preventing relapse during treatment for cocaine addiction. Importantly, these results also suggest strongly that the primary pharmacologically active metabolite in coca leaf infusions is actually cocaine and not the secondary alkaloids.\n\nThe cocaine metabolite benzoylecgonine can be detected in the urine of people a few hours after drinking one cup of coca leaf infusion.\n\nThe first synthesis and elucidation of the cocaine molecule was by Richard Willstätter in 1898. Willstätter's synthesis derived cocaine from tropinone. Since then, Robert Robinson and Edward Leete have made significant contributions to the mechanism of the synthesis. (-NO)\n\nThe additional carbon atoms required for the synthesis of cocaine are derived from acetyl-CoA, by addition of two acetyl-CoA units to the \"N\"-methyl-Δ-pyrrolinium cation. The first addition is a Mannich-like reaction with the enolate anion from acetyl-CoA acting as a nucleophile towards the pyrrolinium cation. The second addition occurs through a Claisen condensation. This produces a racemic mixture of the 2-substituted pyrrolidine, with the retention of the thioester from the Claisen condensation. In formation of tropinone from racemic ethyl [2,3-13C](Nmethyl-2-pyrrolidinyl)-3-oxobutanoate there is no preference for either stereoisomer. In the biosynthesis of cocaine, however, only the (S)-enantiomer can cyclize to form the tropane ring system of cocaine. The stereoselectivity of this reaction was further investigated through study of prochiral methylene hydrogen discrimination. This is due to the extra chiral center at C-2. This process occurs through an oxidation, which regenerates the pyrrolinium cation and formation of an enolate anion, and an intramolecular Mannich reaction. The tropane ring system undergoes hydrolysis, SAM-dependent methylation, and reduction via NADPH for the formation of methylecgonine. The benzoyl moiety required for the formation of the cocaine diester is synthesized from phenylalanine via cinnamic acid. Benzoyl-CoA then combines the two units to form cocaine.\n\nThe biosynthesis begins with L-Glutamine, which is derived to L-ornithine in plants. The major contribution of L-ornithine and L-arginine as a precursor to the tropane ring was confirmed by Edward Leete. Ornithine then undergoes a pyridoxal phosphate-dependent decarboxylation to form putrescine. In animals, however, the urea cycle derives putrescine from ornithine. L-ornithine is converted to L-arginine, which is then decarboxylated via PLP to form agmatine. Hydrolysis of the imine derives \"N\"-carbamoylputrescine followed with hydrolysis of the urea to form putrescine. The separate pathways of converting ornithine to putrescine in plants and animals have converged. A SAM-dependent \"N\"-methylation of putrescine gives the \"N\"-methylputrescine product, which then undergoes oxidative deamination by the action of diamine oxidase to yield the aminoaldehyde. Schiff base formation confirms the biosynthesis of the \"N\"-methyl-Δ-pyrrolinium cation.\n\nThe biosynthesis of the tropane alkaloid, however, is still uncertain. Hemscheidt proposes that Robinson's acetonedicarboxylate emerges as a potential intermediate for this reaction. Condensation of \"N\"-methylpyrrolinium and acetonedicarboxylate would generate the oxobutyrate. Decarboxylation leads to tropane alkaloid formation.\n\nThe reduction of tropinone is mediated by NADPH-dependent reductase enzymes, which have been characterized in multiple plant species. These plant species all contain two types of the reductase enzymes, tropinone reductase I and tropinone reductase II. TRI produces tropine and TRII produces pseudotropine. Due to differing kinetic and pH/activity characteristics of the enzymes and by the 25-fold higher activity of TRI over TRII, the majority of the tropinone reduction is from TRI to form tropine.\n\nCocaine and its major metabolites may be quantified in blood, plasma, or urine to monitor for abuse, confirm a diagnosis of poisoning, or assist in the forensic investigation of a traffic or other criminal violation or a sudden death. Most commercial cocaine immunoassay screening tests cross-react appreciably with the major cocaine metabolites, but chromatographic techniques can easily distinguish and separately measure each of these substances. When interpreting the results of a test, it is important to consider the cocaine usage history of the individual, since a chronic user can develop tolerance to doses that would incapacitate a cocaine-naive individual, and the chronic user often has high baseline values of the metabolites in his system. Cautious interpretation of testing results may allow a distinction between passive or active usage, and between smoking versus other routes of administration. In 2011, researchers at John Jay College of Criminal Justice reported that dietary zinc supplements can mask the presence of cocaine and other drugs in urine. Similar claims have been made in web forums on that topic.\n\nAccording to a 2007 United Nations report, Spain is the country with the highest rate of cocaine usage (3.0% of adults in the previous year). Other countries where the usage rate meets or exceeds 1.5% are the United States (2.8%), England and Wales (2.4%), Canada (2.3%), Italy (2.1%), Bolivia (1.9%), Chile (1.8%), and Scotland (1.5%).\n\nCocaine is the second most popular illegal recreational drug in Europe (behind cannabis). Since the mid-1990s, overall cocaine usage in Europe has been on the rise, but usage rates and attitudes tend to vary between countries. European countries with the highest usage rates are the United Kingdom, Spain, Italy, and the Republic of Ireland.\n\nApproximately 12 million Europeans (3.6%) have used cocaine at least once, 4 million (1.2%) in the last year, and 2 million in the last month (0.5%).\n\nAbout 3.5 million or 87.5% of those who have used the drug in the last year are young adults (15–34 years old). Usage is particularly prevalent among this demographic: 4% to 7% of males have used cocaine in the last year in Spain, Denmark, Republic of Ireland, Italy, and the United Kingdom. The ratio of male to female users is approximately 3.8:1, but this statistic varies from 1:1 to 13:1 depending on country.\n\nIn 2014 London had the highest amount of cocaine in their sewage out of 50 European cities.\n\nCocaine is the second most popular illegal recreational drug in the United States (behind cannabis) and the U.S. is the world's largest consumer of cocaine. Cocaine is commonly used in middle to upper-class communities and is known as a \"rich man's drug\". It is also popular amongst college students, as a party drug. A study throughout the entire United States has reported that around 48 percent of people who graduated high school in 1979 have used Cocaine recreationally during some point in their lifetime, compared to approximately 20 percent of students who graduated between the years of 1980 and 1995.\n\nIts users span over different ages, races, and professions. In the 1970s and 1980s, the drug became particularly popular in the disco culture as cocaine usage was very common and popular in many discos such as Studio 54.\n\nFor over a thousand years South American indigenous peoples have chewed the leaves of \"Erythroxylon coca\", a plant that contains vital nutrients as well as numerous alkaloids, including cocaine. The coca leaf was, and still is, chewed almost universally by some indigenous communities. The remains of coca leaves have been found with ancient Peruvian mummies, and pottery from the time period depicts humans with bulged cheeks, indicating the presence of something on which they are chewing. There is also evidence that these cultures used a mixture of coca leaves and saliva as an anesthetic for the performance of trepanation.\n\nWhen the Spanish arrived in South America, most at first ignored aboriginal claims that the leaf gave them strength and energy, and declared the practice of chewing it the work of the Devil. But after discovering that these claims were true, they legalized and taxed the leaf, taking 10% off the value of each crop. In 1569, Nicolás Monardes described the indigenous peoples' practice of chewing a mixture of tobacco and coca leaves to induce \"great contentment\":\nIn 1609, Padre Blas Valera wrote:\nAlthough the stimulant and hunger-suppressant properties of coca had been known for many centuries, the isolation of the cocaine alkaloid was not achieved until 1855. Various European scientists had attempted to isolate cocaine, but none had been successful for two reasons: the knowledge of chemistry required was insufficient at the time, and contemporary conditions of sea-shipping from South America could degrade the cocaine in the plant samples available to European chemists. \n\nThe cocaine alkaloid was first isolated by the German chemist Friedrich Gaedcke in 1855. Gaedcke named the alkaloid \"erythroxyline\", and published a description in the journal \"Archiv der Pharmazie.\"\n\nIn 1856, Friedrich Wöhler asked Dr. Carl Scherzer, a scientist aboard the \"Novara\" (an Austrian frigate sent by Emperor Franz Joseph to circle the globe), to bring him a large amount of coca leaves from South America. In 1859, the ship finished its travels and Wöhler received a trunk full of coca. Wöhler passed on the leaves to Albert Niemann, a Ph.D. student at the University of Göttingen in Germany, who then developed an improved purification process.\n\nNiemann described every step he took to isolate cocaine in his dissertation titled \"Über eine neue organische Base in den Cocablättern\" (\"On a New Organic Base in the Coca Leaves\"), which was published in 1860—it earned him his Ph.D. and is now in the British Library. He wrote of the alkaloid's \"colourless transparent prisms\" and said that \"Its solutions have an alkaline reaction, a bitter taste, promote the flow of saliva and leave a peculiar numbness, followed by a sense of cold when applied to the tongue.\" Niemann named the alkaloid \"cocaine\" from \"coca\" (from Quechua \"cuca\") + suffix \"ine\". Because of its use as a local anesthetic, a suffix \"-caine\" was later extracted and used to form names of synthetic local anesthetics.\n\nThe first synthesis and elucidation of the structure of the cocaine molecule was by Richard Willstätter in 1898. It was the first biomimetic synthesis of an organic structure recorded in academic chemical literature. The synthesis started from tropinone, a related natural product and took five steps. The name comes from \"coca\" and the alkaloid suffix \"-ine\", forming \"cocaine\".\n\nWith the discovery of this new alkaloid, Western medicine was quick to exploit the possible uses of this plant.\n\nIn 1879, Vassili von Anrep, of the University of Würzburg, devised an experiment to demonstrate the analgesic properties of the newly discovered alkaloid. He prepared two separate jars, one containing a cocaine-salt solution, with the other containing merely salt water. He then submerged a frog's legs into the two jars, one leg in the treatment and one in the control solution, and proceeded to stimulate the legs in several different ways. The leg that had been immersed in the cocaine solution reacted very differently from the leg that had been immersed in salt water.\n\nKarl Koller (a close associate of Sigmund Freud, who would write about cocaine later) experimented with cocaine for ophthalmic usage. In an infamous experiment in 1884, he experimented upon himself by applying a cocaine solution to his own eye and then pricking it with pins. His findings were presented to the Heidelberg Ophthalmological Society. Also in 1884, Jellinek demonstrated the effects of cocaine as a respiratory system anesthetic. In 1885, William Halsted demonstrated nerve-block anesthesia, and James Leonard Corning demonstrated peridural anesthesia. 1898 saw Heinrich Quincke use cocaine for spinal anesthesia.\n\nToday, cocaine has a very limited medical use.\n\nIn 1859, an Italian doctor, Paolo Mantegazza, returned from Peru, where he had witnessed first-hand the use of coca by the local indigenous peoples. He proceeded to experiment on himself and upon his return to Milan he wrote a paper in which he described the effects. In this paper he declared coca and cocaine (at the time they were assumed to be the same) as being useful medicinally, in the treatment of \"a furred tongue in the morning, flatulence, and whitening of the teeth.\"\n\nA chemist named Angelo Mariani who read Mantegazza's paper became immediately intrigued with coca and its economic potential. In 1863, Mariani started marketing a wine called Vin Mariani, which had been treated with coca leaves, to become cocawine. The ethanol in wine acted as a solvent and extracted the cocaine from the coca leaves, altering the drink's effect. It contained 6 mg cocaine per ounce of wine, but Vin Mariani which was to be exported contained 7.2 mg per ounce, to compete with the higher cocaine content of similar drinks in the United States. A \"pinch of coca leaves\" was included in John Styth Pemberton's original 1886 recipe for Coca-Cola, though the company began using decocainized leaves in 1906 when the Pure Food and Drug Act was passed.\n\nIn 1879 cocaine began to be used to treat morphine addiction. Cocaine was introduced into clinical use as a local anesthetic in Germany in 1884, about the same time as Sigmund Freud published his work \"Über Coca\", in which he wrote that cocaine causes:\n\nIn 1885 the U.S. manufacturer Parke-Davis sold cocaine in various forms, including cigarettes, powder, and even a cocaine mixture that could be injected directly into the user's veins with the included needle. The company promised that its cocaine products would \"supply the place of food, make the coward brave, the silent eloquent and render the sufferer insensitive to pain.\"\n\nBy the late Victorian era, cocaine use had appeared as a vice in literature. For example, it was injected by Arthur Conan Doyle's fictional Sherlock Holmes, generally to offset the boredom he felt when he was not working on a case.\n\nIn early 20th-century Memphis, Tennessee, cocaine was sold in neighborhood drugstores on Beale Street, costing five or ten cents for a small boxful. Stevedores along the Mississippi River used the drug as a stimulant, and white employers encouraged its use by black laborers.\n\nIn 1909, Ernest Shackleton took \"Forced March\" brand cocaine tablets to Antarctica, as did Captain Scott a year later on his ill-fated journey to the South Pole.\n\nDuring the mid-1940s, amidst WWII, cocaine was considered for inclusion as an ingredient of a future generation of 'pep pills' for the German military code named D-IX.\n\nIn many countries, cocaine is a popular recreational drug. In the United States, the development of \"crack\" cocaine introduced the substance to a generally poorer inner-city market. Use of the powder form has stayed relatively constant, experiencing a new height of use during the late 1990s and early 2000s in the U.S., and has become much more popular in the last few years in the UK. \n\nCocaine use is prevalent across all socioeconomic strata, including age, demographics, economic, social, political, religious, and livelihood. \n\nThe estimated U.S. cocaine market exceeded US$70 billion in street value for the year 2005, exceeding revenues by corporations such as Starbucks. There is a tremendous demand for cocaine in the U.S. market, particularly among those who are making incomes affording luxury spending, such as single adults and professionals with discretionary income. Cocaine's status as a club drug shows its immense popularity among the \"party crowd\".\n\nIn 1995 the World Health Organization (WHO) and the United Nations Interregional Crime and Justice Research Institute (UNICRI) announced in a press release the publication of the results of the largest global study on cocaine use ever undertaken. However, a decision by an American representative in the World Health Assembly banned the publication of the study, because it seemed to make a case for the positive uses of cocaine. An excerpt of the report strongly conflicted\nwith accepted paradigms, for example \"that occasional cocaine use does not typically lead to severe or even minor physical or social problems.\" In the sixth meeting of the B committee, the US representative threatened that \"If World Health Organization activities relating to drugs failed to reinforce proven drug control approaches, funds for the relevant programs should be curtailed\". This led to the decision to discontinue publication. A part of the study was recuperated and published in 2010, including profiles of cocaine use in 20 countries, but are unavailable .\n\nIn October 2010 it was reported that the use of cocaine in Australia has doubled since monitoring began in 2003.\n\nA problem with illegal cocaine use, especially in the higher volumes used to combat fatigue (rather than increase euphoria) by long-term users, is the risk of ill effects or damage caused by the compounds used in adulteration. Cutting or \"stepping on\" the drug is commonplace, using compounds which simulate ingestion effects, such as Novocain (procaine) producing temporary anesthaesia, as many users believe a strong numbing effect is the result of strong and/or pure cocaine, ephedrine or similar stimulants that are to produce an increased heart rate. The normal adulterants for profit are inactive sugars, usually mannitol, creatine or glucose, so introducing active adulterants gives the illusion of purity and to 'stretch' or make it so a dealer can sell more product than without the adulterants. The adulterant of sugars allows the dealer to sell the product for a higher price because of the illusion of purity and allows to sell more of the product at that higher price, enabling dealers to significantly increase revenue with little additional cost for the adulterants. A 2007 study by the European Monitoring Centre for Drugs and Drug Addiction showed that the purity levels for street purchased cocaine was often under 5% and on average under 50% pure.\n\nThe production, distribution, and sale of cocaine products is restricted (and illegal in most contexts) in most countries as regulated by the Single Convention on Narcotic Drugs, and the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances. In the United States the manufacture, importation, possession, and distribution of cocaine are additionally regulated by the 1970 Controlled Substances Act.\n\nSome countries, such as Peru and Bolivia permit the cultivation of coca leaf for traditional consumption by the local indigenous population, but nevertheless, prohibit the production, sale, and consumption of cocaine. The provisions as to how much a coca farmer can yield annually is protected by laws such as the Bolivian Cato accord. In addition, some parts of Europe and Australia allow processed cocaine for medicinal uses only.\n\nCocaine is a Schedule 8 prohibited substance in Australia under the Poisons Standard (July 2016). A schedule 8 substance is a controlled Drug – Substances which should be available for use but require restriction of manufacture, supply, distribution, possession and use to reduce abuse, misuse and physical or psychological dependence.\n\nIn Western Australia under the Misuse of Drugs Act 1981 4.0g of cocaine is the amount of prohibited drugs determining a court of trial, 2.0g is the amount of cocaine required for the presumption of intention to sell or supply and 28.0g is the amount of cocaine required for purposes of drug trafficking \n\nThe US federal government instituted a national labeling requirement for cocaine and cocaine-containing products through the Food and Drug Act of 1906. The next important federal regulation was the Harrison Narcotics Tax Act of 1914. While this act is often seen as the start of prohibition, the act itself was not actually a prohibition on cocaine, but instead set up a regulatory and licensing regime. The Harrison Act did not recognize addiction as a treatable condition and therefore the therapeutic use of cocaine, heroin or morphine to such individuals was outlawed leading the Journal of American Medicine to remark, \"[the addict] is denied the medical care he urgently needs, open, above-board sources from which he formerly obtained his drug supply are closed to him, and he is driven to the underworld where he can get his drug, but of course, surreptitiously and in violation of the law.\" The Harrison Act left manufacturers of cocaine untouched so long as they met certain purity and labeling standards. Despite that cocaine was typically illegal to sell and legal outlets were rarer, the quantities of legal cocaine produced declined very little. Legal cocaine quantities did not decrease until the Jones-Miller Act of 1922 put serious restrictions on cocaine manufactures.\n\nIn 2004, according to the United Nations, 589 tonnes of cocaine were seized globally by law enforcement authorities. Colombia seized 188 t, the United States 166 t, Europe 79 t, Peru 14 t, Bolivia 9 t, and the rest of the world 133 t.\n\nBecause of the drug's potential for addiction and overdose, cocaine is generally treated as a 'hard drug', with severe penalties for possession and trafficking. Demand remains high, and consequently, black market cocaine is quite expensive. Unprocessed cocaine, such as coca leaves, are occasionally purchased and sold, but this is exceedingly rare as it is much easier and more profitable to conceal and smuggle it in powdered form. The scale of the market is immense: 770 tonnes times $100 per gram retail = up to $77 billion.\n\nUntil 2012, Colombia was the world's leading producer of cocaine. Three-quarters of the world's annual yield of cocaine has been produced in Colombia, both from cocaine base imported from Peru (primarily the Huallaga Valley) and Bolivia, and from locally grown coca. There was a 28% increase from the amount of potentially harvestable coca plants which were grown in Colombia in 1998. This, combined with crop reductions in Bolivia and Peru, made Colombia the nation with the largest area of coca under cultivation after the mid-1990s. Coca is grown for traditional purposes by indigenous communities, a use which is still present and is permitted by Colombian laws only makes up a small fragment of total coca production, most of which is used for the illegal drug trade.\n\nAn interview with a coca farmer published in 2003 described a mode of production by acid-base extraction that has changed little since 1905. Roughly of leaves were harvested per hectare, six times per year. The leaves were dried for half a day, then chopped into small pieces with a strimmer and sprinkled with a small amount of powdered cement (replacing sodium carbonate from former times). Several hundred pounds of this mixture were soaked in of gasoline for a day, then the gasoline was removed and the leaves were pressed for remaining liquid, after which they could be discarded. Then battery acid (weak sulfuric acid) was used, one bucket per of leaves, to create a phase separation in which the cocaine free base in the gasoline was acidified and extracted into a few buckets of \"murky-looking smelly liquid\". Once powdered caustic soda was added to this, the cocaine precipitated and could be removed by filtration through a cloth. The resulting material, when dried, was termed \"pasta\" and sold by the farmer. The 3750 pound yearly harvest of leaves from a hectare produced of \"pasta\", approximately 40–60% cocaine. Repeated recrystallization from solvents, producing \"pasta lavada\" and eventually crystalline cocaine were performed at specialized laboratories after the sale.\n\nAttempts to eradicate coca fields through the use of defoliants have devastated part of the farming economy in some coca growing regions of Colombia, and strains appear to have been developed that are more resistant or immune to their use. Whether these strains are natural mutations or the product of human tampering is unclear. These strains have also shown to be more potent than those previously grown, increasing profits for the drug cartels responsible for the exporting of cocaine. Although production fell temporarily, coca crops rebounded in numerous smaller fields in Colombia, rather than the larger plantations.\n\nThe cultivation of coca has become an attractive economic decision for many growers due to the combination of several factors, including the lack of other employment alternatives, the lower profitability of alternative crops in official crop substitution programs, the eradication-related damages to non-drug farms, the spread of new strains of the coca plant due to persistent worldwide demand.\n\nThe latest estimate provided by the U.S. authorities on the annual production of cocaine in Colombia refers to 290 metric tons.\nAs of the end of 2011, the seizure operations of Colombian cocaine carried out in different countries have totaled 351.8 metric tons of cocaine, i.e. 121.3% of Colombia's annual production according to the U.S. Department of State's estimates.\nSynthetic cocaine would be highly desirable to the illegal drug industry as it would eliminate the high visibility and low reliability of offshore sources and international smuggling, replacing them with clandestine domestic laboratories, as are common for illicit methamphetamine. However, natural cocaine remains the lowest cost and highest quality supply of cocaine. Actual full synthesis of cocaine is rarely done. Formation of inactive enantiomers (cocaine has 4 chiral centres – \"1R\", \"2R\", \"3S\", and \"5S\" – hence a total potential of 16 possible enantiomers and diastereoisomers) plus synthetic by-products limits the yield and purity.\nNames like \"synthetic cocaine\" and \"new cocaine\" have been misapplied to phencyclidine (PCP) and various designer drugs.\n\nOrganized criminal gangs operating on a large scale dominate the cocaine trade. Most cocaine is grown and processed in South America, particularly in Colombia, Bolivia, Peru, and smuggled into the United States and Europe, the United States being the world's largest consumer of cocaine, where it is sold at huge markups; usually in the US at $80–120 for 1 gram, and $250–300 for 3.5 grams (⅛ of an ounce, or an \"eight ball\").\n\n, cocaine shipments from South America transported through Mexico or Central America were generally moved over land or by air to staging sites in northern Mexico. The cocaine is then broken down into smaller loads for smuggling across the U.S.–Mexico border. The primary cocaine importation points in the United States have been in Arizona, southern California, southern Florida, and Texas. Typically, land vehicles are driven across the U.S.–Mexico border. Sixty-five percent of cocaine enters the United States through Mexico, and the vast majority of the rest enters through Florida. , the Sinaloa Cartel is the most active drug cartel involved in smuggling illicit drugs like cocaine into the United States and trafficking them throughout the United States.\n\nCocaine traffickers from Colombia and Mexico have established a labyrinth of smuggling routes throughout the Caribbean, the Bahama Island chain, and South Florida. They often hire traffickers from Mexico or the Dominican Republic to transport the drug using a variety of smuggling techniques to U.S. markets. These include airdrops of in the Bahama Islands or off the coast of Puerto Rico, mid-ocean boat-to-boat transfers of , and the commercial shipment of tonnes of cocaine through the port of Miami.\n\nAnother route of cocaine traffic goes through Chile, which is primarily used for cocaine produced in Bolivia since the nearest seaports lie in northern Chile. The arid Bolivia–Chile border is easily crossed by 4×4 vehicles that then head to the seaports of Iquique and Antofagasta. While the price of cocaine is higher in Chile than in Peru and Bolivia, the final destination is usually Europe, especially Spain where drug dealing networks exist among South American immigrants.\n\nCocaine is also carried in small, concealed, kilogram quantities across the border by couriers known as \"mules\" (or \"mulas\"), who cross a border either legally, for example, through a port or airport, or illegally elsewhere. The drugs may be strapped to the waist or legs or hidden in bags, or hidden in the body. If the mule gets through without being caught, the gangs will reap most of the profits. If he or she is caught, however, gangs will sever all links and the mule will usually stand trial for trafficking alone.\n\nBulk cargo ships are also used to smuggle cocaine to staging sites in the western Caribbean–Gulf of Mexico area. These vessels are typically 150–250-foot (50–80 m) coastal freighters that carry an average cocaine load of approximately 2.5 tonnes. Commercial fishing vessels are also used for smuggling operations. In areas with a high volume of recreational traffic, smugglers use the same types of vessels, such as go-fast boats, as those used by the local populations.\n\nSophisticated drug subs are the latest tool drug runners are using to bring cocaine north from Colombia, it was reported on 20 March 2008. Although the vessels were once viewed as a quirky sideshow in the drug war, they are becoming faster, more seaworthy, and capable of carrying bigger loads of drugs than earlier models, according to those charged with catching them.\n\nCocaine is readily available in all major countries' metropolitan areas. According to the \"Summer 1998 Pulse Check\", published by the U.S. Office of National Drug Control Policy, cocaine use had stabilized across the country, with a few increases reported in San Diego, Bridgeport, Miami, and Boston. In the West, cocaine usage was lower, which was thought to be due to a switch to methamphetamine among some users; methamphetamine is cheaper, three and a half times more powerful, and lasts 12–24 times longer with each dose. Nevertheless, the number of cocaine users remain high, with a large concentration among urban youth.\n\nIn addition to the amounts previously mentioned, cocaine can be sold in \"bill sizes\": for example, $10 might purchase a \"dime bag\", a very small amount (0.1–0.15 g) of cocaine. Twenty dollars might purchase 0.15–0.3 g. However, in lower Texas, it is sold cheaper due to it being easier to receive: a dime for $10 is 0.4 g, a 20 is 0.8–1.0 g and an 8-ball (3.5 g) is sold for $60 to $80, depending on the quality and dealer. These amounts and prices are very popular among young people because they are inexpensive and easily concealed on one's body. Quality and price can vary dramatically depending on supply and demand, and on geographic region.\n\nIn 2008, the European Monitoring Centre for Drugs and Drug Addiction reports that the typical retail price of cocaine varied between €50 and €75 per gram in most European countries, although Cyprus, Romania, Sweden and Turkey reported much higher values.\n\nWorld annual cocaine consumption, as of 2000, stood at around 600 tonnes, with the United States consuming around 300 t, 50% of the total, Europe about 150 t, 25% of the total, and the rest of the world the remaining 150 t or 25%.\n\nThe 2010 UN World Drug Report concluded that \"it appears that the North American cocaine market has declined in value from US$47 billion in 1998 to US$38 billion in 2008. Between 2006 and 2008, the value of the market remained basically stable.\"\n\nIn 2005, researchers proposed the use of cocaine in conjunction with phenylephrine administered in the form of an eye drop as a diagnostic test for Parkinson's disease.\n\n\n", "id": "7701", "title": "Cocaine"}
{"url": "https://en.wikipedia.org/wiki?curid=7706", "text": "Cartesian coordinate system\n\nA Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances to the point from two fixed perpendicular directed lines, measured in the same unit of length. Each reference line is called a \"coordinate axis\" or just \"axis\" of the system, and the point where they meet is its \"origin\", usually at ordered pair . The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.\n\nOne can use the same principle to specify the position of any point in three-dimensional space by three Cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). In general, \"n\" Cartesian coordinates (an element of real \"n\"-space) specify the point in an \"n\"-dimensional Euclidean space for any dimension \"n\". These coordinates are equal, up to sign, to distances from the point to \"n\" mutually perpendicular hyperplanes.\n\nThe invention of Cartesian coordinates in the 17th century by René Descartes (Latinized name: \"Cartesius\") revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2, centered at the origin of the plane, may be described as the set of all points whose coordinates \"x\" and \"y\" satisfy the equation .\n\nCartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design and other geometry-related data processing.\n\nNicole Oresme, a French cleric and friend of the Dauphin (later to become King Charles V) of the 14th Century, used constructions similar to Cartesian coordinates well before the time of Descartes and Fermat.\n\nThe adjective \"Cartesian\" refers to the French mathematician and philosopher René Descartes who published this idea in 1637. It was independently discovered by Pierre de Fermat, who also worked in three dimensions, although Fermat did not publish the discovery. Both authors used a single axis in their treatments and have a variable length measured in reference to this axis. The concept of using a pair of axes was introduced later, after Descartes' \"La Géométrie\" was translated into Latin in 1649 by Frans van Schooten and his students. These commentators introduced several concepts while trying to clarify the ideas contained in Descartes' work.\n\nMany other coordinate systems have been developed since Descartes, such as the polar coordinates for the plane, and the spherical and cylindrical coordinates for three-dimensional space.\n\nThe development of the Cartesian coordinate system would play a fundamental role in the development of the Calculus by Isaac Newton and Gottfried Wilhelm Leibniz. The two-coordinate description of the plane was later generalized into the concept of vector spaces.\n\nChoosing a Cartesian coordinate system for a one-dimensional space that is, for a straight line—involves choosing a point \"O\" of the line (the origin), a unit of length, and an orientation for the line. An orientation chooses which of the two half-lines determined by \"O\" is the positive, and which is negative; we then say that the line \"is oriented\" (or \"points\") from the negative half towards the positive half. Then each point \"P\" of the line can be specified by its distance from \"O\", taken with a + or − sign depending on which half-line contains \"P\".\n\nA line with a chosen Cartesian system is called a number line. Every real number has a unique location on the line. Conversely, every point on the line can be interpreted as a number in an ordered continuum such as the real numbers.\n\nThe Cartesian coordinate system in two dimensions (also called a rectangular coordinate system) is defined by an ordered pair of perpendicular lines (axes), a single unit of length for both axes, and an orientation for each axis. (Early systems allowed \"oblique\" axes, that is, axes that did not meet at right angles.) The lines are commonly referred to as the \"x\"- and \"y\"-axes where the \"x\"-axis is taken to be horizontal and the \"y\"-axis is taken to be vertical. The point where the axes meet is taken as the origin for both, thus turning each axis into a number line. For a given point \"P\", a line is drawn through \"P\" perpendicular to the \"x\"-axis to meet it at \"X\" and second line is drawn through \"P\" perpendicular to the \"y\"-axis to meet it at \"Y\". The coordinates of \"P\" are then \"X\" and \"Y\" interpreted as numbers \"x\" and \"y\" on the corresponding number lines. The coordinates are written as an ordered pair .\n\nThe point where the axes meet is the common origin of the two number lines and is simply called the \"origin\". It is often labeled \"O\" and if so then the axes are called \"Ox\" and \"Oy\". A plane with \"x\"- and \"y\"-axes defined is often referred to as the Cartesian plane or \"xy\"-plane. The value of \"x\" is called the \"x\"-coordinate or abscissa and the value of \"y\" is called the \"y\"-coordinate or ordinate.\n\nThe choices of letters come from the original convention, which is to use the latter part of the alphabet to indicate unknown values. The first part of the alphabet was used to designate known values.\n\nIn the Cartesian plane, reference is sometimes made to a unit circle or a unit hyperbola.\n\nIf a point on a two-dimensional plane is , then the distance of the point from the \"x\"-axis is <nowiki>|</nowiki>\"y\"<nowiki>|</nowiki> and the distance of the point from the \"y\"-axis is |\"x\"|.\n\nChoosing a Cartesian coordinate system for a three-dimensional space means choosing an ordered triplet of lines (axes) that are pair-wise perpendicular, have a single unit of length for all three axes and have an orientation for each axis. As in the two-dimensional case, each axis becomes a number line. The coordinates of a point \"P\" are obtained by drawing a line through \"P\" perpendicular to each coordinate axis, and reading the points where these lines meet the axes as three numbers of these number lines.\n\nAlternatively, the coordinates of a point \"P\" can also be taken as the (signed) distances from \"P\" to the three planes defined by the three axes. If the axes are named \"x\", \"y\", and \"z\", then the \"x\"-coordinate is the distance from the plane defined by the \"y\"- and \"z\"-axes. The distance is to be taken with the + or − sign, depending on which of the two half-spaces separated by that plane contains \"P\". The \"y\"- and \"z\"-coordinates can be obtained in the same way from the \"xz\"- and \"xy\"-planes respectively.\n\nA Euclidean plane with a chosen Cartesian system is called a Cartesian plane. Since Cartesian coordinates are unique and non-ambiguous, the points of a Cartesian plane can be identified with pairs of real numbers; that is with the Cartesian product formula_1, where formula_2 is the set of all reals. In the same way, the points in any Euclidean space of dimension \"n\" be identified with the tuples (lists) of \"n\" real numbers, that is, with the Cartesian product formula_3.\n\nThe concept of Cartesian coordinates generalizes to allow axes that are not perpendicular to each other, and/or different units along each axis. In that case, each coordinate is obtained by projecting the point onto one axis along a direction that is parallel to the other axis (or, in general, to the hyperplane defined by all the other axes). In such an oblique coordinate system the computations of distances and angles must be modified from that in standard Cartesian systems, and many standard formulas (such as the Pythagorean formula for the distance) do not hold (see affine plane).\n\nThe Cartesian coordinates of a point are usually written in parentheses and separated by commas, as in or . The origin is often labelled with the capital letter \"O\". In analytic geometry, unknown or generic coordinates are often denoted by the letters (\"x\", \"y\") in the plane, and (\"x\", \"y\", \"z\") in three-dimensional space. This custom comes from a convention of algebra, which uses letters near the end of the alphabet for unknown values (such as were the coordinates of points in many geometric problems), and letters near the beginning for given quantities.\n\nThese conventional names are often used in other domains, such as physics and engineering, although other letters may be used. For example, in a graph showing how a pressure varies with time, the graph coordinates may be denoted \"t\" and \"p\". Each axis is usually named after the coordinate which is measured along it; so one says the \"x-axis\", the \"y-axis\", the \"t-axis\", etc.\n\nAnother common convention for coordinate naming is to use subscripts, as (\"x\", \"x\", ..., \"x\") for the \"n\" coordinates in an \"n\"-dimensional space, especially when \"n\" is greater than 3 or unspecified. Some authors prefer the numbering (\"x\", \"x\", ..., \"x\"). These notations are especially advantageous in computer programming: by storing the coordinates of a point as an array, instead of a record, the subscript can serve to index the coordinates.\n\nIn mathematical illustrations of two-dimensional Cartesian systems, the first coordinate (traditionally called the abscissa) is measured along a horizontal axis, oriented from left to right. The second coordinate (the ordinate) is then measured along a vertical axis, usually oriented from bottom to top.\n\nHowever, computer graphics and image processing often use a coordinate system with the \"y\"-axis oriented downwards on the computer display. This convention developed in the 1960s (or earlier) from the way that images were originally stored in display buffers.\n\nFor three-dimensional systems, a convention is to portray the \"xy\"-plane horizontally, with the \"z\"-axis added to represent height (positive up). Furthermore, there is a convention to orient the \"x\"-axis toward the viewer, biased either to the right or left. If a diagram (3D projection or 2D perspective drawing) shows the \"x\"- and \"y\"-axis horizontally and vertically, respectively, then the \"z\"-axis should be shown pointing \"out of the page\" towards the viewer or camera. In such a 2D diagram of a 3D coordinate system, the \"z\"-axis would appear as a line or ray pointing down and to the left or down and to the right, depending on the presumed viewer or camera perspective. In any diagram or display, the orientation of the three axes, as a whole, is arbitrary. However, the orientation of the axes relative to each other should always comply with the right-hand rule, unless specifically stated otherwise. All laws of physics and math assume this right-handedness, which ensures consistency.\n\nFor 3D diagrams, the names \"abscissa\" and \"ordinate\" are rarely used for \"x\" and \"y\", respectively. When they are, the \"z\"-coordinate is sometimes called the applicate. The words \"abscissa\", \"ordinate\" and \"applicate\" are sometimes used to refer to coordinate axes rather than the coordinate values.\n\nThe axes of a two-dimensional Cartesian system divide the plane into four infinite regions, called quadrants, each bounded by two half-axes. These are often numbered from 1st to 4th and denoted by Roman numerals: I (where the signs of the two coordinates are +,+), II (−,+), III (−,−), and IV (+,−). When the axes are drawn according to the mathematical custom, the numbering goes counter-clockwise starting from the upper right (\"north-east\") quadrant.\n\nSimilarly, a three-dimensional Cartesian system defines a division of space into eight regions or octants, according to the signs of the coordinates of the points. The convention used for naming a specific octant is to list its signs, e.g. or . The generalization of the quadrant and octant to an arbitrary number of dimensions is the orthant, and a similar naming system applies.\n\nThe Euclidean distance between two points of the plane with Cartesian coordinates formula_4 and formula_5 is\n\nThis is the Cartesian version of Pythagoras's theorem. In three-dimensional space, the distance between points formula_7 and formula_8 is\n\nwhich can be obtained by two consecutive applications of Pythagoras' theorem.\n\nThe Euclidean transformations or Euclidean motions are the (bijective) mappings of points of the Euclidean plane to themselves which preserve distances between points. There are four types of these mappings (also called isometries): translations, rotations, reflections and glide reflections.\n\nTranslating a set of points of the plane, preserving the distances and directions between them, is equivalent to adding a fixed pair of numbers to the Cartesian coordinates of every point in the set. That is, if the original coordinates of a point are , after the translation they will be\n\nTo rotate a figure counterclockwise around the origin by some angle formula_11 is equivalent to replacing every point with coordinates (\"x\",\"y\") by the point with coordinates (\"x<nowiki>'</nowiki>\",\"y<nowiki>'</nowiki>\"), where\n\nThus:\n\nformula_14\n\nIf are the Cartesian coordinates of a point, then are the coordinates of its reflection across the second coordinate axis (the Y-axis), as if that line were a mirror. Likewise, are the coordinates of its reflection across the first coordinate axis (the X-axis). In more generality, reflection across a line through the origin making an angle formula_11 with the x-axis, is equivalent to replacing every point with coordinates by the point with coordinates , where\n\nThus:\nformula_18\n\nA glide reflection is the composition of a reflection across a line followed by a translation in the direction of that line. It can be seen that the order of these operations does not matter (the translation can come first, followed by the reflection).\n\nThese Euclidean transformations of the plane can all be described in a uniform way by using matrices. The result formula_19 of applying a Euclidean transformation to a point formula_20 is given by the formula\n\nwhere \"A\" is a 2×2 orthogonal matrix and is an arbitrary ordered pair of numbers; that is,\n\nwhere\n\nTo be \"orthogonal\", the matrix \"A\" must have orthogonal rows with same Euclidean length of one, that is,\n\nand\n\nThis is equivalent to saying that \"A\" times its transpose must be the identity matrix. If these conditions do not hold, the formula describes a more general affine transformation of the plane provided that the determinant of \"A\" is not zero.\n\nThe formula defines a translation if and only if \"A\" is the identity matrix. The transformation is a rotation around some point if and only if \"A\" is a rotation matrix, meaning that\n\nA reflection or glide reflection is obtained when,\n\nAssuming that translation is not used transformations can be combined by simply multiplying the associated transformation matrices.\n\nAnother way to represent coordinate transformations in Cartesian coordinates is through affine transformations. In affine transformations an extra dimension is added and all points are given a value of 1 for this extra dimension. The advantage of doing this is that point translations can be specified in the final column of matrix \"A\". In this way, all of the euclidean transformations become transactable as matrix point multiplications. The affine transformation is given by:\n\nUsing affine transformations multiple different euclidean transformations including translation can be combined by simply multiplying the corresponding matrices.\n\nAn example of an affine transformation which is not a Euclidean motion is given by scaling. To make a figure larger or smaller is equivalent to multiplying the Cartesian coordinates of every point by the same positive number \"m\". If are the coordinates of a point on the original figure, the corresponding point on the scaled figure has coordinates\n\nIf \"m\" is greater than 1, the figure becomes larger; if \"m\" is between 0 and 1, it becomes smaller.\n\nA shearing transformation will push the top of a square sideways to form a parallelogram. Horizontal shearing is defined by:\n\nShearing can also be applied vertically:\n\nFixing or choosing the \"x\"-axis determines the \"y\"-axis up to direction. Namely, the \"y\"-axis is necessarily the perpendicular to the \"x\"-axis through the point marked 0 on the \"x\"-axis. But there is a choice of which of the two half lines on the perpendicular to designate as positive and which as negative. Each of these two choices determines a different orientation (also called \"handedness\") of the Cartesian plane.\n\nThe usual way of orienting the axes, with the positive \"x\"-axis pointing right and the positive \"y\"-axis pointing up (and the \"x\"-axis being the \"first\" and the \"y\"-axis the \"second\" axis) is considered the \"positive\" or \"standard\" orientation, also called the \"right-handed\" orientation.\n\nA commonly used mnemonic for defining the positive orientation is the \"right hand rule\". Placing a somewhat closed right hand on the plane with the thumb pointing up, the fingers point from the \"x\"-axis to the \"y\"-axis, in a positively oriented coordinate system.\n\nThe other way of orienting the axes is following the \"left hand rule\", placing the left hand on the plane with the thumb pointing up.\n\nWhen pointing the thumb away from the origin along an axis towards positive, the curvature of the fingers indicates a positive rotation along that axis.\n\nRegardless of the rule used to orient the axes, rotating the coordinate system will preserve the orientation. Switching any two axes will reverse the orientation, but switching both will leave the orientation unchanged.\n\nOnce the \"x\"- and \"y\"-axes are specified, they determine the line along which the \"z\"-axis should lie, but there are two possible directions on this line. The two possible coordinate systems which result are called 'right-handed' and 'left-handed'. The standard orientation, where the \"xy\"-plane is horizontal and the \"z\"-axis points up (and the \"x\"- and the \"y\"-axis form a positively oriented two-dimensional coordinate system in the \"xy\"-plane if observed from \"above\" the \"xy\"-plane) is called right-handed or positive.\n\nThe name derives from the right-hand rule. If the index finger of the right hand is pointed forward, the middle finger bent inward at a right angle to it, and the thumb placed at a right angle to both, the three fingers indicate the relative directions of the \"x\"-, \"y\"-, and \"z\"-axes in a \"right-handed\" system. The thumb indicates the \"x\"-axis, the index finger the \"y\"-axis and the middle finger the \"z\"-axis. Conversely, if the same is done with the left hand, a left-handed system results.\n\nFigure 7 depicts a left and a right-handed coordinate system. Because a three-dimensional object is represented on the two-dimensional screen, distortion and ambiguity result. The axis pointing downward (and to the right) is also meant to point \"towards\" the observer, whereas the \"middle\"-axis is meant to point \"away\" from the observer. The red circle is \"parallel\" to the horizontal \"xy\"-plane and indicates rotation from the \"x\"-axis to the \"y\"-axis (in both cases). Hence the red arrow passes \"in front of\" the \"z\"-axis.\n\nFigure 8 is another attempt at depicting a right-handed coordinate system. Again, there is an ambiguity caused by projecting the three-dimensional coordinate system into the plane. Many observers see Figure 8 as \"flipping in and out\" between a convex cube and a concave \"corner\". This corresponds to the two possible orientations of the coordinate system. Seeing the figure as convex gives a left-handed coordinate system. Thus the \"correct\" way to view Figure 8 is to imagine the \"x\"-axis as pointing \"towards\" the observer and thus seeing a concave corner.\nA point in space in a Cartesian coordinate system may also be represented by a position vector, which can be thought of as an arrow pointing from the origin of the coordinate system to the point. If the coordinates represent spatial positions (displacements), it is common to represent the vector from the origin to the point of interest as formula_33. In two dimensions, the vector from the origin to the point with Cartesian coordinates (x, y) can be written as:\n\nwhere formula_35, and formula_36 are unit vectors in the direction of the \"x\"-axis and \"y\"-axis respectively, generally referred to as the \"standard basis\" (in some application areas these may also be referred to as versors). Similarly, in three dimensions, the vector from the origin to the point with Cartesian coordinates formula_37 can be written as:\n\nwhere formula_39 is the unit vector in the direction of the z-axis.\n\nThere is no \"natural\" interpretation of multiplying vectors to obtain another vector that works in all dimensions, however there is a way to use complex numbers to provide such a multiplication. In a two dimensional cartesian plane, identify the point with coordinates with the complex number . Here, i is the imaginary unit and is identified with the point with coordinates , so it is not the unit vector in the direction of the \"x\"-axis. Since the complex numbers can be multiplied giving another complex number, this identification provides a means to \"multiply\" vectors. In a three dimensional cartesian space a similar identification can be made with a subset of the quaternions.\n\nCartesian coordinates are an abstraction that have a multitude of possible applications in the real world. However, three constructive steps are involved in superimposing coordinates on a problem application. 1) Units of distance must be decided defining the spatial size represented by the numbers used as coordinates. 2) An origin must be assigned to a specific spatial location or landmark, and 3) the orientation of the axes must be defined using available directional cues for (n−1) of the n axes.\n\nConsider as an example superimposing 3D Cartesian coordinates over all points on the Earth (i.e. geospatial 3D). What units make sense? Kilometers are a good choice, since the original definition of the kilometer was geospatial...10,000 km equalling the surface distance from the Equator to the North Pole. Where to place the origin? Based on symmetry, the gravitational center of the Earth suggests a natural landmark (which can be sensed via satellite orbits). Finally, how to orient X-, Y- and Z-axis directions? The axis of Earth's spin provides a natural direction strongly associated with \"up vs. down\", so positive Z can adopt the direction from geocenter to North Pole. A location on the Equator is needed to define the X-axis, and the prime meridian stands out as a reference direction, so the X-axis takes the direction from geocenter out to [ 0 degrees longitude, 0 degrees latitude ]. Note that with 3 dimensions, and two perpendicular axes directions pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right hand rule, the Y-axis must point out from the geocenter to [ 90 degrees longitude, 0 degrees latitude ]. So what are the geocentric coordinates of the Empire State Building in New York City? Using [ longitude = −73.985656, latitude = 40.748433 ], Earth radius = 40,000/2π, and transforming from spherical --> Cartesian coordinates, you can estimate the geocentric coordinates of the Empire State Building, [ \"x\", \"y\", \"z\" ] = [ 1330.53 km, –4635.75 km, 4155.46 km ]. GPS navigation relies on such geocentric coordinates.\n\nIn engineering projects, agreement on the definition of coordinates is a crucial foundation. One cannot assume that coordinates come predefined for a novel application, so knowledge of how to erect a coordinate system where there is none is essential to applying René Descartes' ingenious thinking.\n\nWhile spatial apps employ identical units along all axes, in business and scientific apps, each axis may have different units of measurement associated with it (such as kilograms, seconds, pounds, etc.). Although four- and higher-dimensional spaces are difficult to visualize, the algebra of Cartesian coordinates can be extended relatively easily to four or more variables, so that certain calculations involving many variables can be done. (This sort of algebraic extension is what is used to define the geometry of higher-dimensional spaces.) Conversely, it is often helpful to use the geometry of Cartesian coordinates in two or three dimensions to visualize algebraic relationships between two or three of many non-spatial variables.\n\nThe graph of a function or relation is the set of all points satisfying that function or relation. For a function of one variable, \"f\", the set of all points , where is the graph of the function \"f\". For a function \"g\" of two variables, the set of all points , where is the graph of the function \"g\". A sketch of the graph of such a function or relation would consist of all the salient parts of the function or relation which would include its relative extrema, its concavity and points of inflection, any points of discontinuity and its end behavior. All of these terms are more fully defined in calculus. Such graphs are useful in calculus to understand the nature and behavior of a function or relation.\n\n\n\n\n", "id": "7706", "title": "Cartesian coordinate system"}
{"url": "https://en.wikipedia.org/wiki?curid=7708", "text": "Commandant of the Marine Corps\n\nThe Commandant of the Marine Corps (CMC) is normally the highest-ranking officer in the United States Marine Corps and is a member of the Joint Chiefs of Staff. The CMC reports directly to the United States Secretary of the Navy and is responsible for ensuring the organization, policy, plans, and programs for the Marine Corps as well as advising the President, the Secretary of Defense, the National Security Council, the Homeland Security Council, and the Secretary of the Navy on matters involving the Marine Corps. Under the authority of the Secretary of the Navy, the CMC designates Marine personnel and resources to the commanders of Unified Combatant Commands. The Commandant performs all other functions prescribed in Section 5043 in Title 10 of the United States Code or delegates those duties and responsibilities to other officers in his administration in his name. As with the other joint chiefs, the Commandant is an administrative position and has no operational command authority over United States Marine Corps forces.\n\nThe Commandant is nominated by the President for a four-year term of office and must be confirmed by the Senate. By statute, the Commandant is appointed as a four-star general while serving in office. \"The Commandant is directly responsible to the Secretary of the Navy for the total performance of the Marine Corps. This includes the administration, discipline, internal organization, training, requirements, efficiency, and readiness of the service. The Commandant is also responsible for the operation of the Marine Corps material support system.\" Since 1801, the official residence of the Commandant has been located in the Marine Barracks in Washington, D.C. and his main offices are in Arlington County, Virginia.\n\nThe responsibilities of the Commandant are outlined in Title 10, Section 5043 the United States Code and the position is \"subject to the authority, direction, and control of the Secretary of the Navy.\" As stated in the U.S. Code, the Commandant \"shall preside over the Headquarters, Marine Corps, transmit the plans and recommendations of the Headquarters, Marine Corps, to the Secretary and advise the Secretary with regard to such plans and recommendations, after approval of the plans or recommendations of the Headquarters, Marine Corps, by the Secretary, act as the agent of the Secretary in carrying them into effect, exercise supervision, consistent with the authority assigned to commanders of unified or specified combatant commands under chapter 6 of this title, over such of the members and organizations of the Marine Corps and the Navy as the Secretary determines, perform the duties prescribed for him by section 171 of this title and other provisions of law and perform such other military duties, not otherwise assigned by law, as are assigned to him by the President, the Secretary of Defense, or the Secretary of the Navy.\"\n\nThirty-seven men have served as the Commandant of the Marine Corps. The first Commandant was Samuel Nicholas, who took office as a captain, though there was no office titled \"Commandant\" at the time, and the Second Continental Congress had authorized that the senior-most Marine could take a rank up to Colonel. The longest-serving was Archibald Henderson, sometimes referred to as the \"\"Grand old man of the Marine Corps\"\" due to his thirty-nine-year tenure. In the 236-year history of the United States Marine Corps, only one Commandant has ever been fired from the job: Anthony Gale, as a result of a court-martial in 1820.\n\n\n", "id": "7708", "title": "Commandant of the Marine Corps"}
{"url": "https://en.wikipedia.org/wiki?curid=7710", "text": "California Department of Transportation\n\nThe California Department of Transportation (Caltrans) is an executive department within the U.S. state of California.\n\nCaltrans manages the state highway system (which includes the California Freeway and Expressway System) and is actively involved with public transportation systems throughout the state. It supports Amtrak California and the \"Capitol Corridor\".\n\nThe department is part of the state cabinet-level California State Transportation Agency (CalSTA).\n\nLike the majority of state government agencies, Caltrans is headquartered in Sacramento.\n\nIn 2015, Caltrans released a new mission statement: \"Provide a safe, sustainable, integrated and efficient transportation system to enhance California’s economy and livability.\" \n\nThe earliest predecessor of Caltrans was the Bureau of Highways, which was created by the California Legislature and signed into law by Governor James Budd in 1895. This agency consisted of three commissioners who were charged with analyzing the state road system and making recommendations. At the time, there was no state highway system, since roads were purely a local responsibility. California's roads consisted of crude dirt roads maintained by county governments, as well as some paved roads within city boundaries, and this \"ad hoc\" system was no longer adequate for the needs of the state's rapidly growing population. After the commissioners submitted their report to the governor on November 25, 1896, the legislature replaced the Bureau with the Department of Highways.\n\nDue to the state's weak fiscal condition and corrupt politics, little progress was made until 1907, when the legislature replaced the Department of Highways with the Department of Engineering, within which there was a Division of Highways. Voters approved an $18 million bond issue for the construction of a state highway system in 1910, and the first Highway Commission was convened in 1911. On August 7, 1912, the department broke ground on its first construction project, the section of El Camino Real between South San Francisco and Burlingame (now part of California State Route 82). The year 1912 also saw the founding of the Transportation Laboratory and the creation of seven administrative divisions (the predecessors of the 12 district offices that exist today).\n\nIn 1913, the legislature started requiring vehicle registration and allocated the resulting funds to support regular highway maintenance, which began the next year.\n\nIn 1921, the legislature turned the Department of Engineering into the Department of Public Works.\n\nThe history of Caltrans and its predecessor agencies during the 20th century was marked by many firsts. It was one of the first agencies in the United States to paint centerlines on highways statewide (thanks to June McCarroll); the first to build a freeway west of the Mississippi River (the Pasadena Freeway); the first to build a four-level stack interchange; the first to develop and deploy non-reflective raised pavement markers, better known as Botts' dots; and one of the first to implement dedicated freeway-to-freeway connector ramps for high-occupancy vehicle lanes.\n\nIn late 1972, the legislature approved a reorganization (suggested in a study initiated by then-Governor Ronald Reagan) in which the Department of Public Works was merged with the Department of Aeronautics to become the modern Department of Transportation.\nFor administrative purposes, Caltrans divides the State of California into 12 districts, supervised by district offices. Most districts cover multiple counties; District 12 (Orange County) is the only district with one county. The largest districts by population are District 4 (San Francisco Bay Area) and District 7 (Los Angeles and Ventura counties). Like most state agencies, Caltrans maintains its headquarters in Sacramento, which is covered by District 3.\n\n", "id": "7710", "title": "California Department of Transportation"}
{"url": "https://en.wikipedia.org/wiki?curid=7712", "text": "Continuation War\n\nThe Continuation War (; ; 25 June 1941 – 19 September 1944) consisted of hostilities between Finland and the Soviet Union from 1941 to 1944. The Continuation War began shortly after the end of the Winter War, which also was fought between Finland and the Soviet Union. In the Soviet Union, the war was considered part of the Great Patriotic War. Germany regarded its operations in the region as part of its overall war efforts on the Eastern Front, and it provided Finland with critical material support and military cooperation.\n\nActs of war between the Soviet Union and Finland recommenced on 22 June 1941, the day Germany launched its invasion of the Soviet Union, with covert Finnish operations. Open warfare began with a Soviet air offensive on 25 June. Subsequent Finnish operations undid its post-Winter War concessions to the Soviet Union on the Karelian Isthmus and Ladoga Karelia, and captured East Karelia by September 1941. On the Karelian Isthmus, the Finns halted their offensive 30 km from Leningrad, at the pre-World War II border between the Soviet Union and Finland. Finnish forces did not participate in the siege of Leningrad directly, holding their pre-World War II territory on the Karelian Isthmus for two and a half years instead. In 1944, Soviet air forces conducted air raids on Helsinki and other major Finnish cities. Eventually, in mid-1944, the Soviet strategic offensive drove the Finns from most of the territories they had gained during the war, but the Finnish Army later brought the offensive to a standstill in July 1944. A ceasefire ended hostilities on 5 September and was followed by the Moscow Armistice on 19 September. The 1947 Paris peace treaty concluded the war formally. Finland ceded Petsamo Province to the Soviets, leased Porkkala peninsula to them, and paid reparations, while retaining its independence.\n\nOn 23 August 1939, the Soviet Union and Germany signed the Molotov–Ribbentrop Pact whereby the parties divided the independent countries of Finland, Estonia, Latvia, Lithuania, Poland, and Romania into spheres of interest, with Finland falling to the Soviet sphere of interest. Shortly afterward, Germany invaded Poland and as a result the United Kingdom and France declared war against Germany. The Soviet Union invaded eastern Poland on 17 September. Next, Moscow demanded that the Baltic states allow the establishment of Soviet military bases and the stationing of troops on their soil. The Baltic governments accepted these ultimatums, signing corresponding agreements in September and October 1939.\n\nIn October 1939, the Soviet Union attempted to negotiate with Finland for the transfer of Finnish territories on the Karelian Isthmus and the islands of the Gulf of Finland to the Soviet Union and for the establishment of a Soviet military base near the Finnish capital Helsinki. The Finnish government refused, and the Red Army attacked Finland on 30 November 1939. Condemnation of the Soviets by the League of Nations and by countries all over the world had no effect on Soviet policy. International help for Finland was planned, but very little actual help materialized, except from Sweden. The Moscow Peace Treaty, which was signed on 12 March 1940, ended the Winter War. By the terms of the treaty, Finland lost one eleventh of its national territory and about 13% of its economic capacity. However, Finland had avoided having the Soviet Union annex the whole country.\n\nFinland's foreign policy had been based on multilateral guarantees for support from the League of Nations and Nordic countries and was considered a failure. Finnish public opinion favored the reconquest of Finnish Karelia. Finland's government declared the country's defense to be its first priority, and military expenditures rose to nearly half of government spending. Finland purchased and received donations of war material during and immediately after the Winter War. On Finland's southern frontier the Soviet Union had acquired a military base in Hanko near the capital Helsinki, which employed over 30,000 Soviet military personnel.\n\nFinland also had to resettle some 420,000 evacuees from the lost territories. To ensure the supply of food, it was necessary to clear new land for the evacuees to cultivate. This was facilitated by the Rapid Settlement Act. The Finnish leadership wanted to preserve the spirit of unanimity that was commonly felt throughout the country during the Winter War. The divisive White Guard tradition of the Civil War 16 May victory day celebration was therefore discontinued. Relations between Finland and the Soviet Union remained strained despite the signing of the one-sided peace treaty, and there were disputes regarding the implementation of the conditions of the treaty. Finland sought security against further territorial depredations by the Soviet Union and proposed mutual defence agreements with Norway and Sweden, but these initiatives were quashed by Moscow.\n\nAfter the Winter War, Germany was not popular in Finland as it was considered an ally of the Soviet Union. However, the Finnish government began to restore diplomatic relations with Germany. Finland continued its Western-oriented policy and negotiated a war trade agreement with the United Kingdom, but the agreement was renounced after the German invasion of Denmark and Norway on 9 April, when Britain cut all trade and traffic communications with Scandinavia. With the fall of France, a policy of Western orientation was no longer considered an option in Finnish foreign policy. On 15 and 16 June, the Soviet Union occupied the Baltic states without resistance. Soviet puppet regimes were installed; and within two months Estonia, Latvia, and Lithuania were incorporated as Soviet republics within the Soviet Union. By mid-1940, the two remaining northern democracies, Finland and Sweden, were encircled by the hostile states of Germany and the Soviet Union.\n\nOn 23 June, a short time after the Soviet occupation of the Baltic states began, Soviet Foreign Minister Vyacheslav Molotov contacted the Finns and demanded a mining licence for the Soviet Union at the nickel mines in Petsamo or alternately the establishing of a joint Soviet-Finnish company to operate there. The licence to mine the deposit had earlier been granted to a British-Canadian company, and the proposition was rejected. The next month, the Soviets demanded that Finland destroy the fortifications built in the Åland islands and give the Soviets the right to use Finnish railways to transport Soviet troops to the newly acquired Soviet base at Hanko. The Finns very reluctantly agreed to these demands. On 24 July, Molotov accused the Finnish government of persecuting the so-called Society for Peace and Friendship between Finland and USSR, a pro-communist group; and soon afterwards, he publicly supported this group. The society organized demonstrations, some of which turned into riots.\n\nOn 31 July 1940, the German leader Adolf Hitler gave the order to start planning an assault on the Soviet Union. This meant that Germany had to reassess its positions regarding both Finland and Romania. Until then, Germany had rejected Finnish appeals to purchase arms, but in August the Germans allowed the secret sale of weapons to Finland. German and Finnish military authorities made an agreement on 12 September, and an official exchange of diplomatic notes was sent on 22 September. At the same time, German troops were allowed to transit through Sweden and Finland. In practice, this meant Germany had redrawn the border of German and Soviet spheres of influence.\n\nDue to the changed situation, Molotov made a visit to Berlin on 12–13 November. He wanted Germany to withdraw its troops from Finland and stop enabling Finnish anti-Soviet sentiments. He also reminded the Germans of the 1939 Soviet–German non-aggression pact. Hitler asked how the Soviet Union planned to settle the \"Finnish question\". Molotov answered that it would happen in the same manner as in Bessarabia and the Baltic states. Hitler rejected this. The following December, the Soviet Union, Germany, and the United Kingdom all voiced opinions concerning suitable Finnish presidential candidates. Risto Ryti was the only candidate none of these three powers objected to. He was elected on 19 December.\n\nIn January 1941, the Soviet Union demanded to take control of the Petsamo mining area. Finland rejected this, as it by then had a rebuilt defense force and was encouraged by Germany to reject the Soviet demand. On 18 December 1940, Hitler had officially approved Operation Barbarossa, the German invasion of the Soviet Union. He expected both Finland and Romania to join the German campaign. Two days earlier, Finnish Major General Paavo Talvela had met German Colonel General Franz Halder and, a couple days later, Reichsmarschall Hermann Göring, in Berlin. This was the first time the Germans advised the Finns, in carefully couched diplomatic terms, that they were preparing for a war with the Soviet Union. Outlines of the actual plan were revealed in January 1941 and regular contacts between Finnish and German military leaders started from February.\n\nIn late spring 1941, the Soviet Union made a number of goodwill gestures in order to prevent Finland from completely falling under German influence. Soviet ambassador Ivan Zotov was replaced with the more flexible Pavel Orlov. Furthermore, the Soviet government announced that it no longer opposed a rapprochement between Finland and Sweden. However, these conciliatory measures did not have any effect on Finnish policy.\n\nOn 20 May 1941, the Germans invited some Finnish officers to discuss the coordination of Operation Barbarossa. The participants met on 25–28 May in Salzburg and Berlin, and continued their meeting in Helsinki from 3 to 6 June. They agreed upon the arrival of German troops, Finnish mobilization, and a general division of operations. They also agreed that the Finnish Army would start mobilization on 15 June, but the Germans did not reveal the final date for the assault. The Finnish decisions were made by a small group of political and military leaders; the rest of the government was largely kept in the dark. The government was not informed until 9 June that the country would start mobilization of reservists due to tensions between Germany and the Soviet Union.\n\nThe Germans took responsibility for the stretch of the front in northern Finland consisting of Finnish Lapland. The Finnish army was now much stronger than it had been during the Winter War, boasting 475,000 men. The artillery, too, was relatively strong. However, there was only one tank battalion and a lack of motorized transportation.\n\nAt the beginning of the war, the Soviet Union had eighteen divisions in the region, against fifteen Finnish and four German divisions. The Finns enjoyed air supremacy. Furthermore, the Soviet Union needed its best units and most up-to-date materiel on its western front.\n\nGerman forces invaded the Soviet Union on 22 June, but not from Finland. However, German minelayers hiding in the Archipelago Sea laid two large minefields across the Gulf of Finland in the late hours of 21 June. Later the same night, German bombers flew along the Gulf of Finland to Leningrad and mined the harbour and the river Neva. On the return trip, these bombers landed for refueling on an airfield in Utti. In the early hours of 22 June, Finnish forces launched Operation Kilpapurjehdus, which aimed to man the demilitarized Åland Islands. An international treaty on the status of the islands called for Finland to defend them in case of the threat of an attack. However, the operation was coordinated with the Nazi invasion, and the personnel of the Soviet consulate there were arrested. According to Finnish historian Mauno Jokipii, Finland knew that it had violated international norms.\n\nOn 21 June, Finnish units began to concentrate at the Finnish-Soviet border, where they were arranged into defensive formations. Finland mobilised 16 infantry divisions, one cavalry brigade, and two jäger brigades, which were all standard infantry brigades, except for an armoured battalion in the 1st Jäger Brigade. Separate battalions were mostly formed from border guard units and were used mainly for reconnaissance. Soviet military plans estimated that Finland would be able to mobilise only ten infantry divisions, as it had done in the Winter War, but they failed to take into account the materiel Finland had purchased between the wars and its training of all available men. Two German mountain divisions were stationed at Petsamo and two infantry divisions at Salla. On the morning of 22 June, the German Mountain Corps Norway began its advance from northern Norway to Petsamo. Finland did not allow direct German attacks from its soil into the Soviet Union. On the same day, another German infantry division was moved from Oslo to face Ladoga Karelia.\n\nOn the Soviet side, the Karelian Isthmus was covered by the 23rd Army. Ladoga Karelia was defended by the 7th Army. In the Murmansk–Salla region, there was the 14th Army with the 42nd Corps. The Red Army also had around 40 battalions of separate regiments and fortification units present. Leningrad was garrisoned by three infantry divisions and one mechanized corps. As the initial devastating German strike against the Soviet Air Force had not affected air units located near Finland, the Soviets deployed 700 planes as well as some aircraft from the Navy against 300 Finnish planes.\n\nOn the morning of 25 June, the Soviet Union launched an air offensive of 460 fighters and bombers targeting 19 airfields in Finland. However, inaccurate intelligence and poor bombing accuracy caused several raids to hit Finnish cities or municipalities. There was considerable destruction in the cities. Twenty-three Soviet bombers were lost, while the Finns lost no aircraft.\n\nThe Soviet Union stated that the air attack was directed against German targets, especially airfields, in Finland. At the same time, Soviet artillery stationed at the Hanko base began to shell Finnish targets, and a minor Soviet infantry attack was launched over the Finnish side of the border in Parikkala.\n\nThe bombings offered the Finnish government a ground for claiming that the country had become the target of a new assault, and the Finnish parliament approved the \"defensive war\" as a \"fait accompli\". According to historian David Kirby, the message was intended more for public opinion in Finland than abroad, where it was seen that the country was in the German camp.\n\nIn July the Finnish military began its planned offensive. According to Finnish historian Olli Vehviläinen, in 1941 most Finns thought that the scope of the new offensive was only to regain what had been wrongly taken in the Winter War.\n\nThe Soviet Union struggled to contain the German invasion, and soon the Soviet High Command had to call all available units stationed along the Finnish border to the rapidly deteriorating front line. According to Finnish historian Ohto Manninen, because of this, the initial air offensive against Finland could not be followed up with a supporting land offensive as allegedly planned. Moreover, the 237th Infantry Division and, excluding the 198th Motorized Division, the Soviet 10th Mechanized Corps were withdrawn from Ladoga Karelia, thus stripping most of the reserves from the remaining defending Soviet units.\n\nThe Finnish plans for the offensive in Ladoga Karelia were completed on 28 June. The offensive was launched on 10 July, and by 16 July, the Finns reached the shore of Lake Ladoga and cut the defending Soviet army in two, hindering the Soviets' defense of the area. Finnish headquarters halted the offensive in Ladoga Karelia on 25 July after reconquering the area of Ladoga Karelia lost to the Soviet Union in 1940 and after advancing as far as Vitele. The Finnish offensive then shifted to other sections of the front.\n\nThe Finnish II Corps (II AK) started its offensive in the region of the Karelian Isthmus on 31 July. Finnish troops reached the shores of Lake Ladoga on 9 August, surrounding most of three defending Soviet divisions on the northwestern coast of the lake; the Soviet divisions were evacuated across the lake. On 22 August, the Finnish IV AK Corps started its offensive from the 1940 border between the Gulf of Finland and the II AK, and advanced towards Viborg. By 23 August, the Finnish II Corps had reached the Vuoksi waterway from the east and continued to surround the Soviet forces defending Viborg. The Soviet withdrawal order came too late, and the Soviet divisions lost much of their equipment, although a sizable portion of their manpower was later evacuated via the Koivisto islands. The badly mauled defending Soviet army was unable to halt the Finnish offensive, and by 2 September the Finns had reached the 1939 border along its whole length. On 31 August, Finnish headquarters ordered the 2nd and 4th Army Corps, which had advanced the furthest, to halt their offensive after reaching a line just past the former border that ran from the mouth of the River Sestra via Retukylä, Aleksandrovka, and the eastern edge of the village of S. Beloostrov () to Ohta and form for defense.\n\nAccording to Soviet sources, the Finns advanced and took the settlement of Novyi Beloostrov with its train station on 4 September, but a Soviet counter-attack threw them out the next day. The war diary of the Finnish 12th Division facing this settlement notes that it was quiet at the time, while the neighboring 18th Division had orders on the morning of 4 September 1941 to form a line of defense north of N. Beloostrov, and the Finnish 6th Regiment responsible for the Finnish 18th Division's front line facing N. Beloostrov formed for defense along the small stream (Serebryanyy ruchey) north of N. Belootrov on 4 September 1941. According to Finnish sources, Soviet forces advanced north from N. Beloostrov and attacked the Finnish positions along the small stream on the morning of 5 September 1941, but the Finns managed to repel them. Staryi Beloostrov (Valkeasaari) was taken by the Finns on September 4 and the Soviet counterattacks failed to retake the settlement. Finnish forces captured N. Beloostrov again on 10 or 11 September 1941. According to the war diary of the Finnish 12th Division, this was done to strengthen their lines. The Soviet war correspondent Luknitsky noted that this created a dangerous bulge in the Soviet defensive line. According to Russian historian Nazarenko, the Finns were not able to advance further due to stronger Soviet defensive positions. Fighting for the settlement continued until 20 September, when the Soviets managed to force the Finns out. After that the front stabilized.\n\nThe Finnish offensive in East Karelia started in early July in the northern section of the front. In early September, the attack in the northern section reached Rukajärvi (Ругозеро, Rugozero) village and Finnish headquarters halted the offensive there. On August 27, Finnish headquarters ordered the offensive in the south to reach the Svir River. Finnish troops cut the Kirov railroad on 7 September, crossed the Svir on 15 September, and then halted the offensive. Advance troops reached the shores of Lake Onega on 24 September. The town of Petrozavodsk was captured on 1 October after the Soviets withdrew to avoid encirclement. On 6 November, Finnish headquarters ordered their forces to capture Karhumäki and then shift to defense. The Finnish forces captured the area of Karhumäki and Povenets, and halted the offensive in early December.\n\nRelated to the Finnish advance to the Svir, the German Army Group North advanced from the south towards the Svir River and managed to capture Tikhvin before Soviet counterattacks forced the Germans to withdraw to the Volkhov River. Soviet forces also made several attempts to force the Finns out from their bridgehead south of the Svir during October and December 1941; however, the Soviet efforts to reduce the bridgehead were blocked by the Finns. Soviet forces also attacked the German 163rd Division, which was operating under Finnish command across the Svir in October 1941, but the Soviet forces that had crossed the river were pushed back soon after.\n\nThe German objective in northern Finland was to take Murmansk and seize control of the Murman Railway. Murmansk was the only year-round ice-free port in the north, and it was a threat to the nickel mine at Petsamo. Operation Silver Fox was run by the German AOK Norwegen and had two Finnish divisions under its command. The German soldiers were from central Europe and they had difficulty moving over the roadless terrain of swamp and forest. The troops managed to advance some distance with heavy casualties, but the terrain offered good defensive positions for the Soviet resistance. The German–Finnish troops were ordered on 17 November to move to defensive operations, when attempts to reach the Murmansk Railway had failed.\n\nAlthough the Soviet Red Banner Baltic fleet started the war in a strong position, German naval mine warfare and aerial supremacy and the rapid advance by German land forces forced the Soviet Navy to evacuate its bases to Kronstadt and Leningrad. The Soviets' evacuations from Tallinn and Hanko proved to be very costly operations for them. As the Soviet Navy withdrew to the eastern end of the Gulf of Finland, it left nearly the whole Baltic Sea, as well as many of the islands, to the German and Finnish navies. Although Soviet submarines caused some threat to German traffic on the Baltic, the withdrawal of the Soviet Navy made the Baltic Sea a \"German lake\" until the second half of 1944. Although the Soviet Navy left in a hurry, the naval mines it had managed to lay before and during the evacuations caused casualties both to the Germans and the Finns, including the loss of one of the two Finnish coastal defence ships, the .\n\nGermany's main forces advanced rapidly deep into Soviet territory during the first weeks of the Operation Barbarossa campaign. The Finns believed the Germans would defeat the Soviet Union quickly. President Ryti envisioned Greater Finland, where the country and other Finnic people would live inside a \"natural defence borderline\" by incorporating the Kola Peninsula, East Karelia, and perhaps even northern Ingria. In public, the proposed frontier was introduced by the slogan \"A short border – a long peace\". Some members of the Finnish parliament, such as the Social Democrats and the Swedish People's Party, opposed the idea, arguing that maintaining the 1939 frontier would be enough. On 10 July, Finnish Commander-in-Chief C. G. E. Mannerheim gave an order of the day, the Sword Scabbard Declaration, in which he pledged to liberate Karelia. The Finnish government assured the Americans that it was unaware of the order.\n\nFinland had prepared for a short war, but in late autumn it was clear that there would be no decisive outcome in the short term. Finnish troops suffered losses during their advance; and, overall, German victory became uncertain as German troops were halted near Moscow. The Finnish economy suffered from a lack of labour, food shortages, and increased prices. The Finnish government had to demobilize part of the army so that industrial and agricultural production would not collapse. In October, Finland informed Germany that it would need of grain to manage until next year's harvest. The German authorities would have rejected the request, but Hitler himself agreed. Annual grain deliveries of equaled almost one half of the Finnish domestic crop. In November, Finland decided to join the Anti-Comintern Pact. The advance in East Karelia was halted on 6 December. The Finns had suffered 75,000 casualties, of whom 25,000 were Finnish deaths during the advance.\n\nFinland maintained good relations with the Western powers. The Finnish government stressed that Finland was fighting as a co-belligerent with Germany against the Soviet Union only to protect itself. Furthermore, Finland stressed that it was still the same democratic country as it had been in the Winter War. However, on 12 July 1941, the United Kingdom signed an agreement of joint action with the Soviet Union. Furthermore, under German pressure, Finland had to close the British legation in Helsinki. As a result, diplomatic relations between Finland and the United Kingdom were broken on 1 August. On 28 November, Britain presented Finland an ultimatum demanding that Finland cease military operations by 3 December. Unofficially, Finland informed the Western powers that Finnish troops would halt their advance in the next few days. The reply did not satisfy the United Kingdom, which declared war on Finland on 6 December. The Commonwealth member states of Canada, Australia, British Raj, and New Zealand followed suit.\n\nRelations between Finland and the United States were more complex; the American public was sympathetic to the \"brave little democracy\", and there were anti-communist feelings. At first, the United States empathised with the Finnish cause; however, the situation became problematic after Finnish troops crossed the 1939 border. Finnish and German troops were a threat to the Murmansk Railway and northern communication supply line between the Western Allies and the Soviet Union. On 25 October 1941, the United States demanded that Finland cease all hostilities against the Soviet Union and withdraw behind the 1939 border. In public, President Ryti rejected the demands, but in private he wrote to Mannerheim on 5 November, asking him to halt the offensive. Mannerheim agreed and secretly instructed General Hjalmar Siilasvuo to break off the assault against the Murmansk Railway.\n\nAlthough military operations during 1942 and 1943 were limited, the front did see some action. In early 1942, Soviet Karelian Front forces attempted to retake Medvezhyegorsk, which had been lost to the Finns in late 1941. As spring came, the Soviet forces also went on the offensive on the Svir front as well as in Kiestinki region. All Soviet offensives started promisingly, but due either to the Soviets overextending their lines or stubborn defensive resistance, the Soviet offensives were stopped and repulsed. After Finnish and German counterattacks in Kiestinki, the eventual front lines had moved very little. In September 1942, the Soviets tried again at Kriv near Medvezhyegorsk, but despite five days of fighting, the Soviets managed to push the Finnish lines back only on a roughly -long stretch of the front.\n\nUnconventional warfare was fought in both the Finnish and Soviet wilderness. Finnish long-range reconnaissance patrols, organized both by Finnish HQ—4th Separate Battalion (Er.P 4)—and by local units, patrolled beyond Soviet lines. In summer 1942, the Soviet Union formed the 1st Partisan Brigade. The unit was only 'partisan' in name, as it was essentially more than 600 men and women on long-range patrol. The 1st Partisan Brigade was able to infiltrate beyond Finnish patrol lines, but was found and largely destroyed.\n\nOn the naval front, the Soviet Baltic Fleet still operated from the besieged city of Leningrad. In early 1942, Soviet forces recaptured the island of Gogland, but lost both Gogland and Bolshoy Tyuters to the Finns later in spring 1942. During the winter of 1941/1942, the Soviet Baltic Fleet made the decision to use the large submarine fleet to carry the fight to the enemy. Though initial submarine operations in the summer of 1942 were successful, the German Kriegsmarine and Finnish Navy soon stepped up their anti-submarine efforts, making Soviet submarine operations later in 1942 very costly. The underwater offensive carried out by the Soviets convinced the Germans to lay anti-submarine nets as well as supporting minefields between Porkkala and Naissaar which proved to be an insurmountable obstacle for Soviet submarines.\n\nOperation Barbarossa was planned as a \"blitzkrieg\" intended to last a few weeks. In the autumn of 1941, this turned out to be wrong, and leading Finnish military officers started to doubt Germany's capability to finish the war quickly. German troops in northern Finland faced circumstances they were not properly prepared for, and failed to reach their targets, most importantly Murmansk. As the lines stabilized, Finland sent out peace feelers to the Soviet Union several times. Germany was alarmed by this, and reacted by drawing down shipments of desperately needed materials each time. The idea that Finland had to continue the war while putting its own forces in the least possible danger gained increasing support, perhaps in the hope that the \"Wehrmacht\" and the Red Army would wear each other down enough for negotiations to begin, or to at least get them out of the way of Finland's independent decisions. Nationalist elements, including the IKL, may also have continued to hope for an eventual victory by Germany.\n\nFinland's participation in the war brought major benefits to Germany. The Soviet fleet was blockaded in the Gulf of Finland, so that the Baltic was freed for the training of German submarine crews as well as for German shipping, especially for the transport of vital iron ore from northern Sweden and nickel and rare metals (needed in steel processing) from the Petsamo area. The Finnish front secured the northern flank of the German Army Group North in the Baltic states. The sixteen Finnish divisions tied down numerous Soviet troops, put pressure on Leningrad (although Mannerheim refused to attack it directly), and threatened the Murmansk railway. Additionally, Sweden was further isolated and was increasingly pressured to comply with German and Finnish wishes, though with limited success.\nDespite Finland's contributions to the German cause, the Western Allies had ambivalent feelings, torn between residual goodwill for Finland and the need to accommodate their vital ally, the Soviet Union. As a result, Britain declared war against Finland, but the United States did not. With few exceptions, there was no combat between these countries and Finland, but Finnish sailors were interned overseas. In the United States, Finland was denounced for naval attacks made on American Lend-Lease shipments, but received approval for continuing to make payments on its World War I debt throughout the inter-war period.\n\nBecause Finland joined the Anti-Comintern Pact and signed other agreements with Germany, Italy, and Japan, the Allies characterized Finland as one of the Axis Powers, although the term used in Finland is \"co-belligerence with Germany\", emphasizing the lack of a formal military alliance.\n\nForeigners to Finland from Sweden and Estonia were among international personnel who fought during the Continuation war.\n\nAs in the Winter War, Swedish volunteers were recruited. Until December 1941, these formed the Swedish Volunteer Battalion, which was tasked with guarding the Soviet naval base at Hanko. When it was evacuated by sea in December 1941, the Swedish unit was officially disbanded. During the Continuation War, the volunteers signed up for three to six months of service. In all, over 1,600 Swedish volunteers fought for Finland, although only about 60 remained by the summer of 1944. About a third of the volunteers had previously participated in the Winter War. Another significant group—about a quarter of the men—were Swedish officers on leave.\n\nFrom 1942 to 1944 there was also a \"Schutzstaffel\" (SS) battalion of volunteers on the northern Finnish front recruited from Norway, then under German occupation, and similarly, some Danes. About 3,400 Estonian volunteers took part. On other occasions, the Finns received a total of about 2,100 Soviet prisoners of war in return for those Soviet POWs they turned over to the Germans. These POWs were mainly Estonians and Karelians who were willing to join the Finnish army. These, as well as some volunteers from occupied Eastern Karelia, formed the \"Kinship Battalion\" (Finnish language: \"Heimopataljoona\"). At the end of the war, the USSR requested members of the Kinship Battalion to be handed over. Some managed to escape before or during transport, but most of them were either sent to the labor camps or executed.\n\nOn 19 July 1941, the Finns set up the military administration in occupied East Karelia. The goal of the administration was to prepare the region for eventual incorporation into Finland. In the early stage, the Finns aimed at ethnic cleansing where the Russian population would be expelled from the area once the war was over. They would be replaced with Finnic peoples such as Karelians, Finns, Estonians, Ingrians, and Vepsians. The Russian population was deemed \"non-national\". Most of the East Karelian population had been evacuated before the Finnish forces arrived. About 85,000 people—mostly the elderly, women, and children—were left behind, and less than half of them were Karelians. A significant number of civilians—almost 30% of the remaining Russians—were interned in concentration camps.\n\nThe winter of 1941–42 was an ordeal for the Finnish urban population, due to poor harvests and a shortage of agricultural laborers. However, for the Russians captured in Finnish concentration camps it was disastrous; more than 3,500 people died, mostly from starvation. This figure amounted to 13.8% of the inmates, while the corresponding figure for the free population of the occupied territories was 2.6%, and for Finland proper 1.4%. Conditions gradually improved; ethnic discrimination in wage levels and food rations was terminated the following year after the Red Cross commission from Switzerland inspected the camps, and new schools were established for the Russian-speaking population. By the end of the occupation, mortality rates dropped to the same levels as in Finland proper.\n\nSoviet partisans conducted a number of operations in Finland and in Eastern Karelia from 1941 to 1944. The major one failed when the 1st Partisan Brigade was destroyed in the beginning of August 1942 at Lake Seesjärvi. Partisans distributed propaganda newspapers, \"Pravda\" in Finnish and \"Lenin's Banner\" in Russian. One of the leaders of the partisan movement in Finland and Karelia was Yuri Andropov.\n\nFinnish sources state that partisan activity in East Karelia focused mainly on Finnish military supply and communication targets, but almost two thirds of the attacks on the Finnish side of the border targeted civilians, killing 200 and injuring 50, including children and the elderly.\n\nFinland had a small (approx. 2,300) Jewish population. They had full civil rights and fought with other Finns in the ranks of the Finnish Army. The Germans mentioned the Finnish Jews at the Wannsee Conference in Germany during January 1942, wishing to transport them to Majdanek in General Government. SS leader Heinrich Himmler mentioned the Finnish Jews during his visit in Finland in the summer of 1942. Finnish Prime Minister Jukka Rangell replied that Finland had no \"Jewish question\". However, there were differences for Jewish refugees in Finland. In November 1942, the Finns handed eight Jewish refugees over to the Gestapo. This raised protests among the Finnish Social Democrat ministers, and after this event no more refugees were handed over. Over 500 Jewish refugees were granted asylum.\n\nThe field synagogue in Eastern Karelia was one of the very few functioning synagogues on the Axis side during the war. There were even several cases of Jewish officers of Finland's army being awarded the German Iron Cross, which they declined. German soldiers were treated by Jewish medical officers who sometimes saved the soldiers' lives.\n\nThe Continuation War represents the only case of a genuinely democratic state participating in World War II on the side of the Axis powers, albeit without being a signatory of the Tripartite Pact. The United Kingdom declared war on Finland on 6 December 1941 (Finnish Independence Day), with Canada and New Zealand declaring war on Finland on 7 December and Australia and South Africa declaring war the next day. U.S. Secretary of State Cordell Hull congratulated the Finnish envoy on 3 October 1941 for the liberation of Karelia but warned Finland not to enter Soviet territory; furthermore, the United States did not declare war on Finland when it went to war alongside the Axis countries and, together with the UK, approached Soviet Premier Joseph Stalin at the Tehran Conference about acknowledging Finnish independence. However, the U.S. government seized Finnish merchant ships in American ports, and in the summer of 1944 shut down Finnish diplomatic and commercial offices in the United States as a result of President Ryti's treaty with Germany. The U.S. government later warned Finland about the consequences of continued adherence to the Axis.\n\nThe best-known British action on Finnish soil was an aircraft carrier strike on German and Finnish ships in the Finnish harbour of Petsamo on 31 July 1941. This attack achieved little except the loss of three British aircraft, but it was intended as a demonstration of British support for its Soviet ally. Later in 1941, Hurricanes of No. 151 Wing RAF, based at Murmansk, provided local air cover for Soviet troops and fighter escorts for Soviet bombers.\n\nFinnish radio intelligence is said to have participated effectively in German actions against British convoys to Murmansk. Throughout the war, German aircraft operating from airfields in northern Finland attacked British air and naval units based in Murmansk and Archangelsk.\n\nFinland began to actively seek a way out of the war after the disastrous German defeat at the Battle of Stalingrad in February 1943. Edwin Linkomies formed a new cabinet with peace as the top priority. Negotiations were conducted intermittently in 1943–44 between Finland and its representative, Juho Kusti Paasikivi, on one side, and the Western Allies and the Soviet Union on the other, but no agreement was reached. Stalin decided to force Finland to surrender; a bombing campaign on Helsinki followed. The air campaign in February 1944 included three major air attacks involving a total of over 6,000 sorties. Finnish anti-aircraft defences managed to repel the raids as only five percent of the dropped bombs hit their planned targets. Helsinki's air defense included the strategic placing of searchlights and fires as decoys outside the city to lure the Soviet bombers to drop their payloads in what were actually unpopulated areas. Major air attacks also hit Oulu and Kotka, but because of radio intelligence and effective anti-aircraft defences, the number of casualties was small.\n\nMeanwhile, the lengthy and ferocious German defence in Narva aided by the Estonians eliminated Soviet-occupied Estonia as a favorable base for Soviet amphibious invasions and air attacks against Helsinki and other Finnish cities. The tactical success of the army detachment \"Narwa\" from mid-February to April diminished the hopes of the Stavka to assault Finland and force it into capitulation from Estonia. Finland terminated the negotiations in mid-April 1944, because they considered the Soviet terms to be impossible to fulfill.\n\nOn 9 June 1944, the Soviet Union opened a major offensive against Finnish positions on the Karelian Isthmus and in the area of Lake Ladoga (it was timed to accompany D-Day). On the -wide breakthrough segment the Red Army had concentrated 3,000 guns and mortars. In some places, the concentration of artillery pieces exceeded 200 guns for every kilometer of the front (one every ). On that day, Soviet artillery fired over 80,000 rounds along the front on the Karelian Isthmus. On the second day of the offensive, the Soviet forces broke through the Finnish front lines. The Soviets penetrated the second line of defence by the sixth day. The Soviet pressure on the Karelian Isthmus forced the Finns to reinforce the area. This allowed the second Soviet offensive in Eastern Karelia to meet less resistance and to capture Petrozavodsk by 28 June 1944. According to Erickson (1991), James Gebhardt (1989), and Glantz (1998), the main objective of the Soviet offensives was to force Finland from the war.\n\nFinland especially lacked modern antitank weaponry which could stop Soviet heavy tanks, and German Foreign Minister Joachim von Ribbentrop offered these in exchange for a guarantee that Finland would not seek a separate peace again. On 26 June, President Risto Ryti gave this guarantee as a personal undertaking, which he intended to last only for the remainder of his presidency. In addition to delivering thousands of hand-held \"Panzerfaust\" and \"Panzerschreck\" antitank weapons, Hitler sent the 122nd Infantry Division, the half-strength 303rd Assault Gun Brigade, and \"Luftwaffe\" \"Detachment Kuhlmey\" to provide temporary support in the most threatened defense sectors.\n\nWith new supplies from Germany, the Finnish army halted the Soviet advance in early July 1944. At this point, the Finnish forces had retreated about one hundred kilometres, which brought them to approximately the same line of defence they had held at the end of the Winter War. This line was known as the VKT-line (short for \"\"Viipuri–Kuparsaari–Taipale\"\"; it ran from Viborg to the River Vuoksi to Lake Ladoga at Taipale), where the Finnish Army stopped the Soviet offensive in the Battle of Tali-Ihantala in spite of Soviet numerical and materiel superiority. The front stabilized once again.\n\nA few battles were fought in the latter stages of the war. The last of them was the Battle of Ilomantsi, a Finnish victory, from 26 July to 13 August 1944. The struggle to contain the Soviet offensive was exhausting Finnish resources. The German support under the Ryti-Ribbentrop Agreement had prevented a disaster, but it was believed the country would not be able to hold another major attack. The Soviet advances against German Army Groups Center and North further complicated matters for Finland.\n\nWith the front being stable so far, it was a good time for Finland to seek a way out of the war. At the beginning of August President Ryti resigned to allow Finland to sue for peace again, which the new government did in late August. The Soviet peace terms were harsh, but the $600,000,000 reparations demanded in the spring were reduced to $300,000,000, most likely due to pressure from the United States and Britain. However, after the ceasefire the Soviets insisted that the payments should be based on 1938 prices, which doubled the amount. This sum constituted half of Finland's annual gross domestic product in 1939.\n\nThe number of Soviet prisoners of war was estimated to be around 64,000. Of these, 56,000 were captured in 1941. About 2,600 to 2,800 Soviet prisoners of war were handed over to the Germans in exchange for roughly 2,200 Finnic prisoners of war. Out of 64,188 Soviet POWs, at least 18,318 were documented to have died in Finnish prisoner of war camps.\n\nThere are two views of the number of Finnish prisoners of war. The Soviet and Russian view is that of 2,377 Finnish prisoners of war who reached the prison camps 1,954 were returned after the Moscow Armistice. The Finnish view is that of the original approximately 3,500 Finnish prisoners of war, only about 2,000 were returned (more than 40% perished). The difference can be at least partially explained by the Soviet practice of counting only the prisoners who survived to reach a prison camp.\n\nMannerheim had repeatedly reminded the Germans that in case their troops in Estonia retreated, Finland would be forced to make peace even on extremely unfavourable terms. The territory of Estonia would have provided the Soviet army a favourable base for amphibious invasions and air attacks against Finland's capital, Helsinki, and other strategic targets in Finland, and would have severed Finnish access to the sea. The initial German reaction to Finland's announcement of ambitions for a separate peace was limited to only verbal opposition. However, the Germans then arrested hundreds of sailors on Finnish merchant ships in Germany, Denmark, and Norway.\n\nPreviously, in return for critically needed food and defense materiel from the Germans, President Ryti had personally committed, in writing, that no separate peace with the Soviets would be attempted. Accordingly, it became clear that he must resign, paving the way for a separate peace. Finland's military leader Mannerheim was appointed president in an extraordinary procedure by the Finnish parliament. In agreeing to take office, he accepted responsibility for ending the war.\n\nOn 4 September 1944, the cease-fire ended military actions on the Finnish side. The Soviet Union ended hostilities exactly 24 hours after the Finns. An armistice between the Soviet Union and Finland was signed in Moscow on 19 September. Finland had to make many concessions: the Soviet Union regained the borders of 1940, with the addition of the Petsamo area (now Pechengsky District, Russia); the Porkkala peninsula (adjacent to Helsinki) was leased to the USSR as a naval base for fifty years; and transit rights were granted. Finland's army was to be demobilized with haste, but Finland was first required to expel all German troops from its territory within 14 days. As the Germans did not leave Finland by the given deadline, the Finns fought their former co-belligerents in the Lapland War. Finland was also required to clear the minefields in Karelia (including East Karelia) and in the Gulf of Finland. Retreating German forces had also mined northern Finland heavily. The demining was a long operation, especially in the sea areas, lasting until 1952. One-hundred Finnish army personnel were killed and over 200 wounded during this process, most of them in Lapland.\n\nAs sizable numbers of civilians who had been relocated into Finland from Karelia in 1939-40 had moved back into Karelia during the war, they had to be evacuated again; of the 260,000 civilians who had moved back into the Karelia, only 19 chose to remain and become Soviet citizens.\n\nMost of the Ingrian Finns together with Votes and Izhorians living in German-occupied Ingria had been evacuated to Finland in 1943-1944. After armistice Finland was forced to return the evacuees. Soviet authorities did not allow the 55,733 people who had been handed over to settle back in Ingria, and instead deported them to central regions of Russia.\n\nNevertheless, in contrast to the rest of the Eastern front countries, where the war was fought to the end, a Soviet occupation of Finland did not occur and the country retained sovereignty. Neither did the Communists rise to power as they had in the Eastern Bloc countries. A policy called the \"Paasikivi–Kekkonen line\" formed the basis of Finnish foreign policy towards the Soviet Union until the Soviet Union's dissolution in 1991.\n\nFinland re-entered World War II mainly because of the Soviet invasion of Finland during the Winter War, which had taken place after Finnish intentions of relying on the League of Nations and Nordic neutrality to avoid conflicts had failed from lack of outside support. During the Continuation War, Finland primarily aimed to reverse its territorial losses under the March 1940 Moscow Peace Treaty and, depending on the success of the German invasion of the Soviet Union, to possibly expand, especially into East Karelia (Karelo-Finnish Soviet Socialist Republic). Some right-wing groups also supported a Greater Finland ideology. Henrik Lunde notes that, unlike many of Germany's allies, Finland survived World War II without losing its independence, although the price for war was high in war casualties, reparation payments, territorial loss, a bruised international reputation according to Olli Vehviläinen, and according to some, subsequent Soviet influence on Finland's foreign policy during the Cold War. According to Tuulikki Vuonokari, the Finnish–German alliance was different from most of the other Axis relationships, an example of which was the participation of Finnish Jews in the fight against the Soviet Union. The Finnish government did not take any anti-Jewish measures, despite repeated requests from Nazi Germany. One remarkable aspect of the Finnish–German relationship was that Finland never signed the Tripartite Pact, which was signed by all \"de jure\" Axis countries. The Finns, and Mannerheim in particular, clearly stated they would fight against the Soviets only to the extent necessary to redress the balance of the 1940 treaty. However, for Hitler the matter was irrelevant; and he saw Finland as an ally.\nFinland adopted the concept of a \"parallel war\" whereby it sought to pursue its own objectives in concert with, but separate from, Nazi Germany, as \"co-belligerents\".\n\nMajor events across Europe and the tides of war in general had a significant impact on the course of World War II in Finland:\n\nSoviet sources maintain that Soviet policies up to the Continuation War were best explained as defensive measures by offensive means: The Soviet division of occupied Poland with Germany, the Soviet occupations of Lithuania, Latvia and Estonia, and the Soviet invasion of Finland in the Winter War are described as elements in the Soviets' construction of a security zone or buffer region between the perceived threat from the capitalist powers of Western Europe and the Communist Soviet Union. These Soviet sources see the post-war establishment of Soviet satellite states in the Warsaw Pact countries and the Finnish-Soviet Agreement of Friendship, Cooperation, and Mutual Assistance as the conclusion of this Soviet defense plan. Western historians such as Norman Davies and John Lukacs dispute this view and describe the pre-war Soviet policy as an attempt to stay out of the war and regaining land lost after the fall of the Russian Empire.\n\nSeveral Western historians, while noting the Soviets' assertion of their alleged need for a Soviet security buffer, contend the Soviet designs on Finland were no different from their designs on other Baltic countries. American Dan Reiter (1990) notes, “[Finland recognized] that the Soviet Union was unlikely to be satisfied with territorial concessions as a means to increase its security. [T]he Soviets viewed the control of small buffer states as critical to their security...This was the motivation\", he asserts, \"behind the de facto 1940 Soviet annexation of the Baltic States, and Moscow saw the control of Finland also as ultimately being necessary.\" Reiter and British historian Victor Rothwell quote Soviet Foreign Minister Molotov as telling his Lithuanian counterpart at the time Lithuania was effectively absorbed into the USSR, “[S]mall states will disappear...Baltic states, including Finland, will be included within the honourable family of Soviet peoples. However, contends Reiter, \"[T]he fear of rising costs of fighting pushed Stalin to accept a limited war outcome with Finland, rather than pursue absolute victory\", although a contemporary \"Soviet document... called for the brutal military occupation of Finland at war’s end.\" “The Finnish victory [at Ilomantsi] ended the Soviet offensive in Finland and persuaded the Soviets to give up their demand for Finland's unconditional surrender\". Peter Provis (1999) concludes his essay on point, “By following [self-censorship and limited appeasement] policies and fulfilling the Soviet Union's demands [for great reparations]...Finland avoided the same fate as other nations that were 'liberated' by the Red Army...Finland had once again defended her independence in a global conflict that engulfed and destroyed many other nations...The Finns had once again demonstrated their determination to avoid defeat by the Soviet Union and maintained their independence\".\n\nRussian historian Nikolai Baryshnikov disputes the view that the Soviet Union wanted to deprive Finland of its independence, and that Finnish \"defensive victories\" prevented this. He argues that there is no documentary evidence for such claims and that the Soviet government was always open for negotiations. Baryshnikov cites the former head of the Office of Information of the Finnish General Staff, Kalle Lehmus, and other Finnish sources to show that the Finnish leaders already knew of the limited Soviet plans for Finland in the first half of July 1944, after intelligence indicated that some Soviet divisions were to be transferred to reserve in Leningrad.\n\n\n\n\n\n", "id": "7712", "title": "Continuation War"}
{"url": "https://en.wikipedia.org/wiki?curid=7713", "text": "Chinese remainder theorem\n\nThe Chinese remainder theorem is a theorem of number theory, which states that, if one knows the remainders of the division of an integer by several integers, then one can determine uniquely the remainder of the division of by the product of these integers, under the condition that the divisors are pairwise coprime.\n\nThis theorem has this name because it is a theorem about \"remainders\", which was first discovered in the 3rd century AD by the Chinese mathematician Sunzi in \"Sunzi Suanjing\".\n\nThe Chinese remainder theorem is widely used for computing with large integers, as it allows replacing a computation for which one knows a bound on the size of the result by several similar computations on small integers.\n\nThe Chinese remainder theorem (expressed in terms of congruences) is true over every principal ideal domain. It has been generalized to any commutative ring, with a formulation involving ideals.\n\nThe earliest known statement of the theorem, as a problem with specific numbers, appears in the 3rd-century book \"Sunzi Suanjing\" by the Chinese mathematician Sunzi:\n\nSunzi's work contains neither a proof nor a full algorithm. What amounts to an algorithm for solving this problem was described by Aryabhata (6th century). Special cases of the Chinese remainder theorem were also known to Brahmagupta (7th century), and appear in Fibonacci's Liber Abaci (1202). The result was later generalized with a complete solution called \"Dayanshu\" () in Qin Jiushao's 1247 \"Mathematical Treatise in Nine Sections\" (, \"Shushu Jiuzhang\").\n\nThe notion of congruences was first introduced and used by Gauss in his \"Disquisitiones Arithmeticae\" of 1801. Gauss illustrates the Chinese remainder theorem on a problem involving calendars, namely, \"to find the years that have a certain period number with respect to the solar and lunar cycle and the Roman indiction.\" Gauss introduces a procedure for solving the problem that had already been used by Euler but was in fact an ancient method that had appeared several times.\n\nLet be integers greater than 1, which are often called \"moduli\" or \"divisors\". Let us denote by the product of the .\n\nThe Chinese remainder theorem asserts that if the are pairwise coprime, and if are integers such that for every , then there is one and only one integer , such that and the remainder of the Euclidean division of by is for every .\n\nThis may be restated as follows in term of congruences:\nIf the are pairwise coprime, and if are any integers, then there exists an integer such that\nand any two such are congruent modulo .\n\nIn abstract algebra, the theorem is often restated as: if the are pairwise coprime, the map\ndefines a ring isomorphism\n\nbetween the ring of integers modulo and the direct product of the rings of integers modulo the . This means that for doing a sequence of arithmetic operations in formula_4 one may do the same computation independently in each formula_5 and then get the result by applying the isomorphism (from the right to the left). This may be much faster than the direct computation if and the number of operations are large. This is widely used, under the name \"multi-modular computation\", for linear algebra over the integers or the rational numbers.\n\nThe theorem can also be restated in the language of combinatorics as the fact that the infinite arithmetic progressions of integers form a Helly family.\n\nThe existence and the uniqueness of the solution may be proved independently. However the first proof of existence, given below, uses the uniqueness.\n\nSuppose that and are both solutions to all the congruences. As and give the same remainder, when divided by , their difference is a multiple of each . As the are pairwise coprime, their product divides also , and thus and are congruent modulo . If and are supposed to be non negative and less than (as in the first statement of the theorem), then their difference may be a multiple of only if .\n\nThe map\nmaps congruence classes modulo to sequences of congruence classes modulo . The proof of uniqueness shows that this map is injective. As the domain and the codomain of this map have the same number of elements, the map is also surjective, which proves the existence of the solution.\n\nThis proof is very simple, but does not provide any direct way for computing a solution. Moreover, it cannot be generalized to other situations where the following proof can.\n\nExistence may be established by an explicit construction of . This construction may be split in two steps, firstly by solving the problem in the case of two moduli, and the second one by extending this solution to the general case by induction on the number of moduli.\n\nWe want to solve the system\nwhere formula_8 and formula_9 are coprime.\n\nBézout's identity asserts the existence of two integers formula_10 and formula_11 such that \nThe integers formula_10 and formula_11 may be computed by Extended Euclidean algorithm.\n\nA solution is given by\nIn fact \nThis shows that formula_17 The second congruence is proved similarly, by exchanging the indices.\n\nLet us consider a sequence of congruence equations\nwhere the formula_19 are pairwise coprime. The two first equations have a solution formula_20 provided by the method of the previous section. The set of the solutions of these two first equations is the set of all solutions of the equation\n\nAs the other formula_19 are coprime with formula_23 this reduces solving the initial problem of equations to a similar problem with formula_24 equations. Iterating the process, one gets eventually the solutions of the initial problem\n\nFor constructing a solution, it is not necessary to make an induction on the number of moduli. However, such a direct construction involves more computation with large numbers, which makes it less efficient and less used. Nevertheless, Lagrange interpolation is a special case of this construction, applied to polynomials instead of integers.\n\nLet formula_25 be the product of all moduli but one. As the formula_19 are pairwise coprime, formula_27 and formula_19 are coprime. Thus Bézout's identity applies, and there exist integers formula_29 and formula_30 such that\n\nA solution of the system of congruences is\nIn fact, as formula_27 is a multiple of formula_34 for formula_35\nwe have\nfor every formula_37\n\nLet us consider a system of congruences\nwhere the formula_19 are pairwise coprime, and let formula_40 In this section several methods are described for computing the unique solution for formula_41, such that formula_42 and these methods are applied on the example\n\nIt is easy to check whether a value of is a solution: it suffices to compute the remainder of the Euclidean division of by each . Thus, to find the solution, it suffices to check successively the integers from to until finding the solution.\n\nAlthough very simple this method is very inefficient: for the simple example considered here, integers (including ) have to be checked for finding the solution . This is an exponential time algorithm, as the size of the input is, up to a constant factor, the number of digits of , and the average number of operations is of the order of .\n\nTherefore, this method is rarely used, for hand-written computation as well on computers.\n\nThe search of the solution may be made dramatically faster by sieving. For this method, we suppose, without loss of generality, that formula_44 (if it were not the case, it would suffice to replace each formula_45 by the remainder of its division by formula_19). This implies that the solution belongs to the arithmetic progression\nBy testing the values of these numbers modulo formula_48 one eventually finds a solution formula_49 of the two first congruences. Then the solution belongs to the arithmetic progression \nTesting the values of these numbers modulo formula_51, and continuing until every modulus has been tested gives eventually the solution.\n\nThis method is faster if the moduli have been ordered by decreasing value, that is if formula_52 For the example, this gives the following computation. We consider first the numbers that are congruent to 4 modulo 5 (the largest modulus), which are 4, , , ... For each of them, compute the remainder by 4 (the second largest modulus) until getting a number congruent to 3 modulo 4. Then one can proceed by adding at each step, and computing only the remainders by 3. This gives\n\nThis method works well for hand-written computation with a product of moduli that is not too big. However it is much slower than other methods, for very large products of moduli. Although dramatically faster than the systematic search, this method has also an exponential time complexity, and is therefore not used on computers.\n\nThe constructive existence proof shows that, in the case of two moduli, the solution may be obtained by the computation of the Bézout coefficients of the moduli, followed by a few multiplications, additions and reductions modulo formula_53 (for getting a result in the interval formula_54). As the Bézout's coefficients may be computed with the extended Euclidean algorithm, the whole computation, at most, has a quadratic time complexity of formula_55 where formula_56 denotes the number of digits of formula_57\n\nFor more than two moduli, the method for two moduli allows the replacement of any two congruences by a single congruence modulo the product of the moduli. Iterating this process provides eventually the solution with a complexity, which is quadratic in the number of digits of the product of all moduli. This quadratic time complexity does not depend on the order in which the moduli are regrouped. One may regroup the two first moduli, then regrouping the resulting modulus with the next one, and so on. This is the strategy which is the easiest to implement, but it needs more computation involving large numbers.\n\nAnother strategy consists in partitioning the moduli in pairs whose product have comparable sizes (as much as possible), applying parallely the method of two moduli to each pair, and iterating with a number of moduli approximatively divided by two. This method allows an easy parallelization of the algorithm. Also, if fast algorithms (that is algorithms working in quasilinear time) are used for the basic operations, this method provides an algorithm for the whole computation that works in quasilinear time.\n\nOn the current example (which has only three moduli), both strategies are identical, and works as follows.\n\nBézout's identity for 3 and 4 is\nPutting this in the formula given for proving the existence gives \nfor a solution of the two first congruences, the other solutions being obtained by adding to −9 any multiple of . One may continue with any of these solutions, but the solution is smaller (in absolute value) and thus leads probably to an easier computation\n\nBézout identity for 5 and 3×4 = 12 is\nApplying the same formula again, we get a solution of the problem:\nThe other solutions are obtained by adding any multiple of , and the smallest positive solution is .\n\nThe system of congruences solved by the Chinese remainder theorem may be rewritten as a system of simultaneous linear Diophantine equations\nwhere the unknown integers are formula_41 and the formula_64 Therefore, every general method for solving such systems may be used for finding the solution of Chinese remainder theorem, such as the reduction of the matrix of the system to Smith normal form or Hermite normal form. However, as usual when using a general algorithm for a more specific problem, this approach is less efficient than the method of the preceding section, based on a direct use of Bézout's identity.\n\nIn , the Chinese remainder theorem has been stated in three different ways: in terms of remainders, of congruences and of a ring isomorphism. The statement in terms of remainders does not apply, in general to principal ideal domains, as remainders are not defined in such rings. However, the two other version make sense over a principal ideal domain : it suffices to replace \"integer\" by \"element of the domain\", and formula_65 by . These two versions of the theorem are true in this context, because the proofs (except for the first existence proof), are based on Euclid's lemma and Bézout's identity, which are true over every principal domain.\n\nHowever, in general, the theorem is only an existence theorem, and does not provide any way for computing the solution, unless if one has an algorithm for computing the coefficients of Bézout's identity.\n\nThe statement in terms of remainders given in cannot be generalized to any principal ideal domain, but its generalization to Euclidean domains is straightforward. The univariate polynomials over a field is the typical example of a Euclidean domain, which is not the integers. therefore, we state the theorem for the case of a ring of univariate domain formula_66 over a field formula_67 For getting the theorem for a general Euclidean domain, it suffices to replace the degree by the Euclidean function of the Euclidean domain.\n\nThe Chinese remainder theorem for polynomials is thus: Let formula_68 (the moduli) be, for , pairwise coprime polynomials in formula_66. Let formula_70 be the degree of formula_68, and formula_72 be the sum of the formula_73\nIf formula_74 are polynomials such that formula_75 or formula_76 for every , then, there is one and only one polynomial formula_77, such that formula_78 and the remainder of the Euclidean division of formula_77 by formula_68 isformula_81 for every .\n\nThe construction of the solution may be done as in or . However, the latter construction may be simplified by using, as follows, partial fraction decomposition instead of extended Euclidean algorithm.\n\nThus, we want to find a polynomial formula_77, which satisfies the congruences\nfor formula_84\n\nLet us consider the polynomials\n\nThe partial fraction decomposition of formula_86 gives polynomials formula_87 with degrees formula_88 such that\nand thus\n\nThen a solution of the simultaneous congruence system is given by the polynomial\n\nIn fact, we have\nfor formula_93\n\nThis solution may have a degree larger that formula_94 The unique solution of degree less than formula_72 may be deduced by considering the remainder formula_96 of the Euclidean division of formula_97 by formula_98 This solution is \n\nA special case of Chinese remainder theorem for polynomials is Lagrange interpolation. For this, let us consider monic polynomials of degree one:\nThey are pairwise coprime if the formula_101 are all different. The remainder of the division by formula_68 of a polynomial formula_77 is formula_104\n\nNow, let formula_105be constants (polynomials of degree 0) in formula_67 Both Lagrange interpolation and Chinese remainder theorem assert the existence of a unique polynomial formula_107 of degree less than such that\nfor every .\n\nLagrange interpolation formula is exactly the result, in this case, of the above construction of the solution. More precisely, let \nThe partial fraction decomposition of formula_86 is \nIn fact, reducing the right-hand side to a common denominator one gets \nformula_112\nand the numerator is equal to one, as being a polynomial of degree less than k, which takes the value one for formula_113 different values of formula_114\n\nUsing the above general formula, we get the Lagrange interpolation formula\n\nHermite interpolation is an application of Chinese remainder theorem for univariate polynomials, which may involve moduli of arbitrary degrees (Lagrange interpolation involves only moduli of degree one).\n\nThe problem consists of finding a polynomial of the least possible degree, such that the polynomial and its first derivatives take given values at some fixed points.\n\nMore precisely, let formula_116 be formula_113 elements of the ground field formula_118 and, for formula_119 let formula_120 be the values of the first formula_121 derivatives of the sought polynomial at formula_101 (including the 0th derivative, which is the value of the polynomial itself). The problem is to find a polynomial formula_77 such that its \"j\"th derivative takes the value formula_124 at formula_125 for formula_126 and formula_127\n\nLet us consider the polynomial\nThis is the Taylor polynomial of order formula_129 at formula_101, of the unknown polynomial formula_131 Therefore, we must have\n\nConversely, any polynomial formula_133 that satisfies these formula_113 congruences, in particular verifies, for any formula_135\ntherefore formula_68 is its Taylor polynomial of order formula_138 at formula_101, that is, formula_77 solves the initial Hermite interpolation problem.\nThe Chinese remainder theorem asserts that there exists exactly one polynomial of degree less than the sum of the formula_141 which satisfies these formula_113 congruences.\n\nThere are several ways for computing the solution formula_131 One may use the method described at the beginning of . One may also use the constructions given in or .\n\nThe Chinese remainder theorem can be generalized to any ring, by using coprime ideals (also called comaximal ideals). Two ideals and are coprime if there are elements formula_144 and formula_145 such that formula_146 This relation plays the role of Bézout's identity in the proofs related to this generalization, which, otherwise are very similar. The generalization may be stated as follows.\n\nLet be two-sided ideals of a ring formula_147 that are pairwise coprime, and be their intersection. Then we have the isomorphism \nbetween the quotient ring formula_149 and the direct product of the formula_150\nwhere \"formula_151\" denotes the image of the element formula_41 in the quotient ring defined by the ideal formula_153\nMoreover, if formula_147 is commutative, then the ideal intersection is equal to the product of the ideals formula_155\n\nThe Chinese remainder theorem has been used to construct a Gödel numbering for sequences, which is involved in the proof of Gödel's incompleteness theorems.\n\nThe prime-factor FFT algorithm (also called Good-Thomas algorithm) uses the Chinese remainder theorem for reducing the computation of a fast Fourier transform of size formula_53 to the computation of two fast Fourier transforms of smaller sizes formula_8 and formula_9 (providing that formula_8 and formula_9 are coprime).\n\nMost implementations of RSA use the Chinese remainder theorem during signing of HTTPS certificates and during decryption.\n\nThe Chinese remainder theorem can also be used in secret sharing, which consists of distributing a set of shares among a group of people who, all together (but no one alone), can recover a certain secret from the given set of shares. Each of the shares is represented in a congruence, and the solution of the system of congruences using the Chinese remainder theorem is the secret to be recovered. Secret sharing using the Chinese remainder theorem uses, along with the Chinese remainder theorem, special sequences of integers that guarantee the impossibility of recovering the secret from a set of shares with less than a certain cardinality.\n\nThe range ambiguity resolution techniques used with medium pulse repetition frequency radar can be seen as a special case of the Chinese remainder theorem.\n\nDedekind's Theorem on the Linear Independence of Characters. Let be a monoid and an integral domain, viewed as a monoid by considering the multiplication on . Then any finite family of distinct monoid homomorphisms is linearly independent. In other words, every family of elements satisfying \n\nmust be equal to the family .\n\nProof. First assume that is a field, otherwise, replace the integral domain by its quotient field, and nothing will change. We can linearly extend the monoid homomorphisms to -algebra homomorphisms , where is the monoid ring of over . Then, by linearity, the condition\n\nyields\n\nNext, for the two -linear maps and are not proportional to each other. Otherwise and would also be proportional, and thus equal since as monoid homomorphisms they satisfy: , which contradicts the assumption that they are distinct.\n\nTherefore, the kernels and are distinct. Since is a field, is a maximal ideal of for every . Because they are distinct and maximal the ideals and are coprime whenever . The Chinese Remainder Theorem (for general rings) yields an isomorphism:\n\nwhere\n\nConsequently, the map\n\nis surjective. Under the isomorphisms the map corresponds to:\n\nNow,\n\nyields\n\nfor every vector in the image of the map . Since is surjective, this means that\n\nfor every vector\n\nConsequently, . QED.\n\n\n\n\n", "id": "7713", "title": "Chinese remainder theorem"}
{"url": "https://en.wikipedia.org/wiki?curid=7716", "text": "Cyril M. Kornbluth\n\nCyril M. Kornbluth (July 2, 1923 – March 21, 1958) was an American science fiction author and a member of the Futurians. He used a variety of pen-names, including Cecil Corwin, S. D. Gottesman, Edward J. Bellin, Kenneth Falconer, Walter C. Davies, Simon Eisner, Jordan Park, Arthur Cooke, Paul Dennis Lavond and Scott Mariner. The \"M\" in Kornbluth's name may have been in tribute to his wife, Mary Byers; Kornbluth's colleague and collaborator Frederik Pohl confirmed Kornbluth's lack of any actual middle name in at least one interview.\n\nKornbluth was born and grew up in the uptown Manhattan neighborhood of Inwood, in New York City, He was of Polish Jewish descent. the son of a \"second-generation [American] Jew\" who ran his own tailor shop. According to his widow, Kornbluth was a \"precocious child\", learning to read by the age of three and writing his own stories by the time he was seven. He graduated high school at thirteen, received a CCNY scholarship at fourteen, and was \"thrown out for leading a student strike\" before graduating.\n\nAs a teenager, he became a member of the Futurians, an influential group of science fiction fans and writers. While a member of the Futurians, he met and became friends with Isaac Asimov, Frederik Pohl, Donald A. Wollheim, Robert A. W. Lowndes, and his future wife Mary Byers. He also participated in the Fantasy Amateur Press Association.\n\nKornbluth served in the US Army during World War II (European 'Theatre'). He received a Bronze Star for his service in the Battle of the Bulge, where he served as a member of a heavy machine gun crew. Upon his discharge, he returned to finish his education, which had been interrupted by the war, at the University of Chicago. While living in Chicago he also worked at Trans-Radio Press, a news wire service. In 1951 he started writing full-time, returning to the East Coast where he collaborated on novels with his old Futurian friends Frederik Pohl and Judith Merril.\n\nKornbluth began writing at 15. His first solo story, \"The Rocket of 1955\", was published in Richard Wilson's fanzine \"Escape\" (Vol. 1, No 2, August 1939); his first collaboration, \"Stepsons of Mars,\" written with Richard Wilson and published under the name \"Ivar Towers\", appeared in the April 1940 \"Astonishing\". His other short fiction includes \"The Little Black Bag\", \"The Marching Morons\", \"The Altar at Midnight\", \"MS. Found in a Chinese Fortune Cookie\", \"Gomez\" and \"The Advent on Channel 12\".\n\n\"The Little Black Bag\" was first adapted for television live on the television show \"Tales of Tomorrow\" on May 30, 1952. It was later adapted for television by the BBC in 1969 for its \"Out of the Unknown\" series. In 1970, the same story was adapted by Rod Serling for an episode of his \"Night Gallery\" series. This dramatization starred Burgess Meredith as the alcoholic Dr. William Fall, who had long lost his doctor's license and become a homeless alcoholic. He finds a bag containing advanced medical technology from the future (2098), which, after an unsuccessful attempt to pawn it, he uses benevolently.\n\n\"The Marching Morons\" is a look at a far future in which the world's population consists of five billion idiots and a few million geniuses – the precarious minority of the \"elite\" working desperately to keep things running behind the scenes. In his introduction to \"The Best of C.M. Kornbluth\", Pohl states that \"The Marching Morons\" is a direct sequel to \"The Little Black Bag\": it is easy to miss this, as \"Bag\" is set in the contemporary present while \"Morons\" takes place several centuries from now, and there is no character who appears in both stories. The titular black bag in the first story is actually an artifact from the time period of \"The Marching Morons\": a medical kit filled with self-driven instruments enabling a far-future moron to \"play doctor\". A future Earth similar to \"The Marching Morons\" – a civilisation of morons protected by a small minority of hidden geniuses – is used again in the final stages of Kornbluth & Pohl's \"Search the Sky\".\n\n\"MS. Found in a Chinese Fortune Cookie\" (1957) is supposedly written by Kornbluth using notes by \"Cecil Corwin\", who has been declared insane and incarcerated, and who smuggles out in fortune cookies the ultimate secret of life. This fate is said to be Kornbluth's response to the unauthorized publication of \"Mask of Demeter\" (as by \"Corwin\" and \"Martin Pearson\" (Donald A. Wollheim)) in Wollheim's anthology \"Prize Science Fiction\" in 1953.\n\nBiographer Mark Rich describes the 1958 story \"Two Dooms\" as one of several stories which are \"concern[ed] with the ethics of theoretical science\" and which \"explore moral quandaries of the atomic age\":\n\nMany of Kornbluth's novels were written as collaborations: either with Judith Merril (using the pseudonym Cyril Judd), or with Frederik Pohl. These include \"Gladiator-At-Law\" and \"The Space Merchants\". \"The Space Merchants\" contributed significantly to the maturing and to the wider academic respectability of the science fiction genre, not only in America but also in Europe. Kornbluth also wrote several novels under his own name, including \"The Syndic\" and \"Not This August\".\n\nKornbluth died at age 34 in Levittown, New York. Scheduled to meet with Bob Mills in New York City to interview for the position of editor of \"The Magazine of Fantasy & Science Fiction\", Kornbluth had to shovel out his driveway, which left him running behind. Racing to make his train, he suffered a heart attack on the platform of the train station.\n\nA number of short stories remained unfinished at Kornbluth's death; these were eventually completed and published by Pohl. One of these stories, \"The Meeting\" (\"The Magazine of Fantasy & Science Fiction\", November 1972), was the co-winner of the 1973 Hugo Award for Best Short Story; it tied with R. A. Lafferty's \"Eurema's Dam.\" Almost all of Kornbluth's solo SF stories have been collected as \"His Share of Glory: The Complete Short Science Fiction of C. M. Kornbluth\" (NESFA Press, 1997).\n\nFrederik Pohl, in his autobiography \"The Way the Future Was\", Damon Knight, in his memoir \"The Futurians\", and Isaac Asimov, in his memoirs \"In Memory Yet Green\" and \"I. Asimov: A Memoir\", all give descriptions of Kornbluth as a man of odd personal habits and eccentricities. \n\nKornbluth, for example, decided to educate himself by reading his way through an entire encyclopedia from A to Z; in the course of this effort, he acquired a great deal of esoteric knowledge that found its way into his stories, in alphabetical order by subject. When Kornbluth wrote a story that mentioned the \"ballista\", an Ancient Roman weapon, Pohl knew that Kornbluth had finished the A's and had started on the B's.\n\nAccording to Pohl, Kornbluth never brushed his teeth, and they were literally green. Deeply embarrassed by this, Kornbluth developed the habit of holding his hand in front of his mouth when speaking.\n\nKornbluth disliked black coffee, but felt obliged to acquire a taste for it because he believed that professional authors were \"supposed to\" drink black coffee. He trained himself by putting gradually less cream into each cup of coffee he drank, until he eventually \"weaned himself\" (Knight's description) and switched to black coffee.\n\n\n\nAnthony Boucher praised the collection, saying \"Kornbluth's sharp observation is everywhere present, and in most of the stories his bitter insight.\" P. Schuyler Miller reviewed the collection favorably for \"Astounding Science Fiction\".\n\nSpider Robinson praised this collection, saying \"I haven't enjoyed a book so much in years.\" Mark Rich wrote, \"Critics judging Kornbluth by this anthology, edited by Pohl, have seen a growing bitterness in his later stories. This reflects editorial choice more than reality, because Kornbluth also wrote delightful humor in his last years, in stories not collected here. These tales demonstrate Kornbluth's effective use of everyday individuals from a variety of ethnic backgrounds as well as his well-tuned ear for dialect.\"\n\n\n\n\nKornbluth's name is mentioned in Lemony Snicket's \"Series of Unfortunate Events\" as a member of V.F.D., a secret organization dedicated to the promotion of literacy, classical learning, and crime prevention.\n\n\n", "id": "7716", "title": "Cyril M. Kornbluth"}
{"url": "https://en.wikipedia.org/wiki?curid=7720", "text": "Coprophagia\n\nCoprophagia or coprophagy is the consumption of feces. The word is derived from the Greek κόπρος \"copros\", \"feces\" and φαγεῖν \"phagein\", \"to eat\". \"Coprophagy\" refers to many kinds of feces-eating, including eating feces of other species (heterospecifics), of other individuals (allocoprophagy), or one's own (autocoprophagy) – those once deposited or taken directly from the anus.\n\nIn humans, coprophagia has been observed in individuals with mental illness. Some animal species eat feces as a normal behavior; other species may not normally consume feces but do so under very unusual conditions.\n\nCoprophagia has been observed in individuals with schizophrenia and pica. Individuals with Prader–Willi syndrome also often exhibit coprophagia.\n\nConsuming feces carries the risk of spreading bacteria such as \"E. coli\" and of contracting diseases such as Hepatitis A, Hepatitis E, pneumonia, polio and influenza. Coprophagia also carries a risk of contracting intestinal parasites.\n\nCoprophagous insects consume and redigest the feces of large animals. These feces contain substantial amounts of semi-digested food, particularly in the case of herbivores, owing to the inefficiency of the large animals' digestive systems. Two feces-eating insects are the dung-beetle and certain species of fly.\n\nTermites eat one another's feces as a means of obtaining their hindgut protists. Termites and protists have a symbiotic relationship (e.g. with the protozoan that allows the termites to digest the cellulose in their diet). For example, in one group of termites, there is a three-way symbiotic relationship: termites of the family Rhinotermitidae, cellulolytic protists of the genus \"Pseudotrichonympha\" in the guts of these termites, and intracellular bacterial symbionts of the protists.\n\nDomesticated and wild mammals are known to consume feces. \nIn the wild they either bury or eat waste to protect their trail from predators. Mothers of certain species are known to eat the feces of their newborn young during the earliest phase after birth, presumably to eliminate cues to potential predators and to keep the den clean.\n\nDogs may be coprophagic, possibly to rebalance their microbiome or to ingest missing nutrients.\n\nSpecies within the Lagomorpha (rabbits, hares, and pikas) produce two types of fecal pellets: hard ones, and soft ones called cecotropes. Animals in these species reingest their cecotropes, to extract further nutrients. Cecotropes derive from chewed plant material that collects in the cecum, a chamber between the large and small intestine, containing large quantities of symbiotic bacteria that help with the digestion of cellulose and also produce certain B vitamins. After excretion of the soft cecotrope, it is again eaten whole by the animal and redigested in a special part of the stomach. The pellets remain intact for up to six hours in the stomach; the bacteria within continue to digest the plant carbohydrates. This double-digestion process enables these animals to use nutrients that they may have missed during the first passage through the gut, as well as the nutrients formed by the microbial activity and thus ensures that maximum nutrients are derived from the food they eat. This process serves the same purpose within these animals as rumination (cud-chewing) does in cattle and sheep. \n\nCattle in the United States are often fed chicken litter. There are concerns that the practice of feeding chicken litter to cattle could lead to bovine spongiform encephalopathy (mad-cow disease) because of the crushed bone meal in chicken feed. The U.S. Food and Drug Administration regulates this practice by attempting to prevent the introduction of any part of a cow's brain or spinal cord into livestock feed. Other countries, like Canada, have banned chicken litter for use as a livestock feed.\n\nThe young of elephants, giant pandas, koalas and hippos eat the feces of their mothers or other animals in the herd, in order to obtain the bacteria required to properly digest vegetation found in their ecosystems. When such animals are born, their intestines are sterile and do not contain these bacteria. Without doing this they would be unable to obtain any nutritional value from plants.\n\nHamsters, guinea pigs, chinchillas and naked mole-rat eat their own droppings, which are thought to be a source of vitamins B and K, produced by gut bacteria. Gorillas have been recorded to consume their feces extremely rarely, possibly out of boredom, a desire for warm food, or to reingest seeds contained in the feces.\n\nPigs sometimes eat the feces of herbivores that leave a significant amount of semi-digested matter, including their own. In some cultures it was common for poor families to collect horse feces to feed their pigs, which contributes to the risk of parasite infection.The pig toilet is an application of porcine coprophagy to human sanitation.\n\nSome carnivorous plants, such as pitcher plants of the genus \"Nepenthes\", obtain nourishment from the feces of commensal animals.\n\nLewin reported that \"... consumption of fresh, warm camel feces has been recommended by Bedouins as a remedy for bacterial dysentery; its efficacy (probably attributable to the antibiotic subtilisin from \"Bacillus subtilis\") was anecdotally confirmed by German soldiers in Africa during World War II\".\n\nCenturies ago, physicians tasted their patients' feces, to better judge their state and condition.\n\nCoprophagia is depicted in pornography, usually under the term \"scat\" (from \"scatology\").\n\n\"The 120 Days of Sodom\", a novel by the Marquis de Sade written in 1785, is full of detailed descriptions of erotic sadomasochistic coprophagia. Thomas Pynchon's award-winning 1973 novel \"Gravity's Rainbow\" contains a detailed scene of coprophagia. François Rabelais, in his classic \"Gargantua and Pantagruel\", often employs the expression \"mâche-merde\" or \"mâchemerde\", meaning \"shit-chewer\". This in turn comes from the Greek comedians Aristophanes and particularly Menander, who often use the term skatophagos (\"σκατοϕάγος\"). The Austrian actor and pornographic director Simon Thaur created the series \"Avantgarde Extreme\" and \"Portrait Extrem\", which explores coprophagy, coprophilia and urophagia. Modern Russian writer Vladimir Sorokin's novel \"Norma\" describes a society where coprophagia is institutionalised and mandatory. The comic book author Kalem Enginar used the Turkish name for a feces-eating fly, the Ferdogan, as a character in his graphic novel \"Dark Lightbulbs\".\n\n\n", "id": "7720", "title": "Coprophagia"}
{"url": "https://en.wikipedia.org/wiki?curid=7721", "text": "C. L. Moore\n\nCatherine Lucille Moore (January 24, 1911 – April 4, 1987) was an American science fiction and fantasy writer, who most often used the pen name C. L. Moore. She was among the first women to write in either genre (though earlier woman writers in these genres include Sophie Wenzel Ellis, Clare Winger Harris, Lilith Lorraine, Greye La Spina, Francis Stevens, Leslie F. Stone, and Everil Worrell). Moore's work paved the way for many other female speculative fiction writers. She and her first husband Henry Kuttner were prolific co-authors under their own names and three pseudonyms.\n\nMoore was born on January 24, 1911 in Indianapolis, Indiana. She was chronically ill as a child and spent much of her time reading literature of the fantastic. She left college during the Great Depression to work as a secretary at the Fletcher Trust Company in Indianapolis.\n\nHer first stories appeared in pulp magazines during the mid-1930s, including two significant series in \"Weird Tales\", then edited by Farnsworth Wright. One features the rogue and adventurer Northwest Smith wandering through the Solar System; the other features the swordswoman/warrior Jirel of Joiry, one of the first female protagonists in sword-and-sorcery fiction. Both series are sometimes named for their lead characters. One of the Northwest Smith stories, \"Nymph of Darkness\" (\"Fantasy Magazine\" (April 1935); expurgated version, \"Weird Tales\" (Dec 1939)), was written in collaboration with Forrest J Ackerman. \n\nThe most famous Northwest Smith story is \"Shambleau\", which was also Moore's first professional sale. It originally appeared in the November 1933 issue of \"Weird Tales\",netting her $100, and later becoming a popular anthology reprint. Her most famous Jirel story is also the first one, \"Black God's Kiss\", which was the cover story in the October 1934 issue of \"Weird Tales\", subtitled \"the weirdest story ever told\" (see figure). Moore's early stories were notable for their emphasis on the senses and emotions, which was unusual in genre fiction at the time.\n\nMoore's work also appeared in \"Astounding Science Fiction\" magazine throughout the 1940s. Several stories written for that magazine were later collected in her first published book, \"Judgment Night\" (1952) One of them, the novella \"No Woman Born\" (1944), was to be included in more than 10 different science fiction anthologies including \"The Best of C. L. Moore\".\n\nIncluded in that collection were \"Judgment Night\" (first published in August and September 1943), the lush rendering of a future galactic empire with a sober meditation on the nature of power and its inevitable loss; \"The Code\" (July 1945), an homage to the classic Faust with modern theories and Lovecraftian dread; \"Promised Land\" (February 1950) and \"Heir Apparent\" (July 1950), both documenting the grim twisting that mankind must undergo in order to spread into the Solar System; and \"Paradise Street\" (September 1950), a futuristic take on the Old West conflict between lone hunter and wilderness-taming settlers.\n\nMoore met Henry Kuttner, also a science fiction writer, in 1936 when he wrote her a fan letter under the impression that \"C. L. Moore\" was a man. They married in 1940 and thereafter wrote almost all of their stories in collaboration—under their own names and using the joint pseudonyms C. H. Liddell, Lawrence O'Donnell, and Lewis Padgett—most commonly the latter, a combination of their mothers' maiden names.\n\nIn this very prolific partnership they managed to combine Moore's style with Kuttner's more cerebral storytelling. Their works include a classic, \"Mimsy Were the Borogoves\" (February 1943), the basis for the film \"The Last Mimzy\" (2007), and \"Vintage Season\" (September 1946), the basis for the film \"Timescape\" (1992). They also collaborated on a story that combined Moore’s signature characters, Northwest Smith and Jirel of Joiry: \"Quest of the Starstone\" (1937).\n\nAfter Kuttner's death in 1958, Moore continued teaching his writing course at the University of Southern California but wrote almost no fiction. She did write for a few television shows under her married name, but upon marrying Thomas Reggie (who was not a writer) in 1963, she ceased writing entirely.\n\nMoore was the author guest of honor at Kansas City, MO's fantasy and science fiction convention BYOB-Con 6, held over the U. S. Memorial Day weekend in May, 1976. \n\nIn 1981, Moore received two annual awards for her career in fantasy literature: the World Fantasy Award for Life Achievement, chosen by a panel of judges at the World Fantasy Convention, and the Gandalf Grand Master Award, chosen by vote of participants in the World Science Fiction Convention. (Thus she became the eighth and final Grand Master of Fantasy, sponsored by the Swordsmen and Sorcerers' Guild of America, in partial analogy to the Grand Master of Science Fiction sponsored by the Science Fiction Writers of America.)\n\nMoore was an active member of the Tom and Terri Pinckard Science Fiction literary salon and a frequent contributor to literary discussions with the regular membership, including Robert Bloch, George Clayton Johnson, Larry Niven, Jerry Pournelle, Norman Spinrad, A. E. van Vogt, and others, as well as many visiting writers and speakers.\n\nShe developed Alzheimer's disease but that was not obvious for several years. She had ceased to attend the meetings when she was nominated to be the first woman Grand Master of the Science Fiction Writers of America; the nomination was withdrawn at the request of her husband, Thomas Reggie, who said the award and ceremony would be at best confusing and likely upsetting to her, given the progress of her disease. That caused dismay among the former SFWA presidents, for she was a great favorite to receive the award. (Former presidents and current officers select a living writer as Grand Master of SF, no more than one annually.)\n\nMoore died on April 4, 1987 at her home in Hollywood, California after a long battle with Alzheimer's.\n\nThe Science Fiction and Fantasy Hall of Fame inducted Moore in 1998, its third class of two deceased and two living writers.\n\n\nBleiler, E.F. \"Fantasy, Horror...and Sex: The Early Stories of C.L. Moore\". \"Scream Factory\" (1988): 41-47\n\n", "id": "7721", "title": "C. L. Moore"}
{"url": "https://en.wikipedia.org/wiki?curid=7722", "text": "Compactron\n\nCompactrons are a type of thermionic valve, or vacuum tube, which contain arrangements of diodes, triodes, or pentodes in multiple combination arrays, as well as high or low-voltage and power types.\n\nThe Compactron is a 12-pin Duodecar base vacuum tube family introduced in 1961 by General Electric in Owensboro, Kentucky to compete with transistorized electronics during the solid state transition. Television sets were a primary application.\n\nUse was prevalent in televisions because transistors were slow to achieve the high power and frequency capabilities needed particularly in color television sets. The first portable color television, the General Electric Porta-Color, was designed using 13 tubes, 10 of which were Compactrons.\n\nCompactron's integrated valve design helped lower power consumption and heat generation (they were to tubes what integrated circuits were to transistors). Compactrons were also used in a few high end Hi-Fi stereos. They were also used by the Ampeg guitar amplifier company in some of their guitar amps. No modern tube based Hi-Fi systems are known to use this tube type, as simpler and more readily available tubes have again filled this niche.\n\nA distinguishing feature of most Compactrons is the placement of the evacuation tip on the bottom end, rather than the top end as was customary with \"miniature\" tubes, and a characteristic 3/4\" diameter circle pin pattern.\n\nExamples of Compactrons type types include:\n\nDue to their specific applications in television circuits, many different Compactron types were produced. Almost all were assigned using standard US tube numbers.\n\nIntegrated circuits (of the analogue and digital type) gradually took over all of the functions that the Compactron was designed for. \"Hybrid\" television sets produced in the early to mid-1970s made use of a combination of tubes (typically Compactrons), transistors, and integrated circuits in the same set. By the mid-1980s this type of tube was functionally obsolete. Compactrons simply don't exist in any TV sets designed after 1986. Other specialist uses of the tube declined in parallel with the television set manufacture. Manufacture of Compactrons ceased in the early 1990s. New old stock replacements for almost all Compactron types produced are easily found for sale on the Internet.\n\nOf note, in the 1960s, the 6BK11 Triple Triode Compactron tube was used by the Ampeg amplifier company in some of their guitar amps. Ampeg is the only guitar amp manufacturer that used this tube in their amps. The 6BK11 is in increasingly short supply today as an NOS replacement for stock tubes. NOS tubes are very important in guitar amps because they keep the amp performing as it was designed, which means it will produce the desired tone. There is a whole \"cottage industry\" that exists in producing vacuum tubes that are replacements for stock guitar amp tubes. The cottage industry exists because class A, open-ended, hand-wired, tube-driven guitar amps have a warm sound that is preferred by many musicians. The warmer sound of older amps means they command a premium resale value on the open market, and so in turn, any parts in the amp that may wear out over time or with use (including the vacuum tubes), also command similar values. The 6BK11 is a good example of this.\n", "id": "7722", "title": "Compactron"}
{"url": "https://en.wikipedia.org/wiki?curid=7723", "text": "Carmichael number\n\nIn number theory, a Carmichael number is a composite number formula_1 which satisfies the modular arithmetic congruence relation:\nfor all integers <math>1 which are relatively prime to formula_1. They are named for Robert Carmichael. The Carmichael numbers are the subset \"K\" of the Knödel numbers.\n\nFermat's little theorem states that if \"p\" is a prime number, then for any integer \"b\", the number \"b\" − \"b\" is an integer multiple of \"p\". Carmichael numbers are composite numbers which have the same property of modular arithmetic congruence. In fact, Carmichael numbers are also called Fermat pseudoprimes or absolute Fermat pseudoprimes. Carmichael numbers are important because they pass the Fermat primality test but are not actually prime. Since Carmichael numbers exist, this primality test cannot be relied upon to prove the primality of a number, although it can still be used to prove a number is composite. This makes tests based on Fermat's Little Theorem risky compared to other more stringent tests such as the Solovay-Strassen primality test or a strong pseudoprime test. Still, as numbers become larger, Carmichael numbers become very rare. For example, there are 20,138,200 Carmichael numbers between 1 and 10 (approximately one in 50 trillion (5*10) numbers).\n\nAn alternative and equivalent definition of Carmichael numbers is given by Korselt's criterion.\n\nIt follows from this theorem that all Carmichael numbers are odd, since any even composite number that is square-free (and hence has only one prime factor of two) will have at least one odd prime factor, and thus formula_9 results in an even dividing an odd, a contradiction. (The oddness of Carmichael numbers also follows from the fact that formula_10 is a Fermat witness for any even composite number.)\nFrom the criterion it also follows that Carmichael numbers are cyclic. Additionally, it follows that there are no Carmichael numbers with exactly two prime factors.\n\nKorselt was the first who observed the basic properties of Carmichael numbers, but he did not give any examples. In 1910, Carmichael found the first and smallest such number, 561, which explains the name \"Carmichael number\".\n\nThat 561 is a Carmichael number can be seen with Korselt's criterion. Indeed, formula_11 is square-free and formula_12, formula_13 and formula_14.\n\nThe next six Carmichael numbers are :\n\nThese first seven Carmichael numbers, from 561 to 8911, were all found by the Czech mathematician Václav Šimerka in 1885 (thus preceding not just Carmichael but also Korselt, although Šimerka did not find anything like Korselt's criterion). His work, however, remained unnoticed.\n\nJ. Chernick proved a theorem in 1939 which can be used to construct a subset of Carmichael numbers. The number formula_21 is a Carmichael number if its three factors are all prime. Whether this formula produces an infinite quantity of Carmichael numbers is an open question (though it is implied by Dickson's conjecture).\n\nPaul Erdős heuristically argued there should be infinitely many Carmichael numbers. In 1994 it was shown by W. R. (Red) Alford, Andrew Granville and Carl Pomerance that there really do exist infinitely many Carmichael numbers. Specifically, they showed that for sufficiently large formula_1, there are at least formula_23 Carmichael numbers between 1 and formula_1.\n\nLöh and Niebuhr in 1992 found some very large Carmichael numbers, including one with 1,101,518 factors and over 16 million digits.\n\nCarmichael numbers have at least three positive prime factors. For some fixed \"R\", there are infinitely many Carmichael numbers with exactly \"R\" factors; in fact, there are infinitely many such R.\n\nThe first Carmichael numbers with formula_25 prime factors are :\n\nThe first Carmichael numbers with 4 prime factors are :\n\nThe second Carmichael number (1105) can be expressed as the sum of two squares in more ways than any smaller number. The third Carmichael number (1729) is the Hardy-Ramanujan Number: the smallest number that can be expressed as the sum of two cubes (of positive numbers) in two different ways.\n\nLet formula_26 denote the number of Carmichael numbers less than or equal to formula_27. The distribution of Carmichael numbers by powers of 10 :\n\nIn 1953, Knödel proved the upper bound:\n\nfor some constant formula_29.\n\nIn 1956, Erdős improved the bound to\n\nfor some constant formula_31. He further gave a heuristic argument suggesting that this upper bound should be close to the true growth rate of formula_26. The table below gives approximate minimal values for the constant \"k\" in the Erdős bound for formula_33 as \"n\" grows:\n\nIn the other direction, Alford, Granville and Pomerance proved in 1994 that for sufficiently large \"X\",\n\nIn 2005, this bound was further improved by Harman to\n\nand then has subsequently improved the exponent to just over formula_36.\n\nRegarding the asymptotic distribution of Carmichael numbers, there have been several conjectures. In 1956, Erdős conjectured that there were formula_37 Carmichael numbers for \"X\" sufficiently large. In 1981, Pomerance sharpened Erdős' heuristic arguments to conjecture that there are\n\nCarmichael numbers up to \"X\". However, inside current computational ranges (such as the counts of Carmichael numbers performed by Pinch up to 10), these conjectures are not yet borne out by the data.\n\nThe notion of Carmichael number generalizes to a Carmichael ideal in any number field \"K\". For any nonzero prime ideal formula_39 in formula_40, we have <math>\\alpha^\n", "id": "7723", "title": "Carmichael number"}
{"url": "https://en.wikipedia.org/wiki?curid=7727", "text": "Controlled Substances Act\n\nThe Controlled Substances Act (CSA) is the statute establishing federal U.S. drug policy under which the manufacture, importation, possession, use and distribution of certain substances is regulated. It was passed by the 91st United States Congress as Title II of the Comprehensive Drug Abuse Prevention and Control Act of 1970 and signed into law by President Richard Nixon. The Act also served as the national implementing legislation for the Single Convention on Narcotic Drugs.\n\nThe legislation created five Schedules (classifications), with varying qualifications for a substance to be included in each. Two federal agencies, the Drug Enforcement Administration and the Food and Drug Administration, determine which substances are added to or removed from the various schedules, although the statute passed by Congress created the initial listing. Congress has sometimes scheduled other substances through legislation such as the Hillory J. Farias and Samantha Reid Date-Rape Prevention Act of 2000, which placed gamma hydroxybutyrate in Schedule I. Classification decisions are required to be made on criteria including potential for abuse (an undefined term), currently accepted medical use in treatment in the United States, and international treaties.\n\nThe nation first outlawed addictive drugs in the early 1900s and the International Opium Convention helped lead international agreements regulating trade. The Food and Drugs Act of 1906 was the beginning of over 200 laws concerning public health and consumer protections. Others were the Federal Food, Drug, and Cosmetic Act (1938), and the Kefauver Harris Amendment of 1962.\n\nIn 1969, President Richard Nixon announced that the Attorney General, John N. Mitchell, was preparing a comprehensive new measure to more effectively meet the narcotic and dangerous drug problems at the federal level by combining all existing federal laws into a single new statute. The CSA not only combined existing federal drug laws and expanded their scope but it also changed the nature of federal drug law policies, and expanded Federal law enforcement pertaining to controlled substances.\n\nTitle II, Part F of the Comprehensive Drug Abuse Prevention and Control Act of 1970 established the National Commission on Marijuana and Drug Abuse—known as the Shafer Commission after its chairman, Raymond P. Shafer—to study cannabis abuse in the United States. During his presentation of the commission's First Report to Congress, Shafer recommended the decriminalization of marijuana in small amounts, saying,\nRufus King notes that this stratagem was similar to that used by Harry Anslinger when he consolidated the previous anti-drug treaties into the Single Convention and took the opportunity to add new provisions that otherwise might have been unpalatable to the international community. According to David T. Courtwright, \"the Act was part of an omnibus reform package designed to rationalize, and in some respects to liberalize, American drug policy.\" (Courtwright noted that the Act became, not libertarian, but instead repressionistic to the point of tyrannical, in its intent.) It eliminated mandatory minimum sentences and provided support for drug treatment and research. King notes that the rehabilitation clauses were added as a compromise to Senator Jim Hughes, who favored a moderate approach. The bill, as introduced by Senator Everett Dirksen, ran to 91 pages. While it was being drafted, the Uniform Controlled Substances Act, to be passed by state legislatures, was also being drafted by the Department of Justice; its wording closely mirrored the Controlled Substances Act.\n\nSince its enactment in 1970, the Act has been amended several times:\n\nThe Controlled Substances Act consists of 2 subchapters. Subchapter I defines Schedules I-V, lists chemicals used in the manufacture of controlled substances, and differentiates lawful and unlawful manufacturing, distribution, and possession of controlled substances, including possession of Schedule I drugs for personal use; this subchapter also specifies the dollar amounts of fines and durations of prison terms for violations. Subchapter II describes the laws for exportation and importation of controlled substances, again specifying fines and prison terms for violations.\n\nThe Drug Enforcement Administration was established in 1973, combining the Bureau of Narcotics and Dangerous Drugs (BNDD) and Customs' drug agents. Proceedings to add, delete, or change the schedule of a drug or other substance may be initiated by the DEA, the Department of Health and Human Services (HHS), or by petition from any interested party, including the manufacturer of a drug, a medical society or association, a pharmacy association, a public interest group concerned with drug abuse, a state or local government agency, or an individual citizen. When a petition is received by the DEA, the agency begins its own investigation of the drug.\n\nThe DEA also may begin an investigation of a drug at any time based upon information received from laboratories, state and local law enforcement and regulatory agencies, or other sources of information. Once the DEA has collected the necessary data, the Deputy Administrator of DEA, requests from HHS a scientific and medical evaluation and recommendation as to whether the drug or other substance should be controlled or removed from control. This request is sent to the Assistant Secretary of Health of HHS. Then, HHS solicits information from the Commissioner of the Food and Drug Administration and evaluations and recommendations from the National Institute on Drug Abuse and, on occasion, from the scientific and medical community at large. The Assistant Secretary, by authority of the Secretary, compiles the information and transmits back to the DEA a medical and scientific evaluation regarding the drug or other substance, a recommendation as to whether the drug should be controlled, and in what schedule it should be placed.\n\nThe HHS recommendation on scheduling is binding to the extent that if HHS recommends, based on its medical and scientific evaluation, that the substance not be controlled, then the DEA may not control the substance. Once the DEA has received the scientific and medical evaluation from HHS, the DEA Administrator evaluates all available data and makes a final decision whether to propose that a drug or other substance be controlled and into which schedule it should be placed. Under certain circumstances, the Government may temporarily schedule a drug without following the normal procedure. An example is when international treaties require control of a substance. In addition, allows the Attorney General to temporarily place a substance in Schedule I \"to avoid an imminent hazard to the public safety\". Thirty days' notice is required before the order can be issued, and the scheduling expires after a year; however, the period may be extended six months if rulemaking proceedings to permanently schedule the drug are in progress. In any case, once these proceedings are complete, the temporary order is automatically vacated. Unlike ordinary scheduling proceedings, such temporary orders are not subject to judicial review.\n\nThe CSA also creates a closed system of distribution for those authorized to handle controlled substances. The cornerstone of this system is the registration of all those authorized by the DEA to handle controlled substances. All individuals and firms that are registered are required to maintain complete and accurate inventories and records of all transactions involving controlled substances, as well as security for the storage of controlled substances.\n\nThe Congressional findings in 21 USC §§ , , and state that a major purpose of the CSA is to \"enable the United States to meet all of its obligations\" under international treaties. The CSA bears many resemblances to these Conventions. Both the CSA and the treaties set out a system for classifying controlled substances in several Schedules in accordance with the binding scientific and medical findings of a public health authority. Under of the CSA, that authority is the Secretary of Health and Human Services (HHS). Under Article 3 of the Single Convention and Article 2 of the Convention on Psychotropic Substances, the World Health Organization is that authority.\n\nThe domestic and international legal nature of these treaty obligations must be considered in light of the supremacy of the United States Constitution over treaties or acts and the equality of treaties and Congressional acts. In \"Reid v. Covert\" the Supreme Court of the United States addressed both these issues directly and clearly holding:\n\nAccording to the Cato Institute, these treaties only bind (legally obligate) the United States to comply with them as long as that nation agrees to remain a state party to these treaties. The U.S. Congress and the President of the United States have the absolute sovereign right to withdraw from or abrogate at any time these two instruments, in accordance with said nation's Constitution, at which point these treaties will cease to bind that nation in any way, shape, or form.\n\nA provision for automatic compliance with treaty obligations is found at , which also establishes mechanisms for amending international drug control regulations to correspond with HHS findings on scientific and medical issues. If control of a substance is mandated by the Single Convention, the Attorney General is required to \"issue an order controlling such drug under the schedule he deems most appropriate to carry out such obligations,\" without regard to the normal scheduling procedure or the findings of the HHS Secretary. However, the Secretary has great influence over any drug scheduling proposal under the Single Convention, because requires the Secretary the power to \"evaluate the proposal and furnish a recommendation to the Secretary of State which shall be binding on the representative of the United States in discussions and negotiations relating to the proposal.\"\n\nSimilarly, if the United Nations Commission on Narcotic Drugs adds or transfers a substance to a Schedule established by the Convention on Psychotropic Substances, so that current U.S. regulations on the drug do not meet the treaty's requirements, the Secretary is required to issue a recommendation on how the substance should be scheduled under the CSA. If the Secretary agrees with the Commission's scheduling decision, he can recommend that the Attorney General initiate proceedings to reschedule the drug accordingly. If the HHS Secretary disagrees with the UN controls, however, the Attorney General must temporarily place the drug in Schedule IV or V (whichever meets the minimum requirements of the treaty) and exclude the substance from any regulations not mandated by the treaty, while the Secretary is required to request that the Secretary of State take action, through the Commission or the UN Economic and Social Council, to remove the drug from international control or transfer it to a different Schedule under the Convention. The temporary scheduling expires as soon as control is no longer needed to meet international treaty obligations.\n\nThis provision was invoked in 1984 to place Rohypnol (flunitrazepam) in Schedule IV. The drug did not then meet the Controlled Substances Act's criteria for scheduling; however, control was required by the Convention on Psychotropic Substances. In 1999, an FDA official explained to Congress:\nThe Cato Institute's \"Handbook for Congress\" calls for repealing the CSA, an action that would likely bring the United States into conflict with international law, were the United States not to exercise its sovereign right to withdraw from and/or abrogate the Single Convention on Narcotic Drugs and/or the 1971 Convention on Psychotropic Substances prior to repealing the Controlled Substances Act. The exception would be if the U.S. were to claim that the treaty obligations violate the United States Constitution. Many articles in these treaties—such as Article 35 and Article 36 of the Single Convention—are prefaced with phrases such as \"Having due regard to their constitutional, legal and administrative systems, the Parties shall . . .\" or \"Subject to its constitutional limitations, each Party shall . . .\" According to former United Nations Drug Control Programme Chief of Demand Reduction Cindy Fazey, \"This has been used by the USA not to implement part of article 3 of the 1988 Convention, which prevents inciting others to use narcotic or psychotropic drugs, on the basis that this would be in contravention of their constitutional amendment guaranteeing freedom of speech\".\n\nPlacing a drug or other substance in a certain Schedule or removing it from a certain Schedule is primarily based on 21 USC §§ , , , , , , and . Every schedule otherwise requires finding and specifying the \"potential for abuse\" before a substance can be placed in that schedule. The specific classification of any given drug or other substance is usually a source of controversy, as is the purpose and effectiveness of the entire regulatory scheme.\n\nThe term \"controlled substance\" means a drug or other substance, or immediate precursor, included in schedule I, II, III, IV, or V of part B of this subchapter. The term does not include distilled spirits, wine, malt beverages, or tobacco, as those terms are defined or used in subtitle E of the Internal Revenue Code of 1986.\n\nSome have argued that this is an important exemption, since alcohol and tobacco are two of the most widely used drugs in the United States. Also of significance, the exclusion of alcohol includes wine which is sacramentally used by many major religious denominations in the United States.\n\nSchedule I substances are described as those that have the following findings (a drug need not actually have them - this is simply the description given in the Schedule):\nNo prescriptions may be written for Schedule I substances, and such substances are subject to production quotas which the DEA imposes.\n\nUnder the DEA's interpretation of the CSA, a drug does not necessarily have to have the same \"high potential for abuse\" as heroin, for example, to merit placement in Schedule I:\nSentences for first-time, non-violent offenders convicted of trafficking in Schedule I drugs can easily turn into \"de facto\" life sentences when multiple sales are prosecuted in one proceeding. Sentences for violent offenders are much higher.\n\nDrugs listed in this control schedule include:\n\nSchedule II substances are those that have the following findings:\nExcept when dispensed directly by a practitioner, other than a pharmacist, to an ultimate user, no controlled substance in Schedule II, which is a prescription drug as determined under the Federal Food, Drug, and Cosmetic Act (21 USC 301 \"et seq.\"), may be dispensed without the written prescription of a practitioner, except that in emergency situations, as prescribed by the Secretary by regulation after consultation with the Attorney General, such drug may be dispensed upon oral prescription in accordance with section 503(b) of that Act (21 USC 353 (b)). With exceptions, an original prescription is always required even though faxing in a prescription in advance to a pharmacy by a prescriber is allowed. Prescriptions shall be retained in conformity with the requirements of section 827 of this title. No prescription for a controlled substance in schedule II may be refilled. Notably no emergency situation provisions exist outside the Controlled Substances Act's \"closed system\" although this closed system may be unavailable or nonfunctioning in the event of accidents in remote areas or disasters such as hurricanes and earthquakes. Acts which would widely be considered morally imperative remain offenses subject to heavy penalties.\n\nThese drugs vary in potency: for example fentanyl is about 80 times as potent as morphine (heroin is roughly four times as potent). More significantly, they vary in nature. Pharmacology and CSA scheduling have a weak relationship.\n\nBecause refills of prescriptions for Schedule II substances are not allowed, it can be burdensome to both the practitioner and the patient if the substances are to be used on a long-term basis. To provide relief, in 2007, was amended (at ) to allow practitioners to write up to three prescriptions at once, to provide up to a 90-day supply, specifying on each the earliest date on which it may be filled.\n\nDrugs in this schedule include:\n\nSchedule III substances are those that have the following findings:\n\nExcept when dispensed directly by a practitioner, other than a pharmacist, to an ultimate user, no controlled substance in schedule III or IV, which is a prescription drug as determined under the Federal Food, Drug, and Cosmetic Act (21 USC 301 \"et seq.\"), may be dispensed without a written or oral prescription in conformity with section 503(b) of that Act (21 USC 353 (b)). Such prescriptions may not be filled or refilled more than six months after the date thereof or be refilled more than five times after the date of the prescription unless renewed by the practitioner. A prescription for controlled substances in Schedules III, IV, and V issued by a practitioner, may be communicated either orally, in writing, or by facsimile to the pharmacist, and may be refilled if so authorized on the prescription or by call-in. Control of wholesale distribution is somewhat less stringent than Schedule II drugs. Provisions for emergency situations are less restrictive within the \"closed system\" of the Controlled Substances Act than for Schedule II though no schedule has provisions to address circumstances where the closed system is unavailable, nonfunctioning or otherwise inadequate.\n\nDrugs in this schedule include:\n\n\"Placement on schedules; findings required\nSchedule IV substances are those that have the following findings:\nControl measures are similar to Schedule III. Prescriptions for Schedule IV drugs may be refilled up to five times within a six-month period. A prescription for controlled substances in Schedules III, IV, and V issued by a practitioner, may be communicated either orally, in writing, or by facsimile to the pharmacist, and may be refilled if so authorized on the prescription or by call-in.\n\nDrugs in this schedule include:\n\nSchedule V substances are those that have the following findings:\nNo controlled substance in schedule V which is a drug may be distributed or dispensed other than for a medical purpose. A prescription for controlled substances in Schedules III, IV, and V issued by a practitioner, may be communicated either orally, in writing, or by facsimile to the pharmacist, and may be refilled if so authorized on the prescription or by call-in.\n\nDrugs in this schedule include:\n\nDue to pseudoephedrine (PSE) and ephedrine being widely used in the manufacture of methamphetamine, the U.S. Congress passed the Methamphetamine Precursor Control Act which places restrictions on the sale of any medicine containing pseudoephedrine. That bill was then superseded by the Combat Methamphetamine Epidemic Act of 2005, which was passed as an amendment to the Patriot Act renewal and included wider and more comprehensive restrictions on the sale of PSE-containing products. This law requires customer signature of a \"log-book\" and presentation of valid photo ID in order to purchase PSE-containing products from all retailers.\n\nAdditionally, the law restricts an individual to the retail purchase of no more than three packages or 3.6 grams of such product per day per purchase – and no more than 9 grams in a single month. A violation of this statute constitutes a misdemeanor. Retailers now commonly require PSE-containing products to be sold behind the pharmacy or service counter. This affects many preparations which were previously available over-the-counter without restriction, such as Actifed and its generic equivalents.\n\nThe UK Science and Technology Select Committee which suggested that the current system of recreational drug classification in the UK was arbitrary and unscientific and that a more scientific measure of harm should be used for classifying drugs. The new classification system suggested that heroin, cocaine, alcohol, benzodiazepines, methamphetamine, and tobacco have a high or a very high risk of harm or abuse potential, whilst cannabis, LSD, and MDMA were all below the two legal drugs in harm or abuse potential.\n\nIn 2007, \"The Lancet\" published a journal about researchers having introduced an alternative method for drug classification. This new system uses a \"nine category matrix of harm, with an expert Delphic procedure, to assess the harms of a range of illicit drugs in an evidence-based fashion.\" The new classification system suggested that alcohol and tobacco were in the mid-range of harm, while cannabis, LSD, and MDMA were all less harmful than the two legal drugs.\n\nThere has been criticism against the schedule classifications of the listed drugs and substances in the CSA, citing undefined terms.\nSome criticism has arisen due to research that has found several substances on the list of Schedule I substances to have actual accepted medical uses and low abuse potential, despite the requirement for a Schedule I listing mandating that any substance so scheduled have both a high potential for abuse and no accepted medical use. One such example is the legalization of marijuana in some capacity in over 23 states.\n\nSimilar legislation outside of the United States:\n\n", "id": "7727", "title": "Controlled Substances Act"}
{"url": "https://en.wikipedia.org/wiki?curid=7728", "text": "Claude Piron\n\nClaude Piron (26 February 1931 – 22 January 2008), also known by the pseudonym Johán Valano, was a Swiss psychologist, Esperantist, translator, and writer. He worked as a translator for the United Nations from 1956 to 1961 and then for the World Health Organization.\n\nHe was a prolific author of Esperanto works. He spoke Esperanto from childhood and used it in Japan, the People's Republic of China, Uzbekistan, Kazakhstan, in Africa and Latin America, and in nearly all the countries of Europe.\n\nPiron was a psychotherapist and taught from 1973 to 1994 in the psychology department at the University of Geneva in Switzerland. His French-language book \"Le défi des langues — Du gâchis au bon sens\" (The Language Challenge: From Chaos to Common Sense, 1994) is a kind of psychoanalysis of international communication. A Portuguese version, \"O desafio das linguas\", was published in 2002 (Campinas, São Paulo, Pontes).\n\nIn a lecture on the current system of international communication Piron argued that \"Esperanto relies entirely on innate reflexes\" and \"differs from all other languages in that you can always trust your natural tendency to generalize patterns... The same neuropsychological law...—called by Jean Piaget \"generalizing assimilation\"—applies to word formation as well as to grammar.\"\n\nHis diverse Esperanto writings include instructional books, books for beginners, novels, short stories, poems, articles and non-fiction books. His most famous works are \"Gerda malaperis!\" and \"La Bona Lingvo\" (The Good Language).\n\n\"Gerda malaperis!\" is a novella which uses basic grammar and vocabulary in the first chapter and builds up to expert Esperanto by the end, including word lists so that beginners may easily follow along.\n\nIn \"La Bona Lingvo\", Piron captures the basic linguistic and social aspects of Esperanto. He argues strongly for imaginative use of the basic Esperanto morpheme inventory and word-formation techniques, and against unnecessary importation of neologisms from European languages. He also presents the idea that, once one has learned enough vocabulary to express himself, it is easier to think clearly in Esperanto than in many other languages.\n\nPiron is the author of a book in French, \"Le bonheur clés en main\" (The Keys to Happiness), which distinguishes among pleasure, happiness and joy. He shows how one may avoid contributing to his own \"anti-happiness\" (\"l'anti-bonheur\") and how one may expand the areas of happiness in his life. Piron's view is that, while one may desire happiness, desire is not enough. Just as people must do certain things in order to become physically stronger, they must do certain things in order to become happier. Those are the things that he describes in this book.\n\n\n", "id": "7728", "title": "Claude Piron"}
{"url": "https://en.wikipedia.org/wiki?curid=7729", "text": "Captain America\n\nCaptain America is a fictional superhero appearing in American comic books published by Marvel Comics. Created by cartoonists Joe Simon and Jack Kirby, the character first appeared in \"Captain America Comics\" #1 (cover dated March 1941) from Timely Comics, a predecessor of Marvel Comics. Captain America was designed as a patriotic supersoldier who often fought the Axis powers of World War II and was Timely Comics' most popular character during the wartime period. The popularity of superheroes waned following the war and the \"Captain America\" comic book was discontinued in 1950, with a short-lived revival in 1953. Since Marvel Comics revived the character in 1964, Captain America has remained in publication.\n\nCaptain America wears a costume that bears an American flag motif, and is armed with a nearly indestructible shield that he throws at foes. The character is usually depicted as the alter ego of Steve Rogers, a frail young man enhanced to the peak of human perfection by an experimental serum to aid the United States government's efforts in World War II. Near the end of the war, he was trapped in ice and survived in suspended animation until he was revived in the present day. Although Captain America often struggles to maintain his ideals as a man out of his time with its modern realities, he remains a highly respected figure in his community which includes becoming the long-time leader of the Avengers.\n\nCaptain America was the first Marvel Comics character to have appeared in media outside comics with the release of the 1944 movie serial, \"Captain America\". Since then, the character has been featured in other films and television series, more recently in the Marvel Cinematic Universe (MCU) portrayed by Chris Evans in \"\", \"The Avengers\", \"\", \"\", \"\", and the upcoming \"\" (2018) and (2019).\n\nCaptain America is ranked sixth on IGN's \"Top 100 Comic Book Heroes of All Time\" in 2011, second in their list of \"The Top 50 Avengers\" in 2012, and second in their \"Top 25 best Marvel superheroes\" list in 2014.\n\nIn 1940, writer Joe Simon conceived the idea for Captain America and made a sketch of the character in costume. \"I wrote the name 'Super American' at the bottom of the page,\" Simon said in his autobiography, and then considered: \nSimon recalled in his autobiography that Timely Comics publisher Martin Goodman gave him the go-ahead and directed that a Captain America solo comic book series be published as soon as possible. Needing to fill a full comic with primarily one character's stories, Simon did not believe that his regular creative partner, artist Jack Kirby, could handle the workload alone:\n\nAl Liederman would ink that first issue, which was lettered by Simon and Kirby's regular letterer, Howard Ferguson.\n\nSimon said Captain America was a consciously political creation; he and Kirby were morally repulsed by the actions of Nazi Germany in the years leading up to the United States' involvement in World War II and felt war was inevitable: \"The opponents to the war were all quite well organized. We wanted to have our say too.\"\n\n\"Captain America Comics\" #1 — cover-dated March 1941 and on sale December 20, 1940, a year before the attack on Pearl Harbor, but a full year into World War II — showed the protagonist punching Nazi leader Adolf Hitler; it sold nearly one million copies. While most readers responded favorably to the comic, some took objection. Simon noted, \"When the first issue came out we got a lot of ... threatening letters and hate mail. Some people really opposed what Cap stood for.\" The threats, which included menacing groups of people loitering out on the street outside of the offices, proved so serious that police protection was posted with New York Mayor Fiorello La Guardia personally contacting Simon and Kirby to give his support.\n\nThough preceded as a \"patriotically themed superhero\" by MLJ's The Shield, Captain America immediately became the most prominent and enduring of that wave of superheroes introduced in American comic books prior to and during World War II, as evidenced by the unusual move at the time of premiering the character in his own title instead of an anthology title first. This popularity drew the attention and a complaint from MLJ that the character's triangular shield too closely resembled the chest symbol of their Shield character. In response, Goodman had Simon and Kirby create a distinctive round shield for issue 2, which went on to become an iconic element of the character. With his sidekick Bucky, Captain America faced Nazis, Japanese, and other threats to wartime America and the Allies. Stanley Lieber, now better known as Stan Lee, contributed to the character in issue #3 in the filler text story \"Captain America Foils the Traitor's Revenge\", which introduced the character's use of his shield as a returning throwing weapon. Captain America soon became Timely's most popular character and even had a fan-club called the \"Sentinels of Liberty\".\n\nCirculation figures remained close to a million copies per month after the debut issue, which outstripped even the circulation of news magazines such as \"Time\" during the period. After the Simon and Kirby team moved to DC Comics in late 1941, having produced \"Captain America Comics\" through issue #10 (January 1942), Al Avison and Syd Shores became regular pencillers of the celebrated title, with one generally inking over the other. The character was featured in \"All Winners Comics\" #1–19 (Summer 1941 – Fall 1946), \"Marvel Mystery Comics\" #80–84 and #86–92, \"USA Comics\" #6–17 (Dec. 1942 – Fall 1945), and \"All Select Comics\" #1–10 (Fall 1943 – Summer 1946).\n\nIn the post-war era, with the popularity of superheroes fading, Captain America led Timely's first superhero team, the All-Winners Squad, in its two published adventures, in \"All Winners Comics\" #19 and #21 (Fall–Winter 1946; there was no issue #20). After Bucky was shot and wounded in a 1948 \"Captain America\" story, he was succeeded by Captain America's girlfriend, Betsy Ross, who became the superheroine Golden Girl. \"Captain America Comics\" ran until issue #73 (July 1949), at which time the series was retitled \"Captain America's Weird Tales\" for two issues, with the finale being a horror/suspense anthology issue with no superheroes.\n\nAtlas Comics attempted to revive its superhero titles when it reintroduced Captain America, along with the original Human Torch and the Sub-Mariner, in \"Young Men\" #24 (Dec. 1953). Billed as \"Captain America, Commie Smasher!\" Captain America appeared during the next year in \"Young Men\" #24–28 and \"Men's Adventures\" #27–28, as well as in issues #76–78 of an eponymous title. Atlas' attempted superhero revival was a commercial failure, and the character's title was canceled with \"Captain America\" #78 (Sept. 1954).\n\nIn the Human Torch story titled \"Captain America\" in Marvel Comics' \"Strange Tales\" #114 (Nov. 1963), writer-editor Stan Lee and artist and co-plotter Jack Kirby depicted the brash young Fantastic Four member Johnny Storm, the Human Torch, in an exhibition performance with Captain America, described as a legendary World War II and 1950s superhero who has returned after many years of apparent retirement. The 18-page story ends with this Captain America revealed as an impostor: it was actually the villain the Acrobat, a former circus performer the Torch had defeated in \"Strange Tales\" #106, who broke two thieves out of jail, hoping to draw the police away while trying to rob the local bank. Afterward, Storm digs out an old comic book in which Captain America is shown to be Steve Rogers. A caption in the final panel says this story was a test to see if readers would like Captain America to return. According to Lee, fan response to the tryout was very enthusiastic.\n\nCaptain America was then formally reintroduced in \"The Avengers\" #4 (March 1964), which explained that in the final days of World War II, he had fallen from an experimental drone plane into the North Atlantic Ocean and spent decades frozen in a block of ice in a state of suspended animation. The hero found a new generation of readers as leader of that superhero team. Following the success of other Marvel characters introduced during the 1960s, Captain America was recast as a hero \"haunted by past memories, and trying to adapt to 1960s society\".\n\nAfter then guest-starring in the feature \"Iron Man\" in \"Tales of Suspense\" #58 (Oct. 1964), Captain America gained his own solo feature in that \"split book\", beginning the following issue. Issue #63 (March 1965), which retold Captain America's origin, through issue #71 (Nov. 1965) was a period feature set during World War II and co-starred Captain America's Golden Age sidekick, Bucky. Kirby drew all but two of the stories in \"Tales of Suspense,\" which became \"Captain America\" with #100 (April 1968); Gil Kane and John Romita Sr., each filled in once. Several stories were finished by penciller-inker George Tuska over Kirby layouts, with one finished by Romita Sr. and another by penciller Dick Ayers and inker John Tartaglione. Kirby's regular inkers on the series were Frank Giacoia (as \"Frank Ray\") and Joe Sinnott, though Don Heck and Golden Age Captain America artist Syd Shores inked one story each.\n\nThis series — considered \"Captain America\" volume one by comics researchers and historians, following the 1940s \"Captain America Comics\" and its 1950s numbering continuation of \"Tales of Suspense\" — ended with #454 (Aug. 1996).\n\nThis series was almost immediately followed by the 13-issue \"Captain America\" vol. 2 (Nov. 1996 – Nov. 1997, part of the \"Heroes Reborn\" crossover), the 50-issue \"Captain America\" vol. 3 (Jan. 1998 – Feb. 2002), the 32-issue \"Captain America\" vol. 4 (June 2002 – Dec. 2004), and \"Captain America\" vol. 5 (Jan. 2005 – Aug. 2011). Beginning with the 600th overall issue (Aug. 2009), \"Captain America\" resumed its original numbering, as if the series numbering had continued uninterrupted after #454.\n\nAs part of the aftermath of Marvel Comics' company-crossover storyline \"Civil War\", Steve Rogers was ostensibly killed in \"Captain America\" vol. 5, #25 (March 2007).\n\nThe storyline of Rogers' return began in issue #600. Rogers, who was not dead but caroming through time, returned to the present day in the six-issue miniseries \"\" (Sept. 2009 – March 2010).\n\nAfter Rogers' return, Barnes, at Rogers' insistence, continued as Captain America, beginning in the one-shot comic \"Captain America: Who Will Wield the Shield?\" (Feb. 2010). While Bucky Barnes continued adventuring in the pages of \"Captain America\", Steve Rogers received his own miniseries (\"Steve Rogers: Super-Soldier\") as well as taking on the leadership position in a new \"Secret Avengers\" ongoing series.\n\nSpinoff series included \"Captain America Sentinel of Liberty\" (Sept. 1998 – Aug. 1999) and \"Captain America and the Falcon\" (May 2004 – June 2005). The 1940s Captain America appeared alongside the 1940s Human Torch and Sub-Mariner in the 12-issue miniseries \"Avengers/Invaders\". The 2007 mini-series \"Captain America: The Chosen\", written by David Morrell and penciled by Mitchell Breitweiser, depicts a dying Steve Rogers' final minutes, at S.H.I.E.L.D. headquarters, as his spirit guides James Newman, a young American Marine fighting in Afghanistan. \"The Chosen\" is not part of the main Marvel Universe continuity.\n\nThe character, first as agent Steve Rogers and later after resuming his identity as Captain America, appeared as a regular character throughout the 2010–2013 \"Avengers\" series, from issue #1 (July 2010) through its final issue #34 (January 2013). The character appeared as agent Steve Rogers as a regular character in the 2010–2013 \"Secret Avengers\" series, from issue #1 (July 2010) through issue #21 (March 2012); the character made guest appearances as Captain America in issues #21.1, #22–23, #35, and the final issue of the series #37 (March 2013).\n\nMarvel stated in May 2011 that Rogers, following the public death of Bucky Barnes in the \"Fear Itself\" miniseries, would resume his Captain America identity in a sixth volume of \"Captain America\", by writer Ed Brubaker and artist Steve McNiven. The \"Captain America\" title continued from issue #620 featuring team up stories with Bucky (#620-#628), Hawkeye (#629-#632), Iron Man (#633–635), Namor (#635.1), and Black Widow (#636-#640), and the title ended its print run with issue #640.\n\nCaptain America is a regular character in \"Uncanny Avengers\" (2012), beginning with issue #1 as part of Marvel NOW!. \"Captain America\" vol. 7 was launched in November 2012 with a January 2013 cover date by writer Rick Remender and artist John Romita Jr..\n\nOn July 16, 2014 Marvel Comics announced that the mantle of Captain America would be passed on by Rogers (who in the most recent storyline has been turned into a 90-year-old man) to his long-time ally The Falcon, with the series being relaunched as \"All-New Captain America\".\n\nMarvel announced that Rogers will become Captain America once again in the comic series \"Captain America: Steve Rogers\".\n\nIn 1966 Joe Simon sued the owners of Marvel Comics, asserting that he – not Marvel – was legally entitled to renew the copyright upon the expiration of the original 28-year term. The two parties settled out of court, with Simon agreeing to a statement that the character had been created under terms of employment by the publisher, and therefore it was work for hire owned by them.\n\nIn 1999, Simon filed to claim the copyright to Captain America under a provision of the Copyright Act of 1976 which allowed the original creators of works that had been sold to corporations to reclaim them after the original 56-year copyright term (but not the longer term enacted by the new legislation) had expired. Marvel Entertainment challenged the claim, arguing that the settlement of Simon's 1966 suit made the character ineligible for termination of the copyright transfer. Simon and Marvel settled out of court in 2003, in a deal that paid Simon royalties for merchandising and licensing use of the character.\n\nSteven Rogers was born in the Lower East Side of Manhattan, New York City, in 1925 to poor Irish immigrants, Sarah and Joseph Rogers. Joseph died when Steve was a child, and Sarah died of pneumonia while Steve was a teen. By early 1940, before America's entry into World War II, Rogers is a tall, scrawny fine arts student specializing in illustration and a comic book writer and artist.\n\nDisturbed by the rise of the Third Reich, Rogers attempts to enlist but is rejected due to his frail body. His resolution attracts the notice of U.S. Army General Chester Phillips and \"Project: Rebirth\". Rogers is used as a test subject for the Super-Soldier project, receiving a special serum made by \"Dr. Josef Reinstein\", later retroactively changed to a code name for the scientist Abraham Erskine.\n\nThe serum is a success and transforms Steve Rogers into a nearly perfect human being with peak strength, agility, stamina, and intelligence. The success of the program leaves Erskine wondering about replicating the experiment on other human beings. The process itself has been inconsistently detailed: While in the original material Rogers is shown receiving injections of the Super-Serum, when the origin was retold in the 1960s, the Comic Code Authority had already put a veto over graphic description of drug intake and abuse, and thus the Super-Serum was retconned into an oral formula. Later accounts hint at a combination of oral and intravenous treatments with a strenuous training regimen, culminating in the Vita-Ray exposure.\n\nErskine refused to write down every crucial element of the treatment, leaving behind a flawed, imperfect knowledge of the steps. Thus, when the Nazi spy Heinz Kruger killed him, Erskine's method of creating new Super-Soldiers died. Captain America, in his first act after his transformation, avenges Erskine. In the 1941 origin story and in \"Tales of Suspense\" #63, Kruger dies when running into machinery but is not killed by Rogers; in the \"Captain America\" #109 and #255 revisions, Rogers causes the spy's death by punching him into machinery.\n\nUnable to create new Super-Soldiers and willing to hide the Project Rebirth fiasco, the American government casts Rogers as a patriotic superhero, able to counter the menace of the Red Skull as a counter-intelligence agent. He is supplied with a patriotic uniform of his own design, a bulletproof shield, a personal side arm, and the codename Captain America, while posing as a clumsy infantry private at Camp Lehigh in Virginia. He forms a friendship with the camp's teenage mascot, James Buchanan \"Bucky\" Barnes.\n\nBarnes learns of Rogers' dual identity and offers to keep the secret if he can become Captain America's sidekick. During their adventures, Franklin D. Roosevelt presents Captain America with a new shield, forged from an alloy of steel and vibranium, fused by an unknown catalyst, so effective that it replaces his own firearm. Throughout World War II, Captain America and Bucky fight the Nazi menace both on their own and as members of the superhero team the Invaders as seen in the 1970s comic of the same name. Captain America fights in numerous battles in World War II, primarily as a member of 1st Battalion, 26th Infantry Regiment \"Blue Spaders\". Captain America battles a number of criminal menaces on American soil, including a wide variety of costumed villains: the Wax Man, the Hangman, the Fang, the Black Talon, and the White Death, among others.\n\nIn addition to Bucky, Captain America was occasionally assisted by the Sentinels of Liberty. Sentinels of Liberty was the title given to members of the \"Captain America Comics\" fan club who Captain America sometimes addressed as an aside, or as characters in the \"Captain America Comics\" stories.\n\nIn late April 1945, during the closing days of World War II, Captain America and Bucky try to stop the villainous Baron Zemo from destroying an experimental drone plane. Zemo launches the plane with an armed explosive on it with Rogers and Barnes in hot pursuit. The pair reaches the plane just before take off. When Bucky tries to defuse the bomb, it explodes in mid-air. Rogers is hurled into the freezing waters of the North Atlantic. Both are presumed dead, though it is later revealed that neither one died.\n\nCaptain America appeared in comics for the next few years, changing from World War II-era hero fighting the Nazis to confronting the United States' newest enemy, Communism. The revival of the character in the mid-1950s was short-lived, and events during that time period are later retconned to show that multiple people operated using the code name to explain the changes in the character. These post World War II successors are listed as William Naslund and Jeffrey Mace.\n\nThe last of these other official Captains, William Burnside, was a history graduate enamored with the Captain America mythos, having his appearance surgically altered to resemble Rogers and legally changing his name to \"Steve Rogers\", becoming the new \"1950s Captain America\". He self-administered to himself and his pupil James \"Jack\" Monroe a flawed, incomplete copy of the Super-Serum, which made no mention about the necessary Vita-Ray portion of the treatment. As a result, while Burnside and Monroe became the new Captain America and Bucky, they became violently paranoid, often raving about innocent people being communist sympathizers during the height of the Red Scare of the 1950s. Their insanity forced the U.S. government to place them in indefinite cryogenic storage until they could be cured of their mental illness. Monroe would later be cured and assume the Nomad identity.\n\nYears later, the superhero team the Avengers discovers Steve Rogers' body in the North Atlantic. After he revives, they piece together that Rogers has been preserved in a block of ice since 1945, surviving because of his enhancements from Project: Rebirth. The block began to melt after the Sub-Mariner, enraged that an Inuit tribe is worshipping the frozen figure, throws it into the ocean. Rogers accepts membership in the Avengers, and his experience in individual combat service and his time with the Invaders makes him a valuable asset. He quickly assumes leadership and has typically returned to that position throughout the team's history.\n\nCaptain America is plagued by guilt for having been unable to prevent Bucky's death. Although he takes the young Rick Jones (who closely resembles Bucky) under his tutelage, he refuses for some time to allow Jones to take up the Bucky identity, not wishing to be responsible for another youth's death. Insisting that his hero move on from that loss, Jones convinces Rogers to let him don the Bucky costume, but this partnership lasts only a short time; a disguised Red Skull, impersonating Rogers with the help of the Cosmic Cube, drives Jones away.\n\nRogers reunites with his old war comrade Nick Fury, who is similarly well-preserved due to the \"Infinity Formula\". As a result, Rogers regularly undertakes missions for the security agency S.H.I.E.L.D., for which Fury is public director. Through Fury, Rogers befriends Sharon Carter, a S.H.I.E.L.D. agent, with whom he eventually begins a romantic relationship.\n\nRogers later meets and trains Sam Wilson, who becomes the superhero the Falcon, the first African-American superhero in mainstream comic books. The characters established an enduring friendship and adventuring partnership, sharing the series title for some time as \"Captain America and the Falcon\". The two later encounter the revived but still insane 1950s Captain America. Although Rogers and the Falcon defeat the faux Rogers and Jack Monroe, Rogers becomes deeply disturbed that he could have suffered his counterpart's fate. During this period, Rogers temporarily gains super strength.\n\nThe series dealt with the Marvel Universe's version of the Watergate scandal, making Rogers so uncertain about his role that he abandons his Captain America identity in favor of one called Nomad, emphasizing the word's meaning as \"man without a country\". During this time, several men unsuccessfully assume the Captain America identity. Rogers eventually re-assumes it after coming to consider that the identity could be a symbol of American ideals and not its government; it's a personal conviction epitomized when he later confronted a corrupt Army officer attempting to manipulate him by appealing to his loyalty, \"I'm loyal to nothing, General ... except the [American] Dream.\" Jack Monroe, cured of his mental instability, later takes up the Nomad alias. Sharon Carter is believed to have been killed while under the mind control of Dr. Faustus.\n\nThe 1980s included a run by writer Roger Stern and artist John Byrne. Stern had Rogers consider a run for President of the United States in \"Captain America\" #250 (June 1980), an idea originally developed by Roger McKenzie and Don Perlin. Stern, in his capacity as editor of the title, originally rejected the idea but later changed his mind about the concept. McKenzie and Perlin received credit for the idea on the letters page at Stern's insistence. Stern additionally introduced a new love interest, law student Bernie Rosenthal, in \"Captain America\" #248 (Aug. 1980).\nWriter J. M. DeMatteis revealed the true face and full origin of the Red Skull in \"Captain America\" #298–300, and had Captain America take on Jack Monroe, Nomad, as a partner for a time. Around this time, the heroes gathered by the Beyonder elect Rogers as leader during their stay on Battleworld in the 1984 miniseries \"Secret Wars\". Homophobia is dealt with as Rogers runs into a childhood friend named Arnold Roth who is gay.\n\nMark Gruenwald became the writer of the series with issue #307 (July 1985) and wrote 137 issues for 10 consecutive years from until #443 (Sept. 1995), the most issues by any single author in the character's history. Gruenwald created several new foes, including Crossbones and the Serpent Society. Other Gruenwald characters included Diamondback, Super Patriot, and Demolition Man. Gruenwald explored numerous political and social themes as well, such as extreme idealism when Captain America fights the anti-nationalist terrorist Flag-Smasher; and vigilantism when he hunts the murderous Scourge of the Underworld.\n\nRogers receives a large back-pay reimbursement dating back to his disappearance at the end of World War II, and a government commission orders him to work directly for the U.S. government. Already troubled by the corruption he had encountered with the Nuke incident in New York City, Rogers chooses instead to resign his identity, and then takes the alias of \"the Captain\". A replacement Captain America, John Walker, struggles to emulate Rogers' ideals until pressure from hidden enemies helps to drive Walker insane. Rogers returns to the Captain America identity while a recovered Walker becomes the U.S. Agent.\n\nSometime afterward, Rogers avoids the explosion of a methamphetamine lab, but the drug triggers a chemical reaction in the Super-Soldier serum in his system. To combat the reaction, Rogers has the serum removed from his body and trains constantly to maintain his physical condition. A retcon later establishes that the serum was not a drug \"per se\", which would have metabolized out of his system, but in fact a virus-like organism that effected a biochemical and genetic change. This additionally explained how nemesis the Red Skull, who at the time inhabited a body cloned from Rogers' cells, has the formula in his body.\n\nBecause of his altered biochemistry, Rogers' body begins to deteriorate, and for a time he must wear a powered exoskeleton and is eventually placed again in suspended animation. During this time, he is given a transfusion of blood from the Red Skull, which cures his condition and stabilizes the Super-Soldier virus in his system. Captain America returns to crime fighting and the Avengers.\n\nFollowing Gruenwald's departure from the series, Mark Waid took over and resurrected Sharon Carter as Cap's love interest. The title was then relaunched under Rob Liefeld as Cap became part of the Heroes Reborn universe for 13 issues before another relaunch restored Waid to the title in an arc that saw Cap lose his shield for a time using an energy based shield as a temporary replacement. Following Waid's run, Dan Jurgens took over and introduced new foe Protocide, a failed recipient of the Super Soldier serum prior to the experiment that successfully created Rogers. Some time after this, Rogers' original shield was retrieved, but subtle damage sustained during the battle with the Beyonder resulted in it being shattered and a 'vibranium cancer' being triggered that would destroy all vibranium in the world, with Rogers nearly being forced to destroy the shield before a confrontation with the villain Klaw saw Klaw's attacks unwittingly repair the shield's fractured molecular bonds and negate the cancer.\n\nIn the aftermath of the September 11 terrorist attacks, Rogers reveals his identity to the world and establishes a residence in the Red Hook neighborhood of Brooklyn, New York, as seen in \"Captain America\" vol. 4, #1–7 (June 2002 – Feb. 2003). Following the disbandment of the Avengers in the \"Avengers Disassembled\" story arc, Rogers, now employed by S.H.I.E.L.D., discovers Bucky is alive, having been saved and deployed by the Soviets as the Winter Soldier. Rogers resumes his on-again, off-again relationship with S.H.I.E.L.D. agent Sharon Carter. After a mass supervillain break-out of the Raft, Rogers and Tony Stark assemble a new team of Avengers to hunt the escapees.\n\nIn the 2006–2007 company-wide story arc \"Civil War\", and its anchoring, seven-issue miniseries, \"Civil War\" (July 2006 – Jan. 2007), Rogers opposes the new mandatory federal registration of super-powered beings, and leads the underground anti-registration movement. After significant rancor and danger to the public as the two sides clash, Captain America voluntarily surrenders and orders the Anti-Registration forces to stand down, feeling that the fight had reached a point where they were just fighting rather than trying to make a stand on principle.\n\nIn the story arc \"The Death of Captain America\", Rogers is indicted on criminal charges for his anti-registration efforts, and in \"Captain America\" vol. 5, #25 (April 2007) is shot outside a federal courthouse; taken to a hospital, he is pronounced dead. The assassination, orchestrated by the Red Skull, involves Crossbones as a sniper and Dr. Faustus, who poses as a S.H.I.E.L.D. psychiatrist and gives Carter a hypnotic suggestion to surreptitiously shoot Rogers at close range during the chaos surrounding the sniper shot.\n\nThe miniseries \"\" #1–5 (June–Aug. 2007) follows the stunned superhero community after the apparent assassination. Captain America is purportedly laid to rest in Arlington National Cemetery, but Tony Stark (Iron Man) and others have actually returned Rogers' body to the Arctic where Rogers had been found years before, and whereupon Namor swore to guard him. In \"Captain America\" vol. 5, #30 (Sept. 2007), Stark, who previously had tried to convince close friend and colleague of both Rogers and Stark Clint Barton to take up the role, receives a letter containing Rogers' request that Bucky become the next Captain America, which Bucky agrees to do four issues later. Adopting the original shield, he dons a new costume incorporating a pistol and a knife. The Norse god superhero Thor communicates with what appears to be Rogers' spirit on the first anniversary of Rogers' death, setting up a massive electromagnetic surge to shut down global communications and give Rogers' spirit a moment of silence instead of the debates about what he would have done if he was still alive.\n\n\"\" #1 (Aug. 2009) reveals that Rogers did not die, and that the gun Sharon Carter had been hypnotized to use had caused Rogers to phase in and out of space and time, appearing at events in his lifetime and fighting various battles. Although Rogers manages to relay a message to the future by giving a time-delayed command to the Vision during the Kree-Skrull War, the Skull returns Rogers to the present, where he takes control of Rogers' mind and body. Rogers eventually regains control, and, with help from his allies, defeats the Skull in the fourth and final issues of this miniseries. In the subsequent one-shot comic \"Captain America: Who Will Wield the Shield?\", Rogers formally grants Bucky his Captain America shield and asks his former sidekick to continue as Captain America. The American President grants Rogers a full pardon for his anti-registration actions.\n\nFollowing the company-wide \"Dark Reign\" and \"Siege\" story arcs, the Steve Rogers character became part of the \"Heroic Age\" arc.\n\nThe U.S. president appoints Rogers, in his civilian identity, as \"\"America's top cop\"\" and head of the nation's security, replacing Norman Osborn as the tenth \"Executive Director of S.H.I.E.L.D.\". The Superhuman Registration Act is repealed and Rogers re-establishes the superhero team the Avengers, spearheaded by Iron Man, Thor, and Bucky as Captain America. In the miniseries \"Steve Rogers: Super Soldier\", he encounters Jacob Erskine, the grandson of Professor Abraham Erskine and the son of Tyler Paxton, one of Rogers' fellow volunteers in the Super-Soldier program. Shortly afterward, Rogers becomes leader of the Secret Avengers, a black-ops superhero team.\n\nDuring the \"Fear Itself\" storyline, Steve Rogers is present when the threat of the Serpent is known. Following the apparent death of Bucky at the hands of Sin (in the form of Skadi), Steve Rogers ends up changing into his Captain America uniform. When the Avengers and the New Avengers are fighting Skadi, the Serpent ends up joining the battle and breaks Captain America's shield with his bare hands. Captain America and the Avengers teams end up forming a militia for a last stand against the forces of the Serpent. When it comes to the final battle, Captain America uses Thor's hammer to fight Skadi until Thor manages to kill the Serpent. In the aftermath of the battle, Iron Man presents him with his reforged shield, now stronger for its uru-infused enhancements despite the scar it bears. It is then revealed that Captain America, Nick Fury, and Black Widow are the only ones who know that Bucky actually survived the fight with Skadi as Bucky resumes his identity as Winter Soldier.\n\nIn the \"Avengers vs. X-Men\" story arc, Captain America attempts to apprehend Hope Summers of the X-Men. She is the targeted vessel for the Phoenix Force, a destructive cosmic entity. Captain America believes that this Phoenix Force is too dangerous to entrust in one person and seeks to prevent Hope from having it. Cyclops and the X-Men believe that the Phoenix Force will save their race, and oppose Captain America's wishes. The result is a series of battles that eventually take both teams to the blue area of the moon. The Phoenix Force eventually possesses the five X-Men present, leaving the Avengers at an extreme disadvantage. The \"Phoenix Five\", who become corrupted by the power of the Phoenix, are eventually defeated and scattered, with Cyclops imprisoned for turning the world into a police state and murdering Charles Xavier after being pushed too far, only for him to note that, in the end, he was proven right about the Phoenix's intentions. From there, Captain America proceeds to assemble the Avengers Unity Squad, a new team of Avengers composed of both classic Avengers and X-Men.\n\nAfter Cyclops was incarcerated, and Steve accepted the Avengers should have done more to help mutants, and allowed the world to hate them, he started planning a new sub-team of Avengers in the hopes of unifying mutant and humankind alike. He chose Havok to lead his team and become the new face to represent mutants as Professor X and Cyclops once were.\n\nTheir first threat was the return of the Red Skull- more specifically, a clone of the Skull created in 1942 and kept in stasis in the event of the original's death- who usurped Professor X's body to provide himself with telepathic powers, which he would use to provoke citizens of New York into a mass assault against mutants, or anyone who could be one, and force the Scarlet Witch and Rogue to allow themselves to be attacked. With the help of the S-Man Honest John, he managed to even manipulate Thor.\n\nThe Red Skull's skills were still erratic, and could not completely control Captain America, an attack against him was enough of a distraction to lose control of Rogue and the Scarlet Witch. After being overpowered by the rest of the Uncanny Avengers, the Red Skull escapes, but promises to return. In the aftermath, both Rogue and the Scarlet Witch joined the team.\n\nDuring a battle with an enemy called the Iron Nail, the Super-Soldier Serum within Rogers's body was neutralized, causing him to age rapidly to match his chronological age of over 90 years. No longer able to take part in field missions but retaining his sharp mind, Rogers decided to take on a role as mission coordinator, organizing the Avengers' plans of attack from the mansion, while appointing Sam Wilson as his official \"replacement\" as Captain America. When various Avengers and X-Men were inverted into villains and several villains inverted into heroism due to a miscast spell by the Scarlet Witch and Doctor Doom, Rogers not only coordinated the efforts of Spider-Man and the inverted villains, now called the \"Astonishing Avengers\", but also donned his old armor to battle the inverted Falcon, until the heroes and villains could be returned to normal with the aid of the White Skull (the inverted Red Skull).\n\nDuring the \"Time Runs Out\" storyline, Steve Rogers wears armor when he confronts Iron Man. The ensuing fight between the two old friends led Steve Rogers to force Iron Man to admit that he had lied to him and all of their allies, when he had known about the incursions all along, but Iron Man also confessed that he wouldn't change a thing. The final incursion started and Earth-1610 started approaching Earth-616 while Iron Man and Steve Rogers kept fighting. Earth-1610's S.H.I.E.L.D. launched a full invasion to destroy Earth-616, where Tony Stark and Steve Rogers were crushed by a Helicarrier.\n\nAs part of the \"All-New, All-Different Marvel\", Steve Rogers became the new Chief of Civilian Oversight for S.H.I.E.L.D. He returned to the Uncanny Avengers where the team is now using the Schaefer Theater as their headquarters.\n\nSteve Rogers later has an encounter with an alternate Logan from Earth-807128. After defeating Logan and bringing him to Alberta, Canada, Rogers tried to \"reassure\" Logan that this was not \"his\" past by showing him the adamantium-frozen body of Earth-616's Logan. This site reminds Logan of the need to enjoy being alive rather than brooding over the ghosts of his past. Although he told Steve Rogers what he had experienced in his timeline, Logan declined Steve's offer of help.\n\nDuring the \"\" storyline, Steve Rogers learns from Rick Jones that S.H.I.E.L.D. has established Pleasant Hill, a gated community where they use Kobik to transform villains into ordinary citizens. When Rogers is brought to Pleasant Hill, he confronts Maria Hill about the Kobik project. Their argument is interrupted when Baron Helmut Zemo and Fixer restore the inmates to normal. After Hill is injured, Rogers convinces Zemo to let Hill get medical attention. Rogers is then escorted to Dr. Erik Selvig's clinic by Father Patrick. Selvig tells Rogers that Kobik is at the Pleasant Hill Bowling Alley. During an attempt to reason with Kobik, Rogers is attacked by Crossbones. Before Rogers can be killed, Kobik uses her abilities to restore him back to his prime. Declaring that \"It's good to be back,\" Steve defeats Crossbones as Captain America and the Winter Soldier catch up with him. They resume their search for Kobik, and discover that Baron Zemo had Fixer invent a device that would make Kobik subservient to them. Rogers rallies the heroes so that they can take the fight to Zemo. In the aftermath of the incident, Steve and Sam plan to keep what happened at Pleasant Hill under wraps for the time being.\n\nIn \"Captain America: Steve Rogers\" #1 (July 2016), the final panel apparently revealed that Rogers has been a Hydra double-agent since his early youth. This is subsequently revealed to be the result of false memories implanted by Kobik when she restored Rogers' youth as she was secretly under the control of the Red Skull's clone at the time, although he has some true memories on how he knows Hydra in the past. Some of Rogers' original heroic attributes remain intact, such as covering the death of another Hydra member within S.H.I.E.L.D., Erik Selvig, as well as knowing of Jack Flag's tragic life and his immortality, which is why Steve pushes him from Zemo's airplane (resulting in coma, not death). Additionally, it is revealed that Rogers' abusive father, Joseph, was actually killed by Hydra, and that Hydra deceived him into thinking Joseph died of a heart attack. It is also revealed that Rogers witnessed his mother, Sarah, being killed by Sinclair's Hydra goons and kidnapped him, which is the reason why Steve held a grudge towards Hydra's evilness and plans to kill the Red Skull's clone and restore Hydra's lost honor. As part of his long-term plans, Steve further compromised Sam Wilson's current image as 'the' Captain America by using his greater familiarity with the shield to deliberately put Wilson in a position where he would be unable to use the shield to save a senator from Flag-Smasher, with the final goal of demoralizing Sam to the point where he will return the shield to Rogers of his own free will, not wanting to kill Wilson and risk creating a martyr.\n\nDuring the \"Civil War II\" storyline, with the discovery of new Inhuman Ulysses – who has the ability to \"predict\" the future by calculating complex patterns – Rogers has set out to prevent Ulysses from learning of his true plans and allegiance. Rogers does this by \"forcing\" certain predictions on him, such as anonymously providing Bruce Banner with new gamma research to provoke a vision that would drive the Avengers to kill Banner, although this plan has apparently backfired with a recent vision showing the new Spider-Man standing over the dead Steve Rogers. Despite this revelation, Rogers presents himself as the voice of reason by allowing Spider-Man to flee with Thor. This inspires doubt in Tony Stark for his current stance by suggesting that he is just acting against Danvers because he does not like being top dog. He then goes to Washington, D.C., the location seen in Ulysses' vision, to talk to Spider-Man, who was trying to understand the vision like he was. When Captain Marvel attempts to arrest Spider-Man, Tony, wearing the War Machine armor, confronts her and the two begin to fight.\n\nLater, Rogers goes to Sokovia and joins forces with Black Widow to liberate freedom fighters from a prison so they can reclaim their country. After that, he goes to his base where Doctor Selvig expresses concern of his plan to kill the Red Skull. He then reveals that he has Baron Zemo in a cell, planning to recruit him.\n\nCaptain America has no superhuman powers, but through the Super-Soldier Serum and \"Vita-Ray\" treatment, he is transformed and his strength, endurance, agility, speed, reflexes, durability, and healing are at the zenith of natural human potential. Rogers' body regularly replenishes the super-soldier serum; it does not wear off.\n\nAlthough he lacks superhuman strength, Captain America is one of the few mortal beings who has been deemed worthy enough to wield Thor's hammer Mjolnir.\n\nRogers' battle experience and training make him an expert tactician and an excellent field commander, with his teammates frequently deferring to his orders in battle. Thor has stated that Rogers is one of the very few humans he will take orders from and follow \"through the gates of Hades\". Rogers' reflexes and senses are extraordinarily keen. He has blended judo, western boxing, kickboxing, and gymnastics into his own unique fighting style and is a master of multiple martial arts. Years of practice with his near-indestructible shield make him able to aim and throw it with almost unerring accuracy. His skill with his shield is such that he can attack multiple targets in succession with a single throw or even cause a boomerang-like return from a throw to attack an enemy from behind. In canon, he is regarded by other skilled fighters as one of the best hand-to-hand combatants in the Marvel Universe, limited only by his human physique. Although the super-soldier serum is an important part of his strength, Rogers has shown himself still sufficiently capable against stronger opponents, even when the serum has been deactivated reverting him to his pre-Captain America physique.\n\nRogers has vast U.S. military knowledge and is often shown to be familiar with ongoing, classified Defense Department operations. He is an expert in combat strategy, survival, acrobatics, parkour, military strategy, piloting, and demolitions. Despite his high profile as one of the world's most popular and recognizable superheroes, Rogers has a broad understanding of the espionage community, largely through his ongoing relationship with S.H.I.E.L.D. He is a talented artist, and has worked on the \"Captain America\" comic book published in the Marvel universe. Other career fields include commercial arts, teaching high school history, and law enforcement.\n\nThe formula enhances all of his metabolic functions and prevents the build-up of fatigue poisons in his muscles, giving him endurance far in excess of an ordinary human being. This accounts for many of his extraordinary feats, including bench pressing 1200 pounds (545 kg) and running a mile (1.6 km) in 73 seconds (49 mph/78 kph). Furthermore, his enhancements are the reason why he was able to survive being frozen in suspended animation for decades. He is highly resistant to hypnosis or gases that could limit his focus. The secrets of creating a super-soldier were lost with the death of its creator, Dr. Abraham Erskine. In the ensuing decades there have been numerous attempts to recreate Erskine's treatment, only to have them end in failure. Even worse, the attempts have instead often created psychopathic supervillains of which Captain America's 1950s imitator and Nuke are the most notorious examples.\n\nCaptain America has used multiple shields throughout his history, the most prevalent of which is a nigh-indestructible disc-shaped shield made from an experimental alloy of steel and the fictional vibranium. The shield was cast by American metallurgist Dr. Myron MacLain, who was contracted by the U.S. government, from orders of President Franklin D. Roosevelt, to create an impenetrable substance to use for tanks during World War II. This alloy was created by accident and never duplicated, although efforts to reverse-engineer it resulted in the discovery of adamantium.\n\nCaptain America often uses his shield as an offensive throwing weapon. The first instance of Captain America's trademark ricocheting shield-toss occurs in Stan Lee's first comics writing, the two-page text story \"Captain America Foils the Traitor's Revenge\" in \"Captain America Comics\" #3 (May 1941). The legacy of the shield among other comics characters includes the time-traveling mutant superhero Cable telling Captain America that his shield still exists in one of the possible futures; Cable carries it into battle and brandishes it as a symbol.\n\nWhen without his trademark shield, Captain America sometimes uses other shields made from less durable metals such as steel, or even a photonic energy shield designed to mimic a vibranium matrix. Rogers, having relinquished his regular shield to Barnes, carried a variant of the energy shield which can be used with either arm, and used to either block attacks or as an improvised offensive weapon able to cut through metal with relative ease. Much like his Vibranium shield, the energy shield can be thrown, including ricocheting off multiple surfaces and returning to his hand.\n\nCaptain America's uniform is made of a fire-retardant material, and he wears a lightweight, bulletproof duralumin scale armor beneath his uniform for added protection. Originally, Rogers' mask was a separate piece of material, but an early engagement had it dislodged, thus almost exposing his identity. To prevent a recurrence of the situation, Rogers modified the mask with connecting material to his uniform, an added benefit of which was extending his armor to cover his previously exposed neck. As a member of the Avengers, Rogers has an Avengers priority card, which serves as a communications device.\n\nCaptain America has used a custom specialized motorcycle, modified by the S.H.I.E.L.D. weapons laboratory, as well as a custom-built battle van, constructed by the Wakanda Design Group with the ability to change its color for disguise purposes (red, white and blue), and fitted to store and conceal the custom motorcycle in its rear section with a frame that allows Rogers to launch from the vehicle riding it.\n\nCaptain America has faced numerous foes in over 70 years of published adventures. Many of his recurring foes embody ideologies contrary to the American values that Captain America is shown to strive for and believes in. Some examples of these opposing values are Nazism (Red Skull, Baron Zemo), Neo-Nazism (Crossbones, Doctor Faustus), technocratic fascism (AIM, Arnim Zola), Communism (Aleksander Lukin), anarchism (Flag Smasher) and international and domestic terrorism (Hydra).\n\n", "id": "7729", "title": "Captain America"}
{"url": "https://en.wikipedia.org/wiki?curid=7730", "text": "Cyclops (disambiguation)\n\nA cyclops is a one-eyed monster in Greek mythology.\n\nCyclops or The Cyclops may also refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "7730", "title": "Cyclops (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=7731", "text": "Christian countercult movement\n\nThe Christian countercult movement or Christian anti-cult movement is a social movement of certain Protestant evangelical and fundamentalist and other Christian ministries (\"discernment ministries\") and individual activists who oppose religious sects they consider \"cults\".\n\nChristian countercult activism stems mainly from evangelicalism or fundamentalism. The countercult movement asserts that particular Christian sects whose beliefs they deem to be partially or wholly not in accordance with the Bible are erroneous. It also states that a religious sect can be considered a cult if its beliefs involve a denial of what they view as any of the essential Christian teachings such as salvation, the Trinity, Jesus himself as a person, the ministry and miracles of Jesus, his crucifixion, his resurrection, the Second Coming and the Rapture.\n\nCountercult ministries often concern themselves with religious sects that consider themselves Christian but hold beliefs thought to contradict the Bible, including The Church of Jesus Christ of Latter-day Saints, the Unification Church, Christian Science, and Jehovah's Witnesses. Anti-Catholic movements have led Protestants to classify Catholicism as a cult. John Highham described anti-Catholicism as \"the most luxuriant, tenacious tradition of paranoiac agitation in American history\". Some also denounce non-Christian religions such as Islam, Wicca, Paganism, New Age groups, Buddhism, Hinduism and other religions.\n\nCountercult literature usually expresses doctrinal or theological concerns and a missionary or apologetic purpose. It presents a rebuttal by emphasizing the teachings of the Bible against the beliefs of non-fundamental Christian sects. Christian countercult activist writers also emphasize the need for Christians to evangelize to followers of cults. Some Christians also share concerns similar to those of the secular anti-cult movement.\n\nThe movement publishes its views through a variety of media including books, magazines, and newsletters, radio broadcasting; audio and videocassette production, direct mail appeals, proactive evangelistic encounters, professional and avocational Internet Websites, as well as lecture series, training workshops and counter cult conferences.\n\nChristians have applied theological criteria to assess the teachings of non-orthodox movements throughout church history. The Apostles themselves were involved in challenging the doctrines and claims of various teachers. The Apostle Paul wrote an entire epistle, Galatians, antagonistic to the teachings of a Jewish sect that claimed adherence to the teachings of both Jesus and Moses (cf. Acts 15: & Gal. 1:6-10). The Apostle John devoted his first Epistle to countering early proto-gnostic cults that had arisen in the first century, all claiming to be \"Christian\" (1 Jn. 2:19).\n\nThe early Church in the post-apostolic period was much more involved in \"defending its frontiers against alternative soteriologies — either by defining its own position with greater and greater exactness, or by attacking other religions, and particularly the Hellenistic mysteries.\" In fact, a good deal of the early Christian literature is devoted to the exposure and refutation of unorthodox theology, mystery religions and Gnostic groups. Irenaeus, Tertullian and Hippolytus of Rome were among the greatest early Christian apologists who engaged in critical analyses of unorthodox theology, Greco-Roman pagan religions, and Gnostic groups.\n\nIn the Protestant traditions some of the earliest writings opposing unorthodox groups like Swedenborg's teachings, can be traced back to John Wesley, Alexander Campbell and Princeton Theological Seminary theologians like Charles Hodge and B. B. Warfield. The first known usage of the term \"cult\" by a Protestant apologist to denote a group is heretical or unorthodox is in \"Anti-Christian Cults\" by A. H. Barrington, published in 1898.\n\nQuite a few of the pioneering apologists were Baptist pastors, like I. M. Haldeman, or participants in the Plymouth Brethren, like William C. Irvine and Sydney Watson. Watson wrote a series of didactic novels like \"Escaped from the Snare: Christian Science\", \"Bewitched by Spiritualism\", and \"The Gilded Lie\", as warnings of the dangers posed by cultic groups. Watson's use of fiction to counter the cults has been repeated by later novelists like Frank E. Peretti.\n\nThe early twentieth century apologists generally applied the words \"heresy\" and \"sects\" to groups like the Christadelphians, Mormons, Jehovah's Witnesses, Spiritualists, and Theosophists. This was reflected in several chapters contributed to the multi-volume work released in 1915 \"The Fundamentals\", where apologists criticised the teachings of Charles Taze Russell, Mary Baker Eddy, the Mormons and Spiritualists.\n\nSince at least the 1940s, the approach of traditional Christians was to apply the meaning of \"cult\" such that it included those religious groups who use other scriptures beside the Bible or have teachings and practices deviating from traditional Christian teachings and practices. Some examples of sources (with published dates where known) that documented this approach are:\n\n\nOne of the first prominent counter-cult apologists was Jan Karel van Baalen (1890–1968), an ordained minister in the Christian Reformed Church in North America. His book, \"The Chaos of Cults\", which was first published in 1938, became a classic in the field as it was repeatedly revised and updated until 1962.\n\nHistorically, one of the most important protagonists of the movement was Walter Martin (1928–89), whose numerous books include the 1955 \"The Rise of the Cults: An Introductory Guide to the Non-Christian Cults\" and the 1965 \"The Kingdom of the Cults: An Analysis of Major Cult Systems in the Present Christian Era\", which continues to be influential. He became well known in conservative Christian circles through a radio program, \"The Bible Answer Man\", currently hosted by Hank Hanegraaff.\n\nIn \"The Rise of the Cults\" Martin gave the following definition of a cult:\n\nBy cultism we mean the adherence to doctrines which are pointedly contradictory to orthodox Christianity and which yet claim the distinction of either tracing their origin to orthodox sources or of being in essential harmony with those sources. Cultism, in short, is any major deviation from orthodox Christianity relative to the cardinal doctrines of the Christian faith.\n\nAs Martin's definition suggests, the countercult ministries concentrate on non-traditional groups that claim to be Christian, so chief targets have been The Church of Jesus Christ of Latter-day Saints (i.e., \"Mormons\"), Jehovah's Witnesses, Armstrongism, Christian Science and the Unification Church, but also smaller groups like the Swedenborgian Church\n\nVarious other conservative Christian leaders—among them John Ankerberg and Norman Geisler—have emphasized themes similar to Martin's. Perhaps more importantly, numerous other well-known conservative Christian leaders as well as many conservative pastors have accepted Martin's definition of a cult as well as his understanding of the groups to which he gave that label. Dave Breese summed up this kind of definition in these words:\nA cult is a religious perversion. It is a belief and practice in the world of religion which calls for devotion to a religious view or leader centered in false doctrine. It is an organized heresy. A cult may take many forms but it is basically a religious movement which distorts or warps orthodox faith to the point where truth becomes perverted into a lie. A cult is impossible to define except against the absolute standard of the teaching of Holy Scripture.\n\nSince the 1980s the term \"new religions\" or \"new religious movements\" has slowly entered into Evangelical usage alongside the word \"cult\". Some book titles use both terms.\n\nThe acceptance of these alternatives to the word \"cult\" in Evangelicalism reflects, in part, the wider usage of such language in the sociology of religion.\n\nThe term \"countercult apologetics\" first appeared in Protestant Evangelical literature as a self-designation in the late 1970s and early 1980s in articles by Ronald Enroth and David Fetcho, and by Walter Martin in \"Martin Speaks Out on the Cults\". A mid-1980s debate about apologetic methodology between Ronald Enroth and J. Gordon Melton, led the latter to place more emphasis in his publications on differentiating the Christian countercult from the secular anti-cult. Eric Pement urged Melton to adopt the label \"Christian countercult\", and since the early 1990s the terms has entered into popular usage and is recognised by sociologists such as Douglas Cowan.\n\nThe only existing umbrella organization within the countercult movement in the USA is the EMNR (Evangelical Ministries to New Religions) founded in 1982 which has the evangelical Lausanne Covenant as governing document and which stresses mission, scholarship, accountability and networking.\n\nWhile the greatest number of countercult ministries are found in the USA, ministries exist in Australia, Brazil, Canada, Denmark, England, Ethiopia, Germany, Hungary, Italy, Mexico, New Zealand, Philippines, Romania, Russia, Sweden, and Ukraine. A comparison between the methods employed in the USA and other nations discloses some similarities in emphasis, but also other nuances in emphasis. The similarities are that globally these ministries share a common concern about the evangelization of people in cults and new religions. There is also often a common thread of comparing orthodox doctrines and biblical passages with the teachings of the groups under examination. However, in some of the European and southern hemisphere contexts, confrontational methods of engagement are not always relied on, and dialogical approaches are sometimes advocated.\n\nA group of organizations which originated within the context of established religion is working in more general fields of cult-awareness, especially in Europe. Their leaders are theologians, and they are often social ministries affiliated to big churches.\n\n\n\n\nThe phenomena of \"cults\" has also entered into the discourses of Christian missions and theology of religions. An initial step in this direction occurred in 1980 when the Lausanne Committee for World Evangelization convened a mini-consultation in Thailand. From that consultation a position paper was produced. The issue was revisited at the Lausanne Forum in 2004 with another paper. The latter paper adopts a different methodology to that advocated in 1980.\n\nIn the 1990s discussions in academic missions and theological journals indicate that another trajectory is emerging which reflects the influence of contextual missions theory. Advocates of this approach maintain that apologetics as a tool needs to be retained, but do not favour a confrontational style of engagement.\n\nCountercult apologetics has several variations and methods employed in analysing and responding to cults. The different nuances in countercult apologetics have been discussed by John A. Saliba and Philip Johnson.\n\nThe dominant method is the emphasis on detecting unorthodox or heretical doctrines and contrasting those with orthodox interpretations of the Bible and early creedal documents. Some apologists, such as Francis J. Beckwith, have emphasised a philosophical approach, pointing out logical, epistemological and metaphysical problems within the teachings of a particular group. Another approach involves former members of cultic groups recounting their spiritual autobiographies, which highlight experiences of disenchantment with the group, unanswered questions and doubts about commitment to the group, culminating in the person's conversion to Evangelical Christianity.\n\nApologists like Dave Hunt in \"Peace, Prosperity and the Coming Holocaust\" and Hal Lindsey in \"The Terminal Generation\" have tended to interpret the phenomena of cults as part of the burgeoning evidence of signs that Christ's Second Advent is close at hand. Both Hunt, and Constance Cumbey, have applied a conspiracy model to interpreting the emergence of New Age spirituality and linking that to speculations about fulfilled prophecies heralding Christ's reappearance.\n\n\n\n\n", "id": "7731", "title": "Christian countercult movement"}
{"url": "https://en.wikipedia.org/wiki?curid=7732", "text": "Professor X\n\nProfessor Charles Francis Xavier (also known as Professor X) is a fictional character appearing in American comic books published by Marvel Comics and is the founder and leader of the X-Men. Created by writer Stan Lee and artist Jack Kirby, the character first appeared in \"The X-Men\" #1 (September 1963).\nXavier is a member of a subspecies of humans known as mutants who are born with superhuman abilities. The founder of the X-Men, he himself is an exceptionally powerful telepath who can read and control minds of others. While running a private school in Westchester County, New York to both shelter and train mutants from around the globe, Xavier also fights to serve the greater good by promoting peaceful coexistence and equality between humans and mutants in a world where often zealous anti-mutant bigotry is widespread.\n\nThroughout much of the character's history in comics, Xavier is a paraplegic variously using either a wheelchair or a form of personal hovercraft. One of the world's most powerful telepathic mutant minds, Xavier is also a scientific genius, being an expert in genetics, biophysics, psychology, anthropology, and psychiatry. He's put his combined talents to use by serving as a leading authority on genetics, mutations, and psionic powers. Furthermore, he has shown noteworthy talents in devising equipment to greatly enhance psionic powers. Xavier is perhaps best known in this regard for the creation of a device called Cerebro, a technology that serves to detect and track those individuals possessing the mutant gene, at the same time greatly expanding the gifts of those with existing psionic abilities.\n\nAlthough quite at odds with his consistent use of the X-Men to fight threats of evil by engaging in purely physical battle, from a social policy and philosophical perspective Xavier has been shown to deeply resent the violent methods of those like his former close friend and occasional arch enemy, the supervillain Magneto. Instead, he's presented his platform as espousing uncompromising pacifism in order to see his dream to fruition - one that seeks to live harmoniously alongside humanity, just the same as it desires full-fledged civil rights and equality for all of mutantkind. Xavier's actions and goals in life have therefore quite often been compared to those of Martin Luther King, Jr. for his involvement with the American civil rights struggle, whereas Magneto is often compared with the more militant civil rights activist Malcolm X.\n\nIndeed, the character's creation and development occurred simultaneously with the civil rights struggle, taking place as it did in the 1960s, while Xavier's first appearance dates to 1963. The fictionalized plight on paper of a mutantkind faced with exceptional intolerance, prejudice and blind hatred was done in large part to better illustrate to audiences of the day what was transpiring across the United States, just the same as it also served to further promote ideals of tolerance and equality for all.\n\nPatrick Stewart portrayed the character in five films of the \"X-Men\" film series and in various video games, while James McAvoy portrayed a younger version of the character in the 2011 prequel \"\", with both actors reprising the role in the film \"\". James McAvoy also portrays the character in \"\".\n\nCreated by writer Stan Lee and artist/co-writer Jack Kirby, Professor X first appeared in \"X-Men\" #1 (September 1963).\n\nStan Lee has stated that the physical inspiration of Professor Xavier was from Academy Award–winning actor Yul Brynner. Professor Xavier's character development has been inspired by Martin Luther King, Jr.\n\nWriter Scott Lobdell established Xavier's middle name to be Francis in \"Uncanny X-Men\" #328 (January 1996).\n\nXavier’s goals are to promote the peaceful affirmation of mutant rights, to mediate the co-existence of mutants and humans, to protect mutants from violent humans and to protect society from antagonistic mutants, including his old friend, Magneto. To achieve these aims, he founded Xavier's School for Gifted Youngsters (later named the Xavier Institute) to teach mutants to explore and control their powers. Its first group of students was the original X-Men (Cyclops, Iceman, Marvel Girl, Angel, and Beast). Xavier's students consider him a visionary and often refer to their mission as \"Xavier's dream\". He is highly regarded by others in the Marvel Universe, respected by various governments and trusted by several other superhero teams, including the Avengers and the Fantastic Four. However, he also has a manipulative streak which has resulted in several significant fallings-out with allies and students.\n\nHe often acts as a public advocate for mutant rights and is the authority most of the Marvel superhero community turns to for advice on mutants. Despite this, his status as a mutant himself and originator of the X-Men only became public during the 2001 story \"E Is for Extinction\". He also appears in almost all of the X-Men animated series and in many video games, although usually as a non-playable character because of his disability. Patrick Stewart plays him in the 2000s film series, as well as providing his voice in some of the X-Men videogames (including some not connected to the film series).\nAccording to \"BusinessWeek\", Charles Xavier is listed as one of the top ten most intelligent fictional characters in American comics.\n\nIn a number of comics, Xavier is shown to have a dark side, a part of himself that he struggles to suppress. Perhaps the most notable appearance of this character element is in the Onslaught storyline, in which the crossover event's antagonist is a physical manifestation of that dark side. Also, Onslaught is created in the most violent act Xavier claims to have done: erasing the mind of Magneto. In \"X-Men\" #106 (August 1977), the new X-Men fight images of the original team, which have been created by what Xavier says is his \"evil self ... who would use his powers for personal gain and conquest\", which he says he is normally able to keep in check. In the 1984 four-part series titled \"The X-Men and the Micronauts\", Xavier's dark desires manifest themselves as the Entity and threaten to destroy the Micronauts' universe.\n\nIn other instances, Xavier is shown to be secretive and manipulative. During the Onslaught storyline, the X-Men find Xavier's files, the \"Xavier Protocols\", which detail how to kill many of the characters, including Xavier himself, should the need ever arise, such as if they went rogue. \"Astonishing X-Men\" vol. 3, #12 (August 2005) reveals that when Xavier realizes that the Danger Room has become sentient, he keeps it trapped and experiments on it for years, an act that Cyclops calls \"the oppression of a new life\" and equates to humanity's treatment of mutants (however, \"X-Men Legacy\" #220 - 224 reveals that Xavier did not intend for the Danger Room to become sentient: it was an accident, and Xavier sought a way to free Danger, but was unable to find a way to accomplish this without deleting her sentience as well).\n\nCharles Francis Xavier was born in New York City to the wealthy Brian Xavier, a well-respected nuclear scientist, and Sharon Xavier. After Brian dies in an accident, his science partner Kurt Marko comforts and then marries the grieving Sharon. When Xavier's telepathic mutant powers emerge, he discovers Marko cares only about his mother's money.\nAfter the wedding, Kurt moves in with the Xaviers, bringing with him his son Cain. Kurt quickly grows neglectful of Sharon, driving her to alcoholism, and abuses both Charles and Cain. Cain takes out his frustrations and insecurities on his stepbrother. Charles uses his telepathic powers to read Cain's mind and explore the extent of his psychological damage, which only leads to Cain becoming more aggressive toward him and the young Xavier feeling Cain's pain firsthand.\n\nSharon dies soon after, and a fight erupts between Cain and Charles that causes some of Kurt's lab equipment to explode. Mortally wounded, Kurt drags the two children out before dying, and admits he was partly responsible for Brian's death.\n\nWith help from his powers and his natural genius, Xavier becomes an excellent student and athlete, though he gives up the latter, believing his powers give him an unfair advantage. Due to his powers, by the time he graduates from high school, Charles loses all of his hair. He graduates with honors at the age of 16 from Harvard University. In graduate studies, he receives Ph.D.s in Genetics, Biophysics, Psychology, and Anthropology with a two-year residence at Pembroke College, Oxford University. He also receives an M.D. in Psychiatry while spending several years in London. He is later appointed Adjunct Professor at Columbia University. \"Origins of Marvel Comics: X-Men\" #1 (2010) presents a different version of events, suggesting a scholarship to Oxford University rescued him from his abusive home, after which he \"never looked back\", suggesting he began his academic career as a very young man at Oxford. His stepbrother is resentful of him.\n\nAt graduate school, he meets a Scottish girl named Moira Kinross, a fellow genetics student with whom he falls in love. The two agree to get married, but soon, Xavier is drafted into the Korean War. He carves himself a niche as a soldier in search and rescue missions alongside Shadowcat's father, Carmen Pryde, and witnesses Cain's transformation into Juggernaut when he touches a ruby with an inscription on it in an underground temple. During the war, he receives a letter from Moira telling him that she is breaking up with him. He later discovers that Moira married her old boyfriend Joseph MacTaggert, who abuses her.\n\nDeeply depressed when Moira broke off their engagement without explanation, Xavier began traveling around the world as an adventurer after leaving the army. In Cairo, he meets a young girl named Ororo Munroe (later known as Storm), who is a pickpocket, and the Shadow King, a powerful mutant who is posing as Egyptian crime lord Amahl Farouk. Xavier defeats the Shadow King, barely escaping with his life. This encounter leads to Xavier's decision to devote his life to protecting humanity from evil mutants and safeguarding innocent mutants from human oppression.\nXavier visits his friend Daniel Shomron, who runs a clinic for traumatized Holocaust victims in Haifa, Israel. There, he meets a man going by the name of Magnus (who would later become Magneto), a Holocaust survivor who works as a volunteer in the clinic, and Gabrielle Haller, a woman driven into a catatonic coma by the trauma she experienced. Xavier uses his mental powers to break her out of her catatonia and the two fall in love. Xavier and Magneto become good friends, although neither immediately reveals to the other that he is a mutant. The two hold lengthy debates hypothesizing what will happen if humanity is faced with a new super-powered race of humans. While Xavier is optimistic, Magneto's experiences in the Holocaust lead him to believe that humanity will ultimately oppress the new race of humans as they have done with other minorities. The two friends reveal their powers to each other when they fight Nazi Baron Wolfgang von Strucker and his Hydra agents, who kidnap Gabrielle because she knows the location of their secret cache of gold. Magneto attempts to kill Strucker but Xavier stops him. Realizing that his and Xavier's views on mutant-human relations are incompatible, Magneto leaves with the gold. Charles stays in Israel for some time, but he and Gabrielle separate on good terms, neither knowing that she is pregnant with his son, who grows up to become the mutant Legion.\n\nIn a strange town near the Himalayas, Xavier encounters an alien calling himself Lucifer, the advance scout for an invasion by his race, and foils his plans. In retaliation, Lucifer drops a huge stone block on Xavier, crippling his legs. After Lucifer leaves, a young woman named Sage hears Xavier's telepathic cries for help and rescues him, bringing him to safety, beginning a long alliance between the two.\n\nIn a hospital in India he is brought to an American nurse, Amelia Voght, who looks after him and, as she sees to his recovery, they fall in love. When he is released from the hospital, the two moved into an apartment in Bombay together. Amelia is troubled to find Charles studying mutation, as she is a mutant and unsettled by it, though she calms when he reveals himself to be a mutant as well. They eventually move to the United States, living on Xavier's family estate. But the night Scott Summers moves into Xavier's mansion, Amelia leaves him, believing Charles would have changed his view and that mutants should lie low. Yet he is recruiting them to what she believes is a lost cause. Charles tries to force her to stay with his mental powers, but immediately ashamed by this, lets her go. She later becomes a disciple of Magneto.\n\nOver the years, Charles makes a name for himself as geneticist and psychologist, apparently renowned enough that the Greys were referred to him when no other expert could help their catatonic daughter, Jean. Xavier trains her in the use of her telekinesis, while inhibiting her telepathic abilities until she matures. Around this time, he also starts working with fellow mutation expert, Karl Lykos, as well as Moira MacTaggert again, who built a mutant research station on Muir Island. Apparently, Charles had gotten over Moira in his travels to the Greek island of Kirinos. Xavier discusses his candidates for recruitment to his personal strike force, the X-Men, with Moira, including those he passes over, which are Kurt Wagner, Piotr Rasputin, Pietro and Wanda Maximoff, and Ororo Munroe. Xavier also trains Tessa in order to spy on Sebastian Shaw.\n\nXavier founded Xavier's School for Gifted Youngsters, which provides a safe haven for mutants and teaches them to master their abilities. In addition, he seeks to foster mutant-human relations by providing his superhero team, the X-Men, as an example of mutants acting in good faith, as he told FBI agent Fred Duncan. With his inherited fortune, he uses his ancestral mansion at 1407 Graymalkin Lane in Salem Center, Westchester County, New York as a base of operations with technologically advanced facilities, including the Danger Room - later, Fantomex mentions that Xavier is a billionaire with a net worth of $3.5 billion. Presenting the image of a stern teacher, Xavier makes his students endure a rigorous training regime.\n\nXavier's first five students are Marvel Girl, Cyclops, Beast, Iceman, and Angel, who become the original X-Men. After he completes recruiting the original team of X-Men, he sends them into battle with Magneto.\n\nThroughout most of his time with the team, Xavier uses his telepathic powers to keep in constant contact with his students and provides instructions and advice when needed. In addition, he uses a special machine called Cerebro, which enhances his ability to detect mutants and to allow the team to find new students in need of the school.\n\nAmong the obstacles Xavier faces is his old friend, Magneto, who has grown into an advocate of mutant superiority since their last encounter and who believes the only solution to mutant persecution is domination over humanity.\n\nWhen anthropologist Bolivar Trask resurfaces the \"mutant problem\", Xavier counters him in a televised debate, however, he appears arrogant and Trask sends his mutant-hunting robot Sentinels to terrorize mutants. The X-Men dispatch them, but Trask sees the error in his ways too late as he is killed by his creations.\n\nAt one point, Xavier seemingly dies during the X-Men's battle with the sub-human Grotesk, but it is later revealed that Xavier arranged for a reformed former villain named Changeling to impersonate him while he went into hiding to plan a defense against an invasion by the extraterrestrial Z'Nox, imparting a portion of his telepathic abilities to the Changeling to complete the disguise.\n\nWhen the X-Men are captured by the sentient island Krakoa, Xavier assembles a new team to rescue them, including Cyclops' and Havok's long-lost brother, Vulcan, along with Darwin, Petra, and Sway. This new team, composed of students of Dr. Moira MacTaggert, was sent to rescue the original X-Men from Krakoa. However, after rescuing Cyclops, McTaggert's former students were seemingly killed. Upon Cyclops' return, Xavier removed Cyclops' memories of the death of Vulcan and his teammates, and began assembling yet another team of X-Men.\n\nXavier's subsequent rescue team consists of Banshee, Colossus, Sunfire, Nightcrawler, Storm, Wolverine, and Thunderbird. After the mission, the older team of X-Men, except for Cyclops, leave the school, believing they no longer belong there, and Xavier mentors the new X-Men.\n\nXavier forms a psychic bond across galaxies with Princess Lilandra from the Shi'ar Empire. When they finally meet, it is love at first sight. She implores the professor to stop her mad brother, Shi'ar Emperor D'Ken, and he instantly aids her by deploying his X-Men. When Jean Grey returns from the Savage Land to tell him that all the X-Men are dead, he shuts down the school and travels with Lilandra to her kingdom, where she is crowned Empress and he is treated like a child or a trophy husband.\n\nXavier senses the changes taking place in Jean Grey, and returns to Earth to help and resume leadership of the X-Men. Shortly thereafter he battles his pupil after she becomes Dark Phoenix and destroys a populated planet in the Shi'ar Empire. It hurts Xavier to be on the opposite side of Lilandra, but he has no other choice but to challenge the Shi'ar Imperial Guard to a duel over the fate of the Phoenix. Xavier would have lost against the greater power of the Dark Phoenix, but thanks to the help Jean Grey gives him (fighting her Phoenix persona), Xavier emerges victorious; she later commits suicide in order to prevent herself from endangering more innocent lives.\n\nWhen the X-Men fight members of the extraterrestrial race known as the Brood, Xavier is captured by them, and implanted with a Brood egg, which places Xavier under the Brood's control. During this time, Xavier assembles a team of younger mutants called the New Mutants, secretly intended to be prime hosts for reproduction of the aliens. The X-Men discover this and return to free Xavier, but they are too late to prevent his body from being destroyed with a Brood Queen in its place; however, his soul remains intact. The X-Men and Starjammers subdue this monstrous creature containing Xavier's essence, but the only way to restore him is to clone a new body using tissue samples he donated to the Starjammers and transfer his consciousness into the clone body. This new body possesses functional legs, though the psychosomatic pain Xavier experienced after living so long as a paraplegic takes some time to subside. Subsequently, he even joins the X-Men in the field, but later decides not to continue this practice after realizing that his place is at the school, as the teacher of the New Mutants.\n\nAfter taking a teaching position at Columbia University in \"Uncanny X-Men\" #192, Xavier is severely injured and left for dead as the victim of a hate crime. Callisto and her Morlocks, a group of underground-dwelling mutants, get him to safety. One of the Morlocks partially restores Xavier's health, but Callisto warns Xavier that he is not fully healed and that he must spend more time recuperating and restrain himself from exerting his full strength or powers, or his health might fail again. Xavier hides his injuries from the others and resumes his life.\n\nCharles meets with former lover Gabrielle Haller on Muir Isle and discovers that they had a child. The boy, David, is autistic and suffers from multiple personality disorder and has vast psionic powers like his father. After helping him and his team to escape from David's mind, Xavier promises he will always be there for him.\n\nA reformed Magneto is arrested and put on trial. Xavier attends the trial to defend his friend. Andrea and Andreas Strucker, the children of presumed dead Baron von Strucker, crash the courtroom to attack Magneto and Xavier. Xavier is seriously injured. Dying, he asks a shocked Magneto to look after the X-Men for him. Lilandra, who has a psychic bond with Xavier, feels that he is in great danger and heads to Earth. There, she and Corsair take Xavier with them so Shi'ar advanced technology can heal him.\n\nXavier leaves Magneto in charge of the school, but some of the X-Men are unwilling to forgive their former enemy. Cyclops loses a duel for the leadership of the X-Men against Storm, then leaves them and joins the other four original X-Men to form a new team called X-Factor.\n\nIn the meantime, Charles becomes stranded in space with the Starjammers, but he is reunited with his lover Lilandra and relishes his carefree lifestyle. He serves as a member of the Starjammers aboard the starship \"Starjammer\", mobile in the Shi'ar Galaxy. He becomes consort to the Princess-Majestrix Lilandra while in exile, and when she later resumes her throne he takes up residence with her in the Imperial palace on the Shi'ar homeworld. Xavier joins Lilandra in her cause to overthrow her sister Deathbird, taking on the powers of Phoenix temporarily wherein he is named Bald Phoenix by Corsair, but sees that he must return to help the X-Men.\n\nXavier eventually becomes imprisoned by the Skrulls during their attempted invasion of the Shi'ar Empire. Xavier breaks free from imprisonment by Warskrull Prime, and is reunited with the X-Men. A healthy Xavier returns from the Shi'ar Empire and is reunited with both the current and original X-Men teams, and resumes his leadership responsibilities of the united teams. In a battle with his old foe, the Shadow King, in the \"Muir Island Saga\", Xavier's spine is shattered, returning him to his former paraplegic state, while his son David is seemingly killed. In the following months, Xavier rebuilds the mansion, which previously was rebuilt with Shi'ar technology, and restructures the X-Men into two teams.\n\nWhile holding a mutant rights speech, Xavier is nearly assassinated by Stryfe in the guise of Cable, being infected with a fatal techno-organic virus. For reasons of his own, the villain Apocalypse saves him. As a temporary side-effect he gains full use of his legs and devotes his precious time to the youngest recruit on his team, Jubilee.\n\nWith all his students now highly trained adults, Professor Xavier renames his school the Xavier Institute For Higher Learning. Also, he assumes control of a private institution, the Massachusetts Academy, making it a new School for Gifted Youngsters. Another group of young mutants is trained here, Generation X, with Banshee and Emma Frost as headmaster and headmistress, respectively.\n\nProfessor X is for a time the unknowing host of the evil psionic entity Onslaught, the result of a previous battle with Magneto. In that battle, Magneto uses his powers to rip out the adamantium bonded to Wolverine's skeleton, and a furious Xavier wipes Magneto's mind, leaving him in a coma. From the psychic trauma of Xavier using his powers so violently and the mixing of Magneto's and Xavier's repressed anger, Onslaught is born. Onslaught wreaks havoc, destroying much of Manhattan, until many of Marvel's superheroes—including the Avengers, the Fantastic Four and the Hulk—destroy him. Xavier is left without his telepathy and, overcome with guilt, leaves the X-Men and is incarcerated for his actions. He later returns to the X-Men after \"\", in which he is shocked by the cruel act of being turned over to the mutant-hating Bastion, following a clash with the sentient Cerebro and a team of impostor X-Men.\n\nXavier questions his dream again and Magneto shortly thereafter is confronted by the X-Men. After the battle, the UN concedes Genosha to Magnus, and Wolverine is angered by Xavier stopping him from getting his revenge on Magneto. Charles and Logan are later trapped in a dimension with different laws of physics, wherein they have to coordinate their moves together and, in the process, gain a better understanding of the other's views.\n\nApocalypse kidnaps the fabled \"Twelve\" special mutants (Xavier included) whose combined energies would grant him omnipotence. After Apocalypse's defeat with the help of Skrull mutants, Xavier goes with the young Skrulls known as Cadre K to train them and free them from their oppressors, and eventually returns to aid in Legacy Virus research.\n\nMystique and her Brotherhood start a deadly assault on Muir Isle by releasing an altered form of the Legacy Virus, all in retaliation against the election campaign of Robert Kelly, a seeming mutant-hater. Mystique blows up Moira MacTaggert's laboratory complex, fatally wounding her. Charles goes to the astral plane to meet with her and retrieve information on the cure to the Legacy Virus, but after gathering the information does not want to leave her alone. If not for Jean pulling him back, the professor would have died with his first love, who states she has no regrets.\n\nAs Beast cures the Legacy Virus, many infected Genoshan mutants recover overnight, providing Magneto, current ruler of Genosha, with an army to start the third World War. He demands Earth's governments to accept him as their leader, and abducts and crucifies Xavier in Magda Square for all to see. A loyal member of Magneto's Acolytes, Amelia Voght, can't stand to see her former lover punished in such a manner and sets him free. Jean Grey and rather untrained newcomers, as most of the team are elsewhere, distract Magneto and Wolverine guts him. Xavier is too late to intervene.\n\nXavier's evil twin Cassandra Nova, whom Xavier attempted to kill while they were both in their mother's womb, orders a group of rogue Sentinels to destroy the independent mutant nation of Genosha. Magneto, who is Genosha's leader, appears to die along with the vast majority of the nation's inhabitants. Nova then takes over Xavier's body. Posing as Xavier, she reveals his mutation to the world, something he needed to do but did not want to sully his reputation over, before going into space and crippling the Shi'ar Empire. The X-Men restore Xavier, but Lilandra, believing that too much disaster has come from the Shi'ar's involvement with the X-Men, annuls her marriage to Xavier. Lilandra previously had gone insane and tried to assassinate Charles on a trip to Mumbai. During this period, a mutant named Xorn joins the X-Men. Xorn uses his healing power to restore Xavier's use of his legs.\n\nWhen the X-Men receive a distress call from a Scottish island, they are surprised to find Juggernaut with nowhere to go, as the island was destroyed by his further-mutated partner in crime, Black Tom Cassidy, who died. Xavier reaches out to his stepbrother and offers him a place in his mansion, with Cain reluctantly accepting. The Juggernaut redeems himself over the next few weeks and joins the X-Men. Xavier finds out that Cain's father preferred him to his own flesh and blood and that they both thought they deserved the abuse they incurred by Kurt; Cain believed this because his father loved someone else's child more than him, and Charles felt guilty about getting in the way. That it is why neither of them stopped Kurt Marko with their powers.\n\nNow outed as a mutant, Xavier makes speeches to the public about mutant tolerance. He also founds the X-Corporation, or X-Corp (not to be confused with the X-Corps), with offices all over the world. The purpose of the X-Corp is to watch over mutant rights and help mutants in need. As a result of being outed, the school no longer hides the fact that it is a school for mutants and it opens its doors for more mutant (and even human) students to come in. A student named Quentin Quire and members of his gang start a riot at the Xavier Institute during an open house at the school. As a result, Quire and two other students are killed. Uncertain about his dream's validity, Xavier announces that he will step down as headmaster and be succeeded by Jean Grey. Afterwards, Xorn reveals himself to be Magneto, having apparently not died in the Sentinel raid on Genosha. Magneto undoes the restoration of Xavier's ability to walk, kidnaps him, and destroys the X-Mansion (killing several of the students). Then Xorn/Magneto assaults New York, where Cyclops, Fantomex and a few students confront him. After the rest of the X-Men arrive, Xorn/Magneto kills Jean Grey with an electromagnetically induced stroke, and Wolverine decapitates him. With Jean dead, Xavier leaves the school to Cyclops and Emma Frost, to bury Xorn/Magneto in Genosha. In a retcon of Grant Morrison's storyline, there Xavier meets the \"real\" Magneto, who mysteriously survived Cassandra Nova's assault. The two resolve their differences and attempt to restore their friendship, leading a team of mutants, the Genoshan Excalibur, to rebuild and restore order to the destroyed island nation.\n\nAt the mansion, the Danger Room (the X-Men’s simulated reality training chamber) gains sentience, christens itself \"Danger\", assumes a humanoid form, and attacks the X-Men before leaving to kill Xavier. With Magneto's help, Xavier holds off Danger until the X-Men arrive. Danger flees, but not before revealing to Colossus that Xavier has known it to be sentient ever since he upgraded it. Colossus is especially offended by this because he had been held captive and experimented upon by Danger's ally, Ord of the Breakworld. Ashamed, Xavier tries to explain to them that by the time he realized what was happening, he could see no other course. The disgusted X-Men leave.\n\nIn a prelude to \"House of M\", Magneto's daughter Scarlet Witch suffers a mental breakdown and causes the death of several Avengers. Magneto brings her to Xavier and asks him to use his mental powers to help her. Although aided by Doctor Strange and the appearance of Cassandra Nova, Xavier is unsuccessful. Xavier orders a meeting of the X-Men and Avengers to decide Wanda's fate. Her brother Quicksilver, believing the heroes plan to kill her, speeds off to Genosha and convinces Wanda that she could right the wrongs she inflicted by using her powers to alter reality.\n\nQuicksilver somehow forces a tearful Wanda to reveal to him her heart's desires of Magneto, the assembled New Avengers, and the X-Men, and then uses her powers to make them all real. Thanks to Magneto, though, this re-imagined world is a place where a much more numerous mutant-kind are the dominant species, humans a disenfranchised and oppressed 'silent majority', and Magneto himself rules supreme. In this reality, the only proof that Charles Xavier ever existed is a secret monument in Magneto's palace garden, with the engraved message \"He died so Genosha could live\".\n\nAfter mutant Layla Miller restores the memories of some of the X-Men and Avengers, they head to Genosha where they discover that Magneto has erected a memorial garden for Xavier commemorating his death. Emma is horrified until Cloak fades into the grave and discovers there is no body inside. After a battle, Scarlet Witch again uses her powers to restore reality and, as a slight against her father, causes a large majority mutants to lose their powers, leaving the mutant race on the brink of extinction and causing the lost powers to become an energy mass, the Collective. With reality restored, Xavier is still missing and the X-Men are unable to detect him with Cerebro\".\n\nXavier returns when Cyclops' and Havok's long-lost brother, Vulcan, is revived by the Collective energy released as a result of the \"House of M\" incident. Vulcan then attacks the X-Men. Xavier, now depowered but able to walk in the wake of \"House of M\", reveals that he had gathered and trained another team of X-Men (this one composed of students of Dr. Moira MacTaggert) sometime between the original team and the new X-Men team introduced in \"Giant Size X-Men\" #1. This team included Vulcan as a member. Like the \"Giant Size\" X-Men team, McTaggert's former students were sent to rescue the original X-Men from Krakoa, the living island. However, after rescuing Cyclops, McTaggert's former students were seemingly killed. Upon Cyclops' return, Xavier removed Cyclops' memories of the death of Vulcan and his teammates, and began assembling the \"Giant Size\" X-Men. Vulcan skirmishes with the X-Men and eventually flees into space.\n\nIn spite of Cyclops' feelings, Xavier forms a new team including Havok and Darwin, the lone other survivor of Moira's students. Xavier seeks to confront Vulcan before he can enact his vengeance against the Shi'ar empire, which killed Vulcan's mother. While en route to the Shi'ar home world, Xavier is abducted and is later thrown into the M'Kraan Crystal by Vulcan. Darwin follows Xavier into the crystal and pulls Xavier out. This somehow restores Xavier's lost telepathy. With help from his longtime lover, Lilandra, Xavier escapes back to Earth with several of his X-Men.\n\nUpon Xavier's return to Earth as seen in the \"World War Hulk\" storyline, he begins to search for lost mutants such as Magneto. Charles' search for more mutants is interrupted by the Hulk, who was sent into extraterrestal exile by the Illuminati, a group of powerful superbeings to which Xavier belongs. Xavier had no part in (and did not know of) the Hulk exile decision, but Xavier admits to Hulk that he would have concurred to a temporary exile so Bruce Banner could be cured of transforming into the Hulk. However, he also tells the Hulk he would not have agreed to permanent exile. Xavier attempts to surrender to the Hulk, but after viewing the X-Mansion's large graveyard dedicated to post-M-Day mutant deaths, The Hulk concludes the mutants have suffered enough, and leaves the Mansion grounds on his own accord. While the X-Men tend to the wounded, Cyclops finally forgives Professor X.\n\nWhile using Cerebra and talking to Beast during the \"\" storyline, Charles detects a new mutant so powerful it fries Cerebra's system. He asks Cyclops to send out a team to find out about the mutant. Once the team have come back empty handed, he argues with Scott for not telling him about the team he deployed to find former Acolytes. Scott tells him outright that he doesn't need him to run the X-Men anymore. This upsets Charles and annoys him later on when he overhears Cyclops briefing X-Factor on the situation. He also approaches the New X-Men in an attempt to help them figure out a non-violent way to help against the Purifiers, but is quickly rebuked by Surge, who questions where he was when they were getting attacked the first time, and that they didn't need to learn from him. Charles questions Cyclops' decision to send X-Force to hunt down his own son, Cable, in front of the students. Cyclops then tells Xavier that he is a distraction that will keep getting in the way and that he must leave the mansion. Xavier is contacted by Cable, who lost the mutant newborn to the traitorous actions of Bishop, who in turn lost the child to the Marauders, and tells him that he is the only one who can help Cable save the future. In the final fight, Xavier is accidentally shot in the head by Bishop. Immediately afterward Xavier's body disappears and Cyclops declares that there are no more X-Men.\n\nProfessor Xavier survives Bishop's gunshot but falls into a coma. Xavier is kidnapped by Exodus, Tempo, and Karima Shapandar. Exodus tries to heal Xavier, Xavier mentally fights Exodus. Exodus finally approaches Magneto, who is apparently still depowered, for help. Magneto and Karima Shapandar are able to stir Xavier's memories and coax him out of his coma, though Xavier remains slightly confused and partly amnesiac. Later, Exodus confronts Magneto about Joanna Cargill's injury (Magneto was forced to shoot a laser through her eyeball in order to prevent her attempted assassination of Xavier). Exodus nearly kills Magneto, and Xavier drags Exodus onto the Astral Plane, putting Xavier's own newly restored mind at stake. Xavier defeats Exodus after a harrowing psionic battle, and Exodus reveals the reason he abducted Xavier and to restore his mind: Exodus wants Xavier to lead the Acolytes and find the mutant messiah child (now under the guardianship of Cable) in order to indoctrinate the child into their cause. Xavier refuses. Emma Frost's telepathy picks up on the psychic fight, and Emma informs Cyclops that Xavier is alive. Xavier parts company with Magneto and Karima to try to regain his lost memories by visiting people from his past.\n\nThe first person Charles visits is Carter Ryking, who had gone insane after losing his powers. Charles reads Carter's memories and discovers that when the two were children they were used as test subjects by Nathan Milbury of the Black Womb Project, with the approval of Charles' father, Doctor Brian Xavier. Xavier makes the connection Milbury and X-Men villain, Mister Sinister, who has apparently long been manipulating Charles' life in addition to other X-Men. Afterwards, he discovers he has been targeted by assassins.\n\nCharles eventually discovers Mister Sinister had set up Charles, Sebastian Shaw, Juggernaut, and Ryking as potential new hosts for Sinister's mind. Bleeding slowly to death, he apparently gives in to Sinister becoming the new Mister Sinister. But in reality, Xavier is still battling Sinister for control of his body. As Sebastian Shaw and Gambit destroy Sinister's Cronus Machine, the device that he used to transfer his consciousness into new hosts, Xavier drives Sinister out of his body permanently. Xavier thanks Shaw and Gambit for their help and declares he must go and see Cyclops immediately. Professor X returns to the X-Mansion to find it destroyed after recent events. Afterwards, Xavier leaves the ruins of the X-Mansion to secretly meet up Cyclops by psychically coercing his former student for the visit. Xavier explains to Cyclops about the recent events with Mr. Sinister and tries to explain to Cyclops how Sinister has been manipulating Scott's and Jean's lives since when they were children. Xavier attempts to have Scott give him permission to scan Scott's mind for traces of Sinister's influences, but instead Scott turns the tables on Xavier by revealing that he has secretly invited Emma Frost into their entire meeting and also into Xavier's mind. While in his mind, Emma forces Xavier to relive each of his morally ambiguous decisions under altruistic pretenses. As the issue continues, Charles realizes his human arrogance and that while some of his decisions were morally wrong, he must move forward with his life and deal with the consequences. Emma ends her incursion into Xavier's mind by reminding him of Moira MacTaggert's last words. As he reflects on Moira's words, Xavier gives Cyclops his blessing to lead the X-Men and leaves to find his own path. Following his encounter with Wolverine (in the \"Original Sin\" Arc) Professor Xavier seeks out his step brother, the unstoppable Juggernaut in an attempt to reform him. After a conversation about the meaning of the word \"Juggernaut\" and a review of Juggernaut and Xavier's shared history Xavier offers Cain an empty box as a gift. Confused by Xavier's gift Cain attempts to kill the Professor bringing an entire sports bar down over their heads in the process. Later Cain battles the X-Men in his full Juggernaut armor and conquers the planet. Just as everything appears to be under the Juggernaut's control Xavier reappears and informs him that everything that has just taken place except for Juggernaut destroying the bar took place in Cain's mind. A baffled Cain demands to know how Xavier managed to overcome his psychically shielded helmet to which the Professor replies that he decided to visit Cain in his sleep. Professor Xavier then informs him that he now understands Cain as a person and that he will not attempt to get in his way or reform him again. But Xavier also warns Cain that if he gets in the way of the Professor's path to redemption Xavier will stop him permanently. Following his encounter with Cain it has been revealed that Xavier is now searching for Rogue.\n\nAfter his bruising encounter with Cyclops and Emma Frost, Professor X is forced to revisit the biggest challenge and the biggest failure of his career, Wolverine, when the feral mutant asks for Charles' help in freeing his son from the clutches of the Hellfire Club. As the two search for Daken, Wolverine reveals that when he first joined the X-Men he attempted to assassinate Xavier due to some unknown programming. In response, the Professor broke Logan's mind and rebuilt it so that any and all programming he received was forgotten. Logan also revealed that the real reason Xavier asked him to join the X-Men was that Charles \"needed a weapon\". Eventually Professor Xavier and Wolverine locate Sebastian Shaw's mansion and attack his minions, just as they are about to enter a bomb explodes from within catching them both off guard. From the wreckage emerges an angry Sebastian who immobilizes Wolverine. Meanwhile, Miss Sinister knocks Daken unconscious and has him taken to the medlab in the mansion's basement. As Shaw prepares to deliver a killing blow to Xavier, Wolverine recovers and stops him telling Xavier to rescue his son. Professor Xavier locates the medlab and after a quick psychic battle with Miss Sinister enters Daken's fractured mind. While in Daken's mind Xavier discovers Romulus's psychic tampering and comments that Daken's mind is even more broken than Wolverine's was. Before Xavier can heal Daken a psychic bomb explodes causing Xavier to become comatose and Daken to wake up. Miss Sinister arrives and attempts to manipulate Daken who reveals that the psychic bomb in his head restored his memories and stabs Miss Sinister in the chest. Meanwhile, Wolverine defeats Shaw and enters the mansion to find Daken standing over an unconscious Xavier preparing to kill him. Wolverine tells Daken that he won't let him hurt Xavier and the two fight. Overcome with guilt over what happened to Daken and Itsu, Wolverine allows himself to be beaten. Just as Daken appears to have won Xavier pulls both of them onto the astral plane revealing that the psychic bomb had little effect on him because his psyche was already shattered. Xavier then explains to Wolverine and Daken that Romulus is solely responsible for Itsu's death and that he lied to Daken about everything because he wanted Wolverine to become his weapon. As the three converse Daken returns to the physical plane and prevents Shaw from killing Xavier. With the truth revealed Wolverine and Daken decide to kill Romulus. As the two depart Wolverine tells Xavier that he forgives him for all of the dark moments in their history. Wolverine acknowledges that Professor Xavier allowed him to become a hero. Wolverine then tells the Professor that he hopes he will one day be able to forgive him for choosing to kill Romulus.\n\nProfessor Xavier recruits Gambit to go with him to Australia to find and help Rogue who is currently staying at the X-Men's old base in the outback; unaware Danger is using Rogue as a conduit for her revenge against him.\n\nIn a prelude to the \"Secret Invasion\" storyline, Professor X was at the meeting of the Illuminati when it came to the discussion about the Skrulls planning an invasion by taking out Earth's heroes and posing as them. He claims he was unable to distinguish that Black Bolt had been replaced by a Skrull, and his powers were bested quickly by the Black Bolt Skrull. Professor X leaves after learning even he can no longer trust the others, yet appears to have severely restricted the number of people he informs of the forthcoming alien invasion, as the X-Men were not prepared for the Skrulls, at least at first. Xavier is not seen again during the events of \"Secret Invasion\", though his X-Men in San Francisco are successful at repelling the invaders there through the use of the modified Legacy Virus.\n\nDuring the \"Dark Reign\" storyline, Professor X convinces Exodus to disband the Acolytes. A H.A.M.M.E.R. helicopter arrives and from inside appears Norman Osborn, who wants to talk to him. During the Dark Avengers' arrival in San Francisco to enforce martial law and squelch the anti-mutant riots occurring in the city, Xavier appears (back in his wheelchair) in the company of Norman Osborn and publicly denounces Cyclops' actions and urges him to turn himself in. However, this Xavier was revealed to be Mystique who Osborn recruited to impersonate Xavier in public. The real Xavier is shown in prison on Alcatraz and slowly being stripped of his telepathic powers while in psionic contact with Beast, who was arrested earlier for his part in the anti-mutant riots. It was also revealed by Emma Frost that she and Professor X are both Omega Class Telepaths when she manages to detect the real Professor X. Professor X helps Emma Frost enter Sentry's mind. However, as Emma frees him of the Void's influence, a minute sliver of the entity itself remains in her mind. Xavier quickly tells her to remain in her diamond armor state to prevent the Void from gaining access to her psi-powers. Professor X is later seen with Emma Frost where Beast is recuperating.\n\nAfter the events of \"Utopia\", Xavier has come to live on the risen Asteroid M, rechristened Utopia, along with the rest of the X-Men, X-Club, and mutant refugees and is also allowed to join the Utopia lead council (Cyclops, Storm, Namor, Iceman, Beast, Wolverine and Emma Frost). While he no longer continues to openly question every move that Cyclops makes, he is still concerned about some of his leadership decisions. Xavier had wanted to return to the mainland in order to clear his name, but in the aftermath of Osborn declaring Utopia as a mutant detention area, Cyclops refused to let him leave, stating that it would be a tactical advantage to have him as an ace in the hole in case the need arose. To that end, he has kept Xavier out of the field and instead relied on Emma Frost, Psylocke and the Stepford Cuckoos respectively for their own psionic talents. While attending the funeral of Yuriko Takiguchi, Magneto arrives at Utopia, apparently under peaceful motives. Xavier does not believe it, and attacks Magneto telepathically, causing Cyclops to force him to stand down. He later apologizes to Magneto for acting out of his old passions from their complicated relationship, which Magneto accepts.\n\nDuring the \"\" storyline, Professor Xavier is seen on Utopia delivering a eulogy at Nightcrawler's funeral. Like the other X-Men he is deeply saddened by Kurt's death and anxious about the arrival of Cable and Hope. Xavier is seen using his powers to help his son Legion control his many personalities and battle the Nimrods. At the conclusion of Second Coming Professor X is seen surveying the aftermath of the battle from a helicopter. As Hope descends to the ground and cradles Cable's lifeless arm, Xavier reflects on everything that has transpired and states that, while he feels that Hope has indeed come to save mutantkind and revive his dream, she is still only a young woman and will have a long and difficult journey before she can truly achieve her potential.\n\nDuring the \"Avengers vs. X-Men\" storyline, the Phoenix Force is split into five pieces and bonded with Cyclops, Emma Frost, Namor, Colossus and Magik (who become known as the Phoenix Five). Eventually, Cyclops and Frost come to possess the full Phoenix Force, and Professor X is instrumental in confronting them both, and dies in the ensuing battle with Cyclops. The Phoenix Force is subsequently forced to abandon Cyclops as a host by the efforts of the Hope Summers and the Scarlet Witch.\n\nXavier's body is later stolen by the Red Skull's S-Men where the group also captures Rogue and Scarlet Witch. Xavier's brain is removed and fused to the brain of the Red Skull. After Rogue and Scarlet Witch snapped out of the fight they were in, they find the lobotomized body of Professor X. Red Skull uses the new powers conferred upon him by Professor X's brain to provoke anti-mutant riots. His plans are foiled by the Avengers and the X-Men, and the Skull escapes.\n\nProfessor X's spirit is later seen in the Heaven dimension along with Nightcrawler's spirit at the time when Azazel invades Heaven.\n\nDuring the \"AXIS\" storyline, a fragment of Professor X's psyche (which had escaped the scrubbing of his memories) still existed in Red Skull's mind preventing him from unleashing the full potential of Professor X's powers. During a fight with the Stark Sentinels, Doctor Strange and Scarlet Witch attempt to cast a spell to invert the axis of Red Skull's brain and bring out the fragment of Professor X to defeat Onslaught. Doctor Strange was targeted and captured by the Sentinels before they could cast the spell. When Magneto arrived with his supervillain allies, Doctor Doom and Scarlet Witch attempted to cast the inversion spell again and Red Onslaught was knocked unconscious and reverted to his Red Skull form. Although they did not know whether Professor X was now in control, the Avengers decided to be cautious and take Red Skull to Stark Tower. It was later revealed that the spell had actually caused all the heroes and villains present to undergo a \"moral inversion\" rather than simply bringing out Professor X in the Skull, with the result that the Skull and other villains became heroic while the Avengers and X-Men present became villainous. Eventually, the inversion was undone.\n\nProfessor X is a mutant who possesses vast telepathic powers, and is among the strongest and most powerful telepaths in the Marvel Universe. He is able to perceive the thoughts of others or project his own thoughts within a radius of approximately . Xavier's telepathy once covered the entire world but Magneto altered the Earth's electromagnetic field to restrict Xavier's telepathic range. While not on Earth, Xavier's natural telepathic abilities have reached across space to make universal mental contact with multiple alien races. With extreme effort, he can greatly extend the range of his telepathy. He can learn foreign languages by reading the language centers of the brain of someone adept, and alternately \"teach\" languages to others in the same manner; Xavier once trained a new group of mutants mentally, subjectively making them experience months of training together, while only hours passed in the real world.\n\nXavier's vast psionic powers enable him to manipulate the minds of others, warp perceptions to make himself seem invisible, project mental illusions, cause loss of particular memories or total amnesia, and induce pain or temporary mental and/or physical paralysis in others. Within close range, he can manipulate almost any number of minds for such simple feats. However, he can only take full possession of one other mind at a time, and must strictly be within that person's physical presence. He is one of the few telepaths skilled enough to communicate with animals and even share their perceptions. He can also telepathically take away or control people's natural bodily functions and senses, such as sight, hearing, smell, taste, or even mutant powers. A side effect of his telepathy is that he has an eidetic memory. He has displayed telepathic prowess sufficient to confront Ego the Living Planet, while aided by Cadre K as well as narrowly defeat Exodus. However, he cannot permanently \"reprogram\" human minds to believe what he might want them to believe even if he wanted to do so, explaining that the mind is an organism that would always recall the steps necessary for it to reach the present and thus 'rewrite' itself to its original setting if he tried to change it. However, his initial reprogramming of Wolverine lasted several years, despite Wolverine overcoming the reprogramming much faster than an ordinary human because of his healing factor.\n\nHe is able to project from his mind 'bolts' composed of psychic energy, enabling him to stun the mind of another person into unconsciousness, inflict mental trauma, or even cause death. These 'bolts' inflict damage only upon other minds, having a negligible effect on non-mental beings, if any. The manner in which Xavier's powers function indicates that his telepathy is physical in some way, as it can be enhanced by physical means (for example, Cerebro) and can be disrupted by physical means (for example, Magneto's alteration of the Earth's magnetic field).\n\nXavier can perceive the distinct mental presence/brain waves of other superhuman mutants within a small radius of himself. To detect mutants to a wider area beyond this radius, he must amplify his powers through Cerebro and subsequently Cerebra, computer devices of his own design which are sensitive to the psychic/physical energies produced by the mind.\n\nProfessor X can project his astral form into a psychic dimension known as the astral plane. There, he can use his powers to create objects, control his surroundings, and even control and destroy the astral forms of others. He cannot project this form over long distances.\n\n\"Uncanny X-Men\" writer Ed Brubaker has claimed that, after being de-powered by the Scarlet Witch, and then re-powered by the M'Kraan Crystal, Charles' telepathy is more powerful than was previously known. However, the extent of this enhancement is unknown.\n\nCharles is a genius with multiple doctorates. He is a world-renowned geneticist, a leading expert in mutation, possesses considerable knowledge of various life sciences, and is the inventor of Cerebro. He possesses Ph.D.s in Genetics, Biophysics, Psychology, and Anthropology, and an M.D. in Psychiatry. He is highly talented in devising equipment for utilizing and enhancing psionic powers. He is also a great tactician and strategist, effectively evaluating situations and devising swift responses.\n\nDuring his travels in Asia, Xavier learned martial arts, acquiring \"refined combat skills\" according to Magneto. When these skills are coordinated in tandem with his telepathic abilities, Xavier is a dangerous unarmed combatant, capable of sensing the intentions of others and countering them with superhuman efficiency. He also has extensive knowledge of pressure points.\n\nCharles Xavier was also given possession of the Mind Infinity Gem. It allows the user to boost mental power and access the thoughts and dreams of other beings. Backed by the Power Gem, it is possible to access all minds in existence simultaneously. Like all other former Illuminati members, Xavier has sworn to never use the gem and to keep its location hidden.\n\nThe Xavier Protocols are a set of doomsday plans created by Professor X. The Protocols detail the best way to kill many powerful mutant characters, including the X-Men and Xavier himself, should they become too large of a danger. The Xavier Protocols are first mentioned during the \"Onslaught\" crossover and first seen in \"Excalibur\" #100 in Moira MacTaggert's lab. Charles Xavier compiled a list of the Earth's most powerful mutants and plans on how to defeat them if they become a threat to the world. They are first used after Onslaught grows too powerful. Only parts of the actual Protocols are ever shown. In the \"\" crossover Bastion obtains an encrypted copy of the Protocols, intending to use them against the X-Men. However, Cable infiltrates the X-Mansion and secures all encrypted files before Bastion has a chance to decrypt them. Due to the tampering of Bastion and his Sentinels, the X-Mansion computer system Cerebro gains autonomy and seeks to destroy the X-Men by employing its knowledge of the Xavier Protocols. In a virtual environment created by Professor X, Cerebro executes the Xavier Protocols against the X-Men.\n\nEach Protocol is activated by the presence of a different combination of X-Men and were written by Xavier himself:\n\n\nOther X-Men who face their Xavier Protocols are Colossus, Rogue, Shadowcat, Nightcrawler, Storm, and Gambit.\n\n", "id": "7732", "title": "Professor X"}
{"url": "https://en.wikipedia.org/wiki?curid=7734", "text": "Central Pacific Railroad\n\nThe Central Pacific Railroad (CPRR) is the former name of the railroad network built between California and Utah, US, that built eastwards from the West Coast in the 1860s, to complete the western part of the \"First Transcontinental Railroad\" in North America. It is now part of the Union Pacific Railroad.\n\nMany 19th century national proposals to build a transcontinental railroad failed because of the energy consumed by political disputes over slavery. With the secession of the South, the modernizers in the Republican Party controlled the US Congress. They passed legislation authorizing the railroad, with financing in the form of government railroad bonds. These were all eventually repaid with interest. The government and the railroads both shared in the increased value of the land grants, which the railroads developed. The construction of the railroad also secured for the government the economical \"safe and speedy transportation of the mails, troops, munitions of war, and public stores.\"\n\nPlanned by Theodore Judah, the Central Pacific Railroad was authorized by Congress in 1862. It was financed and built through \"The Big Four\" (who called themselves \"The Associates\"): Sacramento, California businessmen Leland Stanford, Collis Huntington, Charles Crocker, and Mark Hopkins. Crocker was in charge of construction. Construction crews comprised 12,000 Chinese emigrant workers by 1868, when they constituted eighty percent of the entire work force. They laid the first rails in 1863. The \"Golden spike\", connecting the western railroad to the Union Pacific Railroad at Promontory, Utah, was hammered on May 10, 1869. Coast-to-coast train travel in eight days became possible, replacing months-long sea voyages and lengthy, hazardous travel by wagon trains.\n\nIn 1885 the Central Pacific Railroad was leased by the Southern Pacific Company. Technically the CPRR remained a corporate entity until 1959, when it was formally merged into Southern Pacific. (It was reorganized in 1899 as the Central Pacific \"Railway\".) The original right-of-way is now controlled by the Union Pacific, which purchased Southern Pacific in 1996.\n\nThe Union Pacific-Central Pacific (Southern Pacific) mainline followed the historic Overland Route from Omaha, Nebraska to San Francisco Bay.\n\nChinese labor was the most vital source for constructing the railroad. Fifty Chinese laborers were hired by the Central Pacific Railroad in February 1865, and soon more and more Chinese men were hired. Working conditions were harsh, and Chinese men were compensated less than their white counterparts. Chinese men were paid thirty-one dollars each month, and while white workers were paid the same, they were also given room and board. \n\nConstruction of the road was financed primarily by 30-year, 6% U.S. government bonds authorized by Sec. 5 of the Pacific Railroad Act of 1862. They were issued at the rate of $16,000 per mile of tracked grade completed west of the designated base of the Sierra Nevada range near Roseville, CA where California state geologist Josiah Whitney had determined were the geologic start of the Sierras' foothills. Sec. 11 of the Act also provided that the issuance of bonds \"shall be treble the number per mile\" (to $48,000) for tracked grade completed over and within the two mountain ranges (but limited to a total of at this rate), and \"doubled\" (to $32,000) per mile of completed grade laid between the two mountain ranges. The U.S. Government Bonds, which constituted a lien upon the railroads and all their fixtures, were repaid in full (and with interest) by the company as and when they became due.\n\nSec. 10 of the 1864 amending Pacific Railroad Act (13 Statutes at Large, 356) additionally authorized the company to issue its own \"First Mortgage Bonds\" in total amounts up to (but not exceeding) that of the bonds issued by the United States. Such company-issued securities had priority over the original Government Bonds. (Local and state governments also aided the financing, although the City and County of San Francisco did not do so willingly. This materially slowed early construction efforts.) Sec. 3 of the 1862 Act granted the railroads of public land for every mile laid, except where railroads ran through cities and crossed rivers. This grant was apportioned in 5 sections on alternating sides of the railroad, with each section measuring by . These grants were later doubled to per mile of grade by the 1864 Act.\n\nAlthough the Pacific Railroad eventually benefited the Bay Area, the City and County of San Francisco obstructed financing it during the early years of 1863-1865. When Stanford was Governor of California, the Legislature passed on April 22, 1863, \"An Act to Authorize the Board of Supervisors of the City and County of San Francisco to take and subscribe One Million Dollars to the Capital Stock of the Western Pacific Rail Road Company and the Central Pacific Rail Road Company of California and to provide for the payment of the same and other matters relating thereto\" (which was later amended by Section Five of the \"Compromise Act\" of April 4, 1864). On May 19, 1863, the electors of the City and County of San Francisco passed this bond by a vote of 6,329 to 3,116, in a highly controversial Special Election.\n\nThe City and County's financing of the investment through the issuance and delivery of Bonds was delayed for two years, when Mayor Henry P. Coon, and the County Clerk, Wilhelm Loewy, each refused to countersign the Bonds. It took legal actions to force them to do so: in 1864 the Supreme Court of the State of California ordered them under Writs of Mandamus (\"The People of the State of California \"ex rel\" the Central Pacific Railroad Company vs. Henry P. Coon, Mayor; Henry M. Hale, Auditor; and Joseph S. Paxson, Treasurer, of the City and County of San Francisco.\" 25 Cal. 635) and in 1865, a legal judgment against Loewy (\"The People \"ex rel\" The Central Pacific Railroad Company of California vs. The Board of Supervisors of the City and County of San Francisco, and Wilhelm Lowey, Clerk\" 27 Cal. 655) directing that the Bonds be countersigned and delivered. In 1863 the legislature's forcing of City and County action became known as the \"Dutch Flat Swindle\". Critics claimed the CPRR intended to build a railroad only as far as Dutch Flat, to connect to the Dutch Flat Wagon Road which they already controlled.\n\nA replica of the Sacramento, California Central Pacific Railroad passenger station is part of the California State Railroad Museum, located in the Old Sacramento State Historic Park.\n\nNearly all the company's early correspondence is preserved at Syracuse University, as part of the Collis Huntington Papers collection. It has been released on microfilm (133 reels). The following libraries have the microfilm: University of Arizona at Tucson; and Virginia Commonwealth University at Richmond. Additional collections of manuscript letters are held at Stanford University and the Mariners' Museum at Newport News, Virginia. Alfred A. Hart was the official photographer of the CPRR construction.\n\nThe Central Pacific's first three locomotives were of the then common 4-4-0 type, although with the American Civil War raging in the east, they had difficulty acquiring engines from eastern builders, who at times only had smaller 4-2-4 or 4-2-2 types available. Until the completion of the Transcontinental rail link and the railroad's opening of its own shops, all locomotives had to be purchased by builders in the northeastern U.S. The engines had to be dismantled, loaded on a ship, which would embark on a four-month journey that went around South America's Cape Horn until arriving in Sacramento where the locomotives would be unloaded, re-assembled, and placed in service.\n\nLocomotives at the time came from many manufacturers, such as Cooke, Schenectady, Mason, Rogers, Danforth, Norris, Booth, and McKay & Aldus, among others. Interestingly, the railroad had been on rather unfriendly terms with the Baldwin Locomotive Works, one of the more well-known firms. It is not clear as to the cause of this dispute, though some attribute it to the builder insisting on cash payment (though this has yet to be verified). Consequently, the railroad refused to buy engines from Baldwin, and three former Western Pacific Railroad (which the CP had absorbed in 1870) engines were the only Baldwin engines owned by the Central Pacific. The Central Pacific's dispute with Baldwin remained unresolved until well after the road had been acquired by the Southern Pacific.\n\nIn the 1870s, the road opened up its own locomotive construction facilities in Sacramento. Central Pacific's 173 was rebuilt by these shops and served as the basis for CP's engine construction.\nThe locomotives built before the 1870s were given names as well as numbers. By the 1870s, it was decided to eliminate the names and as each engine was sent to the shops for service, their names would be removed. However, one engine that was built in the 1880s did receive a name, the El Gobernador.\nConstruction of the rails was often dangerous work. Towards the end of construction, almost all workers were Chinese immigrants . The ethnicity of workers depended largely on the \"gang\" of workers/ specific area on the rails they were working.\n\nThe following CP engines have been preserved:\n\n1861\n\n1862\n1863\n1864\n\n1865\n1866\n\n1867\n\n1868\n1869\n1870\n1876\n1877\n1883\n1885\n1888\n1899\n1959\n\n\n\n\n", "id": "7734", "title": "Central Pacific Railroad"}
{"url": "https://en.wikipedia.org/wiki?curid=7737", "text": "Clairvoyance\n\nThe term clairvoyance (/klɛɹˈvɔɪəns/ or /klɛəˈvɔɪəns/) (from French \"clair\" meaning \"clear\" and \"voyance\" meaning \"vision\") is the alleged ability to gain information about an object, person, location or physical event through extrasensory perception. Any person who is claimed to have some such ability is said accordingly to be a clairvoyant (/klerˈvɔɪənt/) (\"one who sees clearly\").\n\nClaims for the existence of paranormal and psychic abilities such as clairvoyance have not been supported by scientific evidence published in high impact factor peer reviewed journals. Parapsychology explores this possibility, but the existence of the paranormal is not accepted by the scientific community. Parapsychology, including the study of clairvoyance, is an example of pseudoscience.\n\nPertaining to the ability of clear-sightedness, clairvoyance refers to the paranormal ability to see persons and events that are distant in time or space. It can be divided into roughly three classes: precognition, the ability to perceive or predict future events, retrocognition, the ability to see past events, and remote viewing, the perception of contemporary events happening outside of the range of normal perception.\n\nThroughout history, there have been numerous places and times in which people have claimed themselves or others to be clairvoyant. \n\nA number of Christian saints were said to be able to see or know things that were far removed from their immediate sensory perception as a kind of gift from God, including Columba of Iona, Padre Pio and Anne Catherine Emmerich. Jesus Christ in the gospels is also recorded as being able to know things that were far removed from His immediate human perception. \n\nIn other religions, similar stories of certain individuals being able to see things far removed from their immediate sensory perception are commonplace, especially within pagan religions where oracles were used. \n\nIn most of these cases, however, the ability to see things was attributed to a higher power and not thought of as an ability that lay within the person himself.\n\nIn Jainism, clairvoyance is regarded as one of the five kinds of knowledge. The beings of hell and heaven (devas) are said to possess clairvoyance by birth. According to Jain text Sarvārthasiddhi, \"this kind of knowledge has been called \"avadhi\" as it ascertains matter in downward range or knows objects within limits\".\n\nThe earliest record of somnambulistic clairvoyance is credited to the Marquis de Puységur, a follower of Franz Mesmer, who in 1784 was treating a local dull-witted peasant named Victor Race. During treatment, Race reportedly would go into trance and undergo a personality change, becoming fluent and articulate, and giving diagnosis and prescription for his own disease as well as those of others. Clairvoyance was a reported ability of some mediums during the spiritualist period of the late 19th and early 20th centuries, and psychics of many descriptions have claimed clairvoyant ability up to the present day.\n\nEarly researchers of clairvoyance included William Gregory, Gustav Pagenstecher, and Rudolf Tischner. Clairvoyance experiments were reported in 1884 by Charles Richet. Playing cards were enclosed in envelopes and a subject put under hypnosis attempted to identify them. The subject was reported to have been successful in a series of 133 trials but the results dropped to chance level when performed before a group of scientists in Cambridge. J. M. Peirce and E. C. Pickering reported a similar experiment in which they tested 36 subjects over 23, 384 trials which did not obtain above chance scores.\n\nIvor Lloyd Tuckett (1911) and Joseph McCabe (1920) analyzed early cases of clairvoyance and came to the conclusion they were best explained by coincidence or fraud. In 1919, the magician P. T. Selbit staged a séance at his own flat in Bloomsbury. The spiritualist Arthur Conan Doyle attended the séance and declared the clairvoyance manifestations to be genuine.\n\nA significant development in clairvoyance research came when J. B. Rhine, a parapsychologist at Duke University, introduced a standard methodology, with a standard statistical approach to analyzing data, as part of his research into extrasensory perception. A number of psychological departments attempted to repeat Rhine's experiments with failure. W. S. Cox (1936) from Princeton University with 132 subjects produced 25, 064 trials in a playing card ESP experiment. Cox concluded \"There is no evidence of extrasensory perception either in the 'average man' or of the group investigated or in any particular individual of that group. The discrepancy between these results and those obtained by Rhine is due either to uncontrollable factors in experimental procedure or to the difference in the subjects.\" Four other psychological departments failed to replicate Rhine's results. It was revealed that Rhine's experiments contained methodological flaws and procedural errors.\n\nEileen Garrett was tested by Rhine at Duke University in 1933 with Zener cards. Certain symbols that were placed on the cards and sealed in an envelope, and she was asked to guess their contents. She performed poorly and later criticized the tests by claiming the cards lacked a psychic energy called \"energy stimulus\" and that she could not perform clairvoyance to order. The parapsychologist Samuel Soal and his colleagues tested Garrett in May, 1937. Most of the experiments were carried out in the Psychological Laboratory at the University College London. A total of over 12,000 guesses were recorded but Garrett failed to produce above chance level. In his report Soal wrote \"In the case of Mrs. Eileen Garrett we fail to find the slightest confirmation of Dr. J. B. Rhine's remarkable claims relating to her alleged powers of extra-sensory perception. Not only did she fail when I took charge of the experiments, but she failed equally when four other carefully trained experimenters took my place.\"\n\nRemote viewing also known as remote sensing, remote perception, telesthesia and travelling clairvoyance is the alleged paranormal ability to perceive a remote or hidden target without support of the senses.\n\nA well known study of remote viewing in recent times has been the US government-funded project at the Stanford Research Institute during the 1970s through the mid-1990s. In 1972, Harold Puthoff and Russell Targ initiated a series of human subject studies to determine whether participants (the \"viewers\" or \"percipients\") could reliably identify and accurately describe salient features of remote locations or \"targets\". In the early studies, a human \"sender\" was typically present at the remote location, as part of the experiment protocol. A three-step process was used, the first step being to randomly select the target conditions to be experienced by the senders. Secondly, in the viewing step, participants were asked to verbally express or sketch their impressions of the remote scene. Thirdly, in the judging step, these descriptions were matched by separate judges, as closely as possible, with the intended targets. The term remote viewing was coined to describe this overall process. The first paper by Puthoff and Targ on remote viewing was published in \"Nature\" in March 1974; in it, the team reported some degree of remote viewing success. After the publication of these findings, other attempts to replicate the experiments were carried out. and remotely linked groups using computer conferencing.\n\nThe psychologists David Marks and Richard Kammann attempted to replicate Targ and Puthoff's remote viewing experiments that were carried out in the 1970s at the Stanford Research Institute. In a series of 35 studies, they were unable to replicate the results so investigated the procedure of the original experiments. Marks and Kammann discovered that the notes given to the judges in Targ and Puthoff's experiments contained clues as to which order they were carried out, such as referring to yesterday's two targets, or they had the date of the session written at the top of the page. They concluded that these clues were the reason for the experiment's high hit rates. Marks was able to achieve 100 per cent accuracy without visiting any of the sites himself but by using cues. James Randi has written controlled tests by several other researchers, eliminating several sources of cuing and extraneous evidence present in the original tests, produced negative results. Students were also able to solve Puthoff and Targ's locations from the clues that had inadvertently been included in the transcripts.\n\nIn 1980, Charles Tart claimed that a rejudging of the transcripts from one of Targ and Puthoff's experiments revealed an above-chance result. Targ and Puthoff again refused to provide copies of the transcripts and it was not until July 1985 that they were made available for study when it was discovered they still contained sensory cues. Marks and Christopher Scott (1986) wrote \"considering the importance for the remote viewing hypothesis of adequate cue removal, Tart's failure to perform this basic task seems beyond comprehension. As previously concluded, remote viewing has not been demonstrated in the experiments conducted by Puthoff and Targ, only the repeated failure of the investigators to remove sensory cues.\"\n\nIn 1982 Robert Jahn, then Dean of the School of Engineering at Princeton University wrote a comprehensive review of psychic phenomena from an engineering perspective. His paper included numerous references to remote viewing studies at the time. Statistical flaws in his work have been proposed by others in the parapsychological community and within the general scientific community.\n\nAccording to scientific research, clairvoyance is generally explained as the result of confirmation bias, expectancy bias, fraud, hallucination, self-delusion, sensory leakage, subjective validation, wishful thinking or failures to appreciate the base rate of chance occurrences and not as a paranormal power. Parapsychology is regarded by the scientific community as a pseudoscience. In 1988, the US National Research Council concluded \"The committee finds no scientific justification from research conducted over a period of 130 years, for the existence of parapsychological phenomena.\"\n\nSkeptics say that if clairvoyance were a reality it would have become abundantly clear. They also contend that those who believe in paranormal phenomena do so for merely psychological reasons. According to David G. Myers (\"Psychology,\" 8th ed.):\n\nThe search for a valid and reliable test of clairvoyance has resulted in thousands of experiments. One controlled procedure has invited 'senders' to telepathically transmit one of four visual images to 'receivers' deprived of sensation in a nearby chamber (Bem & Honorton, 1994). The result? A reported 32 percent accurate response rate, surpassing the chance rate of 25 percent. But follow-up studies have (depending on who was summarizing the results) failed to replicate the phenomenon or produced mixed results (Bem & others, 2001; Milton & Wiseman, 2002; Storm, 2000, 2003).One skeptic, magician James Randi, has a longstanding offer—now U.S. $1 million—\"to anyone who proves a genuine psychic power under proper observing conditions\" (Randi, 1999). French, Australian, and Indian groups have parallel offers of up to 200,000 euros to anyone with demonstrable paranormal abilities (CFI, 2003). Large as these sums are, the scientific seal of approval would be worth far more to anyone whose claims could be authenticated. To refute those who say there is no ESP, one need only produce a single person who can demonstrate a single, reproducible ESP phenomenon. So far, no such person has emerged. Randi's offer has been publicized for three decades and dozens of people have been tested, sometimes under the scrutiny of an independent panel of judges. Still, nothing. \"People's desire to believe in the paranormal is stronger than all the evidence that it does not exist.\" Susan Blackmore, \"Blackmore's first law\", 2004.\n\nThe words \"clairvoyance\" and \"psychic\" are often used to refer to many different kinds of paranormal sensory experiences, but there are more specific names:\n\nIn the field of parapsychology, clairsentience is a form of extra-sensory perception wherein a person acquires psychic knowledge primarily by feeling. The word \"clair\" is French for \"clear\", and \"sentience\" is derived from the Latin sentire, \"to feel\". Psychometry is related to clairsentience. The word stems from \"psyche\" and \"metric\", which means \"soul-measuring\".\n\nIn the field of parapsychology, clairaudience [from late 17th century French \"clair\" (clear) and audience (hearing)] is a form of extra-sensory perception wherein a person acquires information by paranormal auditory means. It is often considered to be a form of clairvoyance. Clairaudience is essentially the ability to hear in a paranormal manner, as opposed to paranormal seeing (clairvoyance) and feeling (clairsentience).\n\nClairolfactus [presumably from late 17th century French \"clair\" (clear) and \"olfactus\" (smelling from the nasal area)] which comes from parts of the extra-sensory perception (ESP) wherein a person accesses Spiritual/mediumistic ability of knowledge through the physical sense of smell.\n\nIn the field of parapsychology, claircognizance [presumably from late 17th century French \"clair\" (clear) and \"cognizance\" (< ME \"cognisaunce\" < OFr \"conoissance\", knowledge) is a form of extra-sensory perception wherein a person acquires psychic knowledge primarily by means of intrinsic knowledge. It is the ability to know something without a physical explanation why a person knows it, like the concept of mediums.\n\nIn the field of parapsychology, clairgustance is defined as a form of extra-sensory perception that allegedly allows one to taste a substance without putting anything in one's mouth. It is claimed that those who possess this ability are able to perceive the essence of a substance from the spiritual or ethereal realms through taste.\n\n\n", "id": "7737", "title": "Clairvoyance"}
{"url": "https://en.wikipedia.org/wiki?curid=7738", "text": "Chiropractic\n\nChiropractic is a form of alternative medicine concerned with the diagnosis and treatment of unverified mechanical disorders of the musculoskeletal system, especially the spine. Proponents believe that such disorders affect general health via the nervous system. The main chiropractic treatment technique involves manual therapy, especially spinal manipulation therapy (SMT), manipulations of other joints and soft tissues. Its foundation is at odds with mainstream medicine, and chiropractic is sustained by pseudoscientific ideas such as subluxation and \"innate intelligence\".\nMultiple systematic reviews have found no evidence that chiropractic manipulation is effective, with the possible exception of treatment for lower back pain. A critical evaluation found that collectively, spinal manipulation was ineffective at treating any condition. There is not sufficient data to establish the safety of chiropractic manipulations. The rate of adverse events is unknown as they are under–reported. Chiropractic is frequently associated with mild to moderate adverse effects. The incidence of serious complications which can lead to permanent disability or death is probably rare. There is controversy regarding the degree of risk of stroke and death from cervical manipulation. Several deaths have been associated with this technique and it is suggested that the relationship is causative, a claim which is disputed by many chiropractors.\nChiropractic is widely established in the United States, Canada, and Australia. It has had a strong political base and sustained demand for services; in recent decades gaining increased acceptance among conventional physicians and health plans in the U.S., and evidence-based medicine has been used to review research studies and generate practice guidelines. Most who seek chiropractic care do so for low back pain, and back and neck pain are considered the specialties of chiropractic, but many chiropractors treat ailments other than musculoskeletal issues. Many chiropractors describe themselves as primary care providers, despite chiropractic clinical training not supporting the requirements to perform such care, with their role in primary care being limited and disputed. Attitudes towards mainstream medicine vary among chiropractors, who often offer conventional treatments such as physical therapy and lifestyle counseling as well, and it may be difficult for the lay person to distinguish the unscientific from the scientific. \nD. D. Palmer founded chiropractic in the 1890s, and his son B. J. Palmer helped to expand it in the early 20th century. Throughout its history, chiropractic has been controversial.\n\nChiropractic is a form of alternative medicine which focuses on manipulation of the musculoskeletal system, especially the spine. Its founder, D.D. Palmer, called it \"a science of healing without drugs\".\n\nChiropractic's origins lie in the folk medicine of bonesetting, and as it evolved it incorporated vitalism, spiritual inspiration and rationalism. Its early philosophy was based on deduction from irrefutable doctrine, which helped distinguish chiropractic from medicine, provided it with legal and political defenses against claims of practicing medicine without a license, and allowed chiropractors to establish themselves as an autonomous profession. This \"straight\" philosophy, taught to generations of chiropractors, rejects the inferential reasoning of the scientific method, and relies on deductions from vitalistic first principles rather than on the materialism of science. However, most practitioners tend to incorporate scientific research into chiropractic, and most practitioners are \"mixers\" who attempt to combine the materialistic reductionism of science with the metaphysics of their predecessors and with the holistic paradigm of wellness. A 2008 commentary proposed that chiropractic actively divorce itself from the straight philosophy as part of a campaign to eliminate untestable dogma and engage in critical thinking and evidence-based research.\n\nAlthough a wide diversity of ideas exist among chiropractors, they share the belief that the spine and health are related in a fundamental way, and that this relationship is mediated through the nervous system. Some chiropractors claim spinal manipulation can have an effect of a variety of ailments such as irritable bowel syndrome and asthma.\n\nChiropractic philosophy includes the following perspectives:\n\nHolism assumes that health is affected by everything in an individual's environment; some sources also include a spiritual or existential dimension. In contrast, reductionism in chiropractic reduces causes and cures of health problems to a single factor, vertebral subluxation. Homeostasis emphasizes the body's inherent self-healing abilities. Chiropractic's early notion of innate intelligence can be thought of as a metaphor for homeostasis.\n\nA variant of chiropractic called naprapathy originated in Chicago in the early twentieth century. It holds that manual manipulation of soft tissue can reduce \"interference\" in the body and thus improve health.\n\n\"Straight\" chiropractors adhere to the philosophical principles set forth by D.D. and B.J. Palmer, and retain metaphysical definitions and vitalistic qualities. Straight chiropractors believe that vertebral subluxation leads to interference with an \"innate intelligence\" exerted via the human nervous system and is a primary underlying risk factor for many diseases. Straights view the medical diagnosis of patient complaints (which they consider to be the \"secondary effects\" of subluxations) to be unnecessary for chiropractic treatment. Thus, straight chiropractors are concerned primarily with the detection and correction of vertebral subluxation via adjustment and do not \"mix\" other types of therapies into their practice style. Their philosophy and explanations are metaphysical in nature and they prefer to use traditional chiropractic lexicon terminology (e.g., perform spinal analysis, detect subluxation, correct with adjustment).l They prefer to remain separate and distinct from mainstream health care. Although considered the minority group, \"they have been able to transform their status as purists and heirs of the lineage into influence dramatically out of proportion to their numbers.\"\n\n\"Mixer\" chiropractors \"mix\" diagnostic and treatment approaches from chiropractic, medical and/or osteopathic viewpoints and make up the majority of chiropractors. Unlike straight chiropractors, mixers believe subluxation is one of many causes of disease, and hence they tend to be open to mainstream medicine. Many of them incorporate mainstream medical diagnostics and employ conventional treatments including techniques of physical therapy such as exercise, stretching, massage, ice packs, electrical muscle stimulation, therapeutic ultrasound, and moist heat. Some mixers also use techniques from alternative medicine, including nutritional supplements, acupuncture, homeopathy, herbal remedies, and biofeedback.\n\nAlthough mixers are the majority group, many of them retain belief in vertebral subluxation as shown in a 2003 survey of 1100 North American chiropractors, which found that 88% wanted to retain the term \"vertebral subluxation complex\", and that when asked to estimate the percent of disorders of internal organs (such as the heart, the lungs, or the stomach) that subluxation significantly contributes to, the mean response was 62%. A 2008 survey of 6,000 American chiropractors demonstrated that most chiropractors seem to believe that a subluxation-based clinical approach may be of limited utility for addressing visceral disorders, and greatly favored non-subluxation-based clinical approaches for such conditions. The same survey showed that most chiropractors generally believed that the majority of their clinical approach for addressing musculoskeletal/biomechanical disorders such as back pain was based on subluxation.\n\nPalmer hypothesized that vertebral joint misalignments, which he termed \"vertebral subluxations\", interfered with the body's function and its inborn ability to heal itself. D. D. Palmer repudiated his earlier theory that vertebral subluxations caused pinched nerves in the intervertebral spaces in favor of subluxations causing altered nerve vibration, either too tense or too slack, affecting the tone (health) of the end organ. D. D. Palmer, using a vitalistic approach, imbued the term \"subluxation\" with a metaphysical and philosophical meaning. He qualified this by noting that knowledge of innate intelligence was not essential to the competent practice of chiropractic. This concept was later expanded upon by his son, B. J. Palmer, and was instrumental in providing the legal basis of differentiating chiropractic from conventional medicine. In 1910, D. D. Palmer theorized that the nervous system controlled health:\n\nVertebral subluxation, a core concept of traditional chiropractic, remains unsubstantiated and largely untested, and a debate about whether to keep it in the chiropractic paradigm has been ongoing for decades. In general, critics of traditional subluxation-based chiropractic (including chiropractors) are skeptical of its clinical value, dogmatic beliefs and metaphysical approach. While straight chiropractic still retains the traditional vitalistic construct espoused by the founders, evidence-based chiropractic suggests that a mechanistic view will allow chiropractic care to become integrated into the wider health care community. This is still a continuing source of debate within the chiropractic profession as well, with some schools of chiropractic still teaching the traditional/straight subluxation-based chiropractic, while others have moved towards an evidence-based chiropractic that rejects metaphysical foundings and limits itself to primarily neuromusculoskeletal conditions.\n\nIn 2005, the chiropractic subluxation was defined by the World Health Organization as \"a lesion or dysfunction in a joint or motion segment in which alignment, movement integrity and/or physiological function are altered, although contact between joint surfaces remains intact. It is essentially a functional entity, which may influence biomechanical and neural integrity.\" This differs from the medical definition of subluxation as a significant structural displacement, which can be seen with static imaging techniques such as X-rays. The 2008 book \"Trick or Treatment\" states \"X-rays can reveal neither the subluxations nor the innate intelligence associated with chiropractic philosophy, because they do not exist.\" Attorney David Chapman-Smith, Secretary-General of the World Federation of Chiropractic, has stated that \"Medical critics have asked how there can be a subluxation if it cannot be seen on X-ray. The answer is that the chiropractic subluxation is essentially a functional entity, not structural, and is therefore no more visible on static X-ray than a limp or headache or any other functional problem.\" The General Chiropractic Council, the statutory regulatory body for chiropractors in the United Kingdom, states that the chiropractic vertebral subluxation complex \"is not supported by any clinical research evidence that would allow claims to be made that it is the cause of disease.\"\n\nAs of 2014, the National Board of Chiropractic Examiners states \"The specific focus of chiropractic practice is known as the chiropractic subluxation or joint dysfunction. A subluxation is a health concern that manifests in the skeletal joints, and, through complex anatomical and physiological relationships, affects the nervous system and may lead to reduced function, disability or illness.\"\n\nChiropractors emphasize the conservative management of the neuromusculoskeletal system without the use of medicines or surgery, with special emphasis on the spine. Back and neck pain are the specialties of chiropractic but many chiropractors treat ailments other than musculoskeletal issues. There is a range of opinions among chiropractors: some believed that treatment should be confined to the spine, or back and neck pain; others disagreed. For example, while one 2009 survey of American chiropractors had found that 73% classified themselves as \"back pain/musculoskeletal specialists\", the label \"back and neck pain specialists\" was regarded by 47% of them as a \"least\" desirable description in a 2005 international survey. Chiropractic combines aspects from mainstream and alternative medicine, and there is no agreement about how to define the profession: although chiropractors have many attributes of primary care providers, chiropractic has more of the attributes of a medical specialty like dentistry or podiatry. It has been proposed that chiropractors specialize in nonsurgical spine care, instead of attempting to also treat other problems, but the more expansive view of chiropractic is still widespread. Mainstream health care and governmental organizations such as the World Health Organization consider chiropractic to be complementary and alternative medicine (CAM); and a 2008 study reported that 31% of surveyed chiropractors categorized chiropractic as CAM, 27% as integrated medicine, and 12% as mainstream medicine. Many chiropractors believe they are primary care providers, including US and UK chiropractors, but the length, breadth, and depth of chiropractic clinical training do not support the requirements to be considered primary care providers, so their role on primary care is limited and disputed.\n\nChiropractic overlaps with several other forms of manual therapy, including massage therapy, osteopathy, physical therapy, and sports medicine. Chiropractic is autonomous from and competitive with mainstream medicine, and osteopathy outside the US remains primarily a manual medical system; physical therapists work alongside and cooperate with mainstream medicine, and osteopathic medicine in the U.S. has merged with the medical profession. Practitioners may distinguish these competing approaches through claims that, compared to other therapists, chiropractors heavily emphasize spinal manipulation, tend to use firmer manipulative techniques, and promote maintenance care; that osteopaths use a wider variety of treatment procedures; and that physical therapists emphasize machinery and exercise.\n\nChiropractic diagnosis may involve a range of methods including skeletal imaging, observational and tactile assessments, and orthopedic and neurological evaluation. A chiropractor may also refer a patient to an appropriate specialist, or co-manage with another health care provider. Common patient management involves spinal manipulation (SM) and other manual therapies to the joints and soft tissues, rehabilitative exercises, health promotion, electrical modalities, complementary procedures, and lifestyle advice.\n\nChiropractors are not normally licensed to write medical prescriptions or perform major surgery in the United States, (although New Mexico has become the first US state to allow \"advanced practice\" trained chiropractors to prescribe certain medications.). In the US, their scope of practice varies by state, based on inconsistent views of chiropractic care: some states, such as Iowa, broadly allow treatment of \"human ailments\"; some, such as Delaware, use vague concepts such as \"transition of nerve energy\" to define scope of practice; others, such as New Jersey, specify a severely narrowed scope. US states also differ over whether chiropractors may conduct laboratory tests or diagnostic procedures, dispense dietary supplements, or use other therapies such as homeopathy and acupuncture; in Oregon they can become certified to perform minor surgery and to deliver children via natural childbirth. A 2003 survey of North American chiropractors found that a slight majority favored allowing them to write prescriptions for over-the-counter drugs. A 2010 survey found 72% of Switzerland chiropractors judged the current allowance in Switzerland to prescribing nonprescription medication as an advantage for chiropractic treatment.\n\nA related field, veterinary chiropractic, applies manual therapies to animals and is recognized in 40 US states, but is not recognized by the American Chiropractic Association as being chiropractic. It remains controversial within certain segments of the veterinary, and chiropractic profession.\n\nNo single profession \"owns\" spinal manipulation and there is little consensus as to which profession should administer SM, raising concerns by chiropractors that other medical physicians could \"steal\" SM procedures from chiropractors. A focus on evidence-based SM research has also raised concerns that the resulting practice guidelines could limit the scope of chiropractic practice to treating backs and necks. Two U.S. states (Washington and Arkansas) prohibit physical therapists from performing SM, some states allow them to do it only if they have completed advanced training in SM, and some states allow only chiropractors to perform SM, or only chiropractors and physicians. Bills to further prohibit non-chiropractors from performing SM are regularly introduced into state legislatures and are opposed by physical therapist organizations.\n\nSpinal manipulation, which chiropractors call \"spinal adjustment\" or \"chiropractic adjustment\", is the most common treatment used in chiropractic care. Spinal manipulation is a passive manual maneuver during which a three-joint complex is taken past the normal range of movement, but not so far as to dislocate or damage the joint. Its defining factor is a dynamic thrust, which is a sudden force that causes an audible release and attempts to increase a joint's range of motion. High-velocity, low-amplitude spinal manipulation (HVLA-SM) thrusts have physiological effects that signal neural discharge from paraspinal muscle tissues, depending on duration and amplitude of the thrust are factors of the degree in paraspinal muscle spindles activation. Clinical skill in employing HVLA-SM thrusts depends on the ability of the practitioner to handle the duration and magnitude of the load. More generally, spinal manipulative therapy (SMT) describes techniques where the hands are used to manipulate, massage, mobilize, adjust, stimulate, apply traction to, or otherwise influence the spine and related tissues.\n\nThere are several schools of chiropractic adjustive techniques, although most chiropractors mix techniques from several schools. The following adjustive procedures were received by more than 10% of patients of licensed U.S. chiropractors in a 2003 survey: Diversified technique (full-spine manipulation, employing various techniques), extremity adjusting, Activator technique (which uses a spring-loaded tool to deliver precise adjustments to the spine), Thompson Technique (which relies on a drop table and detailed procedural protocols), Gonstead (which emphasizes evaluating the spine along with specific adjustment that avoids rotational vectors), Cox/flexion-distraction (a gentle, low-force adjusting procedure which mixes chiropractic with osteopathic principles and utilizes specialized adjusting tables with movable parts), adjustive instrument, Sacro-Occipital Technique (which models the spine as a torsion bar), Nimmo Receptor-Tonus Technique, applied kinesiology (which emphasises \"muscle testing\" as a diagnostic tool), and cranial. Chiropractic biophysics technique uses inverse functions of rotations during spinal manipulation. Koren Specific Technique (KST) may use their hands, or they may use an electric device known as an \"ArthroStim\" for assessment and spinal manipulations. Insurers in the US and UK that cover other chiropractic techniques exclude KST from coverage because they consider it to be \"experimental and investigational.\" Medicine-assisted manipulation, such as manipulation under anesthesia, involves sedation or local anesthetic and is done by a team that includes an anesthesiologist; a 2008 systematic review did not find enough evidence to make recommendations about its use for chronic low back pain.\n\nMany other procedures are used by chiropractors for treating the spine, other joints and tissues, and general health issues. The following procedures were received by more than one-third of patients of licensed U.S. chiropractors in a 2003 survey: Diversified technique (full-spine manipulation; mentioned in previous paragraph), physical fitness/exercise promotion, corrective or therapeutic exercise, ergonomic/postural advice, self-care strategies, activities of daily living, changing risky/unhealthy behaviors, nutritional/dietary recommendations, relaxation/stress reduction recommendations, ice pack/cryotherapy, extremity adjusting (also mentioned in previous paragraph), trigger point therapy, and disease prevention/early screening advice.\n\nA 2010 study describing Belgium chiropractors and their patients found chiropractors in Belgium mostly focus on neuromusculoskeletal complaints in adult patients, with emphasis on the spine. The diversified technique is the most often applied technique at 93%, followed by the Activator mechanical-assisted technique at 41%. A 2009 study assessing chiropractic students giving or receiving spinal manipulations while attending a U.S. chiropractic college found Diversified, Gonstead, and upper cervical manipulations are frequently used methods.\n\nReviews of research studies within the chiropractic community have been used to generate practice guidelines outlining standards that specify which chiropractic treatments are \"legitimate\" (i.e. supported by evidence) and conceivably reimbursable under managed care health payment systems. Evidence-based guidelines are supported by one end of an ideological continuum among chiropractors; the other end employs antiscientific reasoning and makes unsubstantiated claims. Chiropractic remains at a crossroads, and that in order to progress it would need to embrace science; the promotion by some for it to be a cure-all was both \"misguided and irrational\". A 2007 survey of Alberta chiropractors found that they do not consistently apply research in practice, which may have resulted from a lack of research education and skills.\n\nThere is no good evidence that chiropractic is effective for the treatment of any medical condition, except perhaps for certain kinds of back pain. Generally, the research carried out into the effectiveness of chiropractic has been of poor quality. Numerous controlled clinical studies of treatments used by chiropractors have been conducted, with conflicting results. Research published by chiropractors is distinctly biased. For reviews of SM for back pain chiropractic authors tend to have positive conclusions, while others did not show any effectiveness.\n\nThere is a wide range of ways to measure treatment outcomes. Chiropractic care, like all medical treatment, benefits from the placebo response. It is difficult to construct a trustworthy placebo for clinical trials of spinal manipulative therapy (SMT), as experts often disagree about whether a proposed placebo actually has no effect. The efficacy of maintenance care in chiropractic is unknown.\n\nAvailable evidence covers the following conditions:\n\nThere is not sufficient data to establish the safety of chiropractic manipulations. Manipulation is regarded as relatively safe but complications can arise, and it has known adverse effects, risks and contraindications. Absolute contraindications to spinal manipulative therapy are conditions that should not be manipulated; these contraindications include rheumatoid arthritis and conditions known to result in unstable joints. Relative contraindications are conditions where increased risk is acceptable in some situations and where low-force and soft-tissue techniques are treatments of choice; these contraindications include osteoporosis. Although most contraindications apply only to manipulation of the affected region, some neurological signs indicate referral to emergency medical services; these include sudden and severe headache or neck pain unlike that previously experienced. Indirect risks of chiropractic involve delayed or missed diagnoses through consulting a chiropractor.\n\nSpinal manipulation is associated with frequent, mild and temporary adverse effects, including new or worsening pain or stiffness in the affected region. They have been estimated to occur in 33% to 61% of patients, and frequently occur within an hour of treatment and disappear within 24 to 48 hours; adverse reactions appear to be more common following manipulation than mobilization. Chiropractic is correlated with a very high incidence of minor adverse effects. Chiropractic are more commonly associated with serious related adverse effects than other professionals following manipulation. Rarely, spinal manipulation, particularly on the upper spine, can also result in complications that can lead to permanent disability or death; these can occur in adults and children. Estimates vary widely for the incidence of these complications, and the actual incidence is unknown, due to high levels of underreporting and to the difficulty of linking manipulation to adverse effects such as stroke, which is a particular concern. Adverse effects are poorly reported in recent studies investigating chiropractic manipulations. A 2016 systematic review concludes that the level of reporting is unsuitable and unacceptable. Reports of serious adverse events have occurred, resulting from spinal manipulation therapy of the lumbopelvic region. Estimates for serious adverse events vary from 5 strokes per 100,000 manipulations to 1.46 serious adverse events per 10 million manipulations and 2.68 deaths per 10 million manipulations, though it was determined that there was inadequate data to be conclusive. Several case reports show temporal associations between interventions and potentially serious complications. The published medical literature contains reports of 26 deaths since 1934 following chiropractic manipulations and many more seem to remain unpublished.\n\nVertebrobasilar artery stroke (VAS) is statistically associated with chiropractic services in persons under 45 years of age, but it is similarly associated with general practitioner services, suggesting that these associations are likely explained by preexisting conditions. Weak to moderately strong evidence supports causation (as opposed to statistical association) between cervical manipulative therapy (CMT) and VAS. There is insufficient evidence to support a strong association or no association between cervical manipulation and stroke. While the biomechanical evidence is not sufficient to support the statement that CMT causes cervical artery dissection (CD), clinical reports suggest that mechanical forces have a part in a substantial number of CDs and the majority of population controlled studies found an association between CMT and VAS in young people. It is strongly recommended that practitioners consider the plausibility of CD as a symptom, and people can be informed of the association between CD and CMT before administrating manipulation of the cervical spine. There is controversy regarding the degree of risk of stroke from cervical manipulation. Many chiropractors state that, the association between chiropractic therapy and vertebral arterial dissection is not proven. However, it has been suggested that the causality between chiropractic cervical manipulation beyond the normal range of motion and vascular accidents is probable or definite. There is very low evidence supporting a small association between internal carotid artery dissection and chiropractic neck manipulation. The incidence of internal carotid artery dissection following cervical spine manipulation is unknown. The literature infrequently reports helpful data to better understand the association between cervical manipulative therapy, cervical artery dissection and stroke. The limited evidence is inconclusive that chiropractic spinal manipulation therapy is not a cause of intracranial hypotension.\n\nChiropractors, like other primary care providers, sometimes employ diagnostic imaging techniques such as X-rays and CT scans that rely on ionizing radiation. Although there is no clear evidence for the practice, some chiropractors may still X-ray a patient several times a year. Practice guidelines aim to reduce unnecessary radiation exposure, which increases cancer risk in proportion to the amount of radiation received. Research suggests that radiology instruction given at chiropractic schools worldwide seem to be evidence-based. Although, there seems to be a disparity between some schools and available evidence regarding the aspect of radiography for patients with acute low back pain without an indication of a serious disease, which may contribute to chiropractic overuse of radiography for low back pain.\n\nA 2012 systematic review concluded that no accurate assessment of risk-benefit exists for cervical manipulation. A 2010 systematic review stated that there is no good evidence to assume that neck manipulation is an effective treatment for any medical condition and suggested a precautionary principle in healthcare for chiropractic intervention even if a causality with vertebral artery dissection after neck manipulation were merely a remote possibility. The same review concluded that the risk of death from manipulations to the neck outweighs the benefits. Chiropractors have criticized this conclusion, claiming that the author did not evaluate the potential benefits of spinal manipulation. Edzard Ernst stated \"This detail was not the subject of my review. I do, however, refer to such evaluations and should add that a report recently commissioned by the General Chiropractic Council did not support many of the outlandish claims made by many chiropractors across the world.\"\n\nA 2009 review evaluating maintenance chiropractic care found that spinal manipulation is routinely associated with considerable harm and no compelling evidence exists to indicate that it adequately prevents symptoms or diseases, thus the risk-benefit is not evidently favorable.\n\nA 2012 systematic review suggested that the use of spine manipulation in clinical practice is a cost-effective treatment when used alone or in combination with other treatment approaches. A 2011 systematic review found evidence supporting the cost-effectiveness of using spinal manipulation for the treatment of sub-acute or chronic low back pain; the results for acute low back pain were insufficient.\n\nA 2006 systematic cost-effectiveness review found that the reported cost-effectiveness of spinal manipulation in the United Kingdom compared favorably with other treatments for back pain, but that reports were based on data from clinical trials without sham controls and that the specific cost-effectiveness of the treatment (as opposed to non-specific effects) remains uncertain. A 2005 American systematic review of economic evaluations of conservative treatments for low back pain found that significant quality problems in available studies meant that definite conclusions could not be drawn about the most cost-effective intervention. The cost-effectiveness of maintenance chiropractic care is unknown.\n\nAnalysis of a clinical and cost utilization data from the years 2003 to 2005 by an integrative medicine independent physician association (IPA) which looked the chiropractic services utilization found that the clinical and cost utilization of chiropractic services based on 70,274 member-months over a 7-year period decreased patient costs associate with the following use of services by 60.2% for in-hospital admissions, 59.0% for hospital days, 62.0% for outpatient surgeries and procedures, and 85% for pharmaceutical costs when compared with conventional medicine (visit to a medical doctor primary care provider) IPA performance for the same health maintenance organization product in the same geography and time frame.\n\nRequirements vary between countries. In the U.S. chiropractors obtain a first professional degree in the field of chiropractic. Chiropractic education in the U.S. have been criticized for failing to meet generally accepted standards of evidence-based medicine. The curriculum content of North American chiropractic and medical colleges with regard to basic and clinical sciences has been more similar than not, both in the kinds of subjects offered and in the time assigned to each subject. Accredited chiropractic programs in the U.S. require that applicants have 90 semester hours of undergraduate education with a grade point average of at least 3.0 on a 4.0 scale. Many programs require at least three years of undergraduate education, and more are requiring a bachelor's degree. Canada requires a minimum three years of undergraduate education for applicants, and at least 4200 instructional hours (or the equivalent) of full‐time chiropractic education for matriculation through an accredited chiropractic program. Graduates of the Canadian Memorial Chiropractic College (CMCC) are formally recognized to have at least 7–8 years of university level education. The World Health Organization (WHO) guidelines suggest three major full-time educational paths culminating in either a DC, DCM, BSc, or MSc degree. Besides the full-time paths, they also suggest a conversion program for people with other health care education and limited training programs for regions where no legislation governs chiropractic.\n\nUpon graduation, there may be a requirement to pass national, state, or provincial board examinations before being licensed to practice in a particular jurisdiction. Depending on the location, continuing education may be required to renew these licenses. Specialty training is available through part-time postgraduate education programs such as chiropractic orthopedics and sports chiropractic, and through full-time residency programs such as radiology or orthopedics.\n\nIn the U.S., chiropractic schools are accredited through the Council on Chiropractic Education (CCE) while the General Chiropractic Council (GCC) is the statutory governmental body responsible for the regulation of chiropractic in the UK. The U.S. CCE requires a mixing curriculum, which means a straight-educated chiropractor may not be eligible for licensing in states requiring CCE accreditation. CCEs in the U.S., Canada, Australia and Europe have joined to form CCE-International (CCE-I) as a model of accreditation standards with the goal of having credentials portable internationally. Today, there are 18 accredited Doctor of Chiropractic programs in the U.S., 2 in Canada, 6 in Australasia, and 5 in Europe. All but one of the chiropractic colleges in the U.S. are privately funded, but in several other countries they are in government-sponsored universities and colleges. Of the two chiropractic colleges in Canada, one is publicly funded (UQTR) and one is privately funded (CMCC). In 2005, CMCC was granted the privilege of offering a professional health care degree under the Post-secondary Education Choice and Excellence Act, which sets the program within the hierarchy of education in Canada as comparable to that of other primary contact health care professions such as medicine, dentistry and optometry.\n\nRegulatory colleges and chiropractic boards in the U.S., Canada, Mexico, and Australia are responsible for protecting the public, standards of practice, disciplinary issues, quality assurance and maintenance of competency. There are an estimated 49,000 chiropractors in the U.S. (2008), 6,500 in Canada (2010), 2,500 in Australia (2000), b and 1,500 in the UK (2000).\n\nChiropractors often argue that this education is as good as or better than medical physicians', but most chiropractic training is confined to classrooms with much time spent learning theory, adjustment, and marketing. The fourth year of chiropractic education persistently showed the highest stress levels. Every student, irrespective of year, experienced different ranges of stress when studying. The chiropractic leaders and colleges have had internal struggles. Rather than cooperation, there has been infighting between different factions. A number of actions were posturing due to the confidential nature of the chiropractic colleges in an attempt to enroll students.\n\nThe chiropractic oath is a modern variation of the classical Hippocratic Oath historically taken by physicians and other healthcare professionals swearing to practice their professions ethically. The American Chiropractic Association (ACA) has an ethical code \"based upon the acknowledgement that the social contract dictates the profession’s responsibilities to the patient, the public, and the profession; and upholds the fundamental principle that the paramount purpose of the chiropractic doctor's professional services shall be to benefit the patient.\" The International Chiropractor's Association (ICA) also has a set of professional canons.\n\nA 2008 commentary proposed that the chiropractic profession actively regulate itself to combat abuse, fraud, and quackery, which are more prevalent in chiropractic than in other health care professions, violating the social contract between patients and physicians. According to a 2015 Gallup poll of U.S. adults, the perception of chiropractors is generally favourable; two-thirds of American adults agree that chiropractors have their patient's best interest in mind and more than half also agree that most chiropractors are trustworthy. Less than 10% of US adults disagreed with the statement that chiropractors were trustworthy.\n\nChiropractors, especially in America, have a reputation for unnecessarily treating patients. In many circumstances the focus seems to be put on economics instead of health care. Sustained chiropractic care is promoted as a preventative tool, but unnecessary manipulation could possibly present a risk to patients. Some chiropractors are concerned by the routine unjustified claims chiropractors have made. A 2010 analysis of chiropractic websites found the majority of chiropractors and their associations made claims of effectiveness not supported by scientific evidence, while 28% of chiropractor websites advocate lower back pain care, which has some sound evidence.\n\nThe US Office of the Inspector General (OIG) estimates that for calendar year 2013, 82% of payments to chiropractors under Medicare part B, a total of $359 million, did not comply with Medicare requirements. There have been at least 15 OIG reports about chiropractic billing irregularities since 1986.\n\nIn 2009, a backlash to the libel suit filed by the British Chiropractic Association (BCA) against Simon Singh, has inspired the filing of formal complaints of false advertising against more than 500 individual chiropractors within one 24-hour period, prompting the McTimoney Chiropractic Association to write to its members advising them to remove leaflets that make claims about whiplash and colic from their practice, to be wary of new patients and telephone inquiries, and telling their members: \"If you have a website, take it down NOW.\" and \"Finally, we strongly suggest you do NOT discuss this with others, especially patients.\" An editorial in \"Nature\" has suggested that the BCA may be trying to suppress debate and that this use of British libel law is a burden on the right to freedom of expression, which is protected by the European Convention on Human Rights. The libel case ended with the BCA withdrawing its suit in 2010.\n\nChiropractic is established in the US, Canada, and Australia, and is present to a lesser extent in many other countries. It is viewed as a marginal complementary and alternative medicine health care profession.\n\nIn Australia, most private health insurance funds cover chiropractic care, and the federal government funds chiropractic care when the patient is referred by a medical practitioner.\n\nIn the United Kingdom chiropractic is available on the National Health Service in some areas, such as Cornwall where the treatment is only available for neck or back pain.\n\nA 2010 study by questionnaire presented to UK chiropractors indicated only 45% of chiropractors disclosed with patients the serious risk associated with manipulation of the cervical spine and that 46% believed there was possibility of patient would refuse treatment if risk correctly explained. However 80% acknowledged the ethical/moral responsibility to disclose risk to patient.\n\nThe percentage of the population that utilizes chiropractic care at any given time generally falls into a range from 6% to 12% in the U.S. and Canada, with a global high of 20% in Alberta in 2006. In 2008, chiropractors were reported to be the most common CAM providers for children and adolescents, consuming up to 14% of all visits to chiropractors. In 2002–03, the majority of those who sought chiropractic did so for relief from back and neck pain and other neuromusculoskeletal complaints; most do so specifically for low back pain. The majority of U.S. chiropractors participate in some form of managed care. Although the majority of U.S. chiropractors view themselves as specialists in neuroleptic malignant syndrome conditions, many also consider chiropractic as a type of primary care. In the majority of cases, the care that chiropractors and physicians provide divides the market, however for some, their care is complementary.\n\nIn the U.S., chiropractors perform over 90% of all manipulative treatments. Satisfaction rates are typically higher for chiropractic care compared to medical care, with a 1998 U.S. survey reporting 83% of respondents satisfied or very satisfied with their care; quality of communication seems to be a consistent predictor of patient satisfaction with chiropractors.\n\nUtilization of chiropractic care is sensitive to the costs incurred by the co-payment by the patient. The use of chiropractic declined from 9.9% of U.S. adults in 1997 to 7.4% in 2002; this was the largest relative decrease among CAM professions, which overall had a stable use rate. As of 2007 7% of the U.S. population is being reached by chiropractic. Employment of U.S. chiropractors is expected to increase 14% between 2006 and 2016, faster than the average for all occupations.\n\nIn the U.S., most states require insurers to cover chiropractic care, and most HMOs cover these services.\n\nChiropractic was founded in 1895 by Daniel David (D.D.) Palmer in Davenport, Iowa. Palmer, a magnetic healer, hypothesized that manual manipulation of the spine could cure disease. The first chiropractic patient of D.D. Palmer was Harvey Lillard, a worker in the building where Palmer's office was located. He claimed that he had severely reduced hearing for 17 years, which started soon following a \"pop\" in his spine. A few days following his adjustment, Lillard claimed his hearing was almost completely restored. Chiropractic competed with its predecessor osteopathy, another medical system based on magnetic healing and bonesetting; both systems were founded by charismatic midwesterners in opposition to the conventional medicine of the day, and both postulated that manipulation improved health. Although initially keeping chiropractic a family secret, in 1898 Palmer began teaching it to a few students at his new Palmer School of Chiropractic. One student, his son Bartlett Joshua (B.J.) Palmer, became committed to promoting chiropractic, took over the Palmer School in 1906, and rapidly expanded its enrollment.\n\nEarly chiropractors believed that all disease was caused by interruptions in the flow of innate intelligence, a vitalistic nervous energy or life force that represented God's presence in man; chiropractic leaders often invoked religious imagery and moral traditions. D.D. and B.J. both seriously considered declaring chiropractic a religion, which might have provided legal protection under the U.S. constitution, but decided against it partly to avoid confusion with Christian Science. Early chiropractors also tapped into the Populist movement, emphasizing craft, hard work, competition, and advertisement, aligning themselves with the common man against intellectuals and trusts, among which they included the American Medical Association (AMA).\n\nChiropractic has seen considerable controversy and criticism. Although D.D. and B.J. were \"straight\" and disdained the use of instruments, some early chiropractors, whom B.J. scornfully called \"mixers\", advocated the use of instruments. In 1910 B.J. changed course and endorsed X-rays as necessary for diagnosis; this resulted in a significant exodus from the Palmer School of the more conservative faculty and students. The mixer camp grew until by 1924 B.J. estimated that only 3,000 of the U.S.'s 25,000 chiropractors remained straight. That year, B.J.'s invention and promotion of the neurocalometer, a temperature-sensing device, was highly controversial among B.J.'s fellow straights. By the 1930s chiropractic was the largest alternative healing profession in the U.S.\n\nChiropractors faced heavy opposition from organized medicine. Thousands of chiropractors were prosecuted for practicing medicine without a license, and D.D. and many other chiropractors were jailed. To defend against medical statutes B.J. argued that chiropractic was separate and distinct from medicine, asserting that chiropractors \"analyzed\" rather than \"diagnosed\", and \"adjusted\" subluxations rather than \"treated\" disease. B.J. cofounded the Universal Chiropractors' Association (UCA) to provide legal services to arrested chiropractors. Although the UCA won their first test case in Wisconsin in 1907, prosecutions instigated by state medical boards became increasingly common and in many cases were successful. In response, chiropractors conducted political campaigns to secure separate licensing statutes, eventually succeeding in all fifty states, from Kansas in 1913 through Louisiana in 1974. The longstanding feud between chiropractors and medical doctors continued for decades. The AMA labeled chiropractic an \"unscientific cult\" in 1966, and until 1980 advised its members that it was unethical for medical doctors to associate with \"unscientific practitioners\". This culminated in a landmark 1987 decision, \"Wilk v. AMA\", in which the court found that the AMA had engaged in unreasonable restraint of trade and conspiracy, and which ended the AMA's de facto boycott of chiropractic.\n\nSerious research to test chiropractic theories did not begin until the 1970s, and is continuing to be hampered by antiscientific and pseudoscientific ideas that sustained the profession in its long battle with organized medicine. By the mid 1990s there was a growing scholarly interest in chiropractic, which helped efforts to improve service quality and establish clinical guidelines that recommended manual therapies for acute low back pain. In recent decades chiropractic gained legitimacy and greater acceptance by medical physicians and health plans, and enjoyed a strong political base and sustained demand for services. However, its future seemed uncertain: as the number of practitioners grew, evidence-based medicine insisted on treatments with demonstrated value, managed care restricted payment, and competition grew from massage therapists and other health professions. The profession responded by marketing natural products and devices more aggressively, and by reaching deeper into alternative medicine and primary care.\n\nThe word \"chiropractic\" comes from Greek χειρο- \"chiro-\" (itself from χείρ \"cheir\" \"hand\"), \"hand\" and πρακτικός \"praktikos\", \"practical.\" Chiropractic is classified as a field of pseudomedicine on account of its esoteric origins.\n\nSome chiropractors oppose vaccination and water fluoridation, which are common public health practices. Within the chiropractic community there are significant disagreements about vaccination, one of the most cost-effective public health interventions available. Most chiropractic writings on vaccination focus on its negative aspects, claiming that it is hazardous, ineffective, and unnecessary. Some chiropractors have embraced vaccination, but a significant portion of the profession rejects it, as original chiropractic philosophy traces diseases to causes in the spine and states that vaccines interfere with healing. The extent to which anti-vaccination views perpetuate the current chiropractic profession is uncertain. The American Chiropractic Association and the International Chiropractors Association support individual exemptions to compulsory vaccination laws, and a 1995 survey of U.S. chiropractors found that about a third believed there was no scientific proof that immunization prevents disease. The Canadian Chiropractic Association supports vaccination; a survey in Alberta in 2002 found that 25% of chiropractors advised patients for, and 27% against, vaccinating themselves or their children.\n\nEarly opposition to water fluoridation included chiropractors, some of whom continue to oppose it as being incompatible with chiropractic philosophy and an infringement of personal freedom. Other chiropractors have actively promoted fluoridation, and several chiropractic organizations have endorsed scientific principles of public health. In addition to traditional chiropractic opposition to water fluoridation and vaccination, chiropractors' attempts to establish a positive reputation for their public health role are also compromised by their reputation for recommending repetitive lifelong chiropractic treatment.\n\nThroughout its history chiropractic has been the subject of internal and external controversy and criticism. According to Daniel D. Palmer, the founder of chiropractic, subluxation is the sole cause of disease and manipulation is the cure for all diseases of the human race. A 2003 profession-wide survey found \"most chiropractors (whether 'straights' or 'mixers') still hold views of 'innate intelligence' and of the cause and cure of disease (not just back pain) consistent with those of the Palmers.\" A critical evaluation stated \"Chiropractic is rooted in mystical concepts. This led to an internal conflict within the chiropractic profession, which continues today.\" Chiropractors, including D.D. Palmer, were jailed for practicing medicine without a license. For most of its existence, chiropractic has battled with mainstream medicine, sustained by antiscientific and pseudoscientific ideas such as subluxation. Collectively, systematic reviews have not demonstrated that spinal manipulation, the main treatment method employed by chiropractors, is effective for any medical condition, with the possible exception of treatment for back pain. Chiropractic remains controversial, though to a lesser extent than in past years.\n\n\n", "id": "7738", "title": "Chiropractic"}
{"url": "https://en.wikipedia.org/wiki?curid=7739", "text": "Carbide\n\nIn chemistry, a carbide is a compound composed of carbon and a less electronegative element. Carbides can be generally classified by chemical bonding type as follows: (i) salt-like, (ii) covalent compounds, (iii) interstitial compounds, and (iv) \"intermediate\" transition metal carbides. Examples include calcium carbide (CaC), silicon carbide (SiC), tungsten carbide (WC) (often called simply \"carbide\" when referring to machine tooling), and cementite (FeC), each used in key industrial applications. The naming of ionic carbides is not systematic.\n\nSalt-like carbides are composed of highly electropositive elements such as the alkali metals, alkaline earth metals, and group 3 metals, including scandium, yttrium, and lanthanum. Aluminium from group 13 forms carbides, but gallium, indium, and thallium do not. These materials feature isolated carbon centers, often described as \"C\", in the methanides or methides; two-atom units, \"C\", in the acetylides; and three-atom units, \"C\", in the sesquicarbides. The graphite intercalation compound KC, prepared from vapour of potassium and graphite, and the alkali metal derivatives of C are not usually classified as carbides.\n\nCarbides of this class decompose in water producing methane. Three such examples are aluminium carbide , magnesium carbide and beryllium carbide .\n\nTransition metal carbides are not saline carbides but their reaction with water is very slow and is usually neglected. For example, depending on surface porosity, 5–30 atomic layers of titanium carbide are hydrolyzed, forming methane within 5 minutes at ambient conditions, following by saturation of the reaction.\n\nNote that methanide in this context is a trivial historical name, according to IUPAC systematic naming conventions a compound such as NaCH would be termed a \"methanide\", although this compound is often called methylsodium.\n\nSeveral carbides are assumed to be salts of the acetylide anion C (also called percarbide), which has a triple bond between the two carbon atoms. Alkali metals, alkaline earth metals, and lanthanoid metals form acetylides, e.g., sodium carbide NaC, calcium carbide CaC, and LaC. Lanthanides also form carbides (sesquicarbides, see below) with formula MC. Metals from group 11 also tend to form acetylides, such as copper(I) acetylide and silver acetylide. Carbides of the actinide elements, which have stoichiometry MC and MC, are also described as salt-like derivatives of C.\n\nThe C-C triple bond length ranges from 119.2 pm in CaC (similar to ethyne), to 130.3 pm in LaC and 134 pm in UC. The bonding in LaC has been described in terms of La with the extra electron delocalised into the antibonding orbital on C, explaining the metallic conduction.\n\nThe polyatomic ion C, sometimes called sesquicarbide or allylenide, is found in LiC and MgC. The ion is linear and is isoelectronic with CO. The C-C distance in MgC is 133.2 pm. MgC yields methylacetylene, CHCCH, and propadiene, CHCCH, on hydrolysis, which was the first indication that it contains C.\n\nThe carbides of silicon and boron are described as \"covalent carbides\", although virtually all compounds of carbon exhibit some covalent character. Silicon carbide has two similar crystalline forms, which are both related to the diamond structure. Boron carbide, BC, on the other hand, has an unusual structure which includes icosahedral boron units linked by carbon atoms. In this respect boron carbide is similar to the boron rich borides. Both silicon carbide (also known as \"carborundum\") and boron carbide are very hard materials and refractory. Both materials are important industrially. Boron also forms other covalent carbides, e.g. BC.\n\nThe carbides of the group 4, 5 and 6 transition metals (with the exception of chromium) are often described as interstitial compounds. These carbides have metallic properties and are refractory. Some exhibit a range of stoichiometries, e.g. titanium carbide, TiC. Titanium carbide and tungsten carbide are important industrially and are used to coat metals in cutting tools.\n\nThe long-held view is that the carbon atoms fit into octahedral interstices in a close-packed metal lattice when the metal atom radius is greater than approximately 135 pm:\n\nThe following table shows actual structures of the metals and their carbides. (N.B. the body centered cubic structure adopted by vanadium, niobium, tantalum, chromium, molybdenum and tungsten is not a close-packed lattice.) The notation \"h/2\" refers to the MC type structure described above, which is only an approximate description of the actual structures. The simple view that the lattice of the pure metal \"absorbs\" carbon atoms can be seen to be untrue as the packing of the metal atom lattice in the carbides is different from the packing in the pure metal, although it is technically correct that the carbon atoms fit into the octahedral interstices of a close-packed metal lattice.\n\nFor a long time the non-stoichiometric phases were believed to be disordered with a random filling of the interstices, however short and longer range ordering has been detected.\n\nIn these carbides, the transition metal ion is smaller than the critical 135 pm, and the structures are not interstitial but are more complex. Multiple stoichiometries are common; for example, iron forms a number of carbides, FeC, FeC and FeC. The best known is cementite, FeC, which is present in steels.\nThese carbides are more reactive than the interstitial carbides; for example, the carbides of Cr, Mn, Fe, Co and Ni are all hydrolysed by dilute acids and sometimes by water, to give a mixture of hydrogen and hydrocarbons. These compounds share features with both the inert interstitials and the more reactive salt-like carbides.\n\nMetal complexes containing C are known as metal carbido complexes. Most common are carbon-centered octahedral clusters, such as [AuC(PPh)] and [FeC(CO)]. Similar species are known for the metal carbonyls and the early metal halides. A few terminal carbides have been isolated, e.g., CRuCl(P(CH)).\n\nMetallocarbohedrynes (or \"met-cars\") are stable clusters with the general formula where M is a transition metal (Ti, Zr, V, etc.).\n\nSome metals, such as lead and tin, are believed not to form carbides under any circumstances. There exists however a mixed titanium-tin carbide, which is a two-dimensional conductor. (In 2007, there were two reports of a lead carbide PbC, apparently of the acetylide type; but these claims have yet to be published in reviewed journals.)\n\nIn addition to the carbides, other groups of related carbon compounds exist:\n", "id": "7739", "title": "Carbide"}
{"url": "https://en.wikipedia.org/wiki?curid=7740", "text": "Charles C. Krulak\n\nCharles Chandler Krulak (born March 4, 1942) served as the 31st Commandant of the Marine Corps from July 1, 1995 to June 30, 1999. He is the son of Lieutenant General Victor H. \"Brute\" Krulak, USMC, who served in World War II, Korea, and Vietnam. He was the 13th President of Birmingham-Southern College after his stint as a non-executive director of English association football club Aston Villa.\n\nKrulak was born in 1942 in Quantico, Virginia, the son of Amy (Chandler) and Victor H. Krulak. He graduated from Phillips Exeter Academy in Exeter, New Hampshire in 1960, where he was classmates with novelist John Irving. Krulak then attended the United States Naval Academy, graduating in 1964 with a bachelor's degree. Krulak also holds a master's degree in labor relations from George Washington University (1973). He is a graduate of the Amphibious Warfare School (1968); the Army Command and General Staff College (1976); and the National War College (1982).\n\nAfter his commissioning and graduation from The Basic School at Marine Corps Base Quantico, Krulak held a variety of command and staff positions. His command positions included: commanding officer of a platoon and two rifle companies during two tours of duty in Vietnam; commanding officer of Special Training Branch and Recruit Series at Marine Corps Recruit Depot San Diego, California (1966–1968); commanding officer of Counter-Guerilla Warfare School, Northern Training Area on Okinawa (1970), Company officer at the United States Naval Academy (1970–1973); commanding officer of the Marine Barracks at Naval Air Station North Island, California (1973–1976), and commanding officer, 3rd Battalion 3rd Marines (1983–1985).\n\nKrulak's staff assignments included: operations officer, 2nd Battalion 9th Marines (1977–1978); chief of the Combat Arms Monitor Section at Headquarters Marine Corps, Washington, D.C. (1978–1979); executive assistant to the Director of Personnel Management, Headquarters Marine Corps (1979–1981); Plans Office, Fleet Marine Forces Pacific, Camp H.M. Smith, Hawaii (1982–1983); executive officer, 3rd Marine Regiment, 1st Marine Expeditionary Brigade; assistant chief of staff, maritime pre-positioning ships, 1st MEB; assistant chief of staff for operations, 1st Marine Expeditionary Brigade; and the military assistant to the assistant secretary of defense for command, control, communications and intelligence, Office of the Secretary of Defense.\n\nHe was assigned duty as the deputy director of the White House Military Office in September 1987. While serving in this capacity, he was selected for promotion to brigadier general in November 1988. He was advanced to that grade on June 5, 1989, and assigned duties as the commanding general, 10th <nowiki>MEB/Assistant</nowiki> division commander, 2nd Marine Division, Fleet Marine Force Atlantic, at Marine Corps Base Camp Lejeune, North Carolina on July 10, 1989. On June 1, 1990, he assumed duties as the commanding general, 2nd Force Service Support Group <nowiki>Group/Commanding</nowiki> general, 6th Marine Expeditionary Brigade, Fleet Marine Force Atlantic and commanded the 2d FSSG during the Gulf War. He served in this capacity until July 12, 1991, and was assigned duty as assistant deputy chief of staff for manpower and reserve affairs (personnel <nowiki>Management/Personnel</nowiki> Procurement), Headquarters Marine Corps on August 5, 1991. He was advanced to major general on March 20, 1992. General Krulak was assigned as commanding general, Marine Corps Combat Development Command, Quantico, on August 24, 1992, and was promoted to lieutenant general on September 1, 1992. On July 22, 1994, he was assigned as commander of Marine Forces <nowiki>Pacific/commanding</nowiki> general, Fleet Marine Force Pacific, and in March 1995 he was nominated to serve as the commandant of the Marine Corps. On June, 29, he was promoted to general and assumed duties as the 31st commandant on June 30, 1995. He was relieved on June 30, 1999, by General James L. Jones.\n\nIn 1997, Krulak became a Life Member of the Sons of the Revolution in the State of California.\n\nGeneral Krulak attracted some attention during his tenure as commandant by his custom of delivering Christmas cookies to each marine duty post in the Washington area.\n\nKrulak joined MBNA America in September 1999 as chief administrative officer, responsible for personnel, benefits, compensation, education, and other administrative services. Krulak has served as the Senior Vice Chairman and Chief Executive Officer of MBNA Europe (2001–2005) and was based at the Chester campus in the UK. He was the executive vice chairman and chief administration officer of MBNA Corporation (2004–2005). He retired from MBNA in 2005.\n\nFollowing the takeover of English football club Aston Villa by MBNA Chairman Randy Lerner in August 2006 and as of September 19, 2006, General Krulak joined the board of Aston Villa as non-executive director where he posted on several fans forums. Krulak was generally referred to as \"The General\" by fans on these boards.\n\nKrulak also serves on the boards of ConocoPhillips, Freeport-McMoran (formerly known as Phelps Dodge Corporation) and Union Pacific Corporation. In addition, he serves on the advisory council of Hope For The Warriors, a national non-profit dedicated to provide a full cycle of non-medical care to combat wounded service members, their families, and families of the fallen from each military branch.\n\nHe was elected as the 13th President of Birmingham–Southern College in Birmingham, Alabama on March 21, 2011, and retired in the summer of 2015.\n\nGeneral Krulak is the Vice Chair of the Sweet Briar College Board of Directors. He joined the Board in the Summer of 2015.\n\nGeneral Krulak's decorations and medals include: \n\nGeneral Krulak famously referred to the \"Strategic Corporal\" and the Three Block War as two of the key lessons identified from the deployments in Somalia, Haiti and Bosnia. These concepts are still considered vital in understanding the increasing complexity of modern battlefields.\n\nGeneral Krulak explained some of his warfighting philosophy in an interview with Tom Clancy in Clancy's nonfiction book \"Marine\". Clancy referred to General Krulak as \"Warrior Prince of the Corps.\"\n\nGeneral Krulak also rewrote the Marine Corps' basic combat study text, \"MCDP-1: Warfighting\", incorporating his theories on operations in the modern battlefield.\n\nAs Commandant of the Marine Corps, General Krulak had a habit of always taking a few minutes when visiting Marine units to mingle with the junior ranking enlisted Marines, earning him much respect and popularity, but much to the chagrin of his bodyguards who were always supposed to be within an arm's distance of the Commandant.\n\nGeneral Krulak is married to Zandi Meyers from Annapolis. They have two sons: Captain Dr. David C. Krulak, a surgeon with the US Marine Corps Forces, Pacific and Todd; and five grandchildren: Brian, Katie, Mary, Matthew and Charles.\nHe is the son of Lieutenant General Victor H. Krulak, Sr., and the younger brother of Commander Victor H. Krulak Jr, Navy Chaplain Corps and Colonel William Krulak, USMCR.\nGeneral Krulak stated in an interview that his godfather was Holland M. \"Howling Mad\" Smith.\n\n\n\n \n", "id": "7740", "title": "Charles C. Krulak"}
{"url": "https://en.wikipedia.org/wiki?curid=7742", "text": "Compaq\n\nCompaq (an acronym from Compatibility And Quality; occasionally referred to as CQ prior to its final logo) was a company founded in 1982 that developed, sold, and supported computers and related products and services. Compaq produced some of the first IBM PC compatible computers, being the first company to legally reverse engineer the IBM Personal Computer. It rose to become the largest supplier of PC systems during the 1990s before being overtaken by HP in 2001. Struggling in the aftermath of the dot-com bubble bust, as well as with a risky acquisition of DEC, Compaq was acquired for US$25 billion by HP in 2002. The Compaq brand remained in use by HP for lower-end systems until 2013 when it was discontinued.\n\nThe company was formed by Rod Canion, Jim Harris and Bill Murto—former Texas Instruments senior managers. Murto (SVP of Sales) departed Compaq in 1987, while Canion (President and CEO) and Harris (SVP of Engineering) left under a shakeup in 1991, which saw Eckhard Pfeiffer appointed President and CEO. Pfeiffer served through the 1990s. Ben Rosen provided the venture capital financing for the fledgling company and served as chairman of the board for 18 years from 1983 until September 28, 2000, when he retired and was succeeded by Michael Capellas, who served as the last Chairman and CEO until its merger with HP.\n\nPrior to its takeover the company was headquartered in a facility in northwest unincorporated Harris County, Texas, that now continues as HP's largest United States facility.\n\nCompaq was founded in February 1982 by Rod Canion, Jim Harris and Bill Murto, three senior managers from semiconductor manufacturer Texas Instruments. The three of them had left due to lack of faith and loss of confidence in TI's management, and initially considered but ultimately decided against starting a chain of Mexican restaurants. Each invested $1,000 to form the company, which was founded with the temporary name Gateway Technology. The name \"COMPAQ\" was said to be derived from \"Compatibility and Quality\" but this explanation was an afterthought. The name was chosen from many suggested by Ogilvy and Mather; it being the name least rejected. The first Compaq PC was sketched out on a table napkin by Ted Papajohn while dining with the founders in a Houston pie shop.\nTheir first venture capital came from Benjamin M. Rosen and Sevin Rosen Funds who helped the fledgling company secure $1.5 million to produce their initial computer. Overall, the founders managed to raise $25 million from venture capitalists, as this gave stability to the new company as well as providing assurances the dealers or middlemen.\n\nUnlike many startups, Compaq differentiated its offerings from the many other IBM clones by not focusing mainly on price, but instead concentrating on new features, such as portability and better graphics displays as well as performance—and all at prices comparable to those of IBM’s PCs. In contrast to Dell Computer and Gateway 2000, Compaq hired veteran engineers with an average of 15 years experience, which lent credibility to Compaq's reputation of reliability among customers. Due to its partnership with Intel, Compaq was able to maintain a technological lead in the market place as it was the first one to come out with computers containing the next generation of each Intel processor.\n\nUnder Canion's direction, Compaq sold computers only through dealers to avoid potential competition that a direct sales channel would foster, which helped foster loyalty among resellers. By giving dealers considerable leeway in pricing Compaq's offerings, either a significant markup for more profits or discount for more sales, dealers had a major incentive to advertise Compaq.\n\nDuring its first year of sales (second year of operation), the company sold 53,000 PCs for sales of $111 million, the first start-up to hit the $100 million mark that fast. Compaq went public in 1983 on the NYSE and raised $67 million. In 1986, it enjoyed record sales of $329 million from 150,000 PCs, and being the youngest-ever firm to make the Fortune 500. In 1987, Compaq hit the $1 billion revenue mark, taking the least amount of time to reach that milestone. By 1991, Compaq held the fifth place spot in the PC market with $3 billion in sales that year.\n\nTwo key marketing executives in Compaq's early years, Jim D'Arezzo and Sparky Sparks, had come from IBM's PC Group. Other key executives responsible for the company's meteoric growth in the late 80s and early 90s were Ross A. Cooley, another former IBM associate, who served for many years as SVP of GM North America; Michael Swavely, who was the company's chief marketing officer in the early years, and eventually ran the North America organization, later passing along that responsibility to Cooley when Swavely retired. In the United States, Brendan A. \"Mac\" McLoughlin (another long time IBM executive) led the company's field sales organization after starting up the Western U.S. Area of Operations. These executives, along with other key contributors, including Kevin Ellington, Douglas Johns, Steven Flannigan, and Gary Stimac, helped the company compete against the IBM Corporation in all personal computer sales categories, after many predicted that none could compete with the behemoth.\n\nThe soft-spoken Canion was popular with employees and the culture that he built helped Compaq to attract the best talent. Instead of headquartering the company in a downtown Houston skyscraper, Canion chose a West Coast-style campus surrounded by forests, where every employee had similar offices and no-one (not even the CEO) had a reserved parking spot. At semi-annual meetings, turnout was high as any employee could ask questions to senior managers.\n\nIn 1987, company co-founder Bill Murto resigned to study at a religious education program at the University of St. Thomas. Murto had helped to organize the company's marketing and authorized-dealer distribution strategy, and held the post of senior vice president of sales since June 1985. Murto was succeeded by Ross A. Cooley, director of corporate sales. Cooley would report to Michael S. Swavely, vice president for marketing, who has been given increased responsibility and the title of vice president for sales and marketing.\n\nIn November 1982 Compaq announced their first product, the Compaq Portable, a portable IBM PC compatible personal computer. It was released in March 1983 at $2995, considerably more affordable than the Canadian Hyperion. The Compaq Portable was one of the progenitors of today's laptop; some called it a \"suitcase computer\" for its size and the look of its case. It was the second IBM PC compatible, being capable of running all software that would run on an IBM PC. It was a commercial success, selling 53,000 units in its first year and generating $111 million in sales revenue. The Compaq Portable was the first in the range of the Compaq Portable series. Compaq was able to market a legal IBM clone because IBM mostly used \"off the shelf\" parts for their PC. Furthermore, Microsoft had kept the right to license the operating system to other computer manufacturers. The only part which had to be duplicated was the BIOS, which Compaq did legally by using clean room design at a cost of $1 million.\n\nUnlike other companies, Compaq did not bundle application software with its computers. Vice President of Sales and Service H. L. Sparks said in early 1984:\n\nCompaq instead emphasized PC compatibility. By October 1983, when the company announced the Compaq Plus with a 10MB hard drive, \"PC Magazine\" wrote of \"the reputation for compatibility it built with its highly regarded floppy disk portable\". Compaq computers remained the most compatible PC clones into 1984, and maintained its reputation for compatibility for years, even as clone BIOSes became available from Phoenix Technologies and other companies that also reverse engineered IBM's design, then sold their version to clone manufacturers.\n\nOn June 28, 1984 Compaq released the Compaq Deskpro, a 16-bit desktop computer using an Intel 8086 microprocessor running at 7.14 MHz. It was considerably faster than an IBM PC and was, like the Compaq Portable, also capable of running IBM software. It was Compaq's first non-portable computer and began the Compaq Deskpro line of computers.\n\nWhen Compaq introduced the first PC based on Intel's new 80386 microprocessor, the Compaq Deskpro 386, in 1986, it marked the first CPU change to the PC platform that was not initiated by IBM. An IBM-made 386 machine eventually reached the market seven months later, but by that time Compaq was the 386 supplier of choice and IBM had lost its image of technical leadership.\n\nFor the first three months after announcement the Deskpro 386 shipped with Windows/386. This was a version of Windows 2.1 adapted for the 80386 processor. Support for the virtual 8086 mode was added by Compaq engineers.\n\nCompaq's technical leadership and the rivalry with IBM was emphasized when the SystemPro server was launched in late 1989 – this was a true server product with standard support for a second CPU and RAID, but also the first product to feature the EISA bus, designed in reaction to IBM's MCA (MicroChannel Architecture) which was incompatible with the original AT bus.\n\nAlthough Compaq had become successful by being 100 percent IBM-compatible, it decided to continue with the original AT bus—which it renamed ISA—instead of licensing IBM's MCA. Prior to developing EISA Compaq had invested significant resources into reverse engineering MCA, but its executives correctly calculated that the $80 billion already spent by corporations on IBM-compatible technology would make it difficult for even IBM to force manufacturers to adopt the new MCA design. Instead of cloning MCA, Compaq formed an alliance with Hewlett Packard and seven other major manufacturers, known collectively as the \"Gang of Nine\", to develop EISA.\n\nBy 1989 Compaq was so influential that observers and its executives spoke of \"Compaq compatible\". \"InfoWorld\" reported that \"In the [ISA market] Compaq is already IBM's equal in being seen as a safe bet\", quoting a sell-side analyst describing the company as \"now \"the\" safe choice in personal computers\". Even rival Tandy Corporation acknowledged Compaq's leadership, stating that within the Gang of Nine \"when you have 10 people sit down before a table to write a letter to the president, someone has to write the letter. Compaq is sitting down at the typewriter\".\n\nMichael S. Swavely, president of Compaq's North American division since May 1989, took a six-month sabbatical in January 1991 (which would eventually become retirement effective on July 12, 1991). Eckhard Pfeiffer, then president of Compaq International, was named to succeed him. Pfeiffer also received the title of Chief Operating Officer, with responsibility for the company's operations on a worldwide basis, so that Canion could devote more time to strategy. Swavely's abrupt departure in January led to rumors of turmoil in Compaq's executive suite, including friction between Canion and Swavely, likely as Swavely's rival Pfeiffer had received the number two leadership position. Swavely's U. S. marketing organization was losing ground with only 4% growth for Compaq versus 7% in the market, likely due to short supplies of the LTE 386s from component shortages, rivals that undercut Compaq's prices by as much as 35%, and large customers who did not like Compaq's dealer-only policy. Pfeiffer became President and CEO of Compaq later that year, as a result of a boardroom coup led by board chairman Ben Rosen that forced co-founder Rod Canion to resign as President and CEO.\n\nPfeiffer had joined Compaq from Texas Instruments, and established operations from scratch in both Europe and Asia. Pfeiffer was given $20,000 USD to start up Compaq Europe He started up Compaq's first overseas office in Munich in 1984. By 1990, Compaq Europe was a $2 billion business and number two behind IBM in that region, and foreign sales contributed 54 percent of Compaq's revenues. Pfeiffer, while transplanting Compaq's U. S. strategy of dealer-only distribution to Europe, was more selective in signing up dealers than Compaq had been in the U. S. such that European dealers were more qualified to handle its increasingly complex products.\n\nDuring the 1980s, under Canion's direction Compaq had focused on engineering, research, and quality control, producing high-end, high-performance machines with high profit margins that allowed Compaq to continue investing in engineering and next-generation technology. This strategy was successful as Compaq was considered a trusted brand, while many other IBM clones were untrusted due to being plagued by poor reliability. However, by the end of the eighties many manufacturers had improved their quality and were able to produce inexpensive PCs with off-the-shelf components, incurring none of the R&D costs which allowed them to undercut Compaq's expensive computers. Faced with lower-cost rivals such as Dell Computer, AST Research, and Gateway 2000, Compaq suffered a $71 million loss for that quarter, their first loss as a company, while the stock had dropped by over two-thirds. An analyst stated that \"Compaq has made a lot of tactical errors in the last year and a half. They were trend-setters; now they are lagging\". Canion initially believed that the 1990s recession was responsible for Compaq's declining sales but insisted that they would recover once the economy improved, however Pfeiffer's observation of the European market noted that it was competition as rivals could match Compaq at a fraction of the cost. Under pressure from Compaq's board to control costs as staff was ballooning at their Houston headquarters despite falling U.S. sales, while the number of non-U.S. employees had stayed constant, Compaq made its first-ever layoffs (1400 employees which was 12% of its workforce) while Pfeiffer was promoted to EVP and COO.\n\nRosen and Canion had disagreed about how to counter the cheaper Asian PC imports, as Canion wanted Compaq to build lower cost PCs with components developed in-house in order to preserve Compaq's reputation for engineering and quality, while Rosen believed that Compaq needed to buy standard components from suppliers and reach the market faster. While Canion developed an 18-month plan to create a line of low-priced computers, Rosen sent his own Compaq engineering team to Comdex without Canion's knowledge and discovered that a low-priced PC could be made in half the time and at lower cost than Canion's initiative. It was also believed that Canion's consensus-style management slowed the company's ability to react in the market, whereas Pfeiffer's autocratic style would be suited to price and product competition.\n\nRosen initiated a 14-hour board meeting, and the directors also interviewed Pfeiffer for several hours without informing Canion. At the conclusion, the board was unanimous in picking Pfeiffer over Canion. As Canion was popular with company workers, 150 employees staged an impromptu protest with signs stating \"We love you Rod.\" and taking out a newspaper ad saying \"Rod, you are the wind beneath our wings. We love you.\" Canion declined an offer to remain on Compaq's board and was bitter about his ouster as he didn't speak to Rosen for years, although their relationship became cordial again. In 1999, Canion admitted that his ouster was justified, saying \"I was burned out. I needed to leave. He [Rosen] felt I didn't have a strong sense of urgency\". Two weeks after Canion's ouster, five other senior executives resigned, including remaining company founder James Harris as SVP of Engineering. These departures were motivated by an enhanced severance or early retirement, as well as an imminent demotion as their functions were to be shifted to vice presidents.\n\nUnder Pfeiffer's tenure as chief executive, Compaq entered the retail computer market with the Presario as one of the first manufacturers in the mid-1990s to market a sub-$1000 PC. In order to maintain the prices it wanted, Compaq became the first first-tier computer manufacturer to utilize CPUs from AMD and Cyrix. The two price wars resulting from Compaq's actions ultimately drove numerous competitors from the market, such as Packard Bell and AST Research. From third place in 1993, Compaq had overtaken Apple Computer and even surpassed IBM as the top PC manufacturer in 1994, as both IBM and Apple were struggling considerably during that time. Compaq's inventory and gross margins were better than that of its rivals which enabled it to wage the price wars.\n\nCompaq had decided to make a foray into printers in 1989, and the first models were released to positive reviews in 1992. However, Pfeiffer saw that the prospects of taking on market leader Hewlett Packard (who had 60% market share) was tough, as that would force Compaq to devote more funds and people to that project than originally budgeted. Compaq ending up selling the printer business to Xerox and took a charge of $50 million.\n\nOn June 26, 1995, Compaq reached an agreement with Cisco Systems Inc. in order to get into networking, including digital modems, routers, and switches favored by small businesses and corporate departments, which was now a $4 billion business and the fastest-growing part of the computer hardware market. Compaq also built up a network engineering and marketing staff.\n\nIn 1996, despite record sales and profits at Compaq, Pfeiffer initiated a major management shakeup in the senior ranks. John T. Rose, who previously ran Compaq's desktop PC division, took over the corporate server business from SVP Gary Stimac who had resigned. Rose had joined Compaq in 1993 from Digital Equipment Corporation where he oversaw the personal computer division and worldwide engineering, while Stimac had been with Compaq since 1982 and was one of the longest-serving executives. Senior Vice-President for North America Ross Cooley announced his resignation effective at the end of 1996. CFO Daryl J. White, who joined the company in January, 1983 resigned in May, 1996 after 8 years as CFO. Michael Winkler, who joined Compaq in 1995 to run its portable computer division, was promoted to general manager of the new PC products group.\n\nCompaq had been producing the PC chassis at its plant in Shenzhen, China to cut costs. In 1996, instead of expanding its own plant, Compaq asked a Taiwanese supplier to set up a new factory nearby to produce the mechanicals, with the Taiwanese supplier owning the inventory until it reached Compaq in Houston. Pfeiffer also introduced a new distribution strategy, to build PCs made-to-order which would eliminate the stockpile of computers in warehouses and cut the components inventory down to two weeks, with the supply chain from supplier to dealer linked by complex software.\n\nVice-President for Corporate Development Kenneth E. Kurtzman assembled five teams to examine Compaq's businesses and assess each unit's strategy and that of key rivals. Kurtzman's teams recommended to Pfeiffer that each business unit had to be first or second in its market within three years—or else Compaq should exit that line. Also, the company should no longer use profits from high-margin businesses to carry marginally profitable ones, as instead each unit must show a return on investment. Pfeiffer's vision was to make Compaq a full-fledged computer company, moving beyond its main business of manufacturing retail PCs and into the more lucrative business services and solutions that IBM did well at, such as computer servers which would also require more \"customer handholding\" from either the dealers or Compaq staff themselves.\nUnlike IBM and HP, Compaq was not going to build up field technicians and programmers in-house as these could be costly assets, instead Compaq would leverage its partnerships (including these with Andersen Consulting and software maker SAP) to install and maintain corporate systems. This allowed Compaq to compete in the \"big-iron market\" without incurring the costs of running its own services or software businesses.\n\nIn January 1998, Compaq was at its height. CEO Pfeiffer boldly predicted that the Microsoft/Intel \"Wintel\" duopoly would be replaced by \"Wintelpaq\".\n\nPfeiffer also made several major (and some minor) acquisitions. In 1997, Compaq bought Tandem Computers, known for their NonStop server line. This acquisition instantly gave Compaq a presence in the higher end business computing market. Minor acquisitions centered around building a networking arm and included NetWorth (1998) based in Irving, Texas and Thomas-Conrad (1998) based in Austin, Texas. In 1997 Microcom was also acquired, based in Norwood, MA, which brought a line of modems, Remote Access Servers (RAS) and the popular Carbon Copy software.\n\nIn 1998, Compaq acquired Digital Equipment Corporation for a then-industry record of $9 billion USD. The merger made Compaq, at the time, the world's second largest computer maker in the world in terms of revenue behind IBM. Digital Equipment, which had nearly twice as many employees as Compaq while generating half the revenue, had been a leading computer company during the 1970s and early 1980s. However, Digital had struggled during the 1990s, with high operating costs. For nine years the company had lost money or barely broke even, and had recently refocused itself as a \"network solutions company\". In 1995, Compaq had considered a bid for Digital but only became seriously interested in 1997 after Digital's major divestments and refocusing on the Internet. At the time of the acquisition, services accounted for 45 percent of Digital's revenues (about $6 billion) and their gross margins on services averaged 34 percent, considerably higher than Compaq's 25% margins on PC sales and also satisfying customers who had demanded more services from Compaq for years. Compaq had originally wanted to purchase only Digital's services business but that was turned down. When the announcement was made, it was initially viewed as a master stroke as it immediately gave Compaq a 22,000 person global service operation to help corporations handle major technological purchases (by 2001 services made up over 20% of Compaq's revenues, largely due to the Digital employees inherited from the merger), in order to compete with IBM. However it was also risky merger, as the combined company would have to lay off 2,000 employees from Compaq and 15,000 from Digital which would potentially hurt morale. Furthermore, Compaq fell behind schedule in integrating Digital's operations, which also distracted the company from its strength in low-end PCs where it used to lead the market in rolling out next-generation systems which let rival Dell grab market share. Reportedly Compaq had three consulting firms working to integrate Digital alone.\n\nHowever, Pfeiffer had little vision for what the combined companies should do, or indeed how the three dramatically different cultures could work as a single entity, and Compaq struggled from strategy indecisiveness and lost focus, as a result being caught in between the low end and high end of the market. Mark Anderson, president of Strategic News Service, a research firm based in Friday Harbor, Wash. was quoted as saying \"The kind of goals he had sounded good to shareholders – like being a $50 billion company by the year 2000, or to beat I.B.M. – but they didn't have anything to do with customers. The new C.E.O. should look at everything Eckhard acquired and ask: did the customer benefit from that. If the answer isn't yes, they should get rid of it.\" On one hand, Compaq had previously dominated the PC market with its price war but was now struggling against Dell, which sold directly to buyers, avoiding the dealer channel and its markup, and built each machine to order to keep inventories and costs at a minimum. At the same time, Compaq, through its acquisitions of the Digital Equipment Corporation last year and Tandem Computer in 1997, had tried to become a major systems company, like IBM and Hewlett-Packard. While IBM and HP were able generate repeat business from corporate customers to drive sales of their different divisions, Compaq had not yet managed to make its newly acquired sales and services organizations work as seamlessly.\n\nIn early 1998, Compaq had the problem of bloated PC inventories. By summer 1998, Compaq was suffering from product-quality problems. Robert W. Stearns, SVP of Business Development, said \"In [Pfeiffer's] quest for bigness, he lost an understanding of the customer and built what I call empty market share--large but not profitable\", while Jim Moore, a technology strategy consultant with GeoPartners Research in Cambridge, Mass., says Pfeiffer \"raced to scale without having economies of scale.\" The \"colossus\" that Pfeiffer built up was not nimble enough to adapt to the fast-changing computer industry. That year Compaq forecast demand poorly and overshipped too many PCs, causing resellers to dump them at fire sale prices, and since Compaq protected resellers from heavy losses it cost them two quarters of operating profits.\n\nPfeiffer also refused to develop a potential successor, rebuffing Rosen's suggestion to recruit a few executives to create the separate position of Compaq president. The board complained that Pfeiffer was too removed from management and the rank-and-file, as he surrounded himself with a \"clique\" of Chief Financial Officer Earl Mason, Senior Vice-President John T. Rose, and Senior Vice-President of Human Resources Hans Gutsch. Current and former Compaq employees complained that Gutsch was part of a group of senior executives, dubbed the \"A team\", who controlled access to Pfeiffer. Gutsch was said to be a \"master of corporate politics, pitting senior vice presidents against each other and inserting himself into parts of the company that normally would not be under his purview\". Gutsch, who oversaw security, had an extensive security system and guard station installed on the eight floor of CCA-1, where the company's senior vice presidents worked. There were accusations that Gutsch and others sought to divide top management, although this was regarded by others as sour grapes on the part of executives who were shut out of planning that involved the acquisitions of Tandem and Digital Equipment Corp. Pfeiffer reduced the size of the group working on the deal due to news leaks, saying \"We cut the team down to the minimum number of people - those who would have to be directly involved, and not one person more\". Robert W. Stearns, Compaq's senior vice president for business development, with responsibility for mergers and acquisitions, had opposed the acquisition of Digital as the cultural differences between both companies were too great, and complained that he was placed on the \"B team\" as a result.\n\nCompaq entered 1999 with strong expectations. Fourth-quarter 1998 earnings reported in January 1999 beat expectations by six cents a share with record 48 percent growth. The company launched \"Compaq.com\" as the key for its new direct sales strategy, and planned an IPO for AltaVista toward the end of 1999 in order to capitalize on the dotcom bubble. However, by February 1999, analysts were sceptical of Compaq's plan to sell both direct and to resellers. Compaq was hit with two class-action lawsuits, as a result of CFO Earl Mason, SVP John Rose, and other executives selling $50 million USD of stock before a conference call with analysts, where they noted that demand for PCs was slowing down.\n\nOn April 17, 1999, just nine days after Compaq reported first-quarter profit being at half of what analysts had expected, the latest in a string of earnings disappointments, Pfeiffer was forced to resign as CEO in a coup led by board chairman Ben Rosen. Reportedly, at the special board meeting held on April 15, the directors were unanimous in dismissing Pfeiffer. The company's stock had fallen 50 percent since its all-time high in January 1999. Compaq shares, which traded as high as $51.25 early in 1999, dropped 23 percent on April 12, the first day of trading after the first-quarter announcement and closed the following Friday at $23.62. During three out of the last six quarters of Pfeiffer's tenure, the company's revenues or earnings had missed expectations. While rival Dell Computer had 55% growth in U.S. PC sales in the first quarter of 1999, Compaq could only manage 10%. Rosen suggested that the accelerating change brought about by the Internet had overtaken Compaq's management team, saying \"As a company engaged in transforming its industry for the Internet era, we must have the organizational flexibility necessary to move at Internet speed.\" In a statement, Pfeiffer said \"Compaq has come a long way since I joined the company in 1983\" and \"under Ben's guidance, I know this company will realize its potential.\" Rosen's priority was to have Compaq catchup as an E-commerce competitor, and he also moved to streamline operations and reduce the indecision that plagued the company.\n\nRoger Kay, an analyst at International Data Corporation, observed that Compaq's behavior at times seemed like a personal vendetta, noting that \"Eckhard has been so obsessed with staying ahead of Dell that they focused too hard on market share and stopped paying attention to profitability and liquidity. They got whacked in a price war that they started.\" Subsequent earnings releases from Compaq's rivals, Dell, Gateway, IBM, and Hewlett-Packard suggested that the problems were not affecting the whole PC industry as Pfeiffer had suggested. Dell and Gateway sold direct, which helped them to avoid Compaq's inventory problems and compete on price without dealer markups, plus Gateway sold web access and a broad range of software tailored to small businesses. Hewlett-Packard's PC business had similar challenges like Compaq but this was offset by HP's extremely lucrative printer business, while IBM sold PCs at a loss but used them to lock in multi-year services contracts with customers.\n\nAfter Pfeiffer's resignation, the board established an office of the CEO with a triumvirate of directors; Rosen as interim CEO and vice chairmen Frank P. Doyle and Robert Ted Enloe III. They began \"cleaning house\", as shortly afterward many of Pfeiffer's top executives resigned or were pushed out, including John J. Rando, Earl L. Mason, and John T. Rose. Rando, senior vice president and general manager of Compaq Services, was a key player during the merger discussions and the most senior executive from Digital to remain with Compaq after the acquisition closed and had been touted by some as the heir-apparent to Pfeiffer. Rando's division had performed strongly as it had sales of $1.6 billion for the first quarter compared to $113 million in 1998, which met expectations and was anticipated to post accelerated and profitable growth going forward. At the time of Rando's departure, Compaq Services ranked third behind those of IBM and EDS, while slightly ahead of Hewlett-Packard's and Andersen Consulting, however customers switched from Digital and Tandem technology-based workstations to those of HP, IBM, and Sun Microsystems. Mason, senior vice president and chief financial officer, had previously been offered the job of chief executive of Alliant Foodservice, Inc., a foodservice distributor based in Chicago, and he informed Compaq's board that he accepted the offer. Rose, senior vice president and general manager of Compaq's Enterprise Computing group, resigned effective as of June 3 and was succeeded by Tandem veteran Enrico Pesatori. Rose was reportedly upset that he was not considered for the CEO vacancy, which became apparent once Michael Capellas was named COO. While Enterprise Computing, responsible for engineering and marketing of network servers, workstations and data-storage products, reportedly accounted for one third of Compaq's revenues and likely the largest part of its profits, it was responsible for the earnings shortfall in Q1 of 1999. In addition, Rose was part of the \"old guard\" close to former CEO Pfeiffer, and he and other Compaq executives had been criticized at the company's annual meeting for selling stock before reporting the sales slowdown. Capellas was appointed COO after pressure mounted on Rosen to find a permanent CEO, however it was reported that potential candidates did not want to work under Rosen as chairman.\n\nPfeiffer's permanent replacement was Michael Capellas, who had been serving as Compaq's SVP and CIO for under a year. A couple months after Pfeiffer's ouster, Capellas was elevated to interim chief operating officer on June 2, and was soon appointed President and CEO. Capellas also assumed the title of Chairman on September 28, 2000 when Rosen stepped down from the board of directors. At his retirement, Rosen proclaimed \"These are great achievements—to create 65,000 jobs, $40 billion in sales and $40 billion in market value, all starting with a sketch and a dream\".\n\nCapellas was able to restore some of the luster lost in the latter part of the Pfeiffer era and he repaired the relationship with Microsoft which had deteriorated under his predecessor's tenure.\n\nHowever Compaq still struggled against lower-cost competitors with direct sales channels such as Dell who took over the top spot of PC manufacturer from Compaq in 2001. Compaq relied significantly on reseller channels, so their criticism caused Compaq to retreat from its proposed direct sales plan, although Capellas maintained that he would use the middlemen to provide value-added services. Despite falling to No. 2 among PC manufacturers, Capellas proclaimed \"We are No. 2 in the traditional PC market, but we're focused on industry leadership in the next generation of Internet access devices and wireless mobility. That's where the growth and the profitability will be.\" The company's longer-term strategy involves extending its services to servers and storage products, as well as handheld computers such as the iPAQ PocketPC which accounted for 11 percent of total unit volume.\nDuring November 1999, Compaq began to work with Microsoft to create the first in a line of small-scale, web-based computer systems called MSN Companions.\n\nIn 1998, Compaq also signed new sales and equipment alliance with NaviSite. Under the pact, Compaq agreed to promote and sell NaviSite Web hosting services. In return, NaviSite took Compaq as a preferred provider for its storage and Intel-based servers.\n\nCompaq struggled as a result of the collapse of the Dot-com bubble bust, which hurt sales of their high-end systems in 2001 and 2002, and they managed only a small profit in a few quarters during these years. They also accumulated $1.7 billion in short-term debt around this time. The stock price of Compaq, which was around $25 when Capellas became CEO, was trading at half that by 2002.\n\nIn 2002, Compaq signed a merger agreement with Hewlett-Packard for $24.2 billion, including $14.45 billion for goodwill, where each Compaq share would be exchanged for 0.6325 of a Hewlett-Packard share. There would be a termination fee of $675 million USD that either company would have to pay the other to break the merger. Compaq shareholders would own 36% of the combined company while HP's would have 64%. Hewlett-Packard had reported yearly revenues of $47 billion, while Compaq's was $40 billion, and the combined company would have been close to IBM's $90 billion revenues. It was projected to have $2.5 billion in annual cost savings by mid-2004. The expected layoffs at Compaq and HP, 8500 and 9000 jobs, respectively, would leave the combined company with a workforce of 145,000.\n\nBoth companies had to seek approval from their shareholders through separate special meetings. While Compaq shareholders unanimously approved the deal, there was a public proxy battle within HP as the deal was strongly opposed by numerous large HP shareholders, including the sons of the company founders, Walter Hewlett and David W. Packard, as well as the California Public Employees’ Retirement System (CalPERS) and the Ontario Teachers Pension Plan. Walter Hewlett only reluctantly approved the merger, in his duty as a member of the board of directors, since the merger agreement \"called for unanimous board approval in order to ensure the best possible shareholder reception\". While supporters of the merger argued that there would be economies of scale and that the sales of PCs would drive sales of printers and cameras, Walter Hewlett was convinced that PCs were a low-margin but risky business that would not contribute and would likely dilute the old HP's traditionally profitable Imaging and Printing division. David W. Packard in his opposition to the deal \"[cited] massive layoffs as an example of this departure from HP’s core values...[arguing] that although the founders never guaranteed job security, 'Bill and Dave never developed a premeditated business strategy that treated HP employees as expendable.'\" Packard further stated that \"Fiorina’s high-handed management and her efforts to reinvent the company ran counter to the company’s core values as established by the founders\". The founders' families who controlled a significant amount of HP shares were further irked because Fiorina had made no attempt to reach out to them and consult about the merger, instead they received the same standard roadshow presentation as other investors.\n\nAnalysts on Wall Street were generally critical of the merger, as both companies had been struggling before the announcement, and the stock prices of both companies dropped in the months after the merger agreement was made public. Particularly rival Dell made gains from defecting HP and Compaq customers who were wary of the merger. Carly Fiorina, initially seen as HP's savior when she was hired as CEO back in 1999, had seen the company's stock price drop to less than half since she assumed the position, and her job was said to be on shaky ground before the merger announcement. HP's offer was regarded by analysts to be overvaluing Compaq, due to Compaq's shaky financial performance in the past recent years (there were rumors that it could run out of money in 12 months and be forced to cease business operations had it stayed independent), as well as Compaq's own more conservative valuation of its assets. Detractors of the deal noted that buying Compaq was a \"distraction\" that would not directly help HP take on IBM's breadth or Dell Computer's direct sales model. Plus there were significant cultural differences between HP and Compaq; which made decisions by consensus and rapid autocratic styles, respectively. One of Compaq's few bright spots was its services business, which was outperforming HP's own services division.\n\nThe merger was approved by HP shareholders only after the narrowest of margins, and allegations of vote buying (primarily involving an alleged last-second back-room deal with Deutsche Bank) haunted the new company. It was subsequently disclosed that HP had retained Deutsche Bank's investment banking division in January 2002 to assist in the merger. HP had agreed to pay Deutsche Bank $1 million guaranteed, and another $1 million contingent upon approval of the merger. On August 19, 2003, the U.S. SEC charged Deutsche Bank with failing to disclose a material conflict of interest in its voting of client proxies for the merger and imposed a civil penalty of $750,000. Deutsche Bank consented without admitting or denying the findings.\n\nCompaq's pre-merger ticker symbol was CPQ. This was combined with Hewlett-Packard's ticker symbol (HWP) to create the current ticker symbol (HPQ).\n\nCapellas, Compaq's last Chairman and CEO, became president of the post-merger Hewlett-Packard, under Chairman and CEO Carly Fiorina, to ease the integration of the two companies. However, Capellas was reported not to be happy with his role, being said not to be utilized and being unlikely to become CEO as the board supported Fiorina. Capellas stepped down on November 12, 2002, just six months on the job, to become CEO of MCI Worldcom where he would lead its acquisition by Verizon. Capella's former role of president was not filled as the executives who reported to him then reported directly to the CEO.\n\nFiorina helmed HP for nearly three years after Capellas left. HP laid off thousands of former Compaq, DEC, HP, and Tandem employees, its stock price generally declined and profits did not perk up. Several senior executives from the Compaq side including Jeff Clarke and Peter Blackmore would resign or be ousted from the post-merger HP. Though the combination of both companies' PC manufacturing capacity initially made it the number one, it soon lost the lead and further market share to Dell which squeezed HP on low end PCs. HP was also unable to compete effectively with IBM in the high-end server market. In addition, the merging of the stagnant Compaq computer assembly business with HP's lucrative printing and imaging division was criticized for obstructing the profitability of the printing/imaging segment. Overall, it has been suggested that the purchase of Compaq was not a good move for HP, due to the narrow profit margins in the commoditized PC business, especially in light of IBM's 2005 announcement to sell its PC division to Lenovo. \"The Inquirer\" noted that the continued low return on investment and small margins of HP's personal computer manufacturing business, now named the Personal Systems Group, \"continues to be what it was in the individual companies, not much more than a job creation scheme for its employees\".\n\nIn February 2005, the Board of Directors ousted Fiorina. Former Compaq CEO Capellas was mentioned by some as a potential successor, but several months afterwards, Mark Hurd was hired as President and CEO of HP. Hurd separated the PC division from the imaging and printing division and renamed it the Personal Systems Group, placing it under the leadership of EVP Todd R. Bradley. Hewlett Packard's PC business has since been reinvigorated by Hurd's restructuring and now generates more revenue than the traditionally more profitable printers. By late 2006, HP had retaken the #1 sales position of PCs from Dell, which struggled with missed estimates and poor quality, and has held that rank ever since.\n\nMost Compaq products have been re-branded with the HP nameplate, such as the company's market leading ProLiant server line, while the Compaq brand remains on only some consumer-oriented and budget products, notably Compaq Presario PCs. HP's business computers line was discontinued in favour of the Compaq Evo line, which was rebranded HP Compaq. HP's Jornada PDAs were replaced by Compaq iPAQ PDAs, which were renamed HP iPAQ. All Compaq computers now ship with HP software.\n\nIn May 2007, HP announced in a press release a new logo for their Compaq Division to be placed on the new model Compaq Presarios.\n\nIn 2008, HP reshuffled its business line notebooks. The \"Compaq\" name from its \"HP Compaq\" series was originally used for all of HP's business and budget notebooks. However, the HP EliteBook line became the top of the business notebook lineup while the HP Compaq B series became its middle business line. As of early 2009, the \"HP ProBook\" filled out HP's low end business lineup.\n\nIn 2009, HP sold part of Compaq's former headquarters to the Lone Star College System.\n\nIn 2010, the last Compaq Presario branded laptop was produced at an HP computer assembly facility in China. Since then, the \"Presario\" was rebranded under HP's laptop computer line as the HP 2000 series.\n\nOn August 18, 2011, then-CEO of HP Leo Apotheker announced plans for a partial or full spinoff of the Personal Systems Group. The PC unit had the lowest profit margin although it accounted for nearly a third of HP’s overall revenues in 2010. HP was still selling more PCs than any other vendor, shipping 14.9 million PCs in the second quarter of 2011 (17.5% of the market according to Gartner), while Dell and Lenovo were tied for second place, each with more than a 12% share of the market and shipments of over 10 million units. However, the announcement of the PC spinoff (concurrent with the discontinuation of WebOS, and the purchase of Autonomy Corp. for $10 billion) was poorly received by the market, and after Apotheker's ouster, plans for a divestiture were cancelled. In March 2012, the printing and imaging division was merged into the PC unit. In October 2012, according to Gartner, Lenovo took the lead as the number one PC manufacturer from HP, while IDC ranked Lenovo just right behind HP. In Q2 2013, Forbes reported that Lenovo ranked ahead of HP as the world’s number one PC supplier.\n\nThe Compaq brand name was discontinued in the United States in 2013. In 2015, the Argentinian company Grupo Newsan acquired the brand's license and developed a new line of Presario Notebooks.\n\nCompaq World Headquarters (now HP United States) campus consisted of of land which contained 15 office buildings, 7 manufacturing buildings, a product conference center, an employee cafeteria, mechanical laboratories, warehouses, and chemical handling facilities.\n\nInstead of headquartering the company in a downtown Houston skyscraper, then-CEO Rod Canion chose a West Coast-style campus surrounded by forests, where every employee had similar offices and no-one (not even the CEO) had a reserved parking spot. As it grew, Compaq became so important to Houston that it may have caused the construction of Highway 249 in the late 1980s, and many other technology companies appeared in what became known as the \"249 Corridor\".\n\nSenior Vice-President of Human Resources, Hans W. Gutsch, oversaw the company's facilities and security. Gutsch had an had an extensive security system and guard station installed on the eight floor of CCA-1, where the company's senior vice presidents had their offices. Eckhard Pfeiffer, President and CEO, introduced a whole series of executive perks to a company that had always had an egalitarian culture; for instance he oversaw the construction of an executive parking garage, previously parking places had never been reserved.\n\nOn August 31, 1998, the Compaq Commons was opened in the headquarters campus, which featured a conference center, an employee convenience store, a wellness center, and an employee cafeteria.\n\nIn 2009, HP sold part of Compaq's former headquarters to the Lone Star College System. Hewlett Packard Buildings #7 & #8, two eight-story reinforced concrete buildings totaling 450,000 square feet, plus a 1,200-car parking garage and a central chiller plant, were all deemed by the college to be too robust and costly to maintain so they were demolished by implosion on September 18, 2011.\n\nCompaq originally competed directly against IBM, manufacturing computer systems equivalent with the IBM PC, as well as Apple Computer. In the 1990s, as IBM's own PC division declined, Compaq faced other IBM PC Compatible manufacturers like Dell Computer, Packard Bell, AST Research, and Gateway 2000.\n\nBy the mid-1990s, Compaq's price war had enabled it to overtake IBM and Apple, while other IBM PC Compatible manufacturers such as Packard Bell and AST were driven from the market.\n\nDell became the number one supplier of PCs in 2001.\n\nAt the time of their 2002 merger, Compaq and HP were the second and third largest PC manufacturers, so their combination made them number one. However, the combined HP-Compaq struggled and fell to second place behind Dell from 2003–2006. Due to Dell's struggles in late 2006, HP has led all PC vendors since 2007 onwards.\n\nDuring its existence as a division of HP, Compaq primarily competed against other budget-oriented personal computer series from manufacturers including Acer, Lenovo, and Toshiba. Most of Compaq's competitors except Dell were later acquired by bigger rivals like Acer (Gateway 2000 and Packard Bell) and Lenovo absorbing IBM's PC division.\n\nBefore its merger with HP, Compaq sponsored the Williams Formula One team when it was still powered by BMW engines. HP inherited and continued the sponsorship deal for a few years.\n\n\n", "id": "7742", "title": "Compaq"}
{"url": "https://en.wikipedia.org/wiki?curid=7751", "text": "CPSU (disambiguation)\n\nCPSU can refer to:\n", "id": "7751", "title": "CPSU (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=7755", "text": "Cluny\n\nCluny or Clugny is a commune in the Saône-et-Loire department of the region of Bourgogne-Franche-Comté, in eastern France. It is northwest of Mâcon.\n\nThe town grew up around the Benedictine Abbey of Cluny, founded by Duke William I of Aquitaine in 910. The height of Cluniac influence was from the second half of the 10th century through the early 12th. The abbey was sacked by the Huguenots in 1562, and many of its valuable manuscripts were destroyed or removed.\n\nThe river Grosne flows northward through the commune and crosses the town.\n\nBourgogne-Franche-Comté has a large number of places which are of interest to tourists, such as:\n\n\n", "id": "7755", "title": "Cluny"}
{"url": "https://en.wikipedia.org/wiki?curid=7756", "text": "Chet Atkins\n\nChester Burton \"Chet\" Atkins (June 20, 1924 – June 30, 2001) was an American musician, occasional vocalist, songwriter, and record producer, who along with Owen Bradley and Bob Ferguson, among others, created the country music style that came to be known as the Nashville sound, which expanded country music's appeal to adult pop music fans. He was primarily known as a guitarist. He also played the mandolin, fiddle, banjo, and ukulele.\n\nAtkins' signature picking style was inspired by Merle Travis. Other major guitar influences were Django Reinhardt, George Barnes, Les Paul, and, later, Jerry Reed. His distinctive picking style and musicianship brought him admirers inside and outside the country scene, both in the United States and abroad. Atkins spent most of his career at RCA Victor and produced records for the Browns, Hank Snow, Porter Wagoner, Norma Jean, Dolly Parton, Dottie West, Perry Como, Floyd Cramer, Elvis Presley, the Everly Brothers, Eddy Arnold, Don Gibson, Jim Reeves, Jerry Reed, Skeeter Davis, Waylon Jennings, and many others.\n\nAmong many honors, Atkins received 14 Grammy Awards and the Grammy Lifetime Achievement Award. He also received nine Country Music Association awards for Instrumentalist of the Year. He was inducted into the Rock & Roll Hall of Fame, the Country Music Hall of Fame and Museum, and the Musicians Hall of Fame and Museum.\n\nAtkins was born on June 20, 1924, in Luttrell, Tennessee, near Clinch Mountain. His parents divorced when he was six, after which he was raised by his mother. He was the youngest of three boys and a girl. He started out on the ukulele, later moving on to the fiddle, but traded his brother Lowell an old pistol and some chores for a guitar when he was nine. He stated in his 1974 autobiography, \"We were so poor and everybody around us was so poor that it was the forties before anyone even knew there had been a depression.\" Forced to relocate to Fortson, Georgia, outside of Columbus, to live with his father because of a critical asthma condition, Atkins was a sensitive youth who made music his obsession. Because of his illness, he was forced to sleep in a straight-back chair to breathe comfortably. On those nights, he played his guitar until he fell asleep holding it, a habit which lasted his whole life. While living in Fortson, he attended the historic Mountain Hill School. He returned in the 1990s to play a series of charity concerts to save the school from demolition.\nStories have been told about the very young Chet, who, when a friend or relative would come to visit and play guitar, would crowd in and put his ear so close to the instrument that it became difficult for the visitor to play.\n\nAtkins became an accomplished guitarist while he was in high school. He used the restroom in the school to practice, because it gave better acoustics. His first guitar had a nail for a nut and was so bowed that only the first few frets could be used. He later purchased a semiacoustic electric guitar and amp, but he had to travel many miles to find an electrical outlet, since his home had no electricity.\n\nLater in life, he lightheartedly gave himself (along with John Knowles, Marcel Dadi, Tommy Emmanuel, Steve Wariner, and Jerry Reed) the honorary degree CGP (\"Certified Guitar Player\").\nIn 2011, his daughter Merle Atkins Russell bestowed the CGP degree on his longtime sideman Paul Yandell. She then declared no more CGPs would be allowed by the Atkins estate.\n\nHis half-brother Jim was a successful guitarist who worked with the Les Paul Trio in New York.\n\nAtkins did not have a strong style of his own until 1939, when (while still living in Georgia) he heard Merle Travis picking over WLW radio. This early influence dramatically shaped his unique playing style. Whereas Travis's right hand used his index finger for the melody and thumb for bass notes, Atkins expanded his right-hand style to include picking with his first three fingers, with the thumb on bass.\n\nChet Atkins was a ham radio general class licensee. Formerly using the call sign WA4CZD, he obtained the vanity call sign W4CGP in 1998 to include the CGP designation. He was a member of the American Radio Relay League.\n\nAfter dropping out of high school in 1942, Atkins landed a job at WNOX-AM radio in Knoxville, where he played fiddle and guitar with the singer Bill Carlisle and the comic Archie Campbell and became a member of the station's Dixieland Swingsters, a small swing instrumental combo. After three years, he moved to WLW-AM in Cincinnati, Ohio, where Merle Travis had formerly worked.\n\nAfter six months, he moved to Raleigh and worked with Johnnie and Jack before heading for Richmond, Virginia, where he performed with Sunshine Sue Workman. Atkins's shy personality worked against him, as did the fact that his sophisticated style led many to doubt he was truly \"country\". He was fired often but was soon able to land another job at another radio station on account of his unique playing ability.\n\nAtkins and Jethro Burns (of Homer and Jethro) married twin sisters, Leona and Lois Johnson, who sang as Laverne and Fern Johnson, the Johnson Sisters. Leona Atkins outlived her husband by eight years, dying in 2009 at the age of 85.\n\nTraveling to Chicago, Atkins auditioned for Red Foley, who was leaving his star position on WLS-AM's \"National Barn Dance\" to join the Grand Ole Opry. Atkins made his first appearance at the Opry in 1946 as a member of Foley's band. He also recorded a single for Nashville-based Bullet Records that year. That single, \"Guitar Blues\", was fairly progressive, including a clarinet solo by the Nashville dance band musician Dutch McMillan, with Owen Bradley on piano. He had a solo spot on the Opry, but when that was cut, Atkins moved on to KWTO in Springfield, Missouri. Despite the support of executive Si Siman, however, he soon was fired for not sounding \"country enough\".\n\nWhile working with a Western band in Denver, Colorado, Atkins came to the attention of RCA Victor. Siman had been encouraging Steve Sholes to sign Atkins, as his style (with the success of Merle Travis as a hit recording artist) was suddenly in vogue. Sholes, A&R director of country music at RCA, tracked Atkins down in Denver.\n\nHe made his first RCA Victor recordings in Chicago in 1947. They did not sell. He did some studio work for RCA that year but had relocated to Knoxville again, where he worked with Homer and Jethro on WNOX's new Saturday night radio show \"The Tennessee Barn Dance\" and the popular \"Midday Merry Go Round\".\n\nIn 1949, he left WNOX to join June Carter with Mother Maybelle and the Carter Sisters on KWTO. This incarnation of the old Carter Family featured Maybelle Carter and daughters June, Helen, and Anita. Their work soon attracted attention from the Grand Ole Opry. The group relocated to Nashville in the mid-1950. Atkins began working on recording sessions and performing on WSM-AM and the Opry. Atkins became a member of the Opry in the 1950s.\n\nWhile he had not yet had a hit record for RCA Victor, his stature was growing. He began assisting Sholes as a session leader when the New York–based producer needed help organizing Nashville sessions for RCA Victor artists. Atkins's first hit single was \"Mr. Sandman\", followed by \"Silver Bell\", which he recorded as a duet with Hank Snow. His albums also became more popular. He was featured on ABC-TV's \"The Eddy Arnold Show\" in the summer of 1956 and on \"Country Music Jubilee\" in 1957 and 1958 (by then renamed \"Jubilee USA\").\nIn addition to recording, Atkins was a design consultant for Gretsch, which manufactured a popular Chet Atkins line of electric guitars from 1955–1980. He became manager of RCA Victor's Nashville studio, eventually inspiring and seeing the completion of the legendary RCA Studio B, the first studio built specifically for the purpose of recording on the now-famous Music Row.\n\nWhen Sholes took over pop production in 1957—a result of his success with Elvis Presley—he put Atkins in charge of RCA Victor's Nashville division. With country music record sales declining as rock and roll took over, Atkins and Bob Ferguson took their cue from Owen Bradley and eliminated fiddles and steel guitar as a means of making country singers appeal to pop fans. This became known as the Nashville sound which Atkins said was a label created by the media attached to a style of recording done during that period to keep country (and their jobs) viable.\n\nAtkins used the Jordanaires and a rhythm section on hits such as Jim Reeves's \"Four Walls\" and \"He'll Have to Go\" and Don Gibson's \"Oh Lonesome Me\" and \"Blue Blue Day\". The once-rare phenomenon of having a country hit cross over to pop success became more common. Bradley and he had essentially put the producer in the driver's seat, guiding an artist's choice of material and the musical background.\n\nAtkins made his own records, which usually visited pop standards and jazz, in a sophisticated home studio, often recording the rhythm tracks at RCA and adding his solo parts at home, refining the tracks until the results satisfied him. Guitarists of all styles came to admire various Atkins albums for their unique musical ideas and in some cases experimental electronic ideas. In this period, he became known internationally as \"Mister Guitar\", inspiring an album, \"Mister Guitar\", engineered by both Bob Ferris and Bill Porter, Ferris's replacement.\nAt the end of March 1959, Porter took over as chief engineer at RCA's Nashville studio, in the space now known as Studio B. (At the time, only one RCA studio was in Nashville, with no letter designation.) Porter soon helped Atkins get a better reverberation sound from the studio's German effects device, an EMT plate reverb. With his golden ear, Porter found the studio's acoustics to be problematic, and he devised a set of acoustic baffles to hang from the ceiling, then selected positions for microphones based on resonant room modes. The sound of the recordings improved significantly, and the studio achieved a string of successes. The Nashville sound became more dynamic. In later years, when Bradley asked how he achieved his sound, Atkins told him \"it was Porter.\" Porter described Atkins as respectful of musicians when recording—if someone was out of tune, he would not single that person out by name. Instead, he would say something like, \"we got a little tuning problem ... Everybody check and see what's going on.\" If that did not work, Atkins would instruct Porter to turn the offending player down in the mix. When Porter left RCA in late 1964, Atkins said, \"the sound was never the same, never as great.\"\n\nAtkins's trademark \"Atkins style\" of playing uses the thumb and first two or sometimes three fingers of the right hand. He developed this style from listening to Merle Travis, occasionally on a primitive radio. He was sure no one could play that articulately with just the thumb and index finger (which was exactly how Travis played), and he assumed it required the thumb and two fingers—and that was the style he pioneered and mastered.\n\nHe enjoyed jamming with fellow studio musicians, and they were asked to perform at the Newport Jazz Festival in 1960. That performance was canceled because of rioting, but a live recording of the group (\"After the Riot at Newport\") was released. Atkins performed by invitation at the White House for every president from John Kennedy through George H. W. Bush. Atkins was a member of the Million Dollar Band during the 1980s. He is also well known for his song \"Yankee Doodle Dixie\", in which he played \"Yankee Doodle\" and \"Dixie\" simultaneously, on the same guitar.\n\nBefore his mentor Sholes died in 1968, Atkins had become vice president of RCA's country division. In 1987, he told \"Nine-O-One Network\" magazine that he was \"ashamed\" of his promotion: \"I wanted to be known as a guitarist and I know, too, that they give you titles like that in lieu of money. So beware when they want to make you vice president.\" He had brought Waylon Jennings, Willie Nelson, Connie Smith, Bobby Bare, Dolly Parton, Jerry Reed, and John Hartford to the label in the 1960s and inspired and helped countless others. He took a considerable risk during the mid-1960s, when the Civil Rights Movement sparked violence throughout the South, by signing country music's first African-American singer, Charley Pride, who sang rawer country than the smoother music Atkins had pioneered.\n\nAtkins's biggest hit single came in 1965, with \"Yakety Axe\", an adaptation of \"Yakety Sax\", by his friend, the saxophonist Boots Randolph. He rarely performed in those days and eventually hired other RCA producers, such as Bob Ferguson and Felton Jarvis, to lessen his workload.\n\nIn the 1970s, Atkins became increasingly stressed by his executive duties. He produced fewer records, but could still turn out hits such as Perry Como's 1973 pop hit \"And I Love You So\". He recorded extensively with close friend and fellow picker Jerry Reed, who had become a hit artist in his own right. A 1973 diagnosis of colon cancer, however, led Atkins to redefine his role at RCA, to allow others to handle administration while he went back to his first love, the guitar, often recording with Reed or even Homer & Jethro's Jethro Burns (Atkins' brother-in-law) after Homer died in 1971.\n\nBy the late 1970s, RCA decided to remove Atkins from his producing duties and replace him with younger men. He also felt stifled because the record company would not let him branch into jazz. His mid-1970s collaborations with one of his influences, Les Paul, \"Chester & Lester\" and \"Guitar Monsters\", had already reflected that interest; \"Chester & Lester\" was one of the best-selling recordings of Atkins's career. At the same time, he grew dissatisfied with the direction Gretsch (no longer family-owned) was going and withdrew his authorization for them to use his name and began designing guitars with Gibson. Atkins ended his 35-year association with RCA in 1982 and signed with Columbia Records, for whom he produced a debut album in 1983.\n\nJazz had always been a strong love of his, and often in his career he was criticized by \"pure\" country musicians for his jazz influences. He also said on many occasions that he did not like being called a \"country guitarist\", insisting that he was a guitarist, period. Although he played 'by ear' and was a masterful improviser, he was able to read music and even performed some classical guitar pieces. When Roger C. Field, a friend, suggested to him in 1991 that he record and perform with a female singer, he did so with Suzy Bogguss.\n\nHe returned to his country roots for albums he recorded with Mark Knopfler and Jerry Reed. Knopfler had long mentioned Atkins as one of his earliest influences. Atkins also collaborated with Australian guitar legend Tommy Emmanuel. On being asked to name the ten most influential guitarists of the 20th century, he named Django Reinhardt to the first position, and also placed himself on the list.\n\nIn later years, he even went back to radio, appearing on Garrison Keillor's \"Prairie Home Companion\" radio program, on American Public Media radio, even picking up a fiddle from time to time, and performing songs such as Bob Wills' \"Corrina, Corrina\" and Willie Nelson's \"Seven Spanish Angels\" with Nelson on a 1985 broadcast of the show at the Bridges Auditorium on the campus of Pomona College (then Claremont College).\n\nAtkins received numerous awards, including 14 Grammy awards and nine Country Music Association awards for Instrumentalist of the Year. In 1993, he was honored with the Grammy Lifetime Achievement Award. \"Billboard\" magazine awarded him its Century Award, its \"highest honor for distinguished creative achievement\", in December 1997.\n\nAtkins is notable for his broad influence. His love for numerous styles of music can be traced from his early recording of the stride pianist James P. Johnson's \"Johnson Rag\", all the way to the rock stylings of Eric Johnson, an invited guest on Atkins's recording sessions, who, when Atkins attempted to copy his influential rocker \"Cliffs of Dover\", led to Atkins's creation of a unique arrangement of \"Londonderry Air (Danny Boy)\".\n\nAtkins's recordings of \"Malagueña\" inspired a new generation of flamenco guitarists; the classical guitar selections included on almost all his albums were, for many American artists working in the field today, the first classical guitar they ever heard. He recorded smooth jazz guitar still played on American airwaves today.\n\nAtkins continued performing in the 1990s, but his health declined after he was diagnosed again with cancer in 1996. He died on June 30, 2001, at his home in Nashville, Tennessee, ten days after his 77th birthday. His memorial service was held at Ryman Auditorium in Nashville. He was buried at Harpeth Hills Memory Gardens in Nashville.\n\nA stretch of Interstate 185 in southwest Georgia (between LaGrange and Columbus) is named \"Chet Atkins Parkway\". This stretch of interstate runs through Fortson, where Atkins spent much of his childhood.\n\nIn 2002, Atkins was posthumously inducted into the Rock and Roll Hall of Fame. His award was presented by Marty Stuart and Brian Setzer and accepted by Atkins's grandson, Jonathan Russell. The following year, Atkins ranked number 28 in Country Music Television's \"40 Greatest Men of Country Music\".\n\nAt the age of 13, the future jazz guitarist Earl Klugh was captivated watching Atkins's guitar playing on \"The Perry Como Show.\" Similarly, he was a big influence on Doyle Dykes. Atkins also inspired Drexl Jonez and Tommy Emmanuel.\n\nClint Black's album \"Nothin' but the Taillights\" includes the song \"Ode to Chet\", which includes the lyrics \"'Cause I can win her over like Romeo did Juliet, if I can only show her I can almost pick that legato lick like Chet\" and \"It'll take more than Mel Bay 1, 2, & 3 if I'm ever gonna play like CGP.\" Atkins played guitar on the track. At the end of the song, Black and Atkins had a brief conversation.\n\nChet's song \"Jam Man\" is currently used in commercials for Esurance.\n\nThe opening guitar licks to the Miranda Lambert song \"Only Prettier\" sound very similar to Atkins's guitar-picking style.\n\nIn 1967, a tribute song, \"Chet's Tune\", was produced for his birthday, with contributions by a long list of RCA Victor artists, including Eddy Arnold, Connie Smith, Jerry Reed, Willie Nelson, Hank Snow, and others. The song was written by the Nashville songwriter Cy Coben, a friend of Atkins's. The single reached number 38 on the country charts.\n\nIn 2009, Steve Wariner released an album entitled \"My Tribute to Chet Atkins\". One song from that record, \"Producer's Medley\", featured Wariner's recreation of several famous songs which Atkins both produced and performed. \"Producer's Medley\" won the Grammy for Best Country Instrumental Performance in 2010.\n\nIn November 2011, \"Rolling Stone\" ranked Atkins number 21 on its list of the \"100 Greatest Guitarists of All Time.\n\nCountry Music Association\n\nCountry Music Hall of Fame and Museum\n\nGrammy Awards\n\nRock and Roll Hall of Fame\n\n\n", "id": "7756", "title": "Chet Atkins"}
{"url": "https://en.wikipedia.org/wiki?curid=7757", "text": "Conrad II (disambiguation)\n\nConrad II may refer to:\n\n", "id": "7757", "title": "Conrad II (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=7765", "text": "Cahiers du cinéma\n\nCahiers du Cinéma (, \"Notebooks on Cinema\") is a French language film magazine founded in 1951 by André Bazin, Jacques Doniol-Valcroze and Joseph-Marie Lo Duca. It developed from the earlier magazine \"Revue du Cinéma\" (\"Review of the Cinema\" established in 1928) involving members of two Paris film clubs—\"Objectif 49\" (\"Objective 49\") (Robert Bresson, Jean Cocteau and Alexandre Astruc, among others) and \"Ciné-Club du Quartier Latin\" (\"Cinema Club of the Latin Quarter\"). Initially edited by Doniol-Valcroze and, after 1957, by Éric Rohmer (Maurice Scherer), it included amongst its writers Jacques Rivette, Jean-Luc Godard, Claude Chabrol and François Truffaut. It is the oldest film magazine in publication.\n\nThe first issue of \"Cahiers\" appeared in April 1951. A 1954 article by Truffaut attacked \"La qualité française\" (\"the French Quality\") (usually translated as \"The Tradition of Quality\") and was the manifesto for 'la politique des Auteurs' which Andrew Sarris later termed the auteur theory — resulting in the re-evaluation of Hollywood films and directors such as Alfred Hitchcock, Howard Hawks, Robert Aldrich, Nicholas Ray, and Fritz Lang. \"Cahiers du Cinema\" authors also championed the work of directors Jean Renoir, Roberto Rossellini, Kenji Mizoguchi, Max Ophüls, and Jean Cocteau, by centering their critical evaluations on a film's \"mise en scène\". In turn, auteurs were compared and contrasted, and a true film dialogue was established. The magazine also was essential to the creation of the \"Nouvelle Vague\", or New Wave, of French cinema, which centered on films directed by \"Cahiers\" authors such as Godard and Truffaut. Movement by movement, style by style, the cahiers sought to advance and analyze the growth of world cinema.\n\nJacques Rivette replaced Rohmer as editor in 1963, shifted political and social concerns and paid more attention to the non-Hollywood cinema. The style moved through literary modernism in the early 1960s to radicalism and dialectical materialism by 1970. Moreover, during the mid-1970s the magazine was run by a Maoist editorial collective. In the mid-1970s, a review of the American movie \"Jaws\" marked the magazine's return to more commercial perspectives, and an editorial turnover: (Serge Daney, Serge Toubiana, Thierry Jousse, Antoine de Baecque and Charles Tesson). It led to the rehabilitation of some of the old \"Cahiers\" favourites, as well as some new film makers like Manoel de Oliveira, Raoul Ruiz, Hou Hsiao-Hsien, Youssef Chahine, and Maurice Pialat. Recent writers have included Serge Daney, Serge Toubiana, Thierry Jousse, Antoine de Baecque, Vincent Ostria, Charles Tesson and André Téchiné, Léos Carax, Olivier Assayas, Danièle Dubroux, and Serge Le Péron.\n\nIn 1998, the Editions de l'Etoile (the company publishing \"Cahiers\") was acquired by the press group \"Le Monde\". Traditionally losing money, the magazine attempted a make-over in 1999 to gain new readers, leading to a first split among writers and resulting in a magazine addressing all visual arts in a post-modernist approach. This version of the magazine printed ill-received opinion pieces on reality TV or video games that confused the traditional readership of the magazine.\n\n\"Le Monde\" took full editorial control of the magazine in 2003, appointing Jean-Michel Frodon as editor-in-chief.\n\nIn February 2009, \"Cahiers\" was acquired from \"Le Monde\" by Richard Schlagman, also owner of Phaidon Press, a worldwide publishing group which specialises in books on the visual arts. In July 2009, Stéphane Delorme and Jean-Philippe Tessé have been promoted respectively as editor-in-chief and deputy chief editor.\n\nThe following is a list of the Top 10 films chosen annually by the critics of \"Cahiers du Cinéma\".\n\nNo lists\n\n\n\n", "id": "7765", "title": "Cahiers du cinéma"}
{"url": "https://en.wikipedia.org/wiki?curid=7767", "text": "Circuit Park Zandvoort\n\nCircuit Park Zandvoort is a motorsport race track located in the dunes north of Zandvoort, Netherlands, near the North Sea coast line.\n\nThere were plans for races at Zandvoort before World War II: the first street race was held on June 3, 1939. However, a permanent race track was not constructed until after the war, using communications roads built by the occupying German army. Contrary to popular belief John Hugenholtz cannot be credited with the design of the Zandvoort track, although he was involved as the Nederlandse Automobiel Ren Club chairman (the Dutch Auto Racing Club) before becoming the first track director in 1949. Instead, it was 1927 Le Mans winner, S. C. H. \"Sammy\" Davis who was brought in as a track design advisor in July 1946 although the layout was partly dictated by the existing roads.\n\nThe circuit was inaugurated on August 7, 1948. The following year the race was called the Zandvoort Grand Prix and in 1950 it became the \"Grote Prijs van Nederland\" or Dutch Grand Prix. The Dutch Grand Prix was a round of the World Drivers Championship for the first time [but not a Formula One race, as the World Championship was for Formula Two cars that year and in 1953]. There was no GP at Zandvoort in 1954 (a sportscar-event replaced it), but 1955 saw the first proper Formula One race counting for the World Championship. After 2 more years without a race the Dutch Grand Prix was back on the World Championship(s) calendar in 1958 and from then on remained a permanent fixture (with the exception of 1972) until , when it was held for the last time.\nTo solve a number of problems that had made it impossible to develop and upgrade the track, the most important one being noise pollution for the inhabitants of the part of Zandvoort closest to the track, the track management adopted and developed a plan to move the most southern part of the track away from the housing estate and rebuild a more compact track in the remaining former 'infield'. In January 1987 this plan got the necessary 'green light' when it was formally approved by the Noord-Holland Provincial Council. However, only a couple of months later a new problem arose: the company that commercially ran the circuit (CENAV), called in the receiver and went out of business, marking the end of \"Circuit van Zandvoort\". \nAgain the track, owned by the municipality of Zandvoort, was in danger of being permanently lost for motorsports. However, a new operating company, the Stichting Exploitatie Circuit Park, was formed and started work at the realization of the track's reconstruction plans. Circuit Park Zandvoort was born and in the summer of 1989 the track was remodeled to an interim Club Circuit of , while the disposed southern part of the track was used to build a Vendorado Bungalow Park and new premises for the local football and hockey clubs.\n\nIn 1995, CPZ (\"Circuit Park Zandvoort\") got the \"A Status\" of the Dutch government and began building an international Grand Prix Circuit. This project was finished in 2001 when, after the track was redesigned to a long circuit and a new pits building was realized (by HPG, the development company of John Hugenholtz jr, son of the former director), a new grandstand was situated along the long straight. One of the major events that is held at the circuit, along with DTM and A1GP, is the RTL Masters of Formula 3, where Formula Three cars of several national racing series compete with each other (originally called Marlboro Masters, before tobacco advertising ban). A noise restriction order was responsible for this event moving to the Belgian Circuit Zolder for 2007 and 2008. However, the race returned to its historical home in 2009.\n\nCircuit Park Zandvoort played host to the first race in the 2006/07 season of A1 Grand Prix from 29 September–1 October 2006. On 21 August 2008, the official A1GP site reported that the 2008/09 season's first race has moved from the Mugello Circuit, Italy to Zandvoort on the 4–5 October 2008 due to the delay in the building the new chassis for the new race cars. The Dutch round moved to TT Circuit Assen in 2010. A1GP bankrupted before its fifth season and the Dutch round was replaced with Superleague Formula.\n\nThe circuit gained popularity because of its fast, sweeping corners such as Scheivlak as well as the \"Tarzanbocht\" (Tarzan corner) hairpin at the end of the start/finish straight. Tarzanbocht is the most famous corner in the circuit. Since there is a camber in the corner, it provides excellent overtaking opportunities. It is possible to pass around the outside as well as the easier inside lane. This corner is reportedly named after a local character who had earned the nickname of Tarzan and only wanted to give up his vegetable garden in the dunes if the track's designers named a nearby corner after him. On the other hand, many different stories about Tarzan Corner are known.\n\nThe circuit design has been modified and altered several times:\n\nThe corners are named as follows (the numbers correspond to the image above, starting at the start/finish line):\n\nThe elevation difference is .\n\nIn the history of the circuit, several fatal accidents have occurred.\nIn August 1959 a 26th World Championships Road Race (men) was held here. André Darrigade of France won the race, Tom Simpson (Britain) was 4th.\n\nSince 2008, the course has been used as the venue for the Runner's World Zandvoort Circuit Run, a 5-kilometre road running competition. The 2010 edition of the race attracted Lornah Kiplagat, a multiple world champion, who won the ladies 5 km race.\n\nOn 25./26. May 2013 starting 3 p.m. a 24-hours cycle race open for public for soloists and teams up to 8 was held. On 13./14. June 2015 (12:00) the Cycling Zandvoort - 24-uurs race over 4307-m-laps took place.\n\n\n", "id": "7767", "title": "Circuit Park Zandvoort"}
{"url": "https://en.wikipedia.org/wiki?curid=7768", "text": "Crete Senesi\n\nThe Crete Senesi refers to an area of the Italian region of Tuscany to the south of Siena. It consists of a range of hills and woods among villages and includes the \"comuni\" of \nAsciano, \nBuonconvento,\nMonteroni d'Arbia,\nRapolano Terme and\nSan Giovanni d'Asso, all within the province of Siena.\n\n\"Crete senesi\" are literally ‘Senese clays’, and the distinctive grey colouration of the soil gives the landscape an appearance often described as lunar. This characteristic clay, known as \"mattaione\", represents the sediments of the Pliocene sea which covered the area between 2.5 and 4.5 million years ago. Nearby is also the semi-arid area known as the Accona Desert.\n\nPerhaps the most notable edifice of this area is the monastery of Monte Oliveto Maggiore.\n\nThe region is known for its production of white truffles, and hosts a festival and a museum dedicated to the rare fungus (genus Tuber).\n\n", "id": "7768", "title": "Crete Senesi"}
{"url": "https://en.wikipedia.org/wiki?curid=7770", "text": "Christmas tree\n\nA Christmas tree is a decorated tree, usually an evergreen conifer such as spruce, pine, or fir or an artificial tree of similar appearance, associated with the celebration of Christmas. The modern Christmas tree was developed in early modern Germany (where it is today called \"Weihnachtsbaum\" or \"Christbaum\"), in which devout Christians brought decorated trees into their homes. It acquired popularity beyond the Lutheran areas of Germany, during the second half of the 19th century, at first among the upper classes.\n\nThe tree was traditionally decorated with \"roses made of colored paper, apples, wafers, tinsel, [and] sweetmeats\". In the 18th century, it began to be illuminated by candles which were ultimately replaced by Christmas lights after the advent of electrification. Today, there is a wide variety of traditional ornaments, such as garlands, baubles, tinsel, and candy canes. An angel or star might be placed at the top of the tree to represent the archangel Gabriel or the Star of Bethlehem from the Nativity. Edible items such as gingerbread, chocolate and other sweets are also popular, and are tied to or hung from the tree's branches with ribbons.\n\nIn the Western Christian tradition, Christmas trees are variously erected on days such as the first day of Advent or even as late as Christmas Eve depending on the country; customs of the same faith hold that the two traditional days when Christmas decorations, such as the Christmas tree, are removed are Twelfth Night and if they are not taken down on that day, Candlemas, the latter of which ends the Christmas-Epiphany season in some denominations.\n\nThe Christmas tree is sometimes compared with the \"Yule-tree\", especially in discussions of its folkloric origins.\n\nThe relevance of ancient pre-Christian customs to the 16th Century German initiation of the Christmas tree custom is disputed. Resistance to the custom was often because of its confirmed Lutheran origins.\n\nOther sources have tried to make a connection between the first documented Christmas trees in Alsace around 1600 and pre-Christian traditions. For example, according to the \"Encyclopædia Britannica\", \"The use of evergreen trees, wreaths, and garlands to symbolize eternal life was a custom of the ancient Egyptians, Chinese, and Hebrews. Tree worship was common among the pagan Europeans and survived their conversion to Christianity in the Scandinavian customs of decorating the house and barn with evergreens at the New Year to scare away the devil and of setting up a tree for the birds during Christmas time.\"\n\nDuring the Roman mid-winter festival of Saturnalia, houses were decorated with wreaths of evergreen plants, along with other antecedent customs now associated with Christmas.\n\nThe modern Christmas tree is frequently traced to the symbolism of trees in pre-Christian winter rites, wherein Viking and Saxon worshiped trees. The story of Saint Boniface cutting down Donar's Oak illustrates the pagan practices in 8th century among the Germans. A later folk version of the story adds the detail that an evergreen tree grew in place of the felled oak, telling them about how its triangular shape reminds humanity of the Trinity and how it points to heaven.\n\nAlternatively, it is identified with the \"tree of paradise\" of medieval mystery plays that were given on 24 December, the commemoration and name day of Adam and Eve in various countries. In such plays, a tree decorated with apples (to represent the forbidden fruit) and wafers (to represent the Eucharist and redemption) was used as a setting for the play. Like the Christmas crib, the Paradise tree was later placed in homes. The apples were replaced by round objects such as shiny red balls.\n\nModern Christmas trees originated during the Renaissance of early modern Germany. Its 16th-century origins are sometimes associated with Protestant Christian reformer Martin Luther who is said to have first added lighted candles to an evergreen tree.\n\nThe first recorded Christmas tree can be found on the keystone sculpture of a private home in Turckheim, Alsace (then part of Germany, today France), dating 1576.\n\nThe Georgians have their own traditional Christmas tree called Chichilaki, made from dried up hazelnut or walnut branches that are shaped to form a small coniferous tree. These pale-colored ornaments differ in height from to . Chichilakis are most common in the Guria and Samegrelo regions of Georgia near the Black Sea, but they can also be found in some stores around the capital of Tbilisi.\nGeorgians believe that Chichilaki resembles the famous beard of St. Basil the Great, who is thought to visit people during Christmas similar to the Santa Claus tradition.\n\nThere was an old pagan custom, associated with Koliada, of suspending a branch of fir, spruce or pine called Podłaźniczka from the ceiling. The branches were decorated with apples, nuts, cookies, colored paper, stars made of straw, ribbons and colored wafers. Some people believed that the tree had magical powers that were linked with harvesting and success in the next year.\n\nIn the late eighteenth and early nineteenth century, these traditions were almost completely replaced by the German custom of decorating the Christmas tree.\n\nCustoms of erecting decorated trees in wintertime can be traced to Christmas celebrations in Renaissance-era guilds in Northern Germany and Livonia. The first evidence of decorated trees associated with Christmas Day are trees in guildhalls decorated with sweets to be enjoyed by the apprentices and children. In Livonia (present-day Estonia and Latvia), in 1441, 1442, 1510 and 1514, the Brotherhood of Blackheads erected a tree for the holidays in their guild houses in Reval (now Tallinn) and Riga. On the last night of the celebrations leading up to the holidays, the tree was taken to the Town Hall Square where the members of the brotherhood danced around it.\n\nA Bremen guild chronicle of 1570 reports that a small tree decorated with \"apples, nuts, dates, pretzels and paper flowers\" was erected in the guild-house for the benefit of the guild members' children, who collected the dainties on Christmas Day. In 1584, the pastor and chronicler Balthasar Russow in his \" (1584) wrote of an established tradition of setting up a decorated spruce at the market square where the young men \"went with a flock of maidens and women, first sang and danced there and then set the tree aflame\".\n\nAfter the Protestant Reformation, such trees are seen in the houses of upper-class Protestant families as a counterpart to the Catholic Christmas cribs. This transition from the guild hall to the bourgeois family homes in the Protestant parts of Germany ultimately gives rise to the modern tradition as it developed in the 18th and 19th centuries.\n\nBy the early 18th century, the custom had become common in towns of the upper Rhineland, but it had not yet spread to rural areas. Wax candles, expensive items at the time, are found in attestations from the late 18th century.\n\nAlong the lower Rhine, an area of Roman Catholic majority, the Christmas tree was largely regarded as a Protestant custom. As a result, it remained confined to the upper Rhineland for a relatively long period of time. The custom did eventually gain wider acceptance beginning around 1815 by way of Prussian officials who emigrated there following the Congress of Vienna.\n\nIn the 19th century, the Christmas tree was taken to be an expression of German culture and of \", especially among emigrants overseas.\n\nA decisive factor in winning general popularity was the German army's decision to place Christmas trees in its barracks and military hospitals during the Franco-Prussian War. Only at the start of the 20th century did Christmas trees appear inside churches, this time in a new brightly lit form.\n\nIn the early 19th century, the custom became popular among the nobility and spread to royal courts as far as Russia. Princess Henrietta of Nassau-Weilburg introduced the Christmas tree to Vienna in 1816, and the custom spread across Austria in the following years. In France, the first Christmas tree was introduced in 1840 by the duchesse d'Orléans. In Denmark a Danish newspaper claims that the first attested Christmas tree was lit in 1808 by countess Wilhemine of Holsteinborg. It was the aging countess who told the story of the first Danish Christmas tree to the Danish writer Hans Christian Andersen in 1865. He had published a fairy-tale called \"The Fir-Tree\" in 1844, recounting the fate of a fir-tree being used as a Christmas tree.\n\nAlthough the tradition of decorating the home with evergreens was long established, the custom of decorating an entire small tree was unknown in Britain until some two centuries ago. At the time of the personal union with Hanover, George III's German-born wife, Charlotte of Mecklenburg-Strelitz, introduced a Christmas tree at a party she gave for children in 1800. The custom did not at first spread much beyond the royal family. Queen Victoria as a child was familiar with it and a tree was placed in her room every Christmas. In her journal for Christmas Eve 1832, the delighted 13-year-old princess wrote: \"After dinner... we then went into the drawing-room near the dining-room... There were two large round tables on which were placed two trees hung with lights and sugar ornaments. All the presents being placed round the trees...\" After Victoria's marriage to her German cousin Prince Albert, by 1841 the custom became even more widespread as wealthier middle-class families followed the fashion. In 1842 a newspaper advert for Christmas trees makes clear their smart cachet, German origins and association with children and gift-giving. An illustrated book, \"The Christmas Tree\", describing their use and origins in detail, was on sale in December 1844. In 1847, Prince Albert wrote: \"I must now seek in the children an echo of what Ernest [his brother] and I were in the old time, of what we felt and thought; and their delight in the Christmas-trees is not less than ours used to be\". A boost to the trend was given in 1848 when \"The Illustrated London News\", in a report picked up by other papers, described the trees in Windsor Castle in detail and showed the main tree, surrounded by the royal family, on its cover. In fewer than ten years their use in better-off homes was widespread. By 1856 a northern provincial newspaper contained an advert alluding casually to them, as well as reporting the accidental death of a woman whose dress caught fire as she lit the tapers on a Christmas tree. They had not yet spread down the social scale though, as a report from Berlin in 1858 contrasts the situation there where \"Every family has its own\" with that of Britain, where Christmas trees were still the preserve of the wealthy or the \"romantic\".\n\nTheir use at public entertainments, charity bazaars and in hospitals made them increasingly familiar however, and in 1906 a charity was set up specifically to ensure even poor children in London slums 'who had never seen a Christmas tree' would enjoy one that year. Anti-German sentiment after World War I briefly reduced their popularity but the effect was short-lived and by the mid-1920s the use of Christmas trees had spread to all classes. In 1933 a restriction on the importation of foreign trees led to the 'rapid growth of a new industry' as the growing of Christmas trees within Britain became commercially viable due to the size of demand. By 2013 the number of trees grown in Britain for the Christmas market was approximately 8 million and their display in homes, shops and public spaces a normal part of the Christmas season.\n\nThe tradition was introduced to Canada in the winter of 1781 by Brunswick soldiers stationed in the Province of Quebec to garrison the colony against American attack. General Friedrich Adolf Riedesel and his wife, the Baroness von Riedesel, held a Christmas party at Sorel, delighting their guests with a fir tree decorated with candles and fruits.\n\nThe Christmas tree became very common in the United States in the early nineteenth century. The first image of a Christmas tree was published in 1836 as the frontispiece to \"The Stranger's Gift\" by Hermann Bokum. The first mention of the Christmas tree in American literature was in a story in the 1836 edition of \"The Token and Atlantic Souvenir\", titled \"New Year's Day,\" by Catherine Maria Sedgwick, where she tells the story of a German maid decorating her mistress's tree. Also, a woodcut of the British Royal family with their Christmas tree at Windsor Castle, initially published in \"The Illustrated London News\" December 1848, was copied in the United States at Christmas 1850, in \"Godey's Lady's Book\". \"Godey's\" copied it exactly, except for the removal of the Queen's tiara and Prince Albert's moustache, to remake the engraving into an American scene. The republished \"Godey's\" image became the first widely circulated picture of a decorated evergreen Christmas tree in America. Art historian Karal Ann Marling called Prince Albert and Queen Victoria, shorn of their royal trappings, \"the first influential American Christmas tree\". Folk-culture historian Alfred Lewis Shoemaker states, \"In all of America there was no more important medium in spreading the Christmas tree in the decade 1850–60 than \"Godey's Lady's Book\"\". The image was reprinted in 1860, and by the 1870s, putting up a Christmas tree had become even more common in America.\n\nSeveral cities in the United States with German connections lay claim to that country's first Christmas tree: Windsor Locks, Connecticut, claims that a Hessian soldier put up a Christmas tree in 1777 while imprisoned at the Noden-Reed House, while the \"First Christmas Tree in America\" is also claimed by Easton, Pennsylvania, where German settlers purportedly erected a Christmas tree in 1816. In his diary, Matthew Zahm of Lancaster, Pennsylvania, recorded the use of a Christmas tree in 1821, leading Lancaster to also lay claim to the first Christmas tree in America. Other accounts credit Charles Follen, a German immigrant to Boston, for being the first to introduce to America the custom of decorating a Christmas tree. August Imgard, a German immigrant living in Wooster, Ohio, is said to be the first to popularize the practice of decorating a tree with candy canes. In 1847, Imgard cut a blue spruce tree from a woods outside town, had the Wooster village tinsmith construct a star, and placed the tree in his house, decorating it with paper ornaments, gilded nuts and Kuchen. German immigrant Charles Minnegerode accepted a position as a professor of humanities at the College of William & Mary in Williamsburg, Virginia, in 1842, where he taught Latin and Greek. Entering into the social life of the Virginia Tidewater, Minnigerode introduced the German custom of decorating an evergreen tree at Christmas at the home of law professor St. George Tucker, thereby becoming another of many influences that prompted Americans to adopt the practice at about that time. An 1853 article on Christmas customs in Pennsylvania defines them as mostly \"German in origin\", including the Christmas tree, which is \"planted in a flower pot filled with earth, and its branches are covered with presents, chiefly of confectionary, for the younger members of the family.\" The article distinguishes between customs in different states however, claiming that in New England generally \"Christmas is not much celebrated\", whereas in Pennsylvania and New York it is.\n\nWhen Edward H. Johnson was vice president of the Edison Electric Light Company, a predecessor of Con Edison, he created the first known electrically illuminated Christmas tree at his home in New York City in 1882. Johnson became the \"Father of Electric Christmas Tree Lights\".\n\nThe lyrics sung in the United States to the German tune ' begin \"O Christmas tree...\", giving rise to the mistaken idea that the German word ' (fir tree) means \"Christmas tree\", the German word for which is instead \"\".\n\nIn Russia, the Christmas tree was banned after the October Revolution but then reinstated as a \"New-year spruce\" () in 1935. It became a fully secular icon of the New Year holiday, for example, the crowning star was regarded not as a symbol of Bethlehem Star, but as the Red star. Decorations, such as figurines of airplanes, bicycles, space rockets, cosmonauts, and characters of Russian fairy tales, were produced. This tradition persists after the fall of the USSR, with the New Year holiday outweighing the Christmas (7 January) for a wide majority of Russian people.\n\nThe TV special \"A Charlie Brown Christmas\" (1965) was influential on the pop culture surrounding the Christmas tree. Aluminum Christmas trees were popular during the early 1960s in the US. They were satirized in the Charlie Brown show and came to be seen as symbolizing the commercialization of Christmas. The term \"Charlie Brown Christmas tree\", describing any poor-looking or malformed little tree, also derives from the 1965 TV special, based on the appearance of Charlie Brown's Christmas tree.\n\nSince the early 20th century, it has become common in many cities, towns, and department stores to put up public Christmas trees outdoors, such as the Macy's Great Tree in Atlanta (since 1948), the Rockefeller Center Christmas Tree in New York City, and the large Christmas tree at Victoria Square in Adelaide.\n\nThe use of fire retardant allows many indoor public areas to place real trees and be compliant with code. Licensed applicants of fire retardant solution spray the tree, tag the tree, and provide a certificate for inspection. Real trees are popular with high end visual merchandising displays around the world. Leading global retailers such as Apple often place real trees in their window displays. In 2009, Apple placed two Fraser fir trees in every one of its retail establishments.\n\nThe United States' National Christmas Tree has been lit each year since 1923 on the South Lawn of the White House. Today, the lighting of the National Christmas Tree is part of what has become a major holiday event at the White House. President Jimmy Carter lit only the crowning star atop the tree in 1979 in honor of the Americans being held hostage in Iran. The same was true in 1980, except that the tree was fully lit for 417 seconds, one second for each day the hostages had been in captivity.\n\nDuring most of the 1970s and 1980s, the largest decorated Christmas tree in the world was put up every year on the property of the \"National Enquirer\" in Lantana, Florida. This tradition grew into one of the most spectacular and celebrated events in the history of southern Florida, but was discontinued on the death of the paper's founder in the late 1980s.\n\nIn some cities, a Festival of Trees is organized around the decoration and display of multiple trees as charity events.\n\nThe giving of Christmas trees has also often been associated with the end of hostilities. After the signing of the Armistice in 1918 the city of Manchester sent a tree, and £500 to buy chocolate and cakes, for the children of the much-bombarded town of Lille in northern France. In some cases the trees represent special commemorative gifts, such as in Trafalgar Square in London, where the City of Oslo, Norway presents a tree to the people of London as a token of appreciation for the British support of Norwegian resistance during the Second World War; in Boston, where the tree is a gift from the province of Nova Scotia, in thanks for rapid deployment of supplies and rescuers to the 1917 ammunition ship explosion that leveled the city of Halifax; and in Newcastle upon Tyne, where the main civic Christmas tree is an annual gift from the city of Bergen, in thanks for the part played by soldiers from Newcastle in liberating Bergen from Nazi occupation. Norway also annually gifts a Christmas tree to Washington, D.C. as a symbol of friendship between Norway and the US and as an expression of gratitude from Norway for the help received from the US during World War II.\n\nA \"Chrismon tree\" is a Christmas tree decorated with explicitly Christian symbols in white and gold. \nFirst introduced by North American Lutherans in 1957, the practice has rapidly spread to other Christian denominations, including Anglicans,\nCatholics, Methodists, and the Reformed.\n\n\"Chrismon\" (plural \"Chrismons\") was adopted for the type of Christmas decoration and explained as a portmanteau of \"CHRISt-MONOgram\" (a Christogram).\n\nBoth setting up and taking down a Christmas tree are associated with specific dates. Traditionally, Christmas trees were not brought in and decorated until Christmas Eve (24 December) or, in the traditions celebrating Christmas Eve rather than the first day of Christmas, 23 December, and then removed the day after Twelfth Night (5 January); to have a tree up before or after these dates was even considered bad luck, and that to avoid bad luck from affecting the house's residents, the tree must be left up until after the following Twelfth Night passes.\n\nIn many areas, it has become customary to set up one's Christmas tree at the beginning of the Advent season. Some families in the U.S. and Canada will put up a Christmas tree a week prior to American Thanksgiving (the fourth Thursday of November), and Christmas decorations can show up even earlier in retail stores, often the day after Halloween (31 October). In Canada many families wait until after Remembrance Day, as to show respect to fallen soldiers. Some households do not put up the tree until the second week of December, and leave it up until 6 January (Epiphany). In Germany, traditionally the tree is put up on 24 December and taken down on 7 January, though many start one or two weeks earlier, and in Roman Catholic homes the tree may be kept until February 2 (Candlemas).\n\nIn Italy and Argentina, along with many countries in Latin America, the Christmas tree is put up on 8 December (Immaculate Conception day) and left up until 6 January. In Australia, the Christmas tree is usually put up on 1 December, which occurs about a 2 weeks before the school summer holidays (except for South Australia, where most people put up their tree after in late November following the completion of the Adelaide Christmas Pageant, a time frame that has started to filter into other states as the official time Christmas decorations and in store Santa Claus start to appear) and is left up until it is taken down. Some traditions suggest that Christmas trees may be kept up until no later than 2 February, the feast of the Presentation of Jesus at the Temple (Candlemas), when the Christmas season effectively closes. Superstitions say that it is a bad sign if Christmas greenery is not removed by Candlemas Eve.\n\nChristmas ornaments are decorations (usually made of glass, metal, wood, or ceramics) that are used to decorate a Christmas tree. The first decorated trees were adorned with apples, white candy canes and pastries in the shapes of stars, hearts and flowers. Glass baubles were first made in Lauscha, Germany, and also garlands of glass beads and tin figures that could be hung on trees. The popularity of these decorations grew into the production of glass figures made by highly skilled artisans with clay molds.\n\nTinsel and several types of garland or ribbon are commonly used to decorate a Christmas tree. Silvered saran-based tinsel was introduced later. Delicate mold-blown and painted colored glass Christmas ornaments were a specialty of the glass factories in the Thuringian Forest, especially in Lauscha in the late 19th century, and have since become a large industry, complete with famous-name designers. Baubles are another common decoration, consisting of small hollow glass or plastic spheres coated with a thin metallic layer to make them reflective, with a further coating of a thin pigmented polymer in order to provide coloration.\nLighting with electric lights (Christmas lights or, in the United Kingdom, fairy lights) is commonly done. A tree-topper, sometimes an angel but more frequently a star, completes the decoration.\n\nIn the late 1800s, home-made white Christmas trees were made by wrapping strips of cotton batting around leafless branches creating the appearance of a snow-laden tree.\nIn the 1940s and 1950s, popularized by Hollywood films in the late 1930s, flocking was very popular on the West Coast of the United States. There were home flocking kits that could be used with vacuum cleaners. In the 1980s some trees were sprayed with fluffy white flocking to simulate snow.\n\nEach year, 33 to 36 million Christmas trees are produced in America, and 50 to 60 million are produced in Europe. In 1998, there were about 15,000 growers in America (a third of them \"choose and cut\" farms). In that same year, it was estimated that Americans spent $1.5 billion on Christmas trees.\n\nThe most commonly used species are fir (\"Abies\"), which have the benefit of not shedding their needles when they dry out, as well as retaining good foliage color and scent; but species in other genera are also used.\n\nIn northern Europe most commonly used are:\n\nIn North America, Central America and South America most commonly used are:\n\nSeveral other species are used to a lesser extent. Less-traditional conifers are sometimes used, such as giant sequoia, Leyland cypress, Monterey cypress and eastern juniper. Various types of spruce tree are also used for Christmas trees (including the blue spruce and, less commonly, the white spruce); but spruces begin to lose their needles rapidly upon being cut, and spruce needles are often sharp, making decorating uncomfortable. Virginia pine is still available on some tree farms in the southeastern United States; however, its winter color is faded. The long-needled eastern white pine is also used there, though it is an unpopular Christmas tree in most parts of the country, owing also to its faded winter coloration and limp branches, making decorating difficult with all but the lightest ornaments. Norfolk Island pine is sometimes used, particularly in Oceania, and in Australia, some species of the genera \"Casuarina\" and \"Allocasuarina\" are also occasionally used as Christmas trees. But, by far, the most common tree is the Monterey pine. \"Adenanthos sericeus\" or Albany woolly bush is commonly sold in southern Australia as a potted living Christmas tree. Hemlock species are generally considered unsuitable as Christmas trees due to their poor needle retention and inability to support the weight of lights and ornaments.\n\nSome trees, frequently referred to as \"living Christmas trees\", are sold live with roots and soil, often from a plant nursery, to be stored at nurseries in planters or planted later outdoors and enjoyed (and often decorated) for years or decades. Others are produced in a container and sometimes as topiary for a porch or patio. However, when done improperly, the combination of root loss caused by digging, and the indoor environment of high temperature and low humidity is very detrimental to the tree's health; additionally, the warmth of an indoor climate will bring the tree out of its natural winter dormancy, leaving it little protection when put back outside into a cold outdoor climate. Often Christmas trees are a large attraction for living animals, including mice and spiders. Thus, the survival rate of these trees is low. However, when done properly, replanting provides higher survival rates.\n\nEuropean tradition prefers the open aspect of naturally grown, unsheared trees, while in North America (outside western areas where trees are often wild-harvested on public lands) there is a preference for close-sheared trees with denser foliage, but less space to hang decorations.\n\nIn the past, Christmas trees were often harvested from wild forests, but now almost all are commercially grown on tree farms. Almost all Christmas trees in the United States are grown on Christmas tree farms where they are cut after about ten years of growth and new trees planted. According to the United States Department of Agriculture's agriculture census for 2007, 21,537 farms were producing conifers for the cut Christmas tree market in America, were planted in Christmas trees.\n\nThe life cycle of a Christmas tree from the seed to a tree takes, depending on species and treatment in cultivation, between 8 and 12 years. First, the seed is extracted from cones harvested from older trees. These seeds are then usually grown in nurseries and then sold to Christmas tree farms at an age of 3–4 years. The remaining development of the tree greatly depends on the climate, soil quality, as well as the cultivation and how the trees are tended by the Christmas tree farmer.\n\nThe first artificial Christmas trees were developed in Germany during the 19th century, though earlier examples exist. These \"trees\" were made using goose feathers that were dyed green., as one response by Germans to continued deforestation. Feather Christmas trees ranged widely in size, from a small tree to a large tree sold in department stores during the 1920s. Often, the tree branches were tipped with artificial red berries which acted as candle holders.\n\nOver the years, other styles of artificial Christmas trees have evolved and become popular. In 1930, the U.S.-based Addis Brush Company created the first artificial Christmas tree made from brush bristles. Another type of artificial tree is the aluminum Christmas tree, first manufactured in Chicago in 1958, and later in Manitowoc, Wisconsin, where the majority of the trees were produced. Most modern artificial Christmas trees are made from plastic recycled from used packaging materials, such as polyvinyl chloride (PVC). Approximately 10% of artificial Christmas trees are using virgin suspension PVC resin; despite being plastic most artificial trees are not recyclable or biodegradable.\n\nOther trends have developed in the early 2000s as well. Optical fiber Christmas trees come in two major varieties; one resembles a traditional Christmas tree. One Dallas-based company offers \"holographic mylar\" trees in many hues. Tree-shaped objects made from such materials as cardboard, glass, ceramic or other materials can be found in use as tabletop decorations. Upside-down artificial Christmas trees became popular for a short time and were originally introduced as a marketing gimmick; they allowed consumers to get closer to ornaments for sale in retail stores and opened up floor space for more products.\nArtificial trees became increasingly popular during the late 20th century. Users of artificial Christmas trees assert that they are more convenient, and, because they are reusable, much cheaper than their natural alternative. They are also considered much safer as natural trees can be a significant fire hazard. Between 2001 and 2007 artificial Christmas tree sales in the U.S. jumped from 7.3 million to 17.4 million.\n\nThe debate about the environmental impact of artificial trees is ongoing. Generally, natural tree growers contend that artificial trees are more environmentally harmful than their natural counterparts. However, trade groups such as the American Christmas Tree Association, continue to refute that artificial trees are more harmful to the environment, and maintain that the PVC used in Christmas trees has excellent recyclable properties.\nLive trees are typically grown as a crop and replanted in rotation after cutting, often providing suitable habitat for wildlife. Alternately, live trees can be donated to livestock farmers of such animals like goats who find that such trees uncontaminated by chemical additives are excellent fodder. In some cases management of Christmas tree crops can result in poor habitat since it sometimes involves heavy input of pesticides.\nConcerns have been raised about people cutting down old and rare conifers, such as the \"Keteleeria evelyniana\", for Christmas trees.\nReal or cut trees are used only for a short time, but can be recycled and used as mulch, wildlife habitat, or used to prevent erosion. Real trees are carbon-neutral, they emit no more carbon dioxide by being cut down and disposed of than they absorb while growing. However, emissions can occur from farming activities and transportation. An independent life-cycle assessment study, conducted by a firm of experts in sustainable development, states that a natural tree will generate of greenhouse gases every year (based on purchasing from home) whereas the artificial tree will produce over its lifetime. Some people use living Christmas or potted trees for several seasons, providing a longer life cycle for each tree. Living Christmas trees can be purchased or rented from local market growers. Rentals are picked up after the holidays, while purchased trees can be planted by the owner after use or donated to local tree adoption or urban reforestation services.\n\nMost artificial trees are made of recycled PVC rigid sheets using tin stabilizer in the recent years. In the past, lead was often used as a stabilizer in PVC, but is now banned by Chinese laws.\nThe use of lead stabilizer in Chinese imported trees has been an issue of concern among politicians and scientists over recent years. A 2004 study found that while in general artificial trees pose little health risk from lead contamination, there do exist \"worst-case scenarios\" where major health risks to young children exist. A 2008 United States Environmental Protection Agency report found that as the PVC in artificial Christmas trees aged it began to degrade. The report determined that of the 50 million artificial trees in the United States approximately 20 million were 9 or more years old, the point where dangerous lead contamination levels are reached. A professional study on the life-cycle assessment of both real and artificial Christmas trees revealed that one must use an artificial Christmas tree at least 20 years to leave an environmental footprint as small as the natural Christmas tree.\n\nThe Christmas tree was first used by German Lutherans in the 16th Century, with records indicating that a Christmas tree was placed in the Cathedral of Strassburg in 1539, under the leadership of the Protestant Reformer, Martin Bucer. In the United States, these \"German Lutherans brought the decorated Christmas tree with them; the Moravians put lighted candles on those trees.\" When decorating the Christmas tree, many individuals place a star at the top of the tree symbolizing the Star of Bethlehem, a fact recorded by \"The School Journal\" in 1897. Professor David Albert Jones of Oxford University writes that in the 19th century, it became popular for people to also use an angel to top the Christmas tree in order to symbolize the angels mentioned in the accounts of the Nativity of Jesus.\n\nUnder the Marxist-Leninist doctrine of state atheism in the Soviet Union, after its foundation in 1917, Christmas celebrations—along with other religious holidays—were prohibited as a result of the Soviet anti-religious campaign. The League of Militant Atheists encouraged school pupils to campaign against Christmas traditions, among them being the Christmas tree, as well as other Christian holidays, including Easter; the League established an anti-religious holiday to be the 31st of each month as a replacement. With the Christmas tree being prohibited in accordance with Soviet anti-religious legislation, people supplanted the former Christmas custom with New Year's trees. In 1935 the tree was brought back as New Year tree and became a secular, not a religious holiday.\n\nPope John Paul II introduced the Christmas tree custom to the Vatican in 1982. Although at first disapproved of by some as out of place at the centre of the Roman Catholic Church, the Vatican Christmas Tree has become an integral part of the Vatican Christmas celebrations, and in 2005 Pope Benedict XVI spoke of it as part of the normal Christmas decorations in Catholic homes. In 2004, Pope John Paul called the Christmas tree a symbol of Christ. This very ancient custom, he said, exalts the value of life, as in winter what is evergreen becomes a sign of undying life, and it reminds Christians of the \"tree of life\" of , an image of Christ, the supreme gift of God to humanity. In the previous year he said: \"Beside the crib, the Christmas tree, with its twinkling lights, reminds us that with the birth of Jesus the tree of life has blossomed anew in the desert of humanity. The crib and the tree: precious symbols, which hand down in time the true meaning of Christmas.\" The Catholic Church's official \"Book of Blessings\" has a service for the blessing of the Christmas tree in a home. Likewise the Protestant Episcopal Church in \"The Anglican Family Prayer Book\", which has the imprimatur of The Rt. Rev. Catherine S. Roskam of the Anglican Communion, has long had a ritual titled \"Blessing of a Christmas Tree\", as well as \"Blessing of a Crèche\", for use in the church and the home.\n\nIn 2006, the Seattle–Tacoma International Airport removed all of its Christmas trees in the middle of the night rather than allow a rabbi to put up a menorah near the largest tree display. Officials feared that one display would open the door for other religious displays, and, in 2007, they opted to display a grove of birches in polyethylene terephthalate snow rather than religious symbols or Christmas trees. In 2005, the city of Boston renamed the spruce tree used to decorate the Boston Common a \"Holiday Tree\" rather than a \"Christmas Tree\". The name change drew a poor response from the public and it was reversed after the city was threatened with several lawsuits. At the Bilbao airport 2005 displayed a Christmas tree and a Santa Claus and Christmas elf alongside the Basque Olentzero, as a way of syncretising traditions in Northern Spain.\n\n\"Chrismon trees\" are a variety developed in 1957 by a Lutheran laywoman in Virginia, as a specifically religious version appropriate for a church's Christmas celebrations, although most Christian churches continue to display the traditional Christmas tree in their sanctuaries during Christmastide.\n\n", "id": "7770", "title": "Christmas tree"}
{"url": "https://en.wikipedia.org/wiki?curid=7772", "text": "Carrier battle group\n\nA carrier battle group (CVBG) consists of an aircraft carrier (designated CV) and its large number of escorts, together defining the group. The first naval task forces built around carriers appeared just prior to and during World War II. The Imperial Japanese Navy, IJN, was the first to assemble a large number of carriers into a single task force, known as Kido Butai. This task force was used with devastating effect in the Imperial Japanese Navy's Attack on Pearl Harbor. Kido Butai operated as the IJN's main carrier battle group until four of its carriers were sunk at the Battle of Midway. In contrast, the United States Navy deployed its large carriers in separate formations, with each carrier assigned its own cruiser and destroyer escorts. These single-carrier formations would often be paired or grouped together for certain assignments, most notably the Battle of the Coral Sea and Midway. By 1943, however, large numbers of fleet and light carriers became available, which required larger formations of three or four carriers. These groups eventually formed the Fast Carrier Task Force, which became the primary battle unit of the U.S. Fifth and Third Fleets.\n\nWith the construction of the large super carriers of the Cold War era, the practice of operating each carrier in a single formation was revived. During the Cold War, the main role of the CVBG in case of conflict with the Soviet Union would have been to protect Atlantic supply routes between the United States and Europe, while the role of the Soviet Navy would have been to interrupt these sea lanes, a fundamentally easier task. Because the Soviet Union had no large carriers of its own, a situation of dueling aircraft carriers would have been unlikely. However, a primary mission of the Soviet Navy's attack submarines was to track every allied battle group and, on the outbreak of hostilities, sink the carriers. Understanding this threat, the CVBG expended enormous resources in its own anti-submarine warfare mission.\n\nIn the late 20th and early 21st centuries, most of the uses of CVBGs by the United States as well as that of other nations have been in situations in which their use has been uncontested by other comparable forces.\n\nBritish and French carrier battle groups were involved in the 1956 Suez Crisis.\n\nDuring the Cold War, an important battle scenario was an attack against a CVBG using a large number of antiship missiles.\n\nThe first attempted use of antiship missiles against a carrier battle group was part of Argentina's efforts against British Armed Forces during the Falklands War. This was the last conflict so far in which two belligerents employed aircraft carriers, although Argentina made little use of its sole carrier, originally built in the United Kingdom.\n\nDuring the Indo-Pakistan war of 1971 India used its carrier strike group centered on INS Vikrant to impose a naval blockade upon East Pakistan. Air strikes were carried out initially on shipping in the Chittagong and Cox's Bazar harbours, sinking or incapacitating most ships there. Further strikes were carried out on Cox's Bazar from 60 nautical miles (110 km) offshore. On the evening of 4 December, the air group struck Chittagong Harbour. Later strikes targeted Khulna and the Port of Mongla. Air strikes continued until 10 December 1971.\n\nThe United States Sixth Fleet assembled a force of three carrier battle groups and a battleship during the Lebanese Civil War in 1983. Daily reconnaissance flights were flown over the Bekaa Valley and a strike was flown against targets in the area resulting in loss of an A-6 Intruder and an A-7 Corsair.\n\nCarrier battle groups routinely operated in the Gulf of Sidra inside the \"Line of Death\" proclaimed by Libya resulting in aerial engagements in 1981, 1986 and 1989 between U.S. Navy Tomcats and Libyan Su-22 aircraft, SA-5 surface-to-air missiles and MiG-23 fighters. During the 1986 clashes, three carrier battle groups deployed to the Gulf of Sidra and ultimately two of them conducted strikes against Libya in Operation El Dorado Canyon.\n\nDuring the international military intervention in the 2011 Libyan civil war, the French Navy deployed its aircraft carrier, , off Libya. The \"Charles de Gaulle\" was accompanied by several frigates as , , , the replenishment tanker \"Meuse\" and two nuclear attack submarines.\n\nIn modern United States Navy carrier air operations, the moniker of carrier strike group (CSG) has replaced the traditional term of carrier battle group (CVBG or CARBATGRU). The Navy maintains 11 carrier strike groups, 9 of which are based in the United States and one that is forward deployed in Japan. CSG or CVBG normally consist of 1 Aircraft Carrier, 1 Guided Missile Cruiser (for Air Defense), 2 LAMPS (Light Airborne Multi-Purpose System) Capable Warships (focusing on Anti-Submarine and Surface Warfare), and 1–2 Anti Submarine Destroyers or Frigates. The large number of CSGs used by the United States reflects, in part, a division of roles and missions allotted during the Cold War, in which the United States assumed primary responsibility for blue water operations and for safeguarding supply lines between the United States and Europe, while the NATO allies assumed responsibility for brown and green water operations.\n\nAn Expeditionary Strike Group is composed of an Amphibious Assault Ship (LHA/LHD), a Dock Landing Ship (LSD), an Amphibious transport dock (LPD), a Marine expeditionary unit, AV-8B Harrier II aircraft, CH-53E Super Stallion helicopters and CH-46E Sea Knight helicopters or, more recently, MV-22B tiltrotors. Cruisers, destroyers and attack submarines are deployed with either an Expeditionary Strike Group or a Carrier Strike Group.\n\nDuring the period when the American navy recommissioned all four of its s, it sometimes used a similar formation centered on a battleship, referred to as a battleship battle group (BBBG). It was alternately referred to as a Surface Action Group (SAG).\n\nThe battleship battle group typically consisted of one modernized battleship, one , one or , one , three s and one support ship, such as a fleet oiler.\n\nThe Algerian National Navy operates the amphibious assault ship \"Kalaat Béni Abbès\". \"Kalaat Beni Abbes\" is not capable of operating fix-winged aircraft, but instead acts as the main part of Algeria's amphibious force. When at sea, she is typically escorted by an MEKO 200 frigate for anti-air defense, and two s for anti-submarine defense. A may also be added for anti-submarine patrols. \n\nThe Royal Australian Navy's recently built pair of amphibious assault ship's have allowed Australia naval aviation capability not seen since was decommissioned in 1982. Although they do not currently operate fixed-winged aircraft, they are capable of doing so thanks to their ski-jump's on their flight decks. VTOL fighter jets such as the F-35 Lightning II and the Harrier Jump Jet could be added in the future. Currently however, they are being used as amphibious landing ships by the 2nd Battalion, Royal Australian Regiment for amphibious assaults. When at sea, a typical escort group would likely consist of a for air-defense, two 's for anti-submarine and anti-surface protection, a for anti-submarine patrols, and one of Australia's two replenishment ships, or . \n\nThe forms Brazil's only carrier battle group, together with 4 frigates from Type-22/1 class frigates and Vosper Mk.10 class frigates (known as the ), 1 or 2 s, and one replenishment oiler (), with VF-1 \"Falcão\" Air Wing equipped with 6 to 9 Attack Aircraft AF-1 Skyhawk (A-4Ku), and 3 more Helicopters Squadrons for Attack, ASW and Multi-Mission (between AS332 Super Puma, AS532 Cougar, Super Lynx, Esquilo, EC 725 (16 ordered) SH-3 (being replaced by 6 new SH-60B)) and 2 more Fixed Wings Squadrons for AEW, COD, and REVO (C-1A Trader and S-2 Tracker ordered). The \"São Paulo\" was formerly the , a design used by the French Navy until 1997.\n\nThe Royal Navy did maintain two task forces concurrently (one based on an aircraft carrier and one based on an Amphibious Command Ship). At least one task group would be deployed at any one time. The last of the Royal Navy s were decommissioned in 2014. The Royal Navy also utilises the \"Ocean\"-class LPH as well as the two LPDs as Amphibious Command Ships at the centre of a task group. The two new s are currently under construction and will operate the F-35, replacing the retired \"Invincible\" class in 2019. Plans for the \"Queen Elizabeth\"-class's escorts currently call for two Type 45 destroyers, an , and a . Plans also call for a Type 26 frigate to be added to extra protection.\n\nThe CNS \"Liaoning\" was recently spotted alongside 4 Type 052C/Type 052D destroyers, 2 Type 054A frigates, 1-2 Type 093 Shang nuclear submarine and 1 supply ship. Future carrier battle groups may include the Type 055 destroyer.\n\nThe Egyptian Navy recently acquired two s from France. Although the ships cannot operate fixed-winged aircraft, they have taken up the role providing Egypt with their first naval aviation capability and a modernized amphibious force. When at sea, they are typically escorted by two s or s for anti-air defense. A is also included for anti-submarine defense. Sometimes the FREMM \"Aquitaine\"-class destroyer, ENS \"Tahya Misr\" (FFG-1001) is also included for additional protection. \n\nThe only serving French carrier is the , which also serves as the flagship of the Marine Nationale. The Carrier Battle Group (Groupe Aéronaval, GAN, in French) of the Force d'Action Navale is usually composed, in addition to the aircraft carrier, of:\n\nThis group is commanded by a rear admiral (contre-amiral, in French) on board the aircraft carrier. The commanding officer of the air group (usually a capitaine de frégate—equivalent to commander) is subordinate to the commanding officer of the aircraft carrier, a senior captain. The escort destroyers (called frigates in the French denomination) are commanded by more junior captains.\n\nFrance also operates three s. While incapable of operating fixed-winged aircraft, they function as helicopter carriers and form the backbone of France's amphibious force. These ships are typically escorted by the same escorts the \"Charles De Gaulle\" uses. \n\nThe Indian Navy has been operating carrier battle groups since 1961, with its first carrier battle group formed around the now decommissioned INS \"Vikrant\". As of 2014, the Indian Navy operates two carrier battle groups, one centered on and the other around . \"Viraat\" is an updated \"Centaur\"-class light carrier originally built for the Royal Navy as , which was laid down in 1944 and commissioned in 1959. It was purchased by India in 1986, and is expected to be decommissioned in 2016. India commissioned in 2013 and will follow this with a third carrier, the new INS \"Vikrant\" in 2018. INS \"Vikramaditya\" is the modified , INS \"Vikrant\" will be the first indigenous Indian aircraft carrier. India plans to have three carrier battle groups by 2025, each centered on \"Vikrant\", \"Vikramaditya\" and \"Vishal\", the second, larger and is expected to be nuclear-powered Vikrant-class carrier.\n\nThe Indian Navy's carrier battle group centered on \"Viraat\" consists of two destroyers, usually of the (previously s were used), two or more frigates, usually of the , Godavari or Nilgiri classes, and one support ship.\n\nThe navy's new carrier battle group centered on \"Vikramaditya\" consists of the modern Kolkata class destroyers, Shivalik and Talwar-class frigates, Kamorta-class anti-submarine warfare corvettes and new tankers. INS Chakra II is expected to fill the sub-surface component.\n\nThe CVS–ASW (Aircraft Carrier with Anti-Submarine Warfare) is Italy's first carrier. The battle group based in Taranto called COMFORAL is formed by the carrier \"Giuseppe Garibaldi\", two s, two support ships \"Etna\" and \"Elettra\", and three amphibious/support ships (\"San Giusto\", \"San Marco\" and \"San Giorgio\").\n\nAfter 2010, the Italian battle group will be formed by the new carrier , 5–6 new warships (including destroyers \"Horizon\" and frigates FREMM), one new support ship, some minehunters and new submarines (the COMFORAL will be a reserve group).\n\nThe Japan Maritime Self-Defense Force operates four helicopters destroyers (DDH), which are in fact light aircraft carriers (CVL) in all but name. Two and two make up Japan's naval aviation wing. One of each ship is attached to one of the JMSDF escort flotilla's, and escorts consist of a guided-missile destroyer, usually an , , or , and two other destroyers, usually a , , or . While the \"Huyga\" and \"Izumo\" are not capable of operating fixed-winged aircraft, they could me modified to do so. Japan is also planning to build larger helicopter destroyers in the future that could operate aircraft such as the F-35. \n\nThe \"Kuznetsov\" has been observed sailing together with a (CBGN), (CG), (ASuW), (ASW) and \"Krivak\" I/II FFG (ASW). These escorts, especially the heavily armed \"Kirov\"-class battlecruiser, use advanced sensors and carry a variety of weaponry. During \"Kuznetsov's\" deployment to Syria in November 2016 on her first combat tour, the carrier was escorted by a pair of Udaloy-class destroyers and a Kirov-class battlecruiser en route, while additional Russian Navy warships met her off Syria.\n\nThe \"Admiral Kuznetsov\" is designed specifically to sail alone and carries greater firepower than its U.S. counterparts. This includes 12x SS-N-19 'Shipwreck' (long range, high speed, sea-skimming) SSMs, 24x VLS units loaded with 192 SA-N-9 'Gauntlet' SAMs, and 8x Kashtan CIWS with dual 30 mm guns, and 8x AK-630 CIWS. Compared to the 4x Phalanx CIWS and 4x Sea Sparrow launchers, each with 6 missiles carried by the \"Nimitz\" class, the \"Kuznetsov\" is well armed for both air-defence and offensive operations against hostile shipping.\n\nThe Spanish Navy currently operates the Buque de Proyección Estratégica (Strategic Projection Vessel) , which can be used as a light aircraft carrier. The group includes two escort squadrons: the 41st, with ASW s, and the 31st, with AEGIS AAW frigates.\n\nThe Republic of Korea Navy operates the , the lead ship in her class of amphibious assault ships. She is the flagship of the Republic of Korea Navy and is the country's only flat top ship. While \"Dokdo\" is incapable of operating fixed-wing aircraft, her sister ship, ROKS \"Marado\" (LPH-6112), will be equipped with a ski-jump bow and operated VTOL fighter jets. \"Dokdo\" is the main part of South Korea's rapid response fleet, which consists of: ROKS \"Dokdo\", two cruisers, four s, a , and a Type 214 submarine. When ROKS \"Marado\" is commissioned, South Korea plans to operate her with the same escorts and possible throw in their new s, and increase submarine count to three. \n\n is Thailand's only aircraft carrier, and is based upon the of the Spanish Navy. The Thai Navy no longer operates Harrier fighters. Escorts, if it were to be deployed for non-humanitarian missions would likely consist of the mostly Chinese and US supplied frigates in the Thai fleet. It is not clear that the Thai navy has plans or ability to use the \"Chakri Naruebet\" in any fleet or power projection missions.\n\nSince its origins, the viability of the carrier battle group has been dependent on its ability to remain at sea for extended periods. Specialized ships were developed to provide underway replenishment of fuel (for the carrier and its aircraft), ordnance, and other supplies necessary to sustain operations. Carrier battle groups devote a great deal of planning to efficiently conduct underway replenishment to minimize the time spent conducting replenishment. The carrier can also provide replenishment on a limited basis to its escorts, but typically a replenishment ship such as a fast combat support ship (AOE) or replenishment oiler (AOR) pulls alongside a carrier and conducts simultaneous operations with the carrier on its port side and one of the escorts on its starboard side. The advent of the helicopter provides the ability to speed replenishment by lifting supplies at the same time that fuelling hoses and lines are delivering other goods.\n\nThere is debate in naval warfare circles as to the viability of carrier battle groups in 21st century naval warfare. Proponents of the CVBG argue that it provides unmatched firepower and force projection capabilities. Opponents argue that CVBGs are increasingly vulnerable to arsenal ships and cruise missiles, especially those with supersonic or even hypersonic flight and the ability to perform radical trajectory changes to avoid anti-missile systems. It is also noted that CVBGs were designed for Cold War scenarios, and are less useful in establishing control of areas close to shore. It is argued however that such missiles and arsenal ships pose no serious threat as they would be eliminated due to increasing improvement in ship defenses such as Cooperative Engagement Capability (CEC), DEW technology and missile technology.\n\nHowever, carriers have been called upon to be first responders even when conventional land based aircraft were employed. During Desert Shield, the U.S. Navy sortied additional carriers to augment the on station assets eventually maintaining six carriers for Desert Storm. Although the U.S. Air Force sent fighters such as the F-16 to theater in Desert Shield, they had to carry bombs with them as no stores were in place for sustained operations whereas the carriers arrived on scene with full magazines and had support ships to allow them to conduct strikes indefinitely.\n\nThe Global War on Terror has shown the flexibility and responsiveness of the carrier on multiple occasions when land based air was not feasible or able to respond in a timely fashion. After the September 11 terrorist attacks on the U.S., carriers immediately headed to the Arabian Sea to support Operation Enduring Freedom and took up station, building to a force of three carriers. Their steaming location was closer to the targets in Afghanistan than any land based assets and thereby more responsive. The was adapted to be a support base for special operations helicopters. Carriers were used again in Operation Iraqi Freedom and even provided aircraft to be based ashore on occasion and have done so periodically when special capabilities are needed. This precedent was established during World War II in the Battle of Guadalcanal.\n\nRegardless of the debate over viability, the United States has made a major investment in the development of a new carrier class—the s (formerly designated CVN-X, or the X Carrier)—to replace the existing s. The new \"Ford\"-class carriers are designed to be modular and are easily adaptable as technology and equipment needed on board changes.\n\n\n", "id": "7772", "title": "Carrier battle group"}
{"url": "https://en.wikipedia.org/wiki?curid=7773", "text": "Boeing Vertol CH-46 Sea Knight\n\nThe Boeing Vertol CH-46 Sea Knight is a medium-lift tandem rotor transport helicopter powered by twin turboshaft aircraft engines. It was used by the United States Marine Corps (USMC) to provide all-weather, day-or-night assault transport of combat troops, supplies and equipment until it was replaced by the MV-22 Osprey. Additional tasks included combat support, search and rescue (SAR), support for forward refueling and rearming points, CASEVAC and Tactical Recovery of Aircraft and Personnel (TRAP).\n\nThe Sea Knight was also the United States Navy's standard medium-lift utility helicopter until it was phased out in favor of the MH-60S Knighthawk in the early 2000s. Canada also operated the Sea Knight, designated as CH-113, and operated them in the SAR role until 2004. Other export customers include Japan, Sweden, and Saudi Arabia. The commercial version is the BV 107-II, commonly referred to simply as the \"Vertol\".\n\nPiasecki Helicopter was a pioneering developer of tandem-rotor helicopters, with the most famous previous helicopter being the H-21 \"Flying Banana\". Piasecki Helicopter became Vertol in 1955 and work began on a new tandem rotor helicopter designated the \"Vertol Model 107\" or V-107 in 1956. The V-107 prototype had two Lycoming T53 turboshaft engines, producing 877 shp (640 kW) each. The first flight of the V-107 took place on 22 April 1958. The V-107 was then put through a flight demonstration tour in the United States and overseas. In June 1958, the U.S. Army awarded a contract to Vertol for ten production aircraft designated \"YHC-1A\".\n\nThe order was later decreased to three, so that the Army could divert funds for the V-114, also a turbine powered tandem, but larger than the V-107. The Army's three YHC-1As were powered by GE-T-58 engines. The YHC-1As first flew in August 1959, and were followed by an improved commercial/export model, the 107-II. During 1960, the U.S. Marine Corps evolved a requirement for a medium-lift, twin-turbine troop/cargo assault helicopter to replace the piston-engined types then in use. That same year Boeing acquired Vertol and renamed the group Boeing Vertol. Following a competition, Boeing Vertol was selected to build its model 107M as the HRB-1, early in 1961. In 1962 the U.S. Air Force ordered 12 XCH-46B Sea Knights with the \"XH-49A\" designation, but later cancelled the order due to a delivery delay and opted for the Sikorsky S-61R instead.\n\nFollowing the Sea Knight's first flight in August 1962, the designation was changed to CH-46A. In November 1964, introduction of the Marines' CH-46A and the Navy's UH-46As began. The UH-46A variant was modified for the vertical replenishment role. The CH-46A was equipped with a pair of T58-GE8-8B turboshaft engines rated at 1,250 shp (930 kW) each and could carry 17 passengers or 4,000 pounds (1,815 kg) of cargo.\n\nProduction of the improved CH-46D followed with deliveries beginning in 1966. Its improvements included modified rotor blades and more powerful T58-GE-10 turboshaft engines rated at each. The increased power allowed the D-model to carry 25 troops or of cargo. The CH-46D was introduced to the Vietnam theater in late 1967, supplementing the U.S. Marine Corps' existing unreliable and problematic CH-46A fleet. Along with the USMC's CH-46Ds, the U.S. Navy received a small number of UH-46Ds for ship resupply. Also, approximately 33 CH-46As were upgraded to CH-46Ds.\n\nThe Marines also received CH-46Fs from 1968 to 1971. The F-model retained the D-model's T58-GE-10 engines but revised the avionics and included other modifications. The CH-46F was the final production model. The Sea Knight has undergone upgrades and modifications. Most of the U.S. Marine Corps' Sea Knights were upgraded to CH-46E standard. The CH-46E features fiberglass rotor blades, airframe reinforcement, and further uprated T58-GE-16 engines producing each. Some CH-46Es have been given double fuel capacity. The Dynamic Component Upgrade (DCU), incorporated starting in the mid-1990s, provides for increased capability through strengthened drive systems and rotor controls.\n\nThe commercial variant, the \"BV 107-II\", was first ordered by New York Airways in 1960. They took delivery of their first three aircraft, configured for 25 passengers, in July 1962. In 1965, Boeing Vertol sold the manufacturing rights of the 107 to Kawasaki Heavy Industries. Under this arrangement, all Model 107 civilian and military aircraft built in Japan are known as \"KV 107\". On 15 December 2006, Columbia Helicopters, Inc acquired the type certificate for the Boeing Vertol 107-II, and was in the process of acquiring a Production Certificate from the FAA. Plans for actual production of the aircraft were not announced.\n\nThe CH-46 has tandem counter-rotating rotors powered by two GE T58 turboshaft engines. The engines are mounted on each side of the rear rotor pedestal with a driveshaft to the forward rotor. The engines are coupled so either could power both rotors in an emergency. The rotors feature three blades and can be folded for on-ship operations. The CH-46 has fixed tricycle landing gear, with twin wheels on all three landing gear legs. The gear configuration causes a nose-up stance to facilitate cargo loading and unloading. The main gear are fitted in rear sponsons that also contain fuel tanks with a total capacity of 350 US gallons (1,438 L).\n\nThe CH-46 has a cargo bay with a rear loading ramp that could be removed or left open in flight for extended cargo or for parachute drops. An internal winch is mounted in the forward cabin and can be used to pull external cargo on pallets into the aircraft via the ramp and rollers. A belly sling hook (cargo hook) which is usually rated at . could be attached for carrying external cargo. Although the hook is rated at ., the limited power produced by the engines precludes the lifting of such weight. It usually has a crew of three, but can accommodate a larger crew depending on mission specifics. For example, a Search and Rescue variant will usually carry a crew of five (Pilot, Co-Pilot, Crew Chief, Swimmer, and Medic) to facilitate all aspects of such a mission. A pintle-mounted 0.50 in (12.7 mm) Browning machine gun is mounted on each side of the helicopter for self-defense. Service in southeast Asia resulted in the addition of armor with the guns.\n\nKnown colloquially as the \"Phrog\", the Sea Knight was used in all U.S. Marine operational environments between its introduction during the Vietnam War and its frontline retirement in 2014. The type's longevity and reputation for reliability led to mantras such as \"phrogs phorever\" and \"never trust a helicopter under 30\". CH-46s transported personnel, evacuated wounded, supplied forward arming and refueling points (FARP), performed vertical replenishment, search and rescue, recovered downed aircraft and crews and other tasks.\n\nDuring the Vietnam War, the CH-46 was one of the prime US troop transport helicopters in the theatre, slotting between the smaller Bell UH-1 Iroquois and larger Sikorsky CH-53 Sea Stallion. During the 1972 Easter Offensive, Sea Knights saw heavy use to convey US and South Vietnamese ground forces to and around the front lines. CH-46 operations were plagued by major technical problems; the engines, being prone to foreign object damage (FOD) from debris being ingested when hovering close to the ground and subsequently suffering a compressor stall, had a lifespan as low as 85 flight hours; on 21 July 1966, all CH-46s were grounded until more efficient filters had been fitted. By the end of US military operations in Vietnam, over a hundred Sea Knights had been lost to enemy fire.\n\nIn February 1968 the Marine Corps Development and Education Command obtained several CH-46s to perform herbicide dissemination tests using HIDAL (Helicopter, Insecticide Dispersal Apparatus, Liquid) systems; testing indicated the need for redesign and further study. Tandem-rotor helicopters were often used to transport nuclear warheads; the CH-46A was evaluated to deploy Naval Special Forces with the Special Atomic Demolition Munition (SADM). Nuclear Weapon Accident Exercise 1983 (NUWAX-83), simulating the crash of a Navy CH-46E carrying 3 nuclear warheads, was conducted at the Nevada Test Site on behalf of several federal agencies; the exercise, which used real radiological agents, was depicted in a Defense Nuclear Agency-produced documentary.\n\nU.S. Marine CH-46s were used to deploy the 8th Marine Regiment into Grenada during Operation Urgent Fury, evacuated the surviving crew-member of a downed AH-1 Cobra, and then carried infantry from the 75th Ranger Regiment to secure an evacuate U.S. students at the Grand Anse campus of St. George's University, though one crashed after colliding with a palm tree.\nCH-46E Sea Knights were also used by the U.S. Marine Corps during the 2003 invasion of Iraq. In one incident on 1 April 2003, Marine CH-46Es and CH-53Es carried U.S. Army Rangers and Special Operations troops on an extraction mission for captured Army Private Jessica Lynch from an Iraqi hospital. During the subsequent occupation of Iraq and counter-insurgency operations, the CH-46E was heavily used in the CASEVAC role, being required to maintain 24/7 availability regardless of conditions. According to authors Williamson Murray and Robert H Scales, the Sea Knight displayed serious reliability and maintenance problems during its deployment to Iraq, as well as \"limited lift capabilities\". Following the loss of numerous US helicopters in the Iraqi theatre, the Marines opted to equip their CH-46s with more advanced anti-missile countermeasures.\n\nThe U.S. Navy retired the type on 24 September 2004, replacing it with the MH-60S Seahawk; the Marine Corps maintained its fleet as the MV-22 Osprey was fielded. In March 2006 Marine Medium Helicopter Squadron 263 (HMM-263) was deactivated and redesignated VMM-263 to serve as the first MV-22 squadron. The replacement process continued through the other medium helicopter squadrons into 2014. On 5 October 2014, the Sea Knight performed its final service flight with the U.S. Marine Corps at Marine Corps Air Station Miramar. HMM-364 was the last squadron to use it outside the United States, landing it aboard the USS America (LHA-6) on her maiden transit. On 9 April 2015, the CH-46 was retired by the Marine Medium Helicopter Training Squadron 164, the last Marine Corps squadron to transition to the MV-22. The USMC retired the CH-46 on 1 August 2015 in a ceremony at the Udvar-Hazy Center near Washington DC.\n\nThe Royal Canadian Air Force procured six CH-113 Labrador helicopters for the SAR role and the Canadian Army acquired 12 of the similar \"CH-113A Voyageur\" for the medium-lift transport role. The RCAF Labradors were delivered first with the first one entering service on 11 October 1963. When the larger CH-147 Chinook was procured by the Canadian Forces in the mid-1970s, the Voyageur fleet was converted to Labrador specifications to undertake SAR missions. The refurbished Voyageurs were re-designated as CH-113A Labradors, thus a total of 15 Labradors were ultimately in service.\nThe Labrador was fitted with a watertight hull for marine landings, a 5,000 kilogram cargo hook and an external rescue hoist mounted over the right front door. It featured a 1,110 kilometer flying range, emergency medical equipment and an 18-person passenger capacity. By the 1990s, heavy use and hostile weather conditions had taken their toll on the Labrador fleet, resulting in increasing maintenance costs and the need for prompt replacement. In 1981, a mid-life upgrade of the fleet was carried out by Boeing Canada in Arnprior, Ontario. Known as the SAR-CUP (Search and Rescue Capability Upgrade Program), the refit scheme included new instrumentation, a nose-mounted weather radar, a tail-mounted auxiliary power unit, a new high-speed rescue hoist mounted over the side door and front-mounted searchlights. A total of six CH-113s and five CH-113As were upgraded with the last delivered in 1984.\n\nIn 1992, it was announced that the Labradors were to be replaced by 15 new helicopters, a variant of the AgustaWestland EH101, designated \"CH-149 Chimo\". The order was subsequently cancelled by the Jean Chrétien Liberal government in 1993, resulting in cancellation penalties, as well as extending the service life of the Labrador fleet. However, in 1998, a CH-113 from CFB Greenwood crashed on Quebec's Gaspé Peninsula while returning from a SAR mission, resulting in the deaths of all crewmembers on board. The crash placed pressure upon the government to procure a replacement, thus an order was placed with the manufacturers of the EH101 for 15 aircraft to perform the search-and-rescue mission, designated \"CH-149 Cormorant\". CH-149 deliveries began in 2003, allowing the last CH-113 to be retired in 2004. In October 2005 Columbia Helicopters of Aurora, Oregon purchased eight of the retired CH-113 Labradors to add to their fleet of 15 Vertol 107-II helicopters.\n\nIn 1963, Sweden procured ten UH-46B from the US as a transport and anti-submarine helicopter for the Swedish armed forces, designated Hkp 4A. In 1973, a further eight Kawasaki-built KV-107, which were accordingly designated Hkp 4B, were acquired to replace the older Piasecki H-21. During the Cold War, the fleet's primary missions were anti-submarine warfare and troop transportation, they were also frequently employed in the search and rescue role. In the 1980s, the Hkp 4A was phased out, having been replaced by the Eurocopter AS332 Super Puma; the later Kawasaki-built Sea Knights continued in operational service until 2011, they were replaced by the UH-60 Black Hawk & NH90.\n\nThe civilian version, designated as the BV 107-II \"Vertol\", was developed prior to the military CH-46. It was operated commercially by New York Airways, Pan American World Airways and later on by Columbia Helicopters. Among the diversity of tasks was pulling a hover barge, and constructing transmission towers for overhead power lines.\n\nIn December 2006, Columbia Helicopters purchased the type certificate of the Model 107 from Boeing, with the aim of eventually producing new-build aircraft themselves.\n\n\n\n\n\n\n\n\n\n\nSource:\n\n\n\n\n\n\n\n\n\n\n\n", "id": "7773", "title": "Boeing Vertol CH-46 Sea Knight"}
{"url": "https://en.wikipedia.org/wiki?curid=7774", "text": "Chief of Naval Operations\n\nThe Chief of Naval Operations (CNO) is a statutory office () held by a four-star admiral in the United States Navy, and is the most senior naval officer assigned to serve in the Department of the Navy. The office is a military adviser and deputy to the Secretary of the Navy. In a separate capacity as a member of the Joint Chiefs of Staff () the CNO is a military adviser to the National Security Council, the Homeland Security Council, the Secretary of Defense, and the President. The current Chief of Naval Operations is Admiral John M. Richardson.\n\nThe Chief of Naval Operations is an administrative position based in the Pentagon, and while the CNO does not have operational command authority over Naval forces as the title implies (that is nowadays within the purview of the Combatant Commanders who report to the Secretary of Defense), the CNO does exercise supervision of Navy organizations as the designee of the Secretary of the Navy.\n\nThe CNO reports directly to the Secretary of the Navy for the command, utilization of resources, and operating efficiency of the operating forces of the Navy and of the Navy shore activities assigned by the Secretary. Under the authority of the Secretary of the Navy, the CNO also designates naval personnel and naval resources to the commanders of Unified Combatant Commands. The CNO also performs all other functions prescribed under and those assigned by the secretary or delegates those duties and responsibilities to other officers in his administration. The CNO is typically the highest-ranking officer on active duty in the Navy unless the Chairman and/or the Vice Chairman of the Joint Chiefs of Staff are naval officers. Like the other joint chiefs, the CNO is an administrative position and has no operational command authority over United States naval forces.\n\nThe Chief of Naval Operations presides over the Office of the Chief of Naval Operations (OpNav), which is one of three headquarters staffs in Department of the Navy (the others being the Office of the Secretary of the Navy and Headquarters Marine Corps.)\n\nPolicy documents are issued in the form of OPNAV Instructions.\n\nThe Chief of Naval Operations is nominated by the President for appointment and must be confirmed by the Senate. A requirement for being Chief of Naval Operations is having significant experience in joint duty assignments, which includes at least one full tour of duty in a joint duty assignment as a flag officer. However, the president may waive those requirements if he determines that appointing the officer is necessary for the national interest. By statute, the CNO is appointed as a four-star admiral.\n\nNumber One Observatory Circle, located on the northeast grounds of the United States Naval Observatory in Washington, DC, was built in 1893 for its superintendent. The Chief of Naval Operations liked the house so much that in 1923 he took over the house as his own official residence. It remained the residence of the CNO until 1974, when Congress authorized its transformation to an official residence for the Vice President. The Chief of Naval Operations currently resides in Quarters A in the Washington Naval Yard.\n\nThe position of CNO replaced the position of Aide for Naval Operations, which was a position established by regulation rather than statutory law.\n\n\n", "id": "7774", "title": "Chief of Naval Operations"}
{"url": "https://en.wikipedia.org/wiki?curid=7775", "text": "Clara Petacci\n\nClara Petacci, known as Claretta Petacci (; 28 February 1912 – 28 April 1945) was a mistress of the Italian dictator Benito Mussolini, and was executed with him by partisans.\n\nPetacci had a long-standing relationship with Mussolini while he was married to Rachele Mussolini. Mussolini was twenty-eight years Petacci's senior.\n\nPart of their correspondence is still the subject of a dispute with the National Archives, based on privacy.\n\nOn 27 April 1945, Mussolini and Petacci were captured by partisans while traveling with a convoy of Italian Social Republic members.\n\nOn 28 April, she and Mussolini were taken to Mezzegra and shot. On the following day, 29 April, Mussolini's and Petacci's bodies were taken to the Piazzale Loreto in Milan and hung upside down in front of a petrol station. The bodies were photographed as a crowd vented their rage upon them.\n\n\n\n", "id": "7775", "title": "Clara Petacci"}
{"url": "https://en.wikipedia.org/wiki?curid=7780", "text": "Costa Smeralda\n\nThe Costa Smeralda (, , ) is a coastal area and tourist destination in northern Sardinia, Italy, with a length of some 20 km, although the term originally designated only a small stretch in the commune of Arzachena. With white sand beaches, golf clubs, private jet and helicopter services, and exclusive hotels, the area has drawn celebrities, business leaders, and other affluent visitors.\n\nCosta Smeralda is the most expensive location in Europe. House prices reach up to 300,000 euros ($392,200) per square meter.\n\nThe main towns and villages in the area, built according to a detailed urban plan, are Porto Cervo, Liscia di Vacca, Capriccioli, and Romazzino. Archaeological sites include the Li Muri Giants' graves.\n\nEach September the Sardinia Cup sailing regatta is held off the coast. Polo matches are held between April and October at Gershan near Arzachena. Other attractions include a film festival in Tavolara and a vintage car rally.\n\nDevelopment of the area started in 1961, and was financed by a consortium of companies led by Prince Karim Aga Khan. Spiaggia del Principe, one of the beaches along the Costa Smeralda, was named after this Ishmaelite prince. Architects involved in the project included Michele Busiri Vici, Jacques Couëlle, Savin Couëlle, and Vietti.\n\n", "id": "7780", "title": "Costa Smeralda"}
{"url": "https://en.wikipedia.org/wiki?curid=7781", "text": "Chianti\n\nA Chianti wine is any wine produced in the Chianti region, in central Tuscany, Italy. It was historically associated with a squat bottle enclosed in a straw basket, called a \"fiasco\" (\"flask\"; \"pl. fiaschi\"); however, the \"fiasco\" is only used by a few makers of the wine now; most Chianti is now bottled in more standard shaped wine bottles. Baron Bettino Ricasoli (later Prime Minister of the Kingdom of Italy) created the Chianti recipe of 70% Sangiovese, 15% Canaiolo and 15% Malvasia bianca in the middle of the 19th century.\n\nThe first definition of a wine-area called \"Chianti\" was made in 1716. It described the area near the villages of Gaiole, Castellina and Radda; the so-called \"Lega del Chianti\" and later \"Provincia del Chianti\" (Chianti province). In 1932 the Chianti area was completely re-drawn and divided in seven sub-areas: Classico, Colli Aretini, Colli Fiorentini, Colline Pisane, Colli Senesi, Montalbano and Rùfina. Most of the villages that in 1932 were suddenly included in the new Chianti Classico area added \"in Chianti\" to their name-such as Greve in Chianti which amended its name in 1972. Wines labelled \"Chianti Classico\" come from the biggest sub-area of Chianti, that includes the original Chianti heartland. Only Chianti from this sub-zone may boast the black rooster seal (known in Italian as a \"gallo nero\") on the neck of the bottle, which indicates that the producer of the wine is a member of the Chianti Classico Consortium, the local association of producers. Other variants, with the exception of Rufina from the north-east side of Florence and Montalbano in the south of Pistoia, originate in the respective named provinces: Siena for the Colli Senesi, Florence for the Colli Fiorentini, Arezzo for the Colli Aretini and Pisa for the Colline Pisane. In 1996 part of the Colli Fiorentini sub-area was renamed \"Montespertoli\".\n\nDuring the 1970s producers started to reduce the quantity of white grapes in Chianti. In 1995 it became legal to produce a Chianti with 100% Sangiovese. For a wine to retain the name of Chianti, it must be produced with at least 80% Sangiovese grapes. Aged Chianti (38 months instead of 4–7), may be labelled as Riserva. Chianti that meets more stringent requirements (lower yield, higher alcohol content and dry extract) may be labelled as Chianti Superiore, although Chianti from the \"Classico\" sub-area is not allowed in any event to be labelled as \"Superiore\".\n\nThe earliest documentation of a \"Chianti wine\" dates back to the 13th century when viticulture was known to flourish in the \"\"Chianti Mountains\"\" around Florence. The merchants in the nearby townships of Castellina, Gaiole and Radda formed the \"Lega del Chianti\" (League of Chianti) to produce and promote the local wine. In 1398, records note that the earliest incarnation of Chianti was as a white wine. In 1716 Cosimo III de' Medici, Grand Duke of Tuscany issued an edict legislating that the three villages of the \"Lega del Chianti\" (Castellina in Chianti, Gaiole in Chianti and Radda in Chianti) as well as the village of Greve and a of hillside north of Greve near as the only officially recognised producers of Chianti. This delineation existed until July 1932, when the Italian government expanded the Chianti zone to include the outlying areas of Barberino Val d'Elsa, Chiocchio, , San Casciano in Val di Pesa and . Subsequent expansions in 1967 would eventually bring the Chianti zone to cover a very large area all over central Tuscany.\n\nBy the 18th century, Chianti was widely recognised as a red wine, but the exact composition and grape varieties used to make Chianti at this point is unknown. Ampelographers find clues about which grape varieties were popular at the time in the writings of Italian writer Cosimo Villifranchi who noted that Canaiolo was widely planted variety in the area along with Sangiovese, Mammolo and Marzemino. It was not until the work of the Italian statesman Bettino Ricasoli that the modern \"Chianti recipe\" as a Sangiovese-based wine would take shape. Prior to Ricasoli, Canaiolo was emerging as the dominant variety in the Chianti blend with Sangiovese and Malvasia playing supporting roles. In the mid-19th century, Ricasoli developed a recipe for Chianti that was based primarily on Sangiovese. His recipe called for 70% Sangiovese, 15% Canaiolo, 10% Malvasia (later amended to include Trebbiano) and 5% other local red varieties. In 1967, the Denominazione di origine controllata (DOC) regulation set by the Italian government firmly established the \"Ricasoli formula\" of a Sangiovese-based blend with 10–30% Malvasia and Trebbiano.\n\nThe late 19th century saw a period of economic and political upheaval. First came oidium and then the phylloxera epidemic would take its toll on the vineyards of Chianti just as they had ravaged vineyards across the rest of Europe. The chaos and poverty following the \"Risorgimento\" heralded the beginning of the Italian diaspora that would take Italian vineyard workers and winemakers abroad as immigrants to new lands. Those that stayed behind and replanted choose high-yielding varieties like Trebbiano and Sangiovese clones such as the \"Sangiovese di Romagna\" from the nearby Romagna region. Following the Second World War, the general trend in the world wine market for cheap, easy-drinking wine saw a brief boom for the region. With over-cropping and an emphasis on quantity over quality, the reputation of Chianti among consumers eventually plummeted. By the 1950s, Trebbiano (which is known for its neutral flavours) made up to 30% of many mass-market Chiantis. By the late 20th century, Chianti was often associated with basic Chianti sold in a squat bottle enclosed in a straw basket, called a \"fiasco\". However, during the same period, a group of ambitious producers began working outside the boundaries of DOC regulations to make what they believed would be a higher quality style of Chianti. These wines eventually became known as the \"Super Tuscans\".\n\nMany of the producers behind the Super Tuscan movement were originally Chianti producers who were rebelling against what they felt were antiquated DOC regulations. Some of these producers wanted to make Chiantis that were 100% varietal Sangiovese. Others wanted the flexibility to experiment with blending French grape varieties such as Cabernet Sauvignon and Merlot or to not be required to blend in any white grape varieties. The late 20th century saw a flurry of creativity and innovation in the Chianti zones as producers experimented with new grape varieties and introduced modern wine-making techniques such as the use of new oak barrels. The prices and wine ratings of some Super Tuscans would regularly eclipse those of DOC sanctioned Chiantis. The success of the Super Tuscans encouraged government officials to reconsider the DOC regulations in order to bring some of these wines back into the fold labelled as Chianti.\n\nThe Chianti region covers a vast area of Tuscany and includes within its boundaries several overlapping \"Denominazione di origine controllata\" (DOC) and \"Denominazione di Origine Controllata e Garantita\" (DOCG) regions. Other well known Sangiovese-based Tuscan wines such as Brunello di Montalcino and Vino Nobile di Montepulciano could be bottled and labelled under the most basic designation of \"Chianti\" if their producers chose to do so. Within the collective Chianti region more than 8 million cases of wines classified as DOC level or above are produced each year. Today, most Chianti falls under two major designations of Chianti DOCG, which includes basic level Chianti, as well as that from seven designated sub-zones, and Chianti Classico DOCG. Together, these two Chianti zones produce the largest volume of DOC/G wines in Italy.\n\nThe Chianti DOCG covers all the Chianti wine and includes a large stretch of land encompassing the western reaches of the province of Pisa near the coast of the Tyrrhenian Sea, the Florentine hills in the province of Florence to the north, to the province of Arezzo in the east and the Siena hills to the south. Within this regions are vineyards that overlap the DOCG regions of Brunello di Montalcino, Vino Nobile di Montepulciano and Vernaccia di San Gimignano. Any Sangiovese-based wine made according to the Chianti guidelines from these vineyards can be labelled and marked under the basic Chianti DOCG should the producer wish to use the designation.\n\nWithin the Chianti DOCG there are eight defined sub-zones that are permitted to affix their name to the wine label. Wines that are labelled as simply Chianti are made either from a blend from these sub-zones or include grapes from peripheral areas not within the boundaries of a sub-zone. The sub-zones are (clockwise from the north): the Colli Fiorentini which is located south of the city of Florence; Chianti Rufina in the northeastern part of the zone located around the commune of Rufina; Classico in the centre of Chianti, across the provinces of Florence and Siena; Colli Aretini in the Arezzo province to the east; Colli Senesi south of Chianti Classico in the Siena hills, which is the largest of the sub-zones and includes the Brunello di Montalcino and Vino Nobile di Montepulciano areas; Colline Pisane, the westernmost sub-zone in the province of Pisa; Montespertoli located within the Colli Fiorentini around the commune of Montespertoli; Montalbano in the north-west part of the zone which includes the Carmignano DOCG. , there were under production in Montalbano, in the Colli Fiorentini, in Montespertoli, in Rufina, in the Colli Senesi, in Colline Pisane, in the Colli Aretini, and an additional in the peripheral areas that do not fall within one of the sub-zone classifications. Wines produced from these vineyards are labelled simply \"Chianti\".\n\nThe original area dictated by the edict of Cosimo III de' Medici would eventually be considered the heart of the modern \"Chianti Classico\" subregion. , there were of vineyards in the Chianti Classico subregion. The Chianti Classico subregion covers an area of approximate between the city of Florence to the north and Siena to the south. The four communes of Castellina in Chianti, Gaiole in Chianti, Greve in Chianti and Radda in Chianti are located entirely within the boundaries of the Classico area with parts of Barberino Val d'Elsa, San Casciano in Val di Pesa and Tavarnelle Val di Pesa in the province of Florence as well as Castelnuovo Berardenga and Poggibonsi in the province of Siena included within the permitted boundaries of Chianti Classico. The soil and geography of this subregion can be quite varied, with altitudes ranging from , and rolling hills producing differing macroclimates. There are two main soil types in the area: a weathered sandstone known as \"alberese\" and a bluish-gray chalky marlstone known as \"galestro\". The soil in the north is richer and more fertile with more \"galestro\", with the soil gradually becoming harder and stonier with more \"albarese\" in the south. In the north, the Arno River can have an influence on the climate, keeping the temperatures slightly cooler, an influence that diminishes further south in the warmer Classico territory towards Castelnuovo Berardenga.\n\nChianti Classico are premium Chianti wines that tend to be medium-bodied with firm tannins and medium-high to high acidity. Floral, cherry and light nutty notes are characteristic aromas with the wines expressing more notes on the mid-palate and finish than at the front of the mouth. As with Bordeaux, the different zones of Chianti Classico have unique characteristics that can be exemplified and perceived in some wines from those areas. According to Master of Wine Mary Ewing-Mulligan, Chianti Classico wines from the Castellina area tend to have a very delicate aroma and flavour, Castelnuovo Berardegna wines tend to be the most ripe and richest tasting, wines from Gaiole tend to have been characterised by their structure and firm tannins while wines from the Greve area tend to have very concentrated flavours.\n\nThe production of Chianti Classico is realised under the supervision of , a union of producers in the Chianti Classico subregion. The Consorzio was founded with the aim of promoting the wines of the subregion, improving quality and preventing wine fraud. Since the 1980s, the foundation has sponsored extensive research into the viticultural and winemaking practice of the Chianti Classico area, particularly in the area of clonal research. In the last three decades, more than 50% of the vineyards in the Chianti Classico subregion have been replanted with improved Sangiovese clones and modern vineyard techniques as part of the Consorzio Chianti Classico's project \"Chianti 2000\".\n\nIn 2014 a new category of Chianti Classico was introduced: Chianti Classico Gran Selezione. Gran Selezione is made exclusively from a winery’s own grapes grown according to stricter regulations compared to regular Chianti Classico. Gran Selezione is granted to a Chianti Classico after it passes a suitability test conducted\nby authorised laboratories and after it is approved by a special tasting committee.\n\nOutside of the Chianti Classico area, the wines of the Chianti sub-zone of Rufina are among the most widely recognised and exported from the Chianti region. Located in the Arno valley near the town of Pontassieve, the Rufina region includes much area in the Pomino region, an area that has a long history of wine production. The area is noted for the cool climate of its elevated vineyards located up to . The vineyard soils of the area are predominantly marl and chalk. The Florentine merchant families of the Antinori and Frescobaldi own the majority of the vineyards in Rufina. Chianti from the Rufina area is characterised by its multi-layered complexity and elegance.\n\nThe Colli Fiorentini subregion has seen an influx of activity and new vineyard development in recent years as wealthy Florentine business people move to the country to plant vineyards and open wineries. Many foreign \"flying winemakers\" have had a hand in this development, bringing global viticulture and winemaking techniques to the Colli Fiorentini. Located in the hills between the Chianti Classico area and Arno valley, the wines of the Colli Fiorentini vary widely depending on producer, but tend to have a simple structure with strong character and fruit notes. The Montespertoli sub-zone was part of the Colli Fiorentini sub-zone until 2002 when it became its own tiny enclave.\n\nThe Montalbano subregion is located in the shadow of the Carmignano DOCG, with much of the best Sangiovese going to that wine. A similar situation exists in the Colli Senesi which includes the well known DOCG region of Vino Nobile di Montepulciano. Both regions rarely appear on wine labels that are exported out of Tuscany. The Colli Pisane area produces typical Chiantis with the lightest body and color. The Colli Aretini is a relatively new and emerging area that has seen an influx of investment and new winemaking in recent years.\n\nSince 1996 the blend for Chianti and Chianti Classico has been 75–100% Sangiovese, up to 10% Canaiolo and up to 20% of any other approved red grape variety such as Cabernet Sauvignon, Merlot or Syrah. Since 2006, the use of white grape varieties such as Malvasia and Trebbiano have been prohibited in Chianti Classico. Chianti Classico must have a minimum alcohol level of at least 12% with a minimum of 7 months aging in oak, while Chianti Classico's labeled \"riserva\" must be aged at least 24 months at the winery, with a minimum alcohol level of at least 12.5%. The harvest yields for Chianti Classico are restricted to no more than . For basic Chianti, the minimum alcohol level is 11.5% with yields restricted to .\n\nThe aging for basic Chianti DOCG is much less stringent with most varieties allowed to be released to the market on 1 March following the vintage year. The sub-zones of Colli Fiorentini, Montespertoli and Rufina must be aged for a further three months and not released until 1 June. All Chianti Classicos must be held back until 1 October in the year following the vintage.\n\nJancis Robinson notes that Chianti is sometimes called the \"Bordeaux of Italy\". The flexibility in the blending recipe for Chianti accounts for some of the variability in styles among Chiantis. Lighter bodied styles will generally have a higher proportion of white grape varieties blended in, while Chiantis that have only red grape varieties will be fuller and richer. While only 15% of Cabernet Sauvignon is permitted in the blend, the nature of the grape variety can have a dominant personality in the Chianti blend and be a strong influence in the wine.\n\nChianti Classico wines are characterised in their youth by their predominantly floral and cinnamon spicy bouquet. As the wine ages, aromas of tobacco and leather can emerge. Chiantis tend to have medium-high acidity and medium tannins. The acidity in the wines make them very flexible with food and wine pairings, particularly with Italian cuisines that feature red sauce, as well as with beef, lamb and game. Basic level Chianti is often characterised by its juicy fruit notes of cherry, plum and raspberry and can range from simple quaffing wines to those approaching the level of Chianti Classico. Wine expert Tom Stevenson notes that these basic everyday-drinking Chiantis are at their peak drinking qualities often between three and five years after vintage with premium examples having the potential to age for four to eight years. Well-made examples of Chianti Classico often have the potential to age and improve in the bottle for six to twenty years.\n\nChianti Superiore is an Italian DOCG wine produced in the provinces of Arezzo, Florence, Pisa, Pistoia, Prato and Siena, in Tuscany. Superiore is a specification for wines produced with a stricter rule of production than other Chianti wines. Chianti Superiore has been authorised since 1996. Chianti Superiore wines can be produced only from grapes cultivated in the Chianti wine areas except from those vineyards that are registered in the Chianti Classico sub-zone. Vineyards registered in Chianti sub-zones other than Classico can produce Chianti Superiore wines but must omit the sub-zone name on the label. Aging is calculated from 1 January after the picking. Chianti Superiore cannot be sold to the consumer before nine months of aging, of which three must be in the bottle. Therefore, it cannot be bottled before the June after picking or sold to consumers before the next September.\n\nChianti Classico was promoted as the “Official wine of the 2013 UCI Road World Championships” and sold bottles dedicated to the Championships with special labels.\n\n\n", "id": "7781", "title": "Chianti"}
{"url": "https://en.wikipedia.org/wiki?curid=7783", "text": "Coriolis force\n\nIn physics, the Coriolis force is an inertial force (also called a \"fictitious force\") that acts on objects that are in motion relative to a rotating reference frame. In a reference frame with clockwise rotation, the force acts to the left of the motion of the object. In one with anticlockwise rotation, the force acts to the right. Though recognized previously by others, the mathematical expression for the Coriolis force appeared in an 1835 paper by French scientist Gaspard-Gustave de Coriolis, in connection with the theory of water wheels. Early in the 20th century, the term \"Coriolis force\" began to be used in connection with meteorology. Deflection of an object due to the Coriolis force is called the 'Coriolis effect'.\n\nNewton's laws of motion describe the motion of an object in an inertial (non-accelerating) frame of reference. When Newton's laws are transformed to a rotating frame of reference, the Coriolis force and centrifugal force appear. Both forces are proportional to the mass of the object. The Coriolis force is proportional to the rotation rate and the centrifugal force is proportional to its square. The Coriolis force acts in a direction perpendicular to the rotation axis and to the velocity of the body in the rotating frame and is proportional to the object's speed in the rotating frame. The centrifugal force acts outwards in the radial direction and is proportional to the distance of the body from the axis of the rotating frame. These additional forces are termed inertial forces, fictitious forces or \"pseudo forces\". They allow the application of Newton's laws to a rotating system. They are correction factors that do not exist in a non-accelerating or inertial reference frame.\n\nA commonly encountered rotating reference frame is the Earth. The Coriolis effect is caused by the rotation of the Earth and the inertia of the mass experiencing the effect. Because the Earth completes only one rotation per day, the Coriolis force is quite small, and its effects generally become noticeable only for motions occurring over large distances and long periods of time, such as large-scale movement of air in the atmosphere or water in the ocean. Such motions are constrained by the surface of the Earth, so only the horizontal component of the Coriolis force is generally important. This force causes moving objects on the surface of the Earth to be deflected to the right (with respect to the direction of travel) in the Northern Hemisphere and to the left in the Southern Hemisphere. The horizontal deflection effect is greater near the poles and smallest at the equator, since the rate of change in the diameter of the circles of latitude when travelling north or south, increases the closer the object is to the poles. Rather than flowing directly from areas of high pressure to low pressure, as they would in a non-rotating system, winds and currents tend to flow to the right of this direction north of the equator and to the left of this direction south of it. This effect is responsible for the rotation of large cyclones (see Coriolis effects in meteorology). To explain this intuitively, consider how an object that moves northwards from the equator has a tendency to maintain its greater speed at the equator (rotating around towards the right as you look at the sphere of the Earth), where the \"horizontal diameter\" is larger, and therefore tends to move towards the right as it passed northwards where the \"horizontal diameter\" of the Earth (the rings of latitude) is smaller, and the linear speed of local objects on the Earth's surface at that latitude is slower.\n\nItalian scientist Giovanni Battista Riccioli and his assistant Francesco Maria Grimaldi described the effect in connection with artillery in the 1651 \"Almagestum Novum\", writing that rotation of the Earth should cause a cannonball fired to the north to deflect to the east. In 1674 described in his \"Cursus seu Mundus Mathematicus\" how the rotation of the earth should cause a deflection in the trajectories of both falling bodies and projectiles aimed toward one of the planet's poles. Riccioli, Grimaldi, and Dechales all described the effect as part of an argument against the heliocentric system of Copernicus. In other words, they argued that the Earth's rotation should create the effect, and so failure to detect the effect was evidence for an immobile Earth. The effect was described in the tidal equations of Pierre-Simon Laplace in 1778.\n\nGaspard-Gustave Coriolis published a paper in 1835 on the energy yield of machines with rotating parts, such as waterwheels. That paper considered the supplementary forces that are detected in a rotating frame of reference. Coriolis divided these supplementary forces into two categories. The second category contained a force that arises from the cross product of the angular velocity of a coordinate system and the projection of a particle's velocity into a plane perpendicular to the system's axis of rotation. Coriolis referred to this force as the \"compound centrifugal force\" due to its analogies with the centrifugal force already considered in category one. The effect was known in the early 20th century as the \"acceleration of Coriolis\", and by 1920 as \"Coriolis force\".\n\nIn 1856, William Ferrel proposed the existence of a circulation cell in the mid-latitudes with air being deflected by the Coriolis force to create the prevailing westerly winds.\n\nThe understanding of the kinematics of how exactly the rotation of the Earth affects airflow was partial at first. Late in the 19th century, the full extent of the large scale interaction of pressure gradient force and deflecting force that in the end causes air masses to move 'along' isobars was understood.\n\nIn non-vector terms: at a given rate of rotation of the observer, the magnitude of the Coriolis acceleration of the object is proportional to the velocity of the object and also to the sine of the angle between the direction of movement of the object and the axis of rotation.\n\nThe vector formula for the magnitude and direction of the Coriolis acceleration is derived through vector analysis and is\n\nwhere (here and below) formula_2 is the acceleration of the particle in the rotating system, formula_3 is the velocity of the particle with respect to the rotating system, and Ω is the angular velocity vector having magnitude equal to the rotation rate ω, with direction along the axis of rotation of the rotating reference frame, and the formula_4\nsymbol represents the cross product operator.\n\nThe equation may be multiplied by the mass of the relevant object to produce the Coriolis force:\n\nSee \"fictitious force\" for a derivation.\n\nThe \"Coriolis effect\" is the behavior added by the \"Coriolis acceleration\". The formula implies that the Coriolis acceleration is perpendicular both to the direction of the velocity of the moving mass and to the frame's rotation axis. So in particular:\n\nThe Coriolis force exists only when one uses a rotating reference frame. In the rotating frame it behaves exactly like a real force (that is to say, it causes acceleration and has real effects). However, the Coriolis force is a consequence of inertia, and is not attributable to an identifiable originating body, as is the case for electromagnetic or nuclear forces, for example. From an analytical viewpoint, to use Newton's second law in a rotating system, the Coriolis force is mathematically necessary, but it disappears in a non-accelerating, inertial frame of reference. For example, consider two children on opposite sides of a spinning roundabout (Merry-go-round ), who are throwing a ball to each other. From the children's point of view, this ball's path is curved sideways by the Coriolis force. Suppose the roundabout spins anticlockwise when viewed from above. From the thrower's perspective, the deflection is to the right. From the non-thrower's perspective, deflection is to the left. \"For a mathematical formulation see Mathematical derivation of fictitious forces.\" In meteorology, a rotating frame (the Earth) with its Coriolis force provides a more natural framework for explanation of air movements than a non-rotating, inertial frame without Coriolis forces. In long-range gunnery, sight corrections for the Earth's rotation are based upon the Coriolis force. These examples are described in more detail below.\n\nThe acceleration entering the Coriolis force arises from two sources of change in velocity that result from rotation: the first is the change of the velocity of an object in time. The same velocity (in an inertial frame of reference where the normal laws of physics apply) is seen as different velocities at different times in a rotating frame of reference. The apparent acceleration is proportional to the angular velocity of the reference frame (the rate at which the coordinate axes change direction), and to the component of velocity of the object in a plane perpendicular to the axis of rotation. This gives a term formula_6. The minus sign arises from the traditional definition of the cross product (right hand rule), and from the sign convention for angular velocity vectors.\n\nThe second is the change of velocity in space. Different positions in a rotating frame of reference have different velocities (as seen from an inertial frame of reference). For an object to move in a straight line, it must accelerate so that its velocity changes from point to point by the same amount as the velocities of the frame of reference. The force is proportional to the angular velocity (which determines the relative speed of two different points in the rotating frame of reference), and to the component of the velocity of the object in a plane perpendicular to the axis of rotation (which determines how quickly it moves between those points). This also gives a term formula_6.\n\nThe time, space and velocity scales are important in determining the importance of the Coriolis force. Whether rotation is important in a system can be determined by its Rossby number, which is the ratio of the velocity, \"U\", of a system to the product of the Coriolis parameter,formula_8, and the length scale, \"L\", of the motion:\nThe Rossby number is the ratio of inertial to Coriolis forces. A small Rossby number indicates a system is strongly affected by Coriolis forces, and a large Rossby number idicates a system in which inertial forces dominate. For example, in tornadoes, the Rossby number is large, in low-pressure systems it is low, and in oceanic systems it is around 1. As a result, in tornadoes the Coriolis force is negligible, and balance is between pressure and centrifugal forces. In low-pressure systems, centrifugal force is negligible and balance is between Coriolis and pressure forces. In the oceans all three forces are comparable.\n\nAn atmospheric system moving at \"U\" =  occupying a spatial distance of \"L\" = , has a Rossby number of approximately 0.1.\n\nA baseball pitcher may throw the ball at U =  for a distance of L = . The Rossby number in this case would be 32,000.\n\nBaseball players don't care about which hemisphere they're playing in. However, an unguided missile obeys exactly the same physics as a baseball, but can travel far enough and be in the air long enough to experience the effect of Coriolis force. Long-range shells in the Northern Hemisphere landed close to, but to the right of, where they were aimed until this was noted. (Those fired in the Southern Hemisphere landed to the left.) In fact, it was this effect that first got the attention of Coriolis himself.\n\nThe animation at the top of this article is a classic illustration of Coriolis force. Another visualization of the Coriolis and centrifugal forces is this animation clip.\n\nGiven the radius \"R\" of the turntable in that animation, the rate of angular rotation ω, and the speed of the cannonball (assumed constant) \"v\", the correct angle θ to aim so as to hit the target at the edge of the turntable can be calculated.\n\nThe inertial frame of reference provides one way to handle the question: calculate the time to interception, which is \"t\" = \"R\" / \"v\" . Then, the turntable revolves an angle ω \"t\" in this time. If the cannon is pointed an angle θ = ω \"t\" = ω \"R\" / \"v\", then the cannonball arrives at the periphery at position number 3 at the same time as the target.\n\nNo discussion of Coriolis force can arrive at this solution as simply, so the reason to treat this problem is to demonstrate Coriolis formalism in an easily visualized situation.\n\nThe trajectory in the inertial frame (denoted \"A\") is a straight line radial path at angle θ. The position of the cannonball in (\"x\", \"y\") coordinates at time \"t\" is:\nIn the turntable frame (denoted \"B\"), the \"x\"- \"y\" axes rotate at angular rate ω, so the trajectory becomes:\nand three examples of this result are plotted in the figure.\n\nTo determine the components of acceleration, a general expression is used from the article fictitious force:\nin which the term in Ω × v is the Coriolis acceleration and the term in Ω × ( Ω × r) is the centrifugal acceleration. The results are (let α = θ − ω\"t\"):\n\nProducing a centrifugal acceleration:\nAlso:\nproducing a Coriolis acceleration:\nThese accelerations are shown in the diagrams for a particular example.\n\nIt is seen that the Coriolis acceleration not only cancels the centrifugal acceleration, but together they provide a net \"centripetal\", radially inward component of acceleration (that is, directed toward the center of rotation):\n\nand an additional component of acceleration perpendicular to r \"(t)\":\nThe \"centripetal\" component of acceleration resembles that for circular motion at radius \"r\", while the perpendicular component is velocity dependent, increasing with the radial velocity \"v\" and directed to the right of the velocity. The situation could be described as a circular motion combined with an \"apparent Coriolis acceleration\" of 2ω\"v\". However, this is a rough labelling: a careful designation of the true centripetal force refers to a local reference frame that employs the directions normal and tangential to the path, not coordinates referred to the axis of rotation.\n\nThese results also can be obtained directly by two time differentiations of r \"(t)\". Agreement of the two approaches demonstrates that one could start from the general expression for fictitious acceleration above and derive the trajectories shown here. However, working from the acceleration to the trajectory is more complicated than the reverse procedure used here, which, of course, is made possible in this example by knowing the answer in advance.\n\nAs a result of this analysis an important point appears: \"all\" the fictitious accelerations must be included to obtain the correct trajectory. In particular, besides the Coriolis acceleration, the centrifugal force plays an essential role. It is easy to get the impression from verbal discussions of the cannonball problem, which focus on displaying the Coriolis effect particularly, that the Coriolis force is the only factor that must be considered, but that is not so. A turntable for which the Coriolis force \"is\" the only factor is the parabolic turntable. A somewhat more complex situation is the idealized example of flight routes over long distances, where the centrifugal force of the path and aeronautical lift are countered by gravitational attraction.\nThe figure illustrates a ball tossed from 12:00 o'clock toward the center of a counter-clockwise rotating carousel. On the left, the ball is seen by a stationary observer above the carousel, and the ball travels in a straight line to the center, while the ball-thrower rotates counter-clockwise with the carousel. On the right the ball is seen by an observer rotating with the carousel, so the ball-thrower appears to stay at 12:00 o'clock. The figure shows how the trajectory of the ball as seen by the rotating observer can be constructed.\n\nOn the left, two arrows locate the ball relative to the ball-thrower. One of these arrows is from the thrower to the center of the carousel (providing the ball-thrower's line of sight), and the other points from the center of the carousel to the ball.(This arrow gets shorter as the ball approaches the center.) A shifted version of the two arrows is shown dotted.\n\nOn the right is shown this same dotted pair of arrows, but now the pair are rigidly rotated so the arrow corresponding to the line of sight of the ball-thrower toward the center of the carousel is aligned with 12:00 o'clock. The other arrow of the pair locates the ball relative to the center of the carousel, providing the position of the ball as seen by the rotating observer. By following this procedure for several positions, the trajectory in the rotating frame of reference is established as shown by the curved path in the right-hand panel.\n\nThe ball travels in the air, and there is no net force upon it. To the stationary observer the ball follows a straight-line path, so there is no problem squaring this trajectory with zero net force. However, the rotating observer sees a \"curved\" path. Kinematics insists that a force (pushing to the \"right\" of the instantaneous direction of travel for a \"counter-clockwise\" rotation) must be present to cause this curvature, so the rotating observer is forced to invoke a combination of centrifugal and Coriolis forces to provide the net force required to cause the curved trajectory.\nThe figure describes a more complex situation where the tossed ball on a turntable bounces off the edge of the carousel and then returns to the tosser, who catches the ball. The effect of Coriolis force on its trajectory is shown again as seen by two observers: an observer (referred to as the \"camera\") that rotates with the carousel, and an inertial observer. The figure shows a bird's-eye view based upon the same ball speed on forward and return paths. Within each circle, plotted dots show the same time points. In the left panel, from the camera's viewpoint at the center of rotation, the tosser (smiley face) and the rail both are at fixed locations, and the ball makes a very considerable arc on its travel toward the rail, and takes a more direct route on the way back. From the ball tosser's viewpoint, the ball seems to return more quickly than it went (because the tosser is rotating toward the ball on the return flight).\n\nOn the carousel, instead of tossing the ball straight at a rail to bounce back, the tosser must throw the ball toward the right of the target and the ball then seems to the camera to bear continuously to the left of its direction of travel to hit the rail (\"left\" because the carousel is turning \"clockwise\"). The ball appears to bear to the left from direction of travel on both inward and return trajectories. The curved path demands this observer to recognize a leftward net force on the ball. (This force is \"fictitious\" because it disappears for a stationary observer, as is discussed shortly.) For some angles of launch, a path has portions where the trajectory is approximately radial, and Coriolis force is primarily responsible for the apparent deflection of the ball (centrifugal force is radial from the center of rotation, and causes little deflection on these segments). When a path curves away from radial, however, centrifugal force contributes significantly to deflection.\n\nThe ball's path through the air is straight when viewed by observers standing on the ground (right panel). In the right panel (stationary observer), the ball tosser (smiley face) is at 12 o'clock and the rail the ball bounces from is at position one (1). From the inertial viewer's standpoint, positions one (1), two (2), three (3) are occupied in sequence. At position 2 the ball strikes the rail, and at position 3 the ball returns to the tosser. Straight-line paths are followed because the ball is in free flight, so this observer requires that no net force is applied.\n\nAn important case where the Coriolis force is observed is the rotating Earth. Unless otherwise stated, directions of forces and motion apply to the Northern Hemisphere.\n\nAs the Earth turns around its axis, everything attached to it turns with it (imperceptibly to our senses). An object that is moving without being dragged along with this rotation travels in a straight motion over the turning Earth. From our rotating perspective on the planet, its direction of motion changes as it moves, bending in the opposite direction to our actual motion. When viewed from a stationary point in space directly above the north pole, any land feature in the Northern Hemisphere turns anticlockwise—and, fixing our gaze on that location, any other location in that hemisphere rotates around it the same way. The traced ground path of a freely moving body travelling from one point to another therefore bends the opposite way, clockwise, which is conventionally labeled as \"right,\" where it will be if the direction of motion is considered \"ahead,\" and \"down\" is defined naturally.\n\nConsider a location with latitude \"φ\" on a sphere that is rotating around the north-south axis. A local coordinate system is set up with the \"x\" axis horizontally due east, the \"y\" axis horizontally due north and the \"z\" axis vertically upwards. The rotation vector, velocity of movement and Coriolis acceleration expressed in this local coordinate system (listing components in the order east (\"e\"), north (\"n\") and upward (\"u\")) are:\n\nWhen considering atmospheric or oceanic dynamics, the vertical velocity is small, and the vertical component of the Coriolis acceleration is small compared to gravity. For such cases, only the horizontal (east and north) components matter. The restriction of the above to the horizontal plane is (setting \"v\" = 0):\n\nwhere formula_8 is called the Coriolis parameter.\n\nBy setting \"v\" = 0, it can be seen immediately that (for positive φ and ω) a movement due east results in an acceleration due south. Similarly, setting \"v\" = 0, it is seen that a movement due north results in an acceleration due east. In general, observed horizontally, looking along the direction of the movement causing the acceleration, the acceleration always is turned 90° to the right and of the same size regardless of the horizontal orientation.\n\nAs a different case, consider equatorial motion setting φ = 0°. In this case, Ω is parallel to the north or \"n\"-axis, and:\n\nAccordingly, an eastward motion (that is, in the same direction as the rotation of the sphere) provides an upward acceleration known as the Eötvös effect, and an upward motion produces an acceleration due west.\n\nPerhaps the most important impact of the Coriolis effect is in the large-scale dynamics of the oceans and the atmosphere. In meteorology and oceanography, it is convenient to postulate a rotating frame of reference wherein the Earth is stationary. In accommodation of that provisional postulation, the centrifugal and Coriolis forces are introduced. Their relative importance is determined by the applicable Rossby numbers. Tornadoes have high Rossby numbers, so, while tornado-associated centrifugal forces are quite substantial, Coriolis forces associated with tornadoes are for practical purposes negligible.\n\nBecause ocean currents are driven by the movement of wind over the water's surface, the Coriolis force also affects the movement of ocean currents and cyclones as well. Many of the ocean's largest currents circulate around warm, high-pressure areas called gyres. Though the circulation is not as significant as that in the air, the deflection caused by the Coriolis effect is what creates the spiraling pattern in these gyres. The spiraling wind pattern helps the hurricane form. The stronger the force from the Coriolis effect, the faster the wind spins and picks up additional energy, increasing the strength of the hurricane.\n\nAir within high-pressure systems rotates in a direction such that the Coriolis force is directed radially inwards, and nearly balanced by the outwardly radial pressure gradient. As a result, air travels clockwise around high pressure in the Northern Hemisphere and anticlockwise in the Southern Hemisphere. Air around low-pressure rotates in the opposite direction, so that the Coriolis force is directed radially outward and nearly balances an inwardly radial pressure gradient.\n\nIf a low-pressure area forms in the atmosphere, air tends to flow in towards it, but is deflected perpendicular to its velocity by the Coriolis force. A system of equilibrium can then establish itself creating circular movement, or a cyclonic flow. Because the Rossby number is low, the force balance is largely between the pressure gradient force acting towards the low-pressure area and the Coriolis force acting away from the center of the low pressure.\n\nInstead of flowing down the gradient, large scale motions in the atmosphere and ocean tend to occur perpendicular to the pressure gradient. This is known as geostrophic flow. On a non-rotating planet, fluid would flow along the straightest possible line, quickly eliminating pressure gradients. Note that the geostrophic balance is thus very different from the case of \"inertial motions\" (see below), which explains why mid-latitude cyclones are larger by an order of magnitude than inertial circle flow would be.\n\nThis pattern of deflection, and the direction of movement, is called Buys-Ballot's law. In the atmosphere, the pattern of flow is called a cyclone. In the Northern Hemisphere the direction of movement around a low-pressure area is anticlockwise. In the Southern Hemisphere, the direction of movement is clockwise because the rotational dynamics is a mirror image there. At high altitudes, outward-spreading air rotates in the opposite direction. Cyclones rarely form along the equator due to the weak Coriolis effect present in this region.\n\nAn air or water mass moving with speed formula_44 subject only to the Coriolis force travels in a circular trajectory called an 'inertial circle'. Since the force is directed at right angles to the motion of the particle, it moves with a constant speed around a circle whose radius formula_45 is given by:\n\nwhere formula_47 is the Coriolis parameter formula_48, introduced above (where formula_49 is the latitude). The time taken for the mass to complete a full circle is therefore formula_50. The Coriolis parameter typically has a mid-latitude value of about 10 s; hence for a typical atmospheric speed of the radius is , with a period of about 17 hours. For an ocean current with a typical speed of , the radius of an inertial circle is . These inertial circles are clockwise in the Northern Hemisphere (where trajectories are bent to the right) and anticlockwise in the Southern Hemisphere.\n\nIf the rotating system is a parabolic turntable, then formula_47 is constant and the trajectories are exact circles. On a rotating planet, formula_47 varies with latitude and the paths of particles do not form exact circles. Since the parameter formula_47 varies as the sine of the latitude, the radius of the oscillations associated with a given speed are smallest at the poles (latitude = ±90°), and increase toward the equator.\n\nThe Coriolis effect strongly affects the large-scale oceanic and atmospheric circulation, leading to the formation of robust features like jet streams and western boundary currents. Such features are in geostrophic balance, meaning that the Coriolis and \"pressure gradient\" forces balance each other. Coriolis acceleration is also responsible for the propagation of many types of waves in the ocean and atmosphere, including Rossby waves and Kelvin waves. It is also instrumental in the so-called Ekman dynamics in the ocean, and in the establishment of the large-scale ocean flow pattern called the Sverdrup balance.\n\nThe practical impact of the \"Coriolis effect\" is mostly caused by the horizontal acceleration component produced by horizontal motion.\n\nThere are other components of the Coriolis effect. Westward-travelling objects are deflected downwards (feel heavier), while Eastward-travelling objects are deflected upwards (feel lighter). This is known as the Eötvös effect. This aspect of the Coriolis effect is greatest near the equator. The force produced by this effect is similar to the horizontal component, but the much larger vertical forces due to gravity and pressure mean that it is generally unimportant dynamically.\n\nIn addition, objects travelling upwards (\"i.e.\", out) or downwards (\"i.e.\", in) are deflected to the west or east respectively. This effect is also the greatest near the equator. Since vertical movement is usually of limited extent and duration, the size of the effect is smaller and requires precise instruments to detect. However, in the case of large changes of momentum, such as a spacecraft being launched into orbit, the effect becomes significant. The fastest and most fuel-efficient path to orbit is a launch from the equator that curves to a directly eastward heading.\n\nImagine a train that travels through a frictionless railway line along the equator. Assume that, when in motion, it moves at the necessary speed to complete a trip around the world in one day (465 m/s). The Coriolis effect can be considered in three cases: when the train travels west, when it is at rest, and when it travels east. In each case, the Coriolis effect can be calculated from the rotating frame of reference on Earth first, and then checked against a fixed inertial frame. The image below illustrates the three cases in an inertial frame as observed from a fixed point above Earth along its axis of rotation:\n\nThis also explains why high speed projectiles that travel west are deflected down, and those that travel east are deflected up. This vertical component of the Coriolis effect is called the Eötvös effect.\n\nThe above example can be used to explain why the Eötvös effect starts diminishing when an object is travelling westward as its tangential speed increases above Earth's rotation (465 m/s). If the westward train in the above example increases speed, part of the force of gravity that pushes against the track accounts for the centripetal force needed to keep it in circular motion on the inertial frame. Once the train doubles its westward speed at 930 m/s that centripetal force becomes equal to the force the train experiences when it stops. From the inertial frame, in both cases it rotates at the same speed but in the opposite directions. Thus, the force is the same cancelling completely the Eötvös effect. Any object that moves westward at a speed above 930 m/s experiences an upward force instead. In the figure, the Eötvös effect is illustrated for a 10 kilogram object on the train at different speeds. The parabolic shape is because the centripetal force is proportional to the square of the tangential speed. On the inertial frame, the bottom of the parabola is centered at the origin. The offset is because this argument uses the Earth's rotating frame of reference. The graph shows that the Eötvös effect is not symmetrical, and that the resulting downward force experienced by an object that travels west at high velocity is less than the resulting upward force when it travels east at the same speed.\n\nContrary to popular misconception, water rotation in home bathrooms under \"normal\" circumstances is not related to the Coriolis effect or to the rotation of the Earth, and no consistent difference in rotation direction between toilet drainage in the Northern and Southern Hemispheres can be observed. The formation of a vortex over the plug hole may be explained by the conservation of angular momentum: The radius of rotation decreases as water approaches the plug hole, so the rate of rotation increases, for the same reason that an ice skater's rate of spin increases as they pull their arms in. Any rotation around the plug hole that is initially present accelerates as water moves inward.\n\nOf course, the Coriolis force does still impact the direction of the flow of water, but only minutely. Only if the water is so still that the effective rotation rate of the Earth is faster than that of the water relative to its container, and if externally applied torques (such as might be caused by flow over an uneven bottom surface) are small enough, the Coriolis effect may indeed determine the direction of the vortex. Without such careful preparation, the Coriolis effect is likely to be much smaller than various other influences on drain direction such as any residual rotation of the water and the geometry of the container. Despite this, the idea that toilets and bathtubs drain differently in the Northern and Southern Hemispheres has been popularized by several television programs and films, including \"Escape Plan\", \"Wedding Crashers\", \"The Simpsons\" episode \"Bart vs. Australia\", \"Pole to Pole\", and \"The X-Files\" episode \"Die Hand Die Verletzt\". Several science broadcasts and publications, including at least one college-level physics textbook, have also stated this.\n\nIn 1908, the Austrian physicist Ottokar Tumlirz described careful and effective experiments that demonstrated the effect of the rotation of the Earth on the outflow of water through a central aperture. The subject was later popularized in a famous 1962 article in the journal \"Nature\", which described an experiment in which all other forces to the system were removed by filling a tank with of water and allowing it to settle for 24 hours (to allow any movement due to filling the tank to die away), in a room where the temperature had stabilized. The drain plug was then very slowly removed, and tiny pieces of floating wood were used to observe rotation. During the first 12 to 15 minutes, no rotation was observed. Then, a vortex appeared and consistently began to rotate in an anticlockwise direction (the experiment was performed in Boston, Massachusetts, in the Northern Hemisphere). This was repeated and the results averaged to make sure the effect was real. The report noted that the vortex rotated, \"about 30,000 times faster than the effective rotation of the Earth in 42° North (the experiment's location)\". This shows that the small initial rotation due to the Earth is amplified by gravitational draining and conservation of angular momentum to become a rapid vortex and may be observed under carefully controlled laboratory conditions.\n\nThe Coriolis force is important in external ballistics for calculating the trajectories of very long-range artillery shells. The most famous historical example was the Paris gun, used by the Germans during World War I to bombard Paris from a range of about . The Coriolis force minutely changes the trajectory of a bullet, affecting accuracy at extremely long distances. It is adjusted for by accurate long-distance shooters, such as snipers. Unlike large-scale motions of air in the atmosphere or water in the ocean, a bullet's path is not constrained to be horizontal, and the vertical component of the Coriolis force is often more important than the horizontal component: westward shots hit low, and eastward shots hit high.\n\nThe effects of the Coriolis force on ballistic trajectories should not be confused with the curvature of the paths of missiles, satellites, and similar objects when the paths are plotted on two-dimensional (flat) maps, such as the Mercator projection. The projections of the three-dimensional curved surface of the Earth to a two-dimensional surface (the map) necessarily results in distorted features. The apparent curvature of the path is a consequence of the sphericity of the Earth and would occur even in a non-rotating frame.\n\nTo demonstrate the Coriolis effect, a parabolic turntable can be used.\nOn a flat turntable, the inertia of a co-rotating object forces it off the edge. However, if the turntable surface has the correct paraboloid (parabolic bowl) shape (see the figure) and rotates at the corresponding rate, the force components shown in the figure make the component of gravity tangential to the bowl surface exactly equal to the centripetal force necessary to keep the object rotating at its velocity and radius of curvature (assuming no friction). (See .) This carefully contoured surface allows the Coriolis force to be displayed in isolation.\n\nDiscs cut from cylinders of dry ice can be used as pucks, moving around almost frictionlessly over the surface of the parabolic turntable, allowing effects of Coriolis on dynamic phenomena to show themselves. To get a view of the motions as seen from the reference frame rotating with the turntable, a video camera is attached to the turntable so as to co-rotate with the turntable, with results as shown in the figure. In the left panel of the figure, which is the viewpoint of a stationary observer, the gravitational force in the inertial frame pulling the object toward the center (bottom ) of the dish is proportional to the distance of the object from the center. A centripetal force of this form causes the elliptical motion. In the right panel, which shows the viewpoint of the rotating frame, the inward gravitational force in the rotating frame (the same force as in the inertial frame) is balanced by the outward centrifugal force (present only in the rotating frame). With these two forces balanced, in the rotating frame the only unbalanced force is Coriolis (also present only in the rotating frame), and the motion is an \"inertial circle\". Analysis and observation of circular motion in the rotating frame is a simplification compared to analysis or observation of elliptical motion in the inertial frame.\n\nBecause this reference frame rotates several times a minute rather than only once a day like the Earth, the Coriolis acceleration produced is many times larger and so easier to observe on small time and spatial scales than is the Coriolis acceleration caused by the rotation of the Earth.\n\nIn a manner of speaking, the Earth is analogous to such a turntable. The rotation has caused the planet to settle on a spheroid shape, such that the normal force, the gravitational force and the centrifugal force exactly balance each other on a \"horizontal\" surface. (See equatorial bulge.)\n\nThe Coriolis effect caused by the rotation of the Earth can be seen indirectly through the motion of a Foucault pendulum.\n\nA practical application of the Coriolis effect is the mass flow meter, an instrument that measures the mass flow rate and density of a fluid flowing through a tube. The operating principle involves inducing a vibration of the tube through which the fluid passes. The vibration, though not completely circular, provides the rotating reference frame that gives rise to the Coriolis effect. While specific methods vary according to the design of the flow meter, sensors monitor and analyze changes in frequency, phase shift, and amplitude of the vibrating flow tubes. The changes observed represent the mass flow rate and density of the fluid.\n\nIn polyatomic molecules, the molecule motion can be described by a rigid body rotation and internal vibration of atoms about their equilibrium position. As a result of the vibrations of the atoms, the atoms are in motion relative to the rotating coordinate system of the molecule. Coriolis effects are therefore present, and make the atoms move in a direction perpendicular to the original oscillations. This leads to a mixing in molecular spectra between the rotational and vibrational levels, from which Coriolis coupling constants can be determined.\n\nWhen an external torque is applied to a spinning gyroscope along an axis that is at right angles to the spin axis, the rim velocity that is associated with the spin becomes radially directed in relation to the external torque axis. This causes a Coriolis force to act on the rim in such a way as to tilt the gyroscope at right angles to the direction that the external torque would have tilted it. This tendency has the effect of keeping spinning bodies stably aligned in space.\n\nFlies (Diptera) and some moths (Lepidoptera) exploit the Coriolis effect in flight with specialized appendages and organs that relay information about the angular velocity of their bodies. Coriolis forces resulting from linear motion of these appendages are detected within the rotating frame of reference of the insects' bodies. In the case of flies, their specialized appendages are dumbbell shaped organs located just behind their wings called halteres. The halteres oscillate in a plane at the same beat frequency as the main wings so that any body rotation results in lateral deviation of the halteres from their plane of motion. In moths, their antennae are responsible for the sensing of Coriolis forces in the similar manner as with the halteres in flies. In both flies and moths, a collection of mechanosensors at the base of the appendage are sensitive to deviations at the beat frequency, correlating to rotation in the pitch and roll planes, and at twice the beat frequency, correlating to rotation in the yaw plane.\n\nIn astronomy, Lagrangian points are five positions in the orbital plane of two large orbiting bodies where a small object affected only by gravity can maintain a stable position relative to the two large bodies. The first three Lagrangian points (L, L, L) lie along the line connecting the two large bodies, while the last two points (L and L) each form an equilateral triangle with the two large bodies. The L and L points, although they correspond to maxima of the effective potential in the coordinate frame that rotates with the two large bodies, are stable due to the Coriolis effect. The stability can result in orbits around just L or L, known as Tadpole orbits, where trojans can be found. It can also result in orbits that encircle L, L, and L, known as horseshoe orbits. \n\n", "id": "7783", "title": "Coriolis force"}
{"url": "https://en.wikipedia.org/wiki?curid=7786", "text": "Challenger Deep\n\nThe Challenger Deep is the deepest known point in the Earth's seabed hydrosphere, with a depth of by direct measurement from submersibles, and slightly more by sonar bathymetry. It is in the Pacific Ocean, at the southern end of the Mariana Trench near the Mariana Islands group. The Challenger Deep is a relatively small slot-shaped depression in the bottom of a considerably larger crescent-shaped oceanic trench, which itself is an unusually deep feature in the ocean floor. Its bottom is about long and wide, with gently sloping sides. The closest land to the Challenger Deep is Fais Island (one of the outer islands of Yap), southwest, and Guam, to the northeast. It is located in the ocean territory of the Federated States of Micronesia, from its border with ocean territory associated with Guam.\n\nThe depression is named after the British Royal Navy survey ship HMS \"Challenger\", whose expedition of 1872–1876 made the first recordings of its depth. According to the August 2011 version of the GEBCO Gazetteer of Undersea Feature Names, the location and depth of the Challenger Deep are and ±.\n\nJune 2009 sonar mapping of the Challenger Deep by the Simrad EM120 (sonar multibeam bathymetry system for 300–11,000 m deep water mapping) aboard the RV \"Kilo Moana\" indicated a depth of . The sonar system uses phase and amplitude bottom detection, with a precision of 0.2% to 0.5% of water depth; this is an error of about at this depth.<ref name=\"Daily Reports for R/V KILO MOANA\"></ref><ref name=\"Scientic Equipment aboard the R/V KILO MOANA\"></ref> Further soundings made by the US Center for Coastal & Ocean Mapping in October 2010 are in agreement with this figure, preliminarily placing the deepest part of the Challenger Deep at , with an estimated vertical uncertainty of ±. A 2014 study concludes that with the best of 2010 multibeam echosounder technologies a depth uncertainty of ± (95% confidence level) on 9 degrees of freedom and a positional uncertainty of ± (2drms) remain and the location of the deepest depth recorded in the 2010 mapping is at ().\n\nOnly four descents have ever been achieved. The first descent by any vehicle was by the manned bathyscaphe \"Trieste\" in 1960. This was followed by the unmanned ROVs \"Kaikō\" in 1995 and \"Nereus\" in 2009. In March 2012 a manned solo descent was made by the deep-submergence vehicle \"Deepsea Challenger\".\nThese expeditions measured very similar depths of .\n\nOver many years, the search for the point of maximum depth has involved many different vessels.\n\n\n\n\nIn 2014, a study was conducted regarding the determination of the depth and location of the Challenger Deep based on data collected previous to and during the 2010 sonar mapping of the Mariana Trench with a Kongsberg Maritime EM 122 multibeam echosounder system aboard the USNS Sumner (T-AGS-61). This study by James. V. Gardner et al. of the Center for Coastal & Ocean Mapping-Joint Hydrographic Center (CCOM/JHC), Chase Ocean Engineering Laboratory of the University of New Hampshire splits the measurement attempt history into three main groups: early single-beam echo sounders (1950s - 1970's), early multibeam echo sounders (1980s - 21st century), and modern (i.e., post-GPS, high-resolution) multibeam echo sounders. Taking uncertainties in depth measurements and position estimation into account the raw data of the 2010 bathymetry of the Challenger Deep vicinity consisting of 2,051,371 soundings from eight survey lines was analyzed. The study concludes that with the best of 2010 multibeam echosounder technologies after the analysis a depth uncertainty of ± (95% confidence level) on 9 degrees of freedom and a positional uncertainty of ± (2drms) remain and the location of the deepest depth recorded in the 2010 mapping is at . The depth measurement uncertainty is a composite of measured uncertainties in the spatial variations in sound-speed through the water volume, the ray-tracing and bottom-detection algorithms of the multibeam system, the accuracies and calibration of the motion sensor and navigation systems, estimates of spherical spreading, attenuation throughout the water volume, and so forth.\n\nThe 2009 and 2010 maximal depths were not confirmed by the series of dives \"Nereus\" made to the bottom during an expedition in May–June 2009. The direct descent measurements by the four expeditions which have reported from the bottom, have fixed depths in a narrow range from 10,916 m (\"Trieste\") to 10,911 m (\"Kaikō\"), to 10,902 m (\"Nereus\") to 10,898 m (\"Deepsea Challenger\") Although an attempt was made to correlate locations, it could not be absolutely certain that Nereus (or the other descents) reached exactly the same points found to be maximally deep by the sonar/echo sounders of previous mapping expeditions, even though one of these echo soundings was made by \"Nereus\" mothership.\n\nOn 23 January 1960, the Swiss-designed \"Trieste\", originally built in Italy and acquired by the U.S. Navy, descended to the ocean floor in the trench manned by Jacques Piccard (who co-designed the submersible along with his father, Auguste Piccard) and USN Lieutenant Don Walsh. Their crew compartment was inside a spherical pressure vessel, which was a heavy-duty replacement (of the Italian original) built by Krupp Steel Works of Essen, Germany. Their descent took almost five hours and the two men spent barely twenty minutes on the ocean floor before undertaking the three-hour-and-fifteen-minute ascent. Their early departure from the ocean floor was due to their concern over a crack in the outer window caused by the temperature differences during their descent. The measured depth at the bottom was measured with a manometer at ±.\n\nOn 26 March 2012 (local time), Canadian film director James Cameron made a solo manned descent in the DSV \"Deepsea Challenger\" to the bottom of the Challenger Deep.\nAt approximately 05:15 ChST on 26 March (19:15 UTC on 25 March), the descent began.\nAt 07:52 ChST (21:52 UTC), \"Deepsea Challenger\" arrived at the bottom. The descent lasted 2 hours and 36 minutes and the recorded depth was when \"Deepsea Challenger\" touched down.\nCameron had planned to spend about six hours near the ocean floor exploring but decided to start the ascent to the surface after only 2 hours and 34 minutes. The time on the bottom was shortened because a hydraulic fluid leak in the lines controlling the manipulator arm obscured the visibility out the only viewing port. It also caused the loss of the submersible's starboard thrusters. At around 12:00 ChST (02:00 UTC on 26 March), the Deepsea Challenge website says the sub resurfaced after a 90-minute ascent, although Paul Allen's tweets indicate the ascent took only about 67 minutes.\nDuring a post-dive press conference Cameron said: \"I landed on a very soft, almost gelatinous flat plain. Once I got my bearings, I drove across it for quite a distance ... and finally worked my way up the slope.\" The whole time, Cameron said, he didn't see any fish, or any living creatures more than an inch (2.5 cm) long: \"The only free swimmers I saw were small amphipods\"—shrimplike bottom-feeders.\n\nSeveral other manned expeditions are planned. These include:\n\nOn 24 March 1995, the Japanese robotic deep-sea probe \"Kaikō\" broke the depth record for unmanned probes when it reached close to the surveyed bottom of the Challenger Deep. Created by the Japan Agency for Marine-Earth Science and Technology (JAMSTEC), it was one of the few unmanned deep-sea probes in operation that could dive deeper than . The manometer measured depth of ± at for the Challenger Deep is believed to be the most accurate measurement taken yet. \"Kaikō\" also collected sediment cores containing marine organisms from the bottom of the deep. \"Kaikō\" made many unmanned descents to the Mariana Trench during three expeditions in 1995, 1996 and 1998. The greatest depth measured by \"Kaikō\" in 1996 was at and in 1998 at . It was lost at sea off Shikoku Island during Typhoon Chan-Hom on 29 May 2003.\n\nOn 3 June 2008, the Japanese robotic deep-sea probe \"ABISMO\" (Automatic Bottom Inspection and Sampling Mobile) reached the bottom of the Mariana Trench about east of the Challenger Deep and collected core samples of the deep sea sediment and water samples of the water column. Created by the Japan Agency for Marine-Earth Science and Technology (JAMSTEC), it was the only unmanned deep-sea probe in use that could dive deeper than after that of \"Nereus\". During \"ABISMO\"'s deepest Mariana Trench dive its manometer measured a depth of ±\n\nOn 31 May 2009 the United States sent the \"Nereus\" hybrid remotely operated vehicle (HROV) to the Challenger Deep. Nereus thus became the first vehicle to reach the Mariana Trench since 1998 and the deepest-diving vehicle then in operation. Project manager and developer Andy Bowen heralded the achievement as \"the start of a new era in ocean exploration\". \"Nereus\", unlike \"Kaikō\", did not need to be powered or controlled by a cable connected to a ship on the ocean surface.\n\n\"Nereus\" spent over 10 hours at the bottom of the Challenger Deep and measured a depth of at , while sending live video and data back to its mothership \"RV Kilo Moana\" at the surface and collecting geological and biological samples from the Challenger Deep bottom with its manipulator arm for further scientific analysis.<ref name=\"Daily Reports for R/V KILO MOANA April and May 2009\"></ref>\n\nThe \"Nereus\" was operated by the Woods Hole Oceanographic Institution. It was lost on May 10, 2014.\n\nOn 23 May 2016 the Chinese submersible \"Haidou-1\" dove to a depth of in the Mariana Trench. This autonomous and remotely operated vehicle has a design depth of .\n\nThe Summary Report of the HMS \"Challenger\" expedition lists radiolaria from the two dredged samples taken when the Challenger Deep was first discovered. These (Nassellaria and Spumellaria) were reported in the Report on Radiolaria (1887) written by Ernst Haeckel.\n\nOn their 1960 descent, the crew of the \"Trieste\" noted that the floor consisted of diatomaceous ooze and reported observing \"some type of flatfish\" lying on the seabed.\n\nMany marine biologists are now skeptical of this supposed sighting, and it is suggested that the creature may instead have been a sea cucumber. The video camera on board the \"Kaiko\" probe spotted a sea cucumber, a scale worm and a shrimp at the bottom. At the bottom of the Challenger deep, the \"Nereus\" probe spotted one polychaete worm (a multi-legged predator) about an inch long.\n\nAn analysis of the sediment samples collected by \"Kaiko\" found large numbers of simple organisms at . While similar lifeforms have been known to exist in shallower ocean trenches (> 7,000 m) and on the abyssal plain, the lifeforms discovered in the Challenger Deep possibly represent taxa distinct from those in shallower ecosystems.\n\nMost of the organisms collected were simple, soft-shelled foraminifera (432 species according to National Geographic), with four of the others representing species of the complex, multi-chambered genera \"Leptohalysis\" and \"Reophax\". Eighty-five percent of the specimens were organic, soft-shelled allogromiids, which is unusual compared to samples of sediment-dwelling organisms from other deep-sea environments, where the percentage of organic-walled foraminifera ranges from 5% to 20%. As small organisms with hard, calcareous shells have trouble growing at extreme depths because of the high solubility of calcium carbonate in the pressurized water, scientists theorize that the preponderance of soft-shelled organisms in the Challenger Deep may have resulted from the typical biosphere present when the Challenger Deep was shallower than it is now. Over the course of six to nine million years, as the Challenger Deep grew to its present depth, many of the species present in the sediment died out or were unable to adapt to the increasing water pressure and changing environment. The species that survived the change in depth were the ancestors of the Challenger Deep's current denizens.\n\nOn 17 March 2013, researchers reported data that suggested microbial life forms thrive in the Challenger Deep. Other researchers reported related studies that microbes thrive inside rocks up to 1900 feet below the sea floor under 8500 feet of ocean off the coast of the northwestern United States. According to one of the researchers, \"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are.\"\n\n\n", "id": "7786", "title": "Challenger Deep"}
{"url": "https://en.wikipedia.org/wiki?curid=7787", "text": "Claude Louis Berthollet\n\nClaude Louis Berthollet (9 December 1748 in Talloires, France – 6 November 1822 in Arcueil, France) was a Savoyard-French chemist who became vice president of the French Senate in 1804. He is known for his scientific contributions to theory of chemical equilibria via the mechanism of reverse chemical reactions, and for his contribution to modern chemical nomenclature. On a practical basis, Berthollet was the first to demonstrate the bleaching action of chlorine gas, and was first to develop a solution of sodium hypochlorite as a modern bleaching agent.\n\nClaude Louis Berthollet was born in Talloires, near Annecy, then part of the Duchy of Savoy, in 1749.\n\nHe started his studies at Chambéry and then in Turin where he graduated in medicine. Berthollet's great new developments in works regarding chemistry made him, in a short period of time, an active participant of the Academy of Science in 1780.\n\nBerthollet, along with Antoine Lavoisier and others, devised a chemical nomenclature, or a system of names, which serves as the basis of the modern system of naming chemical compounds.\n\nHe also carried out research into dyes and bleaches, being first to introduce the use of chlorine gas as a commercial bleach in 1785. He first produced a modern bleaching liquid in 1789 in his laboratory on the quay Javel in Paris, France, by passing chlorine gas through a solution of sodium carbonate. The resulting liquid, known as \"\"Eau de Javel\"\" (\"Javel water\"), was a weak solution of sodium hypochlorite. Another strong chlorine oxidant and bleach which he investigated and was the first to produce, potassium chlorate (KClO), is known as \"Berthollet's Salt\".\n\nBertholett first determined the elemental composition of the gas ammonia, in 1785.\n\nBerthollet was one of the first chemists to recognize the characteristics of a reverse reaction, and hence, chemical equilibrium.\n\nBerthollet was engaged in a long-term battle with another French chemist Joseph Proust on the validity of the law of definite proportions. While Proust believed that chemical compounds are composed of a fixed ratio of their constituent elements irrespective of the methods of production, Berthollet believed that this ratio can change according to the ratio of the reactants initially taken. Although Proust proved his theory by accurate measurements, his theory was not immediately accepted partially due to Berthollet's authority. His law was finally accepted when Berzelius confirmed it in 1811. But it was found later that Berthollet was not completely wrong because there exists a class of compounds that do not obey the law of definite proportions. These non-stoichiometric compounds are also named \"berthollides\" in his honor.\n\nBerthollet was one of several scientists who went with Napoleon to Egypt, and was a member of the physics and natural history section of the Institut d'Égypte.\n\nIn April, 1789 Berthollet was elected a Fellow of the Royal Society of London. In 1801, he was elected a foreign member of the Royal Swedish Academy of Sciences. In 1809, Berthollet was elected an associate member first class of the Royal Institute of the Netherlands, predecessor of the Royal Netherlands Academy of Arts and Sciences. He was elected an Honorary Fellow of the Royal Society of Edinburgh in 1820 and a Foreign Honorary Member of the American Academy of Arts and Sciences in 1822.\n\nClaude-Louis Berthollet’s 1788 publication entitled \"Méthode de Nomenclature Chimique\", published with colleagues Antoine Lavoisier, Louis Bernard Guyton de Morveau, and Antoine François, comte de Fourcroy, was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society, presented at the Académie des Sciences (Paris) in 2015.\n\nBerthollet married Marguerite Baur in 1788.\n\nBerthollet was an accused of being an atheist.\n\nHe died in Arcueil, France in 1822.\n\n\n\n", "id": "7787", "title": "Claude Louis Berthollet"}
{"url": "https://en.wikipedia.org/wiki?curid=7791", "text": "Constitution of Chile\n\nThe current Political Constitution of the Republic of Chile, approved by Chilean voters in a controversial plebiscite on September 11, 1980, under the military dictatorship of Augusto Pinochet, partially effective March 11, 1981, fully effective 11 March 1990 and amended considerably on August 17, 1989 (via referendum) and on September 22, 2005 (legislatively), and also in 1991, 1994, 1997, 1999, 2000, 2001, 2003, 2007, 2008, 2009 and 2010, replaced the earlier constitution of 1925. It is Chile's eighth constitution. \n\nAccording to law professor, Camel Cazor Aliste, the Constitution of 1980 has problems of legitimacy stemming from two facts: First, the writing commission was not representative of the political spectrum of Chile. Its members were hand-picked by the dictatorship of Pinochet and deliberately excluded opponents of the regime. Secondly, the constitution \"approval\" was achieved through the controversial and tightly government-controlled referendum of 1980.\n\n\n\n\n", "id": "7791", "title": "Constitution of Chile"}
{"url": "https://en.wikipedia.org/wiki?curid=7794", "text": "Crystallography\n\nCrystallography is the experimental science of determining the arrangement of atoms in the crystalline solids (see crystal structure). The word \"crystallography\" derives from the Greek words \"crystallon\" \"cold drop, frozen drop\", with its meaning extending to all solids with some degree of transparency, and \"grapho\" \"I write\". In July 2012, the United Nations recognised the importance of the science of crystallography by proclaiming that 2014 would be the International Year of Crystallography. X-ray crystallography is used to determine the structure of large biomolecules such as proteins. \nBefore the development of X-ray diffraction crystallography (see below), the study of crystals was based on physical measurements of their geometry. This involved measuring the angles of crystal faces relative to each other and to theoretical reference axes (crystallographic axes), and establishing the symmetry of the crystal in question. This physical measurement is carried out using a goniometer. The position in 3D space of each crystal face is plotted on a stereographic net such as a Wulff net or Lambert net. The pole to each face is plotted on the net. Each point is labelled with its Miller index. The final plot allows the symmetry of the crystal to be established.\n\nCrystallographic methods now depend on analysis of the diffraction patterns of a sample targeted by a beam of some type. X-rays are most commonly used; other beams used include electrons or neutrons. This is facilitated by the wave properties of the particles. Crystallographers often explicitly state the type of beam used, as in the terms \"X-ray crystallography, neutron diffraction\" and \"electron diffraction\". These three types of radiation interact with the specimen in different ways. \nBecause of these different forms of interaction, the three types of radiation are suitable for different crystallographic studies.\n\nAn image of a small object is made using a lens to focus the beam, similar to a lens in a microscope. However, the wavelength of visible light (about 4000 to 7000 ångström) is three orders of magnitude longer than the length of typical atomic bonds and atoms themselves (about 1 to 2 Å). Therefore, obtaining information about the spatial arrangement of atoms requires the use of radiation with shorter wavelengths, such as X-ray or neutron beams. Employing shorter wavelengths implied abandoning microscopy and true imaging, however, because there exists no material from which a lens capable of focusing this type of radiation can be created. (Nevertheless, scientists have had some success focusing X-rays with microscopic Fresnel zone plates made from gold, and by critical-angle reflection inside long tapered capillaries.) Diffracted X-ray or neutron beams cannot be focused to produce images, so the sample structure must be reconstructed from the diffraction pattern. Sharp features in the diffraction pattern arise from periodic, repeating structure in the sample, which are often very strong due to coherent reflection of many photons from many regularly spaced instances of similar structure, while non-periodic components of the structure result in diffuse (and usually weak) diffraction features - areas with a higher density and repetition of atom order tend to reflect more light toward one point in space when compared to those areas with fewer atoms and less repetition.\n\nBecause of their highly ordered and repetitive structure, crystals give diffraction patterns of sharp Bragg reflection spots, and are ideal for analyzing the structure of solids.\n\n\nSome materials that have been analyzed crystallographically, such as proteins, do not occur naturally as crystals. Typically, such molecules are placed in solution and allowed to slowly crystallize through vapor diffusion. A drop of solution containing the molecule, buffer, and precipitants is sealed in a container with a reservoir containing a hygroscopic solution. Water in the drop diffuses to the reservoir, slowly increasing the concentration and allowing a crystal to form. If the concentration were to rise more quickly, the molecule would simply precipitate out of solution, resulting in disorderly granules rather than an orderly and hence usable crystal.\n\nOnce a crystal is obtained, data can be collected using a beam of radiation. Although many universities that engage in crystallographic research have their own X-ray producing equipment, synchrotrons are often used as X-ray sources, because of the purer and more complete patterns such sources can generate. Synchrotron sources also have a much higher intensity of X-ray beams, so data collection takes a fraction of the time normally necessary at weaker sources. Complementary neutron crystallography techniques are used to identify the positions of hydrogen atoms, since X-rays only interact very weakly with light elements such as hydrogen.\n\nProducing an image from a diffraction pattern requires sophisticated mathematics and often an iterative process of modelling and refinement. In this process, the mathematically predicted diffraction patterns of an hypothesized or \"model\" structure are compared to the actual pattern generated by the crystalline sample. Ideally, researchers make several initial guesses, which through refinement all converge on the same answer. Models are refined until their predicted patterns match to as great a degree as can be achieved without radical revision of the model. This is a painstaking process, made much easier today by computers.\n\nThe mathematical methods for the analysis of diffraction data only apply to \"patterns,\" which in turn result only when waves diffract from orderly arrays. Hence crystallography applies for the most part only to crystals, or to molecules which can be coaxed to crystallize for the sake of measurement. In spite of this, a certain amount of molecular information can be deduced from patterns that are generated by fibers and powders, which while not as perfect as a solid crystal, may exhibit a degree of order. This level of order can be sufficient to deduce the structure of simple molecules, or to determine the coarse features of more complicated molecules. For example, the double-helical structure of DNA was deduced from an X-ray diffraction pattern that had been generated by a fibrous sample.\n\nCrystallography is used by materials scientists to characterize different materials. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. In addition, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in poly-crystalline form (i.e., as an aggregate of small crystals with different orientations). Because of this, the powder diffraction method, which takes diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination.\n\nOther physical properties are also linked to crystallography. For example, the minerals in clay form small, flat, platelike structures. Clay can be easily deformed because the platelike particles can slip along each other in the plane of the plates, yet remain strongly connected in the direction perpendicular to the plates. Such mechanisms can be studied by crystallographic texture measurements.\n\nIn another example, iron transforms from a body-centered cubic (bcc) structure to a face-centered cubic (fcc) structure called austenite when it is heated. The fcc structure is a close-packed structure unlike the bcc structure; thus the volume of the iron decreases when this transformation occurs.\n\nCrystallography is useful in phase identification. When manufacturing or using a material, it is generally desirable to know what compounds and what phases are present in the material, as their composition, structure and proportions will influence the material's properties. Each phase has a characteristic arrangement of atoms. X-ray or neutron diffraction can be used to identify which patterns are present in the material, and thus which compounds are present. Crystallography covers the enumeration of the symmetry patterns which can be formed by atoms in a crystal and for this reason is related to group theory and geometry.\n\nX-ray crystallography is the primary method for determining the molecular conformations of biological macromolecules, particularly protein and nucleic acids such as DNA and RNA. In fact, the double-helical structure of DNA was deduced from crystallographic data. The first crystal structure of a macromolecule was solved in 1958, a three-dimensional model of the myoglobin molecule obtained by X-ray analysis. The Protein Data Bank (PDB) is a freely accessible repository for the structures of proteins and other biological macromolecules. Computer programs such as RasMol or Pymol can be used to visualize biological molecular structures.\nNeutron crystallography is often used to help refine structures obtained by X-ray methods or to solve a specific bond; the methods are often viewed as complementary, as X-rays are sensitive to electron positions and scatter most strongly off heavy atoms, while neutrons are sensitive to nucleus positions and scatter strongly even off many light isotopes, including hydrogen and deuterium.\nElectron crystallography has been used to determine some protein structures, most notably membrane proteins and viral capsids.\n\nThe International Tables of Crystallography is an eight book series that outlines the standard notations for formatting, describing and testing crystals. The series contains books that covers analysis methods and the mathematical procedures for determining organic structure though x-ray crystallography, electron diffraction, and neutron diffraction. The International tables are focused on procedures, techniques and descriptions and does not list the physical properties of individual crystals themselves. Each book is about 1000 pages and the titles of the books are:\n\nVol A - Space Group Symmetry\n\nVol A1 - Symmetry Relations Between Space Groups\n\nVol B - Reciprocal Space\n\nVol C - Mathematical, Physical, and Chemical Tables\n\nVol D - Physical Properties of Crystals\n\nVol E - Subperiodic Groups\n\nVol F - Crystallography of Biological Macromolecues\n\nVol G - Definition and Exchange of Crystallographic Data\n\n", "id": "7794", "title": "Crystallography"}
{"url": "https://en.wikipedia.org/wiki?curid=7796", "text": "Claude Auchinleck\n\nField Marshal Sir Claude John Eyre Auchinleck (21 June 1884 – 23 March 1981) was a British Army commander during the Second World War. He was a career soldier who spent much of his military career in India, where he rose to become Commander-in-Chief of the Indian Army by early 1941. In July 1941 he was appointed Commander-in-Chief of the Middle East theatre, but after initial successes the war in North Africa turned against the British, and he was relieved of the post in 1942 during the crucial Alamein campaign. In June 1943 he was once more appointed Commander-in-Chief India, where his support through the organisation of supply, maintenance and training for Slim's Fourteenth Army played an important role in its success. He served as Commander-in-Chief India until Partition in 1947, when he assumed the role of Supreme Commander of all British forces in India and Pakistan until late 1948.\n\nBorn at 89 Victoria Road in Aldershot, the son of Colonel John Auchinleck and Mary Auchinleck, Auchinleck attended Eagle House School at Crowthorne and then Wellington College on scholarships. After attending the Royal Military College, Sandhurst, Auchinleck was commissioned as an unattached second lieutenant in the Indian Army on 21 January 1903 and joined to the 62nd Punjabis in April 1904. He learnt some Punjabi and, able to speak fluently with his soldiers, he absorbed a knowledge of local dialects and customs: this familiarity engendered a lasting mutual respect, enhanced by his own personality. He was promoted to lieutenant on 21 April 1905 and then spent the next two years in Tibet and Sikkim before moving to Benares in 1907 where he caught diphtheria. After briefly serving with the Royal Inniskilling Fusiliers at Aldershot he returned Benares in 1909 and became adjutant of the 62nd Punjabis with promotion to captain on 21 January 1912.\n\nAuchinleck saw active service in the First World War and was deployed with his regiment to defend the Suez Canal: in February 1915 he was in action against the Turks at Ismaïlia. His regiment moved into Aden to counter the Turkish threat there in July 1915. The 6th Indian Division, of which the 62nd Punjabis were a part, was landed at Basra on 31 December 1915 for the Mesopotamian campaign. In July 1916 Auchinleck was promoted acting major and made second in command of the regiment. He took part in a series of fruitless attacks on the Turks at the Battle of Hanna in January 1916 and was one of the few British officers in his regiment to survive these actions. He became acting commanding officer of his regiment in February 1917 and led his regiment at the Second Battle of Kut in February 1917 and the Fall of Baghdad in March 1917. Having been mentioned in despatches and having received the Distinguished Service Order in 1917 for his service in Mesopotamia, he was promoted to the substantive rank of major on 21 January 1918, to temporary lieutenant-colonel on 23 May 1919 and to brevet lieutenant-colonel on 15 November 1919 for his \"distinguished service in Southern and Central Kurdistan\" on the recommendation of the Commander-in-Chief of the Mesopotamia Expeditionary Force.\n\nAuchinleck attended the Staff College, Quetta between 1920 and 1921. He married Jessie Stewart in 1921. Jessie had been born in 1900 in Tacoma, Washington, to Alexander Stewart, head of the Blue Funnel Line that plied the west coast of the United States. When he died about 1919, their mother took her, her twin brother Alan and her younger brother Hepburne back to Bun Rannoch, the family estate at Innerhadden in Perthshire. Holidaying at Grasse on the French Riviera, Auchinleck, who was on leave from India at the time, met Jessie on the tennis courts. She was a high-spirited, blue-eyed beauty. Things moved quickly, and they were married within five months. Sixteen years younger than Auchinleck, Jessie became known as 'the little American girl' in India, but adapted readily to life there.\n\nAuchinleck became temporary deputy assistant quartermaster-general at Army Headquarters in February 1923 and then second-in-command of his regiment, which in the 1923 reorganisation of the British Indian Army had become the 1st battalion, 1st Punjab Regiment, in September 1925. He attended the Imperial Defence College in 1927 and, having been promoted to lieutenant-colonel on 21 January 1929 he was appointed to command his regiment. Promoted to full colonel on 1 February 1930 with seniority from 15 November 1923, he became an instructor at the Staff College, Quetta in February 1930 where he remained until April 1933. He was promoted to temporary brigadier on 1 July 1933 and given command of the Peshawar Brigade, which was active in the pacification of the adjacent tribal areas during the Mohmand and Bajaur Operations between July and October 1933: during his period of command he was mentioned in despatches. He led a second punitive expedition during the Second Mohmand Campaign in August 1935 for which he was again mentioned in despatches, promoted to major-general on 30 November 1935 and appointed a Companion of the Order of the Star of India on 8 May 1936.\n\nOn leaving his brigade command in April 1936 Auchinleck was on the unemployed list (on half pay) until September 1936 when he was appointed Deputy Chief of the General Staff and Director of Staff Duties in Delhi. He was then appointed to command the Meerut District in India in July 1938. In 1938 Auchinleck was appointed to chair a committee to consider the modernisation, composition and re-equipment of the British Indian Army: the committee's recommendations formed the basis of the 1939 Chatfield Report which outlined the transformation of the Indian Army – it grew from 183,000 in 1939 to over 2,250,000 men by the end of the war.\n\nOn the outbreak of war Auchinleck was appointed to command the Indian 3rd Infantry Division but in January 1940 was summoned to the United Kingdom to command IV Corps, the only time in the war that a wholly British corps was commanded by an Indian Army officer. He received promotion to acting lieutenant-general on 1 February 1940 and to the substantive rank of lieutenant-general on 16 March 1940. In May 1940 Auchinleck took over command of the Anglo-French ground forces in Norway, a military operation that was doomed to fail. After the fall of Norway, in June 1940 he briefly commanded V Corps before becoming General Officer Commanding-in-Chief, Southern Command in July 1940, where he had an uneasy relationship with his subordinate Bernard Montgomery, the new V Corps commander. Montgomery later wrote: \"In the 5th Corps I first served under Auchinleck... I cannot recall that we ever agreed on anything.\"\n\nPromoted to full general on 26 December 1940, Auchinleck was recalled to India in January 1941 to become Commander-in-Chief, India in which position he also was appointed to the Executive Council of the Governor-General of India and appointed ADC General to the King which ceremonial position he held until after the end of the War.\n\nIn April 1941 RAF Habbaniya was threatened by the new pro-Axis regime of Rashid Ali. This large Royal Air Force station was west of Baghdad in Iraq and General Archibald Wavell, Commander-in-Chief Middle East Command, was reluctant to intervene, despite the urgings of Winston Churchill, because of his pressing commitments in the Western Desert and Greece. Auchinleck, however, acted decisively, sending a battalion of the King's Own Royal Regiment by air to Habbaniya and shipping the Indian 10th Infantry Division by sea to Basra. Wavell was prevailed upon by London to send \"Habforce\", a relief column, from the British Mandate of Palestine but by the time it arrived in Habbaniya on 18 May the Anglo-Iraqi War was virtually over.\n\nFollowing the see-saw of Allied and Axis successes and reverses in North Africa, Auchinleck was appointed to succeed General Sir Archibald Wavell as Commander-in-Chief Middle East Command in July 1941; Wavell took up Auchinleck's post as Commander-in-Chief of the Indian Army, swapping jobs with him.\n\nAs Commander-in-Chief Middle East Auchinleck, based in Cairo, held responsibility not just for North Africa but also for Persia and the Middle East. He launched an offensive in the Western Desert, Operation Crusader, in November 1941: despite some tactical reverses during the fighting which resulted in Auchinleck replacing the Eighth Army commander Alan Cunningham with Neil Ritchie, by the end of December the besieged garrison of Tobruk had been relieved and Rommel obliged to withdraw to El Agheila. Auchinleck appears to have believed that enemy had been defeated, writing on 12 January 1942 that the Axis forces were \"beginning to feel the strain\" and were \"hard pressed\". In fact the Axis forces had managed to withdraw in good order and a few days after Auchinleck's optimistic appreciation, having reorganised and been reinforced, struck at the dispersed and weakened British forces, driving them back to the Gazala positions near Tobruk. The British Chief of Imperial General staff, Alan Brooke, wrote in his diary that it was \"Nothing less than bad generalship on the part of Auchinleck\". Rommel's attack at the Battle of Gazala of 26 May 1942 resulted in a significant defeat for the British. Auchinleck's appreciation of the situation written to Ritchie on 20 May had suggested that the armoured reserves be concentrated in a position suitable to meet both a flanking attack around the south of the front or a direct attack through the centre (which was the likelihood more favoured by Auchinleck). In the event, Ritchie chose a more dispersed and rearward positioning of his two armoured divisions and when the attack in the centre came, it proved to be a diversion and the main attack, by Rommel's armoured formations, came round the southern flank. Poor initial positioning and subsequent handling and coordination of Allied formations by Ritchie and his corps commanders resulted in their heavy defeat and the Eighth Army retreating into Egypt; Tobruk fell to the Axis on 21 June 1942.\n\nOn 24 June Auchinleck stepped in to take direct command of the Eighth Army, having lost confidence in Neil Ritchie's ability to control and direct his forces. Auchinleck discarded Ritchie's plan to stand at Mersa Matruh, deciding to fight only a delaying action there, while withdrawing to the more easily defendable position at El Alamein. Here Auchinleck tailored a defence that took advantage of the terrain and the fresh troops at his disposal, stopping the exhausted German/Italian advance in the First Battle of El Alamein. Enjoying a considerable superiority of material and men over the weak German/Italian forces, Auchinleck organised a series of counter-attacks. Poorly conceived and badly coordinated, these attacks achieved little.\n\n\"The Auk\", as he was known, appointed a number of senior commanders who proved to be unsuitable for their positions, and command arrangements were often characterised by bitter personality clashes. Auchinleck was an Indian Army officer and was criticised for apparently having little direct experience or understanding of British and Dominion troops. His controversial chief of operations, Major-General Dorman-Smith, was regarded with considerable distrust by many of the senior commanders in Eighth Army. By July 1942 Auchinleck had lost the confidence of Dominion commanders and relations with his British commanders had become strained.\n\nLike his foe Rommel (and his predecessor Wavell and successor Montgomery), Auchinleck was subjected to constant political interference, having to weather a barrage of hectoring telegrams and instructions from Prime Minister Churchill throughout late 1941 and the spring and summer of 1942. Churchill constantly sought an offensive from Auchinleck, and was downcast at the military reverses in Egypt and Cyrenaica. Churchill was desperate for some sort of British victory before the planned Allied landings in North Africa, Operation Torch, scheduled for November 1942. He badgered Auchinleck immediately after the Eighth Army had all but exhausted itself after the first battle of El Alamein. Churchill and the Chief of the Imperial General Staff, Alan Brooke, flew to Cairo in early August 1942, to meet Auchinleck, where it emerged he had lost the confidence of both men. He was replaced as Commander-in-Chief Middle East Command by General Sir Harold Alexander (later Field Marshal Earl Alexander of Tunis).\n\nJoseph M. Horodyski and Maurice Remy both praise Auchinleck as an underrated military leader who contributed the most to the successful defense of El Alamein and consequently the final defeat of Rommel in Africa. The two historians also criticize Churchill for the militarily unreasonable decision of putting the blame on Auchinleck and relieving him.\n\nChurchill offered Auchinleck command of the newly created Persia and Iraq Command (this having been separated from Alexander's command), but Auchinleck declined this post, as he believed that separating the area from the Middle East Command was not good policy and the new arrangements would not be workable. He set his reasons out in his letter to the Chief of the Imperial General Staff dated 14 August 1942. Instead he returned to India, where he spent almost a year \"unemployed\" before in June 1943 being again appointed Commander-in-Chief of the Indian Army, General Wavell meanwhile having been appointed Viceroy: on this appointment it was announced that responsibility for the prosecution of the war with Japan would move from the C-in-C India to a newly created South East Asia Command. However, the appointment of the new command's Supreme Commander, Admiral Louis Mountbatten, was not announced until August 1943 and until Mountbatten could set up his headquarters and assume control (in November) Auchinleck retained responsibility for operations in India and Burma while conducting a review and revision of Allied plans based on the decisions taken by the Allied Combined Chiefs of Staff at the Quadrant Conference which ended in August.\n\nFollowing Mountbatten's arrival, Auchinleck's India Command (which had equal status with South East Asia Command in the military hierarchy) was responsible for the internal security of India, the defence of the North West Frontier and the buildup of India as a base, including most importantly the reorganisation of the Indian Army, the training of forces destined for SEAC and the lines of communication carrying men and material to the forward areas and to China. Auchinleck made the supply of Fourteenth Army, with probably the worst lines of communication of the war, his immediate priority; as William Slim, commander of the Fourteenth Army was later to write:\n\nAuchinleck suffered a personal disappointment when his wife Jessie left him for his friend Air Chief Marshal Sir Richard Peirse. Peirse and Auchinleck had been students together at the Imperial Defence College, but that was long before. Peirse was now Allied Air Commander-in-Chief, South-East Asia, and also based in India. The affair became known to Mountbatten in early 1944, and he passed the information to the Chief of the RAF, Sir Charles Portal, hoping that Peirse would be recalled. The affair was common knowledge by September 1944, and Peirse was neglecting his duties. Mountbatten sent Peirse and Lady Auchinleck back to England on 28 November 1944, where they lived together at a Brighton hotel. Peirse had his marriage dissolved, and Auchinleck obtained a divorce in 1946. Auchinleck was reportedly very badly affected. According to his sister, he was never the same after the break-up. He always carried a photograph of Jessie in his wallet even after the divorce.\n\nAuchinleck continued as Commander-in-Chief of the Indian Army after the end of the war helping, though much against his own convictions, to prepare the future Indian and Pakistani armies for the Partition of India: in November 1945 he was forced to commute the more serious judicial sentences awarded against officers of the Indian National Army in face of growing unease and unrest both within the Indian population, and the British Indian Army. On 1 June 1946 he was promoted to field marshal, but he refused to accept a peerage, lest he be thought associated with a policy (i.e. Partition) that he thought fundamentally dishonourable. \nSending a report to British Government on 28 September 1947 Auchinleck wrote: \"I have no hesitation, whatever, in affirming that the present Indian Cabinet are implacably determined to do all in their power to prevent the establishment of the Dominion of Pakistan on firm basis.\"\n\nWhen partition was effected in August 1947, Auchinleck was appointed Supreme Commander of all British forces remaining in India and Pakistan and remained in this role until the winding up and closure of the Supreme H.Q. at the end of November 1948. This marked his effective retirement from the army (although technically field marshals in the British Army never retire, remaining on the active list on half pay). He left India on 1 December.\n\nAfter a brief period in Italy in connection with an unsuccessful business project, Auchinleck retired to London, where he occupied himself with a number of charitable and business interests and became a respectably skilled watercolour painter. In 1960 he settled in Beccles in the county of Suffolk, remaining there for seven years until, at the age of eighty-four, he decided to emigrate and set up home in Marrakesh, where he died on 23 March 1981.\n\nAuchinleck was buried in Ben M'Sik European Cemetery, Casablanca, in the Commonwealth War Graves Commission plot in the cemetery, next to the grave of Raymond Steed who was the second youngest non-civilian Commonwealth casualty of the Second World War.\n\nA memorial plaque was erected in the crypt of St Paul's Cathedral. The tour guides relate how in 1979, as plaques for the other great Second World War military leaders were being installed, no one in the establishment had been in contact with his family for some years. Cathedral officials telephoned to enquire the date of his death only to be told \"Auchinleck here – but I won't be keeping you much longer!\"\n\n\n\n\n\n\n\n \n", "id": "7796", "title": "Claude Auchinleck"}
{"url": "https://en.wikipedia.org/wiki?curid=7797", "text": "Camilla Hall\n\nCamilla Christine Hall (March 24, 1945 - May 17, 1974) was an artist, college trained social worker, and an early member of the Symbionese Liberation Army. She is most well known for being one of the kidnappers of heiress Patricia Hearst.\n\nOn March 24, 1945, Camilla Christine Hall was born in Saint Peter, Minnesota. Her parents, George Fridolph Hall (1908-2000) and Lorena Daeschner Hall (1911-1995), worked at Gustavus Adolphus College in Saint Peter, Minnesota from 1938-1952. In addition, her father was a minister in the Augustana Evangelical Lutheran Church, Lutheran Church in America, and later the Evangelical Lutheran Church in America. Her mother, Lorena (Daeschner) Hall, helped found Gustavus Adolphus College's Art Department and served as the department head. Camilla Hall was the only surviving child of four; two of her siblings died of a kidney disorder, Peter and Nan, and a third, Terry, of congenital heart disease.\n\nIn 1952, the Hall family moved to what is now Tanzania in East Africa. George and Lorena Hall taught in schools and did mission work, while Camilla and Nan played with the native children. In 1954, when Camilla was nine, the family moved back to Saint Peter, because of seven-year-old Nan's poor health. While Camilla Hall attended elementary school in Minnesota, the family moved to Montclair, New Jersey until Hall was to start high school.\n\nAfter moving back to Minnesota, Hall went to Washburn High School in Minneapolis where she was involved in many activities. The 1963 Washburn Yearbook says, \"Candy was a member of Blue Tri, Class Play, Poplars Staff, Quill Club, Forensics, Pep Club, and Hall of Fame\" Blue Tri club was an organization that encouraged Christian ideals and put together service projects. In addition, Camilla Hall was voted class clown in High School. In 1963, she graduated from Washburn High School.\n\nCamilla Hall attended Gustavus Adolphus College in St. Peter, Minnesota. She transferred to the University of Minnesota after her freshman year at Gustavus. Hall attended special lectures, exhibits, and concerts at the University. On June 10, 1967, Hall graduated with a humanities degree from the University of Minnesota.\n\nFollowing graduation, Hall moved to Duluth, Minnesota where she was a caseworker for St. Louis County, Minnesota. In early 1968 she was elected to carry the Eugene McCarthy banner, in support of the Eugene McCarthy Presidential Campaign, for the St. Louis County precinct. Even though Hall enjoyed helping people in her work, she found it difficult to separate her feelings while being a caseworker. For her job in Duluth, Minnesota, Hall used her musical and poetic talents in an advertising campaign.\n\nIn June 1968, Hall returned to Minneapolis, Minnesota and worked as a caseworker for the Hennepin County, Minnesota welfare office. Co-workers and friends of Hall described her as witty, sympathetic, helpful, and compassionate. Also, she had an outgoing personality and had a passion for literature. At the same time, Hall frequently talked with family and friends about philosophy and how she was disappointed with the state of welfare. In 1968, Hall was 23 years old and carefully monitored the political situation in America, including the 1968 Democratic National Convention. She was active in the peace movement and food boycotts, including the Mobilization Committee to End the War in Vietnam. Despite her active participation in urging social change and working as a caseworker, Hall's mother says Camilla became dissatisfied with her work.\n\nIn November 1969, Hall moved to Topanga, a northern suburb of Los Angeles, California. In March, she moved into Los Angeles proper in west Los Angeles. According to Rachael Hanel, \"She lived off her savings, interest income from a trust, money from her parents, and selling her simple, Rubenesque line drawings.\" Even though Hall didn't express dissatisfaction at being an artist, she decided to move again.\n\nHall moved to Berkeley in February 1971. In May 1971, Hall moved into an apartment complex on Channing Way where she met Patricia Soltysik. Previous to this relationship, Hall had not lived publicly in a lesbian relationship. Patricia Soltysik was the object of Hall's love poem named \"Mizmoon\".\n\nIn Berkeley, Hall continued being politically active. She was one of the activists who took over Berkeley Park in the People's Park demonstration of the summer of 1972. In October 1972, Hall travelled to Europe and stayed with friends while she traveled for three months. Once she returned, she continued being politically active and was involved with the Symbionese Liberation Army.\n\nWhile a member of the radical terrorist organization known as the SLA, Hall participated in numerous terrorist acts including the murder of school superintendent Marcus Foster, various bank robberies, and most famously, the kidnapping and torture of heiress Patricia Hearst.\n\nHall died in a shootout (May 17, 1974) with police in which five other SLA members were killed. As their hideout burned, Hall and fellow SLA member Nancy Ling Perry exited from the back door. Police claimed that Perry came out firing a revolver while Hall was firing an automatic pistol. Police shot them immediately, killing both. Perry was shot twice. One shot hit her right lung, the other shot severed her spine. Hall was shot once in the forehead.\n\nInvestigators working for Hall's parents claimed that Perry had come walking out of the house intending to surrender.\n\n", "id": "7797", "title": "Camilla Hall"}
{"url": "https://en.wikipedia.org/wiki?curid=7800", "text": "Clone\n\nClone or Clones or The Clone may refer to:\n\n\n\n\n\n", "id": "7800", "title": "Clone"}
{"url": "https://en.wikipedia.org/wiki?curid=7801", "text": "Critical psychology\n\nCritical psychology is a perspective on psychology that draws extensively on critical theory. Critical psychology challenges mainstream psychology and attempts to apply psychological understandings in more progressive ways, often looking towards social change as a means of preventing and treating psychopathology.\n\nOne of critical psychology's main criticisms of conventional psychology is that it fails to consider or deliberately ignores the way power differences between social classes and groups can affect the mental and physical well-being of individuals or groups of people. It does this, in part, because it tends to explain behavior at the level of the individual.\n\nCriticisms of mainstream psychology consistent with current critical psychology usage have existed since psychology's modern development in the late 19th century. Use of the term \"critical psychology\" started in the 1970s in Berlin at Freie Universität Berlin. The German branch of critical psychology predates and has developed largely separately from the rest of the field. As of May 2007, only a few works have been translated into English. The German Critical Psychology movement is rooted in the post-war babyboomers' student revolt of the late '60s; see German student movement. Marx's \"Critique of Political Economy\" played an important role in the German branch of the student revolt, which was centered in Berlin. Then Berlin was a capitalist city surrounded by communist-ruled East Germany, represented a \"hot spot\" of political and ideological controversy for the revolting German students. The sociological foundations of critical psychology are decidedly Marxist.\n\nOne of the most important and sophisticated books in the field is the \"Grundlegung der Psychologie\" (\"Foundations of Psychology\") by Klaus Holzkamp, who might be considered the theoretical founder of critical psychology. Holzkamp, who had written two books on theory of science and one on sensory perception before publishing the \"Grundlegung der Psychologie\" in 1983, thought this major work provided a solid paradigm for psychological research, as he viewed psychology as a pre-paradigmatic scientific discipline (T.S. Kuhn had used the term \"pre-paradigmatic\" for social science).\n\nHolzkamp mostly based his sophisticated attempt to provide a comprehensive and integrated set of categories defining the field of psychological research on Aleksey Leontyev's approach to cultural–historical psychology and activity theory. Leontyev had seen human action as a result of biological as well as cultural evolution and, drawing on Marx's materialist conception of culture, stressed that individual cognition is always part of social action which in turn is mediated by man-made tools (cultural artifacts), language and other man-made systems of symbols, which he viewed as a major distinguishing feature of human culture and, thus, human cognition. Another important source was Lucien Séve's theory of personality, which provided the concept of \"social activity matrices\" as mediating structure between individual and social reproduction. At the same time, the \"Grundlegung\" systematically integrated previous specialized work done at Free University of Berlin in the '70s by critical psychologists who also had been influenced by Marx, Leontyev and Seve. This included books on animal behavior/ethology, sensory perception, motivation and cognition. He also incorporated ideas from Freud's psychoanalysis and Merleau-Ponty's phenomenology into his approach.\n\nOne core result of Holzkamp's historical and comparative analysis of human reproductive action, perception and cognition is a very specific concept of meaning that identifies symbolic meaning as historically and culturally constructed, purposeful conceptual structures that humans create in close relationship to material culture and within the context of historically specific formations of social reproduction.\n\nComing from this phenomenological perspective on culturally mediated and socially situated action, Holzkamp launched a devastating and original methodological attack on behaviorism (which he termed S–R (stimulus–response) psychology) based on linguistic analysis, showing in minute detail the rhetorical patterns by which this approach to psychology creates the illusion of \"scientific objectivity\" while at the same time losing relevance for understanding culturally situated, intentional human actions. Against this approach, he developed his own approach to generalization and objectivity, drawing on ideas from Kurt Lewin in Chapter 9 of \"Grundlegung der Psychologie\".\n\nHis last major publication before his death in 1995 was about learning. It appeared in 1993 and contained a phenomenological theory of learning from the standpoint of the subject. One important concept Holzkamp developed was \"reinterpretation\" of theories developed by conventional psychology. This meant to look at these concepts from the standpoint of the paradigm of critical psychology, thereby integrating their useful insights into critical psychology while at the same time identifying and criticizing their limiting implications, which in the case of S–R psychology were the rhetorical elimination of the subject and intentional action, and in the case of cognitive psychology which did take into account subjective motives and intentional actions, methodological individualism. \n\nThe first part of the book thus contains an extensive look at the history of psychological theories of learning and a minute re-interpretation of those concepts from the perspective of the paradigm of critical psychology, which focuses on intentional action situated in specific socio-historical/cultural contexts. The conceptions of learning he found most useful in his own detailed analysis of \"classroom learning\" came from cognitive anthropologists Jean Lave (situated learning) and Edwin Hutchins (distributed cognition). \n\nThe book's second part contained an extensive analysis on the modern state's institutionalized forms of \"classroom learning\" as the cultural–historical context that shapes much of modern learning and socialization. In this analysis, he heavily drew upon Michel Foucault's Discipline and Punish. Holzkamp felt that classroom learning as the historically specific form of learning does not make full use of student's potentials, but rather limits her or his learning potentials by a number of \"teaching strategies.\" Part of his motivation for the book was to look for alternative forms of learning that made use of the enormous potential of the human psyche in more fruitful ways. Consequently, in the last section of the book, Holzkamp discusses forms of \"expansive learning\" that seem to avoid the limitations of classroom learning, such as apprenticeship and learning in contexts other than classrooms. \n\nThis search culminated in plans to write a major work on life leadership in the specific historical context of modern (capitalist) society. Due to his death in 1995, this work never got past the stage of early (and premature) conceptualizations, some of which were published in the journals \"Forum Kritische Psychologie\" and \"Argument\".\n\nIn the 1960s and 1970s the term \"radical psychology\" was used by psychologists to denote a branch of the field which rejected conventional psychology's focus on the individual as the basic unit of analysis and sole source of psychopathology. Instead, radical psychologists examined the role of society in causing and treating problems and looked towards social change as an alternative to therapy to treat mental illness and as a means of preventing psychopathology. Within psychiatry the term \"anti-psychiatry\" was often used and now British activists prefer the term \"critical psychiatry\". \"Critical psychology\" is currently the preferred term for the discipline of psychology keen to find alternatives to the way the discipline of psychology reduces human experience to the level of the individual and thereby strips away possibilities for radical social change.\n\nStarting in the 1990s a new wave of books started to appear on critical psychology, the most influential being the edited book \"Critical Psychology\" by Dennis Fox and Isaac Prilleltensky. Various introductory texts to critical psychology written in the United Kingdom have tended to focus on discourse, but this has been seen by some proponents of critical psychology as a reduction of human experience to language which is as politically dangerous as the way mainstream psychology reduces experience to the individual mind. Attention to language and ideological processes, others would argue, is essential to effective critical psychology - it is not simply a matter of applying mainstream psychological concepts to issues of social change.\n\nIn 1999 Ian Parker published an influential manifesto in both the online journal \"Radical Psychology\" and the Annual Review of Critical Psychology. This manifesto argues that critical psychology should include the following four components:\n\nThere are a few international journals devoted to critical psychology, including the no longer published \"International Journal of Critical Psychology\" (continued in the journal Subjectivity) and the \"Annual Review of Critical Psychology\". The journals still tend to be directed to an academic audience, though the \"Annual Review of Critical Psychology\" runs as an open-access online journal. There are close links between critical psychologists and critical psychiatrists in Britain through the Asylum Collective. Critical psychology courses and research concentrations are available at Manchester Metropolitan University, York St Johns University, the University of East London, the University of Edinburgh, the University of KwaZulu Natal, the Graduate Center of the City University of New York, and the University of West Georgia.\n\nLike many critical applications, critical psychology has expanded beyond Marxist and feminist roots to benefit from other critical approaches. Consider ecopsychology and transpersonal psychology. Critical psychology and related work has also sometimes been labelled radical psychology and liberation psychology. In the field of developmental psychology, the work of Erica Burman has been influential.\n\nVarious sub-disciplines within psychology have begun to establish their own critical orientations. Perhaps the most extensive are critical health psychology and community psychology.\n\nAt FU-Berlin, critical psychology was not really seen as a division of psychology and followed its own methodology, trying to reformulate traditional psychology on an unorthodox Marxist base and drawing from Soviet ideas of cultural–historical psychology, particularly Aleksey Leontyev. Some years ago the department of critical psychology at FU-Berlin was merged into the traditional psychology department.\n\nAn April 2009 issue of the Sage journal \"Theory & Psychology\" (edited by Desmond Painter, Athanasios Marvakis, and Leendert Mos) is devoted to an examination of German critical psychology.\n\nThe University of KwaZulu-Natal in Durban, South Africa, is one of few worldwide to offer a Master's course in critical psychology. For an overview of critical psychology in South Africa, see Desmond Painter and Martin Terre Blanche's article on \"Critical Psychology in South Africa: Looking back and looking forwards\". They have also now started a critical psychology blog.\n\nCritical psychology in the United States and Canada has, for the most part, focused on critiques of mainstream psychology's support for an unjust \"status quo\". No departments of critical psychology exist, with the exception of the Bachelor's Completion Program with a minor in Critical Psychology, offered at the California Institute of Integral Studies in San Francisco, though critical perspectives are sometimes encountered in traditional universities, perhaps especially within community psychology programs. The University of West Georgia offers a Ph.D. in Consciousness and Society with critical psychology being one of the main three theoretical orientations. North American efforts include the 1993 founding of RadPsyNet, the 1997 publication of \"Critical Psychology: An Introduction\" (edited by Dennis Fox and Isaac Prilleltensky; expanded 2009 edition edited by Dennis Fox, Isaac Prilleltensky, and Stephanie Austin), the 2001 Monterey Conference on Critical Psychology, and in underlying themes of many contributions to the \"Journal of Social Action in Counseling and Psychology\".\n\n\n\n\n", "id": "7801", "title": "Critical psychology"}
{"url": "https://en.wikipedia.org/wiki?curid=7803", "text": "Crossfire\n\nA crossfire (also known as interlocking fire) is a military term for the siting of weapons (often automatic weapons such as assault rifles or sub-machine guns) so that their arcs of fire overlap. This tactic came to prominence in World War I.\n\nSiting weapons this way is an example of the application of the defensive principle of \"mutual support\". The advantage of siting weapons that mutually support one another is that it is difficult for an attacker to find a covered approach to any one defensive position. \n\nUse of armour, air support, indirect fire support, and stealth are tactics that may be used to assault a defensive position. However, when combined with land mines, snipers, barbed wire, and air cover, crossfire became a difficult tactic to counter in the early 20th century.\n\nThe tactic of using overlapping arcs of fire came to prominence during World War I where it was a feature of trench warfare. Machine guns were placed in groups, called machine-gun nests, and they protected the front of the trenches. Many lives were lost in futile attempts to charge across the no man's land where these crossfires were set up.\n\nTo be \"caught in the crossfire\" is an expression that often refers to unintended casualties (bystanders, etc.) who were killed or wounded by being exposed to the gunfire of a battle or gun fight, such as in a position to be hit by bullets of either side. The phrase has come to mean any injury, damage or harm (physical or otherwise) caused to a third party due to the action of belligerents (collateral damage).\n", "id": "7803", "title": "Crossfire"}
{"url": "https://en.wikipedia.org/wiki?curid=7805", "text": "CNO\n\nCNO is a three-letter initialism. It can mean:\n\n\nCNO may also refer to:\n\n", "id": "7805", "title": "CNO"}
{"url": "https://en.wikipedia.org/wiki?curid=7806", "text": "Cruising (maritime)\n\nCruising by boat is a lifestyle that involves living for extended time on a vessel while traveling from place to place for pleasure. Cruising generally refers to trips of a few days or more, and can extend to round-the-world voyages.\n\nBoats were almost exclusively used for working purposes prior to the nineteenth century. In 1857, the philosopher Henry David Thoreau, with his book \"Canoeing in Wilderness\" chronicling his canoe voyaging in the wilderness of Maine, was the first to convey the enjoyment of spiritual and lifestyle aspects of cruising.\nThe modern conception of cruising for pleasure was first popularised by the Scottish explorer and sportsman John MacGregor. He was introduced to the canoes and kayaks of the Native Americans on a camping trip in 1858, and on his return to the United Kingdom constructed his own 'double-ended' canoe in Lambeth. The boat, nicknamed 'Rob Roy' after a famous relative of his, was built of lapstrake oak planking, decked in cedar covered with rubberized canvas with an open cockpit in the center. He cruised around the waterways of Britain, Europe and the Middle East and wrote a popular book about his experiences, \"A Thousand Miles in the Rob Roy Canoe\".\n\nIn 1866, Macgregor was a moving force behind the establishment of the Royal Canoe Club, the first club in the world to promote pleasure cruising. The first recorded regatta was held at on 27 April 1867, and it received Royal patronage in 1873. The latter part of the century saw cruising for leisure being enthusiastically taken up by the middle class. The author Robert Louis Stevenson wrote \"An Inland Voyage\" in 1877 as a travelogue on his canoeing trip through France and Belgium. Stevenson and his companion, Sir Walter Grindlay Simpson travelled in two 'Rob Roys' along the Oise River and witnessed the Romantic beauty of rural Europe.\n\nThe Canadian-American Joshua Slocum was one of the first people to carry out a long-distance sailing voyage for pleasure, circumnavigating the world between 1895 and 1898. Despite opinion that such a voyage was impossible, Slocum rebuilt a derelict sloop \"Spray\" and sailed her single-handed around the world. His book \"Sailing Alone Around the World\" was a classic adventure, and inspired many others to take to the seas.\nOther cruising authors have provided both inspiration and instruction to prospective cruisers. Key among these during the post World War II period are Electa and Irving Johnson, Miles and Beryl Smeeton, Bernard Moitessier, Peter Pye, and Eric and Susan Hiscock. During the 1970s - 1990s Robin Lee Graham, Lin and Larry Pardey, Annie Hill, Herb Payson, Linda and Steve Dashew, Margaret and Hal Roth, and Beth Leonard & Evans Starzinger have provided inspiration for people to set off voyaging.\n\nThe development of ocean crossing rallies, most notably the ARC (Atlantic Rally for Cruisers), have encouraged less experienced sailors to undertake ocean crossings. These rallies provide a group of sailors crossing the same ocean at the same time with safety inspections, weather information and social functions.\n\nCruising is done on both sail and power boats, monohulls and multihulls although sail predominates over longer distances, as ocean-going power boats are considerably more expensive to purchase and operate. The size of the typical cruising boat has increased over the years and is currently in the range of 10 to 15 metres (33 to 50 feet) although smaller boats have been used in around-the-world trips, but are generally not recommended given the dangers involved. Many cruisers are \"long term\" and travel for many years, the most adventurous among them circle the globe over a period of three to ten years. Many others take a year or two off from work and school for shorter trips and the chance to experience the cruising lifestyle.\n\nBlue-water cruising is more involved and inherently more dangerous than coastal cruising. \nBefore embarking on an open-ocean voyage, planning and preparation will include studying charts, weather reports/warnings, almanacs and navigation books of the route to be followed. In addition, supplies need to be stocked (including fresh water and fuel), navigation instruments checked and the ship itself needs to be inspected and the crew needs to be given exact instruction on the jobs are expected to perform (e.g. the watch; which is generally 4 hours on and 4 hours off, navigation, steering, rigging sails, ...). In addition, the crew needs to be well trained at working together and with the ship in question. Finally, the sailor must be mentally prepared for dealing with harsh situations. There have been many well-documented cases where sailors had to be rescued simply because they were not sufficiently prepared (the sailors as well as the ship) or lacked experience for their venture and ran into serious trouble.\n\nSailing near the coast (coastal cruising) gives a certain amount of safety. A ship is always granted 'innocent passage' through the country (most countries usually claim up to off the coast). When this method is practiced however, one must still remember that if the ship needs to stop (e.g. for repairs), a trip to a customs checkpoint to have passports checked would be required.\n\nCruisers use a variety of equipment and techniques to make their voyages possible, or simply more comfortable. \nThe use of wind vane self steering was common on long distance cruising yachts but is increasingly being supplemented or replaced by electrical auto-pilots.\n\nThough in the past many cruisers had no means of generating electricity on board and depended on kerosene and dry-cell batteries, today electrical demands are much higher and nearly all cruisers have electrical devices such as lights, communications equipment and refrigeration. Although most boats can generate power from their inboard engines, an increasing number carry auxiliary generators. Carrying sufficient fuel to power engine and generator over a long voyage can be a problem, so many cruising boats are equipped with other ancillary generating devices such as solar panels, wind turbines and towed turbines. Cruisers choosing to spend extended time in very remote locations with minimal access to marinas can opt to equip their vessels with watermakers (reverse-osmosis seawater desalination units) used to convert sea water to potable fresh water.\n\nSatellite communications are becoming more common on cruising boats. Many boats are now equipped with satellite telephone systems; however, these systems can be expensive to use, and may operate only in certain areas. Many cruisers still use short wave maritime SSB and amateur radio, which has no running costs. These radios provide two-way voice communications, can receive weather fax graphics or GRIB files via a laptop computer, and with a compatible modem (e.g. PACTOR) can send and receive email at very slow speed. Such emails are usually limited to basic communication using plain text, without HTML formatting or attachments.\n\nAwareness of impending weather conditions is particularly important to cruising sailors who are often far from safe harbours and need to steer clear of dangerous weather conditions. Most cruising boats are equipped with a barometer or a weather station that records barometric pressure as well as temperature and provides rudimentary forecasting. For more sophisticated weather forecasting, cruisers rely on their ability to receive forecasts by radio, phone or satellite.\n\nIn order to avoid collisions with other vessels, cruisers rely on a maintaining a regular watch schedule. At night, color-coded running lights help determine the position and orientation of vessels. Radar and AIS systems are often employed to detect vessels positions and movement in all conditions (day, night, rain and fog).\n\nCruisers navigate using paper charts and radar. Modern yachts are often also equipped with a chartplotter which enables the use of electronic charts and is linked to GPS satellites that provide position reports. Some chartplotters have the ability to interface charts and radar images. Those that still wish to work with traditional charts as well as with GPS may do so using a Yeoman Plotter. Certain advanced sailing vessels have a completely automated sailing system which includes a plotter, as well as course correcting through a link with the ship's steering organs (e.g. sails, propeller). One such device can be found at the Maltese Falcon.\n\nPurchasing and maintaining a yacht can be costly. Most cruising sailors do not own a house and consider their boat their home during the duration of their cruise. Many cruisers find they spend, on average, 4% of their boat's purchase price annually on boat maintenance.\n\nLike living a conventional life on land, the cost of cruising is variable. How much a person ends up spending depends largely on their spending habits (for example, eating out a lot and frequenting marinas vs. preparing local foods aboard and anchoring out) and the type of boat (fancy modern production boats are very expensive to purchase and maintain, whilst low-key cruising boats often involve much lower expenses). Most long-term cruisers prefer to live a simple life, usually with far lower expenses than people who live ashore.\n\nAn alternative solution is to sail on someone else's yacht. Those who know how to sail can sometimes find boats looking for an extra crewmember for a long trip, whilst some non-sailors are also able to find boats willing to carry a hitch-hiker. Crew-finding websites exist to help match-up people looking for a crossing with yachts with a berth available or looking for a temporary crewmember, Find a Crew for example. Another common tactic for finding a yacht is to visit local yacht clubs and marinas and get to know the sailors there, in the hope that one of them will be able to provide a berth.\n\nTravel by water brings hazards: collision, weather, and equipment failure can lead to dangerous situations such as a sinking or severely disabled and dangerous vessel. For this reason many long distance cruising yachts carry with them emergency equipment such as SARTs, EPIRBs and liferafts or proactive lifeboats. Medical emergencies are also of concern, as a medical emergency can occur on a long passage when the closest port is over a week away. For this reason before going cruising many people go through first aid training and carry medical kits. In some parts of the world (e.g., near the Horn of Africa) piracy can be a problem.\n\n", "id": "7806", "title": "Cruising (maritime)"}
{"url": "https://en.wikipedia.org/wiki?curid=7807", "text": "Cavitation\n\nCavitation is the formation of vapour cavities in a liquid – i.e. small liquid-free zones (\"bubbles\" or \"voids\") – that are the consequence of forces acting upon the liquid. It usually occurs when a liquid is subjected to rapid changes of pressure that cause the formation of cavities where the pressure is relatively low. When subjected to higher pressure, the voids implode and can generate an intense shock wave.\n\nCavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal causing a type of wear also called \"cavitation\". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.\n\nInertial cavitation is the process where a void or bubble in a liquid rapidly collapses, producing a shock wave. Inertial cavitation occurs in nature in the strikes of mantis shrimps and pistol shrimps, as well as in the vascular tissues of plants. In man-made objects, it can occur in control valves, pumps, propellers and impellers.\n\nNon-inertial cavitation is the process in which a bubble in a fluid is forced to oscillate in size or shape due to some form of energy input, such as an acoustic field. Such cavitation is often employed in ultrasonic cleaning baths and can also be observed in pumps, propellers, etc.\n\nSince the shock waves formed by collapse of the voids are strong enough to cause significant damage to moving parts, cavitation is usually an undesirable phenomenon. It is very often specifically avoided in the design of machines such as turbines or propellers, and eliminating cavitation is a major field in the study of fluid dynamics. However, it is sometimes useful and does not cause damage when the bubbles collapse away from machinery, such as in supercavitation.\n\nInertial cavitation was first studied by Lord Rayleigh in the late 19th century, when he considered the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined \"cavitation inception\" and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.\n\nOther ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a perfect vacuum, but has a relatively low gas pressure. Such a low-pressure bubble in a liquid begins to collapse due to the higher pressure of the surrounding medium. As the bubble collapses, the pressure and temperature of the vapor within increases. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.\n\nInertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n\nThe physical process of cavitation inception is similar to boiling. The major difference between the two is the thermodynamic paths that precede the formation of the vapor. Boiling occurs when the local vapor pressure of the liquid rises above its local ambient pressure and sufficient energy is present to cause the phase change to a gas. Cavitation inception occurs when the local pressure falls sufficiently far below the saturated vapor pressure, a value given by the tensile strength of the liquid at a certain temperature.\n\nIn order for cavitation inception to occur, the cavitation \"bubbles\" generally need a surface on which they can nucleate. This surface can be provided by the sides of a container, by impurities in the liquid, or by small undissolved microbubbles within the liquid. It is generally accepted that hydrophobic surfaces stabilize small bubbles. These pre-existing bubbles start to grow unbounded when they are exposed to a pressure below the threshold pressure, termed Blake's threshold.\n\nThe vapor pressure here differs from the meteorological definition of vapor pressure, which describes the partial pressure of water in the atmosphere at some value less than 100% saturation. Vapor pressure as relating to cavitation refers to the vapor pressure in equilibrium conditions and can therefore be more accurately defined as the equilibrium (or saturated) vapor pressure.\n\nNon-inertial cavitation is the process in which small bubbles in a liquid are forced to oscillate in the presence of an acoustic field, when the intensity of the acoustic field is insufficient to cause total bubble collapse. This form of cavitation causes significantly less erosion than inertial cavitation, and is often used for the cleaning of delicate materials, such as silicon wafers.\n\nHydrodynamic cavitation describes the process of vaporisation, bubble generation and bubble implosion which occurs in a flowing liquid as a result of a decrease and subsequent increase in local pressure. Cavitation will only occur if the local pressure declines to some point below the saturated vapor pressure of the liquid and subsequent recovery above the vapor pressure. If the recovery pressure is not above the vapor pressure then flashing is said to have occurred. In pipe systems, cavitation typically occurs either as the result of an increase in the kinetic energy (through an area constriction) or an increase in the pipe elevation.\n\nHydrodynamic cavitation can be produced by passing a liquid through a constricted channel at a specific flow velocity or by mechanical rotation of an object through a liquid. In the case of the constricted channel and based on the specific (or unique) geometry of the system, the combination of pressure and kinetic energy can create the hydrodynamic cavitation cavern downstream of the local constriction generating high energy cavitation bubbles.\n\nThe process of bubble generation, and the subsequent growth and collapse of the cavitation bubbles, results in very high energy densities and in very high local temperatures and local pressures at the surface of the bubbles for a very short time. The overall liquid medium environment, therefore, remains at ambient conditions. When uncontrolled, cavitation is damaging; by controlling the flow of the cavitation, however, the power can be harnessed and non-destructive. Controlled cavitation can be used to enhance chemical reactions or propagate certain unexpected reactions because free radicals are generated in the process due to disassociation of vapors trapped in the cavitating bubbles. .\n\nOrifices and venturi are reported to be widely used for generating cavitation. A venturi has an inherent advantage over an orifice because of its smooth converging and diverging sections, such that it can generate a higher flow velocity at the throat for a given pressure drop across it. On the other hand, an orifice has an advantage that it can accommodate a greater number of holes (larger perimeter of holes) in a given cross sectional area of the pipe.\n\nThe cavitation phenomenon can be controlled to enhance the performance of high-speed marine vessels and projectiles, as well as in material processing technologies, in medicine, etc. Controlling the cavitating flows in liquids can be achieved only by advancing the mathematical foundation of the cavitation processes. These processes are manifested in different ways, the most common ones and promising for control being bubble cavitation and supercavitation. The first exact classical solution should perhaps be credited to the well- known solution by H. Helmholtz in 1868. The earliest distinguished studies of academic type on the theory of a cavitating flow with free boundaries and supercavitation were published in the book \"Jets, wakes and cavities\" followed by \"Theory of jets of ideal fluid\". Widely used in these books was the well-developed theory of conformal mappings of functions of a complex variable, allowing one to derive a large number of exact solutions of plane problems. Another venue combining the existing exact solutions with approximated and heuristic models was explored in the work \"Hydrodynamics of Flows with Free Boundaries\" that refined the applied calculation techniques based on the principle of cavity expansion independence, theory of pulsations and stability of elongated axisymmetric cavities, etc. and in \"Dimensionality and similarity methods in the problems of the hydromechanics of vessels\".\n\nA natural continuation of these studies was recently presented in \"The Hydrodynamics of Cavitating Flows\" – an encyclopedic work encompassing all the best advances in this domain for the last three decades, and blending the classical methods of mathematical research with the modern capabilities of computer technologies. These include elaboration of nonlinear numerical methods of solving 3D cavitation problems, refinement of the known plane linear theories, development of asymptotic theories of axisymmetric and nearly axisymmetric flows, etc. As compared to the classical approaches, the new trend is characterized by expansion of the theory into the 3D flows. It also reflects a certain correlation with current works of an applied character on the hydrodynamics of supercavitating bodies.\n\nHydrodynamic cavitation can also improve some industrial processes. For instance, cavitated corn slurry shows higher yields in ethanol production compared to uncavitated corn slurry in dry milling facilities.\n\nThis is also used in the mineralization of bio-refractory compounds which otherwise would need extremely high temperature and pressure conditions since free radicals are generated in the process due to the dissociation of vapors trapped in the cavitating bubbles, which results in either the intensification of the chemical reaction or may even result in the propagation of certain reactions not possible under otherwise ambient conditions.\n\nIn industry, cavitation is often used to homogenize, or mix and break down, suspended particles in a colloidal liquid compound such as paint mixtures or milk. Many industrial mixing machines are based upon this design principle. It is usually achieved through impeller design or by forcing the mixture through an annular opening that has a narrow entrance orifice with a much larger exit orifice. In the latter case, the drastic decrease in pressure as the liquid accelerates into a larger volume induces cavitation. This method can be controlled with hydraulic devices that control inlet orifice size, allowing for dynamic adjustment during the process, or modification for different substances. The surface of this type of mixing valve, against which surface the cavitation bubbles are driven causing their implosion, undergoes tremendous mechanical and thermal localized stress; they are therefore often constructed of super-hard or tough materials such as stainless steel, Stellite, or even polycrystalline diamond (PCD).\n\nCavitating water purification devices have also been designed, in which the extreme conditions of cavitation can break down pollutants and organic molecules. Spectral analysis of light emitted in sonochemical reactions reveal chemical and plasma-based mechanisms of energy transfer. The light emitted from cavitation bubbles is termed sonoluminescence.\n\nUse of this technology has been tried successfully in alkali refining of vegetable oils.\n\nHydrophobic chemicals are attracted underwater by cavitation as the pressure difference between the bubbles and the liquid water forces them to join together. This effect may assist in protein folding.\n\nCavitation plays an important role for the destruction of kidney stones in shock wave lithotripsy. Currently, tests are being conducted as to whether cavitation can be used to transfer large molecules into biological cells (sonoporation). Nitrogen cavitation is a method used in research to lyse cell membranes while leaving organelles intact.\nCavitation plays a key role in non-thermal non-invasive fractionation of tissue for treatment of a variety of diseases. Cavitation also probably plays a role in HIFU, a thermal noninvasive treatment methodology for cancer.\n\nUltrasound is sometimes used to increase bone formation, for instance in post-surgical applications.\nUltrasound treatments and/or exposure can create cavitation that can potentially \"result in a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness, and fatigue.\".\n\nIt has been suggested that the sound of \"cracking\" knuckles derives from the collapse of cavitation in the synovial fluid within the joint. Movements that cause cracking expand the joint space, thus reducing pressure to the point of cavitation. It remains controversial whether this is associated with clinically significant joint injury such as osteoarthritis. Some physicians say that osteoarthritis is caused by cracking knuckles regularly, as this causes wear and tear and may cause the bone to weaken. It is not the \"bubbles popping,\" but rather the bones' rubbing together that causes osteoarthritis.\n\nIn industrial cleaning applications, cavitation has sufficient power to overcome the particle-to-substrate adhesion forces, loosening contaminants. The threshold pressure required to initiate cavitation is a strong function of the pulse width and the power input. This method works by generating controlled acoustic cavitation in the cleaning fluid, picking up and carrying contaminant particles away so that they do not reattach to the material being cleaned.\n\nCavitation is, in many cases, an undesirable occurrence. In devices such as propellers and pumps, cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. Cavitation has also become a concern in the renewable energy sector as it may occur on the blade surface of tidal stream turbines.\n\nWhen the cavitation bubbles collapse, they force energetic liquid into very small volumes, thereby creating spots of high temperature and emitting shock waves, the latter of which are a source of noise. The noise created by cavitation is a particular problem for military submarines, as it increases the chances of being detected by passive sonar.\n\nAlthough the collapse of a small cavity is a relatively low-energy event, highly localized collapses can erode metals, such as steel, over time. The pitting caused by the collapse of cavities produces great wear on components and can dramatically shorten a propeller or pump's lifetime.\n\nAfter a surface is initially affected by cavitation, it tends to erode at an accelerating pace. The cavitation pits increase the turbulence of the fluid flow and create crevices that act as nucleation sites for additional cavitation bubbles. The pits also increase the components' surface area and leave behind residual stresses. This makes the surface more prone to stress corrosion.\n\nMajor places where cavitation occurs are in pumps, on propellers, or at restrictions in a flowing liquid.\n\nAs an impeller's (in a pump) or propeller's (as in the case of a ship or submarine) blades move through a fluid, low-pressure areas are formed as the fluid accelerates around and moves past the blades. The faster the blade moves, the lower the pressure around it can become. As it reaches vapor pressure, the fluid vaporizes and forms small bubbles of gas. This is cavitation. When the bubbles collapse later, they typically cause very strong local shock waves in the fluid, which may be audible and may even damage the blades.\n\nCavitation in pumps may occur in two different forms:\n\nSuction cavitation occurs when the pump suction is under a low-pressure/high-vacuum condition where the liquid turns into a vapor at the eye of the pump impeller. This vapor is carried over to the discharge side of the pump, where it no longer sees vacuum and is compressed back into a liquid by the discharge pressure. This imploding action occurs violently and attacks the face of the impeller. An impeller that has been operating under a suction cavitation condition can have large chunks of material removed from its face or very small bits of material removed, causing the impeller to look spongelike. Both cases will cause premature failure of the pump, often due to bearing failure. Suction cavitation is often identified by a sound like gravel or marbles in the pump casing.\n\nIn automotive applications, a clogged filter in a hydraulic system (power steering, power brakes) can cause suction cavitation making a noise that rises and falls in synch with engine RPM. It is fairly often a high pitched whine, like set of nylon gears not quite meshing correctly.\n\nDischarge cavitation occurs when the pump discharge pressure is extremely high, normally occurring in a pump that is running at less than 10% of its best efficiency point. The high discharge pressure causes the majority of the fluid to circulate inside the pump instead of being allowed to flow out the discharge. As the liquid flows around the impeller, it must pass through the small clearance between the impeller and the pump housing at extremely high flow velocity. This flow velocity causes a vacuum to develop at the housing wall (similar to what occurs in a venturi), which turns the liquid into a vapor. A pump that has been operating under these conditions shows premature wear of the impeller vane tips and the pump housing. In addition, due to the high pressure conditions, premature failure of the pump's mechanical seal and bearings can be expected. Under extreme conditions, this can break the impeller shaft.\n\nDischarge cavitation in joint fluid is thought to cause the popping sound produced by bone joint cracking, for example by deliberately cracking one's knuckles.\n\nSince all pumps require well-developed inlet flow to meet their potential, a pump may not perform or be as reliable as expected due to a faulty suction piping layout such as a close-coupled elbow on the inlet flange. When poorly developed flow enters the pump impeller, it strikes the vanes and is unable to follow the impeller passage. The liquid then separates from the vanes causing mechanical problems due to cavitation, vibration and performance problems due to turbulence and poor filling of the impeller. This results in premature seal, bearing and impeller failure, high maintenance costs, high power consumption, and less-than-specified head and/or flow.\n\nTo have a well-developed flow pattern, pump manufacturer's manuals recommend about (10 diameters?) of straight pipe run upstream of the pump inlet flange. Unfortunately, piping designers and plant personnel must contend with space and equipment layout constraints and usually cannot comply with this recommendation. Instead, it is common to use an elbow close-coupled to the pump suction which creates a poorly developed flow pattern at the pump suction.\n\nWith a double-suction pump tied to a close-coupled elbow, flow distribution to the impeller is poor and causes reliability and performance shortfalls. The elbow divides the flow unevenly with more channeled to the outside of the elbow. Consequently, one side of the double-suction impeller receives more flow at a higher flow velocity and pressure while the starved side receives a highly turbulent and potentially damaging flow. This degrades overall pump performance (delivered head, flow and power consumption) and causes axial imbalance which shortens seal, bearing and impeller life.\nTo overcome cavitation:\nIncrease suction pressure if possible.\nDecrease liquid temperature if possible.\nThrottle back on the discharge valve to decrease flow-rate.\nVent gases off the pump casing.\n\nCavitation can occur in control valves. If the actual pressure drop across the valve as defined by the upstream and downstream pressures in the system is greater than the sizing calculations allow, pressure drop flashing or cavitation may occur. The change from a liquid state to a vapor state results from the increase in flow velocity at or just downstream of the greatest flow restriction which is normally the valve port. To maintain a steady flow of liquid through a valve the flow velocity must be greatest at the vena contracta or the point where the cross sectional area is the smallest. This increase in flow velocity is accompanied by a substantial decrease in the fluid pressure which is partially recovered downstream as the area increases and flow velocity decreases. This pressure recovery is never completely to the level of the upstream pressure. If the pressure at the vena contracta drops below the vapor pressure of the fluid bubbles will form in the flow stream. If the pressure recovers after the valve to a pressure that is once again above the vapor pressure, then the vapor bubbles will collapse and cavitation will occur.\n\nWhen water flows over a dam spillway, the irregularities on the spillway surface will cause small areas of flow separation in a high-speed flow, and, in these regions, the pressure will be lowered. If the flow velocities are high enough the pressure may fall to below the local vapor pressure of the water and vapor bubbles will form. When these are carried downstream into a high pressure region the bubbles collapse giving rise to high pressures and possible cavitation damage.\n\nExperimental investigations show that the damage on concrete chute and tunnel spillways can start at clear water flow velocities of between 12 and 15 m/s, and, up to flow velocities of 20 m/s, it may be possible to protect the surface by streamlining the boundaries, improving the surface finishes or using resistant materials.\n\nWhen some air is present in the water the resulting mixture is compressible and this damps the high pressure caused\nby the bubble collapses. If the flow velocities near the spillway invert are sufficiently high, aerators (or aeration devices) must be introduced to prevent cavitation. Although these have been installed for some years, the mechanisms of air entrainment at the aerators and the slow movement of the air away from the spillway surface are still challenging.\n\nThe spillway aeration device design is based upon a small deflection of the spillway bed (or sidewall) such as a ramp and offset to deflect the high flow velocity flow away from the spillway surface. In the cavity formed below the nappe, a local subpressure beneath the nappe is produced by which air is sucked into the flow. The complete design includes the deflection device (ramp, offset) and the air supply system.\n\nSome larger diesel engines suffer from cavitation due to high compression and undersized cylinder walls. Vibrations of the cylinder wall induce alternating low and high pressure in the coolant against the cylinder wall. The result is pitting of the cylinder wall, which will eventually let cooling fluid leak into the cylinder and combustion gases to leak into the coolant.\n\nIt is possible to prevent this from happening with the use of chemical additives in the cooling fluid that form a protective layer on the cylinder wall. This layer will be exposed to the same cavitation, but rebuilds itself. Additionally a regulated overpressure in the cooling system (regulated and maintained by the coolant filler cap spring pressure) prevents the forming of cavitation.\n\nFrom about the 1980s, new designs of smaller gasoline engines also displayed cavitation phenomena. One answer to the need for smaller and lighter engines was a smaller coolant volume and a correspondingly higher coolant flow velocity. This gave rise to rapid changes in flow velocity and therefore rapid changes of static pressure in areas of high heat transfer. Where resulting vapor bubbles collapsed against a surface, they had the effect of first disrupting protective oxide layers (of cast aluminium materials) and then repeatedly damaging the newly formed surface, preventing the action of some types of corrosion inhibitor (such as silicate based inhibitors). A final problem was the effect that increased material temperature had on the relative electrochemical reactivity of the base metal and its alloying constituents. The result was deep pits that could form and penetrate the engine head in a matter of hours when the engine was running at high load and high speed. These effects could largely be avoided by the use of organic corrosion inhibitors or (preferably) by designing the engine head in such a way as to avoid certain cavitation inducing conditions.\n\nSome hypotheses relating to diamond formation posit a possible role for cavitation—namely cavitiation in the kimberlite pipes providing the extreme pressure needed to change pure carbon into the rare allotrope that is diamond.\n\nThe loudest three sounds ever recorded, during the 1883 eruption of Krakatoa, are now understood as the bursts of three huge cavitation bubbles, each larger than the last, formed in the volcano's throat. Rising magma, filled with dissolved gasses and under immense pressure, encountered a different magma that compressed easily, allowing bubbles to grow and combine. \n\nCavitation occurs in the xylem of vascular plants when the tension of water within the xylem becomes so great that liquid water (or sap) vaporizes locally and dissolved air within the water expands to fill either the vessel elements or tracheids. Plants are generally able to repair cavitated xylem in a number of ways. For plants less than 50 cm tall, root pressure can be sufficient to redissolve air. For larger plants, they must repair cavitation by importing solutes into the xylem via \"ray cells\", or in tracheids, via osmosis through bordered pits; this causes water to enter as well, which can then redissolve the air. In some trees, the sound of the cavitation is clearly audible, particularly in summer, when the rate of evapotranspiration is highest, and can be used to determine the rate of cavitation. Deciduous trees shed leaves in the autumn partly because cavitation increases as temperatures decrease.\n\nJust as cavitation bubbles form on a fast-spinning boat propeller, they may also form on the tails and fins of aquatic animals. The effects of cavitation are especially important near the surface of the ocean, where the ambient water pressure is relatively low and cavitation is more likely to occur.\n\nFor powerful swimming animals like dolphins and tuna, cavitation may be detrimental, because it limits their maximum swimming speed. Even if they have the power to swim faster, dolphins may have to restrict their speed because collapsing cavitation bubbles on their tail are very painful. Cavitation also slows tuna, but for a different reason. Unlike dolphins, these fish do not feel the painful bubbles, because they have bony fins without nerve endings. Nevertheless, they cannot swim faster because the cavitation bubbles create a vapor film around their fins that limits their speed. Lesions have been found on tuna that are consistent with cavitation damage.\n\nCavitation is not always a limitation for sea life; some animals have found ways to use it to their advantage when hunting prey. The pistol shrimp snaps a specialized claw to create cavitation, which can kill small fish. The mantis shrimp (of the \"smasher\" variety) uses cavitation as well in order to stun, smash open, or kill the shellfish that it feasts upon.\n\nThresher sharks use 'tail slaps' to debilitate their small fish prey and cavitation bubbles have been seen rising from the apex of the tail arc.\n\nIn the last half-decade, coastal erosion in the form of inertial cavitation has been generally accepted. Vapor pockets in an incoming wave are forced into cracks in the cliff being eroded, then the force of the wave compresses the vapor pockets until the bubble implodes, becoming liquid, giving off various forms of energy that blast apart the rock.\n\n\n\n", "id": "7807", "title": "Cavitation"}
{"url": "https://en.wikipedia.org/wiki?curid=7808", "text": "Cyprinodontiformes\n\nCyprinodontiformes is an order of ray-finned fish, comprising mostly small, freshwater fish. Many popular aquarium fish, such as killifish and live-bearers, are included. They are closely related to the Atheriniformes and are occasionally included with them. A colloquial term for the order as a whole is toothcarps, though they are not actually close relatives of the true carps – the latter belong to the superorder Ostariophysi, while the toothcarps are Acanthopterygii.\n\nThe families of Cyprinodontiformes can be divided into three groups: viviparous and ovoviviparous (all species give live birth), and oviparous (all species egg-laying). The live-bearing groups differ in whether the young are carried to term within (ovoviviparous) or without (viviparous) an enclosing eggshell. Phylogenetically however, one of the two suborders – the Aplocheiloidei – contains oviparous species exclusively, as do two of the four superfamilies of the other suborder (the Cyprinodontoidea and Valencioidea of the Cyprinodontoidei). Vivipary and ovovivipary have evolved independently from oviparous ancestors, the latter possibly twice.\n\nMembers of this order are notable for inhabiting harsh environments, such as saline or very warm waters, water of poor quality, or isolated situations where no other types of fish occur. They are typically omnivores, and often live near the surface, where the oxygen-rich water compensates for environmental disadvantages.\n\nThey are small to medium-sized fish, with small mouths, large eyes, a single dorsal fin, and a rounded caudal fin. The largest species is the \"cuatro ojos\" (\"Anableps dowi\"), which measures in length, while the smallest, the least killifish (\"Heterandria formosa\"), is just long as an adult.\n\nCYPRINODONTIFORMES\n", "id": "7808", "title": "Cyprinodontiformes"}
{"url": "https://en.wikipedia.org/wiki?curid=7810", "text": "Church of the Holy Sepulchre\n\nThe Church of the Holy Sepulchre (; also called the Church of the Resurrection or Church of the \"Anastasis\" by Orthodox Christians) is a church in the Christian Quarter of the Old City of Jerusalem, a few steps away from the Muristan. The church contains, according to traditions dating back at least to the fourth century, the two holiest sites in Christianity: the site where Jesus of Nazareth was crucified, known as \"Calvary\" or \"Golgotha\", and Jesus's empty tomb, where he is said to have been buried and resurrected. The tomb is enclosed by the 18th-century shrine, called the Edicule (Aedicule).\n\nWithin the church proper are the last four (or, by some definitions, five) Stations of the Via Dolorosa, representing the final episodes of Jesus' Passion. The church has been a major Christian pilgrimage destination since its creation in the fourth century, as the traditional site of the Resurrection of Christ, thus its original Greek name, Church of the Anastasis.\n\nToday, the wider complex accumulated during the centuries around the Church of the Holy Sepulchre also serves as the headquarters of the Greek Orthodox Patriarch of Jerusalem, while control of the church itself is shared between several Christian denominations and secular entities in complicated arrangements essentially unchanged for over 160 years, and some for much longer. The main denominations sharing property over parts of the church are the Greek Orthodox, Armenian Orthodox and Roman Catholic, and to a lesser degree the Egyptian Copts, Syriacs and Ethiopians. Meanwhile, Protestants, including Anglicans, have no permanent presence in the Church and they generally prefer the Garden Tomb, elsewhere in Jerusalem, as either the true place of Jesus' crucifixion and resurrection, or at least a more evocative site to commemorate those events.\n\nAccording to Eusebius of Caesarea, the Roman emperor Hadrian in the 2nd century AD built a temple dedicated to the goddess Aphrodite in order to bury the cave in which Jesus had been buried. The first Christian emperor, Constantine the Great, ordered in about 325/326 that the temple be replaced by a church. During the building of the Church, Constantine's mother, Helena, is believed to have rediscovered the tomb (although there are some discrepancies among authors). Socrates Scholasticus (born c. 380), in his \"Ecclesiastical History,\" gives a full description of the discovery. \n\nConstantine's church was built as two connected churches over the two different holy sites, including a great basilica (the \"Martyrium\" visited by Egeria in the 380s), an enclosed colonnaded atrium (the \"Triportico\") with the traditional site of \"Golgotha\" in one corner, and a rotunda, called the \"Anastasis\" (\"Resurrection\" in Greek), which contained the remains of a rock-cut room that Helena and Macarius identified as the burial site of Jesus.\n\nAccording to tradition, Constantine arranged for the rockface to be removed from around the tomb, without harming it, in order to isolate the tomb; in the centre of the rotunda is a small building called the \"Kouvouklion\" in Greek or the \"Aedicula\" in Latin, which encloses this tomb. The remains are completely enveloped by a marble sheath placed some 500 years before to protect the ledge from Ottoman attacks. However, there are several thick window wells extending through the marble sheath, from the interior to the exterior that are not marble clad. They appear to reveal an underlying limestone rock, which may be part of the original living rock of the tomb.\n\nThe church was built starting in 325/326, and was consecrated on 13 September 335. From pilgrim reports it seems that the chapel housing the tomb of Jesus was freestanding at first, and that the Rotunda was only erected around the chapel in the 380s.\nEach year, the Eastern Orthodox Church celebrates the anniversary of the consecration of the Church of the Resurrection (Holy Sepulchre) on 13 September.\n\nThis building was damaged by fire in May of 614 when the Sassanid Empire, under Khosrau II, invaded Jerusalem and captured the True Cross. In 630, the Emperor Heraclius restored it and rebuilt the church after recapturing the city. After Jerusalem was captured by the Arabs, it remained a Christian church, with the early Muslim rulers protecting the city's Christian sites. A story reports that the Caliph Umar ibn al-Khattab visited the church and stopped to pray on the balcony; but at the time of prayer, he turned away from the church and prayed outside. He feared that future generations would misinterpret this gesture, taking it as a pretext to turn the church into a mosque. Eutychius added that Umar wrote a decree prohibiting Muslims from praying at this location. The building suffered severe damage due to an earthquake in 746.\n\nEarly in the ninth century, another earthquake damaged the dome of the Anastasis. The damage was repaired in 810 by Patriarch Thomas. In the year 841, the church suffered a fire. In 935, the Orthodox Christians prevented the construction of a Muslim mosque adjacent the Church. In 938, a new fire damaged the inside of the basilica and came close to the rotunda. In 966, due to a defeat of Muslim armies in the region of Syria, a riot broke out and was followed by reprisals. The basilica was burned again. The doors and roof were burnt, and the Patriarch John VII was murdered.\n\nOn 18 October 1009, Fatimid caliph Al-Hakim bi-Amr Allah ordered the complete destruction of the church as part of a more general campaign against Christian places of worship in Palestine and Egypt. The damage was extensive, with few parts of the early church remaining. Christian Europe reacted with shock and expulsions of Jews (for example, Cluniac monk Rodulfus Glaber blamed the Jews, with the result that Jews were expelled from Limoges and other French towns) and an impetus to later Crusades.\n\nIn wide-ranging negotiations between the Fatimids and the Byzantine Empire in 1027–8, an agreement was reached whereby the new Caliph Ali az-Zahir (Al-Hakim's son) agreed to allow the rebuilding and redecoration of the Church. The rebuilding was finally completed with the financing at a huge expense by Emperor Constantine IX Monomachos and Patriarch Nicephorus of Constantinople in 1048. As a concession, the mosque in Constantinople was re-opened and sermons were to be pronounced in az-Zahir's name. Muslim sources say a by-product of the agreement was the recanting of Islam by many Christians who had been forced to convert under Al-Hakim's persecutions. In addition, the Byzantines, while releasing 5,000 Muslim prisoners, made demands for the restoration of other churches destroyed by Al-Hakim and the re-establishment of a Patriarch in Jerusalem. Contemporary sources credit the emperor with spending vast sums in an effort to restore the Church of the Holy Sepulchre after this agreement was made. Despite the Byzantines spending vast sums on the project, \"a total replacement was far beyond available resources. The new construction was concentrated on the rotunda and its surrounding buildings: the great basilica remained in ruins.\" The rebuilt church site consisted of \"a court open to the sky, with five small chapels attached to it.\" The chapels were to the east of the court of resurrection, where the wall of the great church had been. They commemorated scenes from the passion, such as the location of the prison of Christ and of his flagellation, and presumably were so placed because of the difficulties of free movement among shrines in the streets of the city. The dedication of these chapels indicates the importance of the pilgrims' devotion to the suffering of Christ. They have been described as 'a sort of Via Dolorosa in miniature'... since little or no rebuilding took place on the site of the great basilica. Western pilgrims to Jerusalem during the eleventh century found much of the sacred site in ruins.\" Control of Jerusalem, and thereby the Church of the Holy Sepulchre, continued to change hands several times between the Fatimids and the Seljuk Turks (loyal to the Abbasid caliph in Baghdad) until the arrival of the Crusaders in 1099.\n\nMany historians maintain that the main concern of Pope Urban II, when calling for the First Crusade, was the threat to Constantinople from the Turkish invasion of Asia Minor in response to the appeal of Byzantine Emperor Alexios I Komnenos. Historians agree that the fate of Jerusalem and thereby the Church of the Holy Sepulchre was of concern if not the immediate goal of papal policy in 1095. The idea of taking Jerusalem gained more focus as the Crusade was underway. The rebuilt church site was taken from the Fatimids (who had recently taken it from the Abassids) by the knights of the First Crusade on 15 July 1099.\nThe First Crusade was envisioned as an armed pilgrimage, and no crusader could consider his journey complete unless he had prayed as a pilgrim at the Holy Sepulchre. Crusader Prince Godfrey of Bouillon, who became the first crusader monarch of Jerusalem, decided not to use the title \"king\" during his lifetime, and declared himself \"Advocatus Sancti Sepulchri\" (\"Protector [or Defender] of the Holy Sepulchre\"). By the crusader period, a cistern under the former basilica was rumoured to have been the location where Helena had found the True Cross, and began to be venerated as such; although the cistern later became the \"Chapel of the Invention of the Cross,\" there is no evidence of the rumour before the 11th century, and modern archaeological investigation has now dated the cistern to 11th century repairs by Monomachos.\n\nAccording to the German clergyman and orient pilgrim Ludolf von Sudheim, the keys of the Chapel of the Holy Sepulchre were in hands of the \"ancient Georgians\" and the food, alms, candles and oil for lamps were given them by the pilgrims in the south door of the church.\n\nWilliam of Tyre, chronicler of the Crusader Kingdom of Jerusalem, reports on the renovation of the Church in the mid-12th century. The crusaders investigated the eastern ruins on the site, occasionally excavating through the rubble, and while attempting to reach the cistern, they discovered part of the original ground level of Hadrian's temple enclosure; they decided to transform this space into a chapel dedicated to Helena (the Chapel of Saint Helena), widening their original excavation tunnel into a proper staircase. The crusaders began to refurnish the church in a Romanesque style and added a bell tower. These renovations unified the small chapels on the site and were completed during the reign of Queen Melisende in 1149, placing all the Holy places under one roof for the first time. The church became the seat of the first Latin Patriarchs, and was also the site of the kingdom's scriptorium. The church was lost to Saladin, along with the rest of the city, in 1187, although the treaty established after the Third Crusade allowed for Christian pilgrims to visit the site. Emperor Frederick II (r. 1220–50) regained the city and the church by treaty in the 13th century while he himself was under a ban of excommunication, with the curious consequence that the holiest church in Christianity was laid under interdict. The church seems to have been largely in Greek Orthodox Patriarch Athanasius II of Jerusalem's hands, ca. 1231–47, during the Latin control of Jerusalem. Both city and church were captured by the Khwarezmians in 1244.\n\nThe Franciscan friars renovated it further in 1555, as it had been neglected despite increased numbers of pilgrims. The Franciscans rebuilt the Aedicule, extending the structure to create an ante-chamber. After the renovation of 1555, control of the church oscillated between the Franciscans and the Orthodox, depending on which community could obtain a favorable \"firman\" from the \"Sublime Porte\" at a particular time, often through outright bribery, and violent clashes were not uncommon. There was no agreement about this question, although it was discussed at the negotiations to the Treaty of Karlowitz in 1699. In 1767, weary of the squabbling, the \"Porte\" issued a \"firman\" that divided the church among the claimants.\n\nA fire severely damaged the structure again in 1808, causing the dome of the Rotunda to collapse and smashing the Edicule's exterior decoration. The Rotunda and the Edicule's exterior were rebuilt in 1809–1810 by architect Nikolaos Ch. Komnenos of Mytilene in the then current Ottoman Baroque style. The fire did not reach the interior of the Aedicule, and the marble decoration of the Tomb dates mainly to the 1555 restoration, although the interior of the ante-chamber, now known as the \"Chapel of the Angel,\" was partly rebuilt to a square ground-plan, in place of the previously semi-circular western end. Another decree in 1853 from the sultan solidified the existing territorial division among the communities and set a \"status quo\" for arrangements to \"remain forever,\" causing differences of opinion about upkeep and even minor changes, including disagreement on the removal of the \"Immovable Ladder,\" an exterior ladder under one of the windows; this ladder has remained in the same position since then.\nThe cladding of red marble applied to the Aedicule by Komnenos has deteriorated badly and is detaching from the underlying structure; since 1947 it has been held in place with an exterior scaffolding of iron girders installed by the British authorities. A careful renovation is undergoing, funded by a $4 million gift from King Abdullah II of Jordan and a $1.3-million gift from Mica Ertegun.\n\nThe current dome dates from 1870, although it was restored between 1994–1997, as part of extensive modern renovations to the church which have been ongoing since 1959. During the 1970–1978 restoration works and excavations inside the building, and under the nearby Muristan, it was found that the area was originally a quarry, from which white \"meleke\" limestone was struck. To the east of the \"Chapel of Saint Helena\", the excavators discovered a void containing a 2nd-century drawing of a Roman ship, two low walls which supported the platform of Hadrian's 2nd-century temple, and a higher 4th-century wall built to support Constantine's basilica. After the excavations of the early 1970s, the Armenian authorities converted this archaeological space into the Chapel of Saint Vartan, and created an artificial walkway over the quarry on the north of the chapel, so that the new Chapel could be accessed (by permission) from the \"Chapel of Saint Helena\".\n\nIn 2016, restoration works were performed in the Edicule. For the first time since at least 1555, marble cladding which protected the estimated burial bed of Jesus from vandalism and souvenir takers was removed. When the cladding was first removed on October 26, an initial inspection by the National Technical University of Athens team showed only a layer of fill material underneath. By the night of October 28, the original limestone burial bed was revealed intact. This suggested that the tomb location has not changed through time and confirmed the existence of the original limestone cave walls within the Edicule. The tomb was resealed shortly thereafter.\n\nThe entrance to the church, a single door in the south transept—through the crusader façade—is found past a group of streets winding through the outer Via Dolorosa, by way of a local souq in the Muristan. This narrow way of access to such a large structure has proven to be hazardous at times. For example, when a fire broke out in 1840, dozens of pilgrims were trampled to death.\n\nHistorically, two large, arched doors allowed access to the church. However, only the left-hand entrance is currently accessible, as the right door has long since been bricked up. These entrances are located in the parvis of a larger courtyard, or plaza.\n\nAlso located along the parvis are a few smaller structures and openings:\n\n\nBroken columns—once forming part of an arcade—flank the church's front, which is covered in crusader graffiti mostly consisting of crosses. In the 13th century, the tops of the columns were removed and sent to Mecca by the Khwarezmids.\n\nThe church's bell tower is located to the left of the façade. It is currently almost half its original size.\n\nThe historic Immovable Ladder stands beneath a window on the façade.\n\nOn the south side of the altar, via the ambulatory, is a stairway climbing to Calvary (Golgotha), traditionally regarded as the site of Jesus' crucifixion and the most lavishly decorated part of the church. The main altar there belongs to the Greek Orthodox, which contains the Rock of Calvary (12th Station of the Cross). The rock can be seen under glass on both sides of the altar, and beneath the altar there is a hole said to be the place where the cross was raised. Due to the significance of this, it is the most visited site in the Church of the Holy Sepulchre. The Roman Catholics (Franciscans) have an altar to the side, the Chapel of the Nailing of the Cross (11th Station of the Cross). On the left of the altar, towards the Eastern Orthodox chapel, there is a statue of Mary, believed by some to be miraculous (the 13th Station of the Cross, where Jesus' body was removed from the cross and given to his family).\n\nBeneath the Calvary and the two chapels there, on the main floor, there is the Chapel of Adam. According to tradition, Jesus was crucified over the place where Adam's skull was buried. According to some, at the crucifixion, the blood of Christ ran down the cross and through the rocks to fill the skull of Adam. The Rock of Calvary appears cracked through a window on the altar wall, with the crack traditionally claimed to be caused by the earthquake that occurred when Jesus died on the cross, while some scholars claim it to be the result of quarrying against a natural flaw in the rock.\n\nJust inside the entrance to the church is the Stone of Anointing (also Stone of the Anointing or Stone of Unction), which tradition believes to be the spot where Jesus' body was prepared for burial by Joseph of Arimathea. However, this tradition is only attested since the crusader era (notably by the Italian Dominican pilgrim Riccoldo da Monte di Croce in 1288), and the present stone was only added in the 1810 reconstruction.\n\nThe wall behind the stone is defined by its striking blue balconies and tau cross-bearing red banners (depicting the insignia of the Brotherhood of the Holy Sepulchre), and is decorated with lamps. The modern mosaic along the wall depicts the anointing of Jesus' body.\n\nThe wall was a temporary addition to support the arch above it, which had been weakened after the damage in the 1808 fire; it blocks the view of the rotunda, separates the entrance from the Catholicon, sits on top of the now-empty and desecrated graves of four 12th-century crusader kings—including Godfrey of Bouillon and Baldwin I of Jerusalem—and is no longer structurally necessary. There is a difference of opinion as to whether it is the 13th Station of the Cross, which others identify as the lowering of Jesus from the cross and locate between the 11th and 12th stations up on Calvary.\n\nThe lamps that hang over the Stone of Unction, adorned with cross-bearing chain links, are contributed by Armenians, Copts, Greeks and Latins.\n\nImmediately to the left of the entrance is a bench that has traditionally been used by the church's Muslim doorkeepers, along with some Christian clergy, as well as electrical wiring. To the right of the entrance is a wall along the ambulatory containing, to the very right, the staircase leading to Golgatha. Further along the same wall is the entrance to the Chapel of Adam.\n\nThe Rotunda is located in the centre of the Anastasis, beneath the larger of the church's two domes. In the center of the Rotunda is the chapel called the Aedicule, which contains the Holy Sepulchre itself. The Aedicule has two rooms, the first holding the Angel's Stone, which is believed to be a fragment of the large stone that sealed the tomb; the second is the tomb itself. Possibly due to the fact that pilgrims laid their hands on the tomb and/or to prevent eager pilgrims from removing bits of the original rock as souvenirs, a marble plaque was placed in the fourteenth century on the tomb to prevent further damage to the tomb.\n\nUnder the \"status quo\", the Eastern Orthodox, Roman Catholic, and Armenian Apostolic Churches all have rights to the interior of the tomb, and all three communities celebrate the Divine Liturgy or Holy Mass there daily. It is also used for other ceremonies on special occasions, such as the Holy Saturday ceremony of the Holy Fire led by the Greek Orthodox Patriarch (with the participation of the Coptic and Armenian patriarchs). To its rear, in a chapel constructed of iron latticework upon a stone base semicircular in plan, lies the altar used by the Coptic Orthodox. Historically, the Georgians also retained the key to the Aedicule.\n\nBeyond that to the rear of the Rotunda is a rough-hewn chapel containing an opening to a chamber cut from the rock, from which several \"kokh\"-tombs radiate. Although this space was discovered recently, and contains no identifying marks, many Christians believe it to be the tomb of Joseph of Arimathea, and it is where the Syriac Orthodox celebrate their Liturgy on Sundays. To the right of the Sepulchre on the northwestern edge of the Rotunda is the Chapel of the Apparition, which is reserved for Roman Catholic use.\n\nThe Aedicule is being renovated, planned to complete Spring 2017, photographs of the renovation were made, 30 people were allowed to see the open tomb. \n\n\nEast of this is a large iconostasis demarcating the Orthodox sanctuary before which is set the throne of the Greek Orthodox Patriarch of Jerusalem on the south side facing the throne of the Greek Orthodox Patriarch of Antioch on the north side.\n\n\nFurther to the east in the ambulatory are three chapels (from south to north):\n\n\n\nThe three Greek Orthodox chapels of St. James the Just, St. John the Baptist and of the Forty Martyrs of Sebaste, south of the rotunda and on the west side of the front courtyard originally formed the baptistery complex of the Constantinean church. The southernmost chapel was the vestibule, the middle chapel the actual baptistery, and the north chapel the chamber in which the patriarch chrismated the newly baptized before leading them into the rotunda north of this complex.\n\n\nOn the far side of the chapel is the low entrance to two complete 1st-century Jewish tombs. Since Jews always buried their dead outside the city, this proves that the Holy Sepulchre site was outside the city walls at the time of the crucifixion. There is a tradition that Joseph of Arimathea and Nicodemus were buried here.\n\nThe Sultan's firman (decree) of 1853, known as the \"status quo\", pinned down the now permanent statutes of property and the regulations concerning the roles of the different denominations and other custodians.\n\nThe primary custodians are the Greek Orthodox, Armenian Apostolic, and Roman Catholic Churches, with the Greek Orthodox Church having the lion's share. In the 19th century, the Coptic Orthodox, the Ethiopian Orthodox and the Syriac Orthodox acquired lesser responsibilities, which include shrines and other structures in and around the building. Times and places of worship for each community are strictly regulated in common areas. The Greek Orthodox act through the Greek Orthodox Patriarchate as well as through the Brotherhood of the Holy Sepulchre. The Roman Catholics act through the Franciscan Custody of the Holy Land.\n\nThe establishment of the 1853 status quo did not halt the violence, which continues to break out every so often even in modern times. On a hot summer day in 2002, a Coptic monk moved his chair from its agreed spot into the shade. This was interpreted as a hostile move by the Ethiopians, and eleven were hospitalized after the resulting fracas.\n\nIn another incident in 2004, during Orthodox celebrations of the Exaltation of the Holy Cross, a door to the Franciscan chapel was left open. This was taken as a sign of disrespect by the Orthodox and a fistfight broke out. Some people were arrested, but no one was seriously injured.\nOn Palm Sunday, in April 2008, a brawl broke out when a Greek monk was ejected from the building by a rival faction. Police were called to the scene but were also attacked by the enraged brawlers. On Sunday, 9 November 2008, a clash erupted between Armenian and Greek monks during celebrations for the Feast of the Cross.\n\nUnder the \"status quo\", no part of what is designated as common territory may be so much as rearranged without consent from all communities. This often leads to the neglect of badly needed repairs when the communities cannot come to an agreement among themselves about the final shape of a project. Just such a disagreement delayed the renovation of the \"edicule\", but also where any change in the structure might result in a change to the \"status quo\", disagreeable to one or more of the communities. After the Greek Orthodox Church, the Roman Catholic Church and the Armenian Orthodox Church agreed in 1958 to restore the edicule, nearly 50 years of discussion ensued before restoration commenced.\n\nA less grave sign of this state of affairs is located on a window ledge over the church's entrance. A wooden ladder was placed there at some time before 1852, when the \"status quo\" defined both the doors and the window ledges as common ground. This ladder, the \"Immovable Ladder\", remains to this day, in almost exactly the same position it occupied in century-old photographs and engravings. An engraving by David Roberts in 1839 also shows the same ladder in the same position.\n\nNo one controls the main entrance. In 1192, Saladin assigned door keeping responsibilities to the Muslim Nuseibeh family. The wooden doors that compose the main entrance are the original, highly carved doors. The Joudeh Al-Goudia family were entrusted as custodian to the keys of the Holy Sepulchre by Saladin in 1187. This arrangement has persisted into modern times.\n\nThe site of the Church had been a temple of Aphrodite before Constantine's edifice was built. Hadrian's temple had actually been located there because it was the junction of the main north-south road with one of the two main east-west roads and directly adjacent to the forum (which is now the location of the (smaller) Muristan); the forum itself had been placed, as is traditional in Roman towns, at the junction of the main north-south road with the (other) main east-west road (which is now El-Bazar/David Street). The temple and forum together took up the entire space between the two main east-west roads (a few above-ground remains of the east end of the temple precinct still survive in the Alexander Nevsky Church complex of the \"Russian Mission in Exile\").\n\nFrom the archaeological excavations in the 1970s, it is clear that construction took over most of the site of the earlier temple enclosure and that the \"Triportico\" and \"Rotunda\" roughly overlapped with the temple building itself; the excavations indicate that the temple extended at least as far back as the Aedicule, and the temple enclosure would have reached back slightly further. Virgilio Canio Corbo, a Franciscan priest and archaeologist, who was present at the excavations, estimated from the archaeological evidence that the western retaining wall, of the temple itself, would have passed extremely close to the east side of the supposed tomb; if the wall had been any further west any \"tomb\" would have been crushed under the weight of the wall (which would be immediately above it) if it had not already been destroyed when foundations for the wall were made.\n\nOther archaeologists have criticized Corbo's reconstructions. Dan Bahat, the former city archaeologist of Jerusalem, regards them as unsatisfactory, as there is no known temple of Aphrodite matching Corbo's design, and no archaeological evidence for Corbo's suggestion that the temple building was on a platform raised high enough to avoid including anything sited where the Aedicule is now; indeed Bahat notes that many temples to Aphrodite have a rotunda-like design, and argues that there is no archaeological reason to assume that the present rotunda was not based on a rotunda in the temple previously on the site.\n\nThe Bible describes Jesus's tomb as being outside the city wall, as was normal for burials across the ancient world, which were regarded as unclean. Today, the site of the Church is within the current walls of the old city of Jerusalem. It has been well documented by archaeologists that in the time of Jesus, the walled city was smaller and the wall then was to the east of the current site of the Church. In other words, the city had been much narrower in Jesus' time, with the site then having been outside the walls; since Herod Agrippa (41–44) is recorded by history as extending the city to the north (beyond the present northern walls), the required repositioning of the western wall is traditionally attributed to him as well.\n\nThe area immediately to the south and east of the sepulchre was a quarry and outside the city during the early 1st century as excavations under the Lutheran Church of the Redeemer across the street demonstrated.\n\nThe church is a part of the UNESCO World Heritage Site Old City of Jerusalem.\n\nFrom the 9th century, the construction of churches inspired in the Anastasis was extended across Europe. One example is Santo Stefano in Bologna, Italy, an agglomeration of seven churches recreating shrines of Jerusalem.\n\nSeveral churches and monasteries in Europe, for instance, in Germany and Russia, and at least one church in the United States have been modeled on the Church of the Resurrection, some even reproducing other holy places for the benefit of pilgrims who could not travel to the Holy Land. They include the of Görlitz, constructed between 1481 and 1504,the New Jerusalem Monastery in Moscow Oblast, constructed by Patriarch Nikon between 1656 and 1666, and Mount St. Sepulchre Franciscan Monastery built by the Franciscans in Washington, DC in 1898.\n\nThe Church of the Holy Sepulchre features prominently in the 2016 crypto-thriller \"The Apocalypse Fire\" by Dominic Selwood.\n\n\n\n\n", "id": "7810", "title": "Church of the Holy Sepulchre"}
{"url": "https://en.wikipedia.org/wiki?curid=7811", "text": "Cernunnos\n\nCernunnos is the conventional name given in Celtic studies to depictions of the \"horned god\" of Celtic polytheism. Cernunnos was a Celtic god of fertility, life, animals, wealth, and the underworld. The name itself is only attested once, on the 1st-century Pillar of the Boatmen, but he appears all over Gaul, and among the Celtiberians. Cernunnos is depicted with the antlers of a stag, sometimes carries a purse filled with coin, often seated cross-legged and often associated with animals and holding or wearing torcs, are known from over 50 examples in the Gallo-Roman period, mostly in north-eastern Gaul. \n\nNot much is known about the god from literary sources, and details about his name, his followers or his significance in Celtic religion are unknown. Speculative interpretations identify him as a god of nature, life or fertility.\n\nThe theonym \"[C]ernunnos\" appears on the Pillar of the Boatmen, a Gallo-Roman monument dating to the early 1st century CE, to label a god depicted with stag's antlers in their early stage of annual growth. Both antlers have torcs hanging from them.\n\nThe name has been compared to a divine epithet \"Carnonos\" in a Celtic inscription written in Greek characters at Montagnac, Hérault (as καρνονου, \"karnonou\", in the dative case).\nA Gallo-Latin adjective \"carnuātus\", \"horned,\" is also found.\n\nThe Proto-Celtic form of the theonym is reconstructed as either *\"Cerno-on-os\" or *\"Carno-on-os\". The augmentative \"-on-\" is characteristic of theonyms, as in Maponos, Epona, Matronae, and Sirona.\nMaier (2010) states that the etymology of \"Cernunnos\" is unknown, as the Celtic word for \"horn\" has an \"a\" (as in \"Carnonos\").\n\nGaulish \"karnon\" \"horn\" is cognate with Latin \"cornu\" and Germanic \"*hurnaz\", English \"horn\", ultimately from Proto-Indo-European \"\".\nThe etymon \"karn-\" \"horn\" appears in both Gaulish and Galatian branches of Continental Celtic. Hesychius of Alexandria glosses the Galatian word \"karnon\" (κάρνον) as \"Gallic trumpet\", that is, the Celtic military horn listed as the carnyx (κάρνυξ) by Eustathius of Thessalonica, who notes the instrument's animal-shaped bell. The root also appears in the names of Celtic polities, most prominent among them the Carnutes, meaning something like \"the Horned Ones,\" and in several personal names found in inscriptions.\n\nThe name \"Cernunnos\" occurs only on the \"Pillar of the Boatmen\" (\"Pilier des nautes\"), now displayed in the Musée National du Moyen Age in Paris. Constructed by Gaulish sailors probably in 14 CE, it was discovered in 1710 within the foundations of the cathedral of Notre-Dame de Paris, site of ancient Lutetia, the \"civitas\" capital of the Celtic Parisii. The distinctive stone pillar is an important monument of Gallo-Roman religion. Its low reliefs depict and label by name several Roman deities such as Jupiter, Vulcan, and Castor and Pollux, along with Gallic deities such as Esus, Smertrios, and Tarvos Trigaranus. The name \"Cernunnos\" can be read clearly on 18th century drawings of the inscriptions, but the initial letter has been obscured since, so that today only a reading \"[_]ernunnos\" can be verified\n\nAdditional evidence is given by one inscription on a metal plaque from Steinsel-Rëlent in Luxembourg, in the territory of the Celtic Treveri. This inscription read \"Deo Ceruninco\", \"to the God Cerunincos\", assumed to be the same deity. The Gaulish inscription from Montagnac reads αλλετ[ει]νος καρνονου αλ[ι]σο[ντ]εας (\"Alletinos [dedicated this] to Carnonos of Alisontea\"), with the last word possibly a place name based on \"Alisia\", \"service-tree\" or \"rock\" (compare Alesia, Gaulish \"Alisiia\").\n\nThe god labelled \"[C]ernunnos\" on the Pillar of the Boatmen is depicted with stag's antlers in their early stage of annual growth. Both antlers have torcs hanging from them. The lower part of the relief is lost, but the dimensions suggest that the god was sitting cross-legged, providing a direct parallel to the antlered figure on the Gundestrup cauldron.\n\nIn spite of the name \"Cernunnos\" being attested nowhere else, it is commonly used in Celtological literature as describing all comparable depictions of horned/antlered deities.\n\nThis \"Cernunnos\" type in Celtic iconography is often portrayed with animals, in particular the stag, and also frequently associated with the ram-horned serpent, and less frequently bulls (at Rheims), dogs and rats. Because of his frequent association with creatures, scholars often describe Cernunnos as the \"Lord of the Animals\" or the \"Lord of Wild Things\", and Miranda Green describes him as a \"peaceful god of nature and fruitfulness\".\n\nThe \"Pilier des nautes\" links him with sailors and with commerce, suggesting that he was also associated with material wealth as does the coin pouch from the Cernunnos of Rheims (Marne, Champagne, France)—in antiquity, Durocortorum, the \"civitas\" capital of the Remi tribe—and the stag vomiting coins from Niedercorn-Turbelslach (Luxembourg) in the lands of the Treveri. The god may have symbolised the fecundity of the stag-inhabited forest.\n\nOther examples of \"Cernunnos\" images include a petroglyph in Val Camonica in Cisalpine Gaul. The antlered human figure has been dated as early as the 7th century BCE or as late as the 4th. An antlered child appears on a relief from Vendeuvres, flanked by serpents and holding a purse and a torc. The best known image appears on the Gundestrup cauldron found on Jutland, dating to the 1st century BCE, thought to depict Celtic subject matter though usually regarded as of Thracian workmanship.\n\nAmong the Celtiberians, horned or antlered figures of the Cernunnos type include a \"Janus-like\" god from Candelario (Salamanca) with two faces and two small horns; a horned god from the hills of Ríotinto (Huelva); and a possible representation of the deity Vestius Aloniecus near his altars in Lourizán (Pontevedra). The horns are taken to represent \"aggressive power, genetic vigor and fecundity.\"\n\nDivine representations of the Cernunnos type are exceptions to the often-expressed view that the Celts only began to picture their gods in human form after the Roman conquest of Gaul.\nThe Celtic \"horned god\", while well attested in iconography, cannot be identified in description of Celtic religion in Roman ethnography and does not appear to have been given any \"interpretatio romana\", perhaps due to being too distinctive to be translatable into the Roman pantheon.\nWhile Cernunnos was never assimilated, scholars have sometimes compared him functionally to Greek and Roman divine figures such as Mercury, Actaeon, specialized forms of Jupiter, and Dis Pater, the latter of whom Julius Caesar said was considered the ancestor of the Gauls.\n\nThere have been attempts to find the \"cern\" root in the name of Conall Cernach, the foster brother of the Irish hero Cuchulainn in the Ulster Cycle. In this line of interpretation, \"Cernach\" is taken as an epithet with a wide semantic field — \"angular; victorious; bearing a prominent growth\" — and Conall is seen as \"the same figure\" as the ancient Cernunnos.\n\nThere is even greater evidence available to connect Conall Cernach to Cernunnos than the similarity in the names. A brief passage involving Conall in an eighth-century story entitled Táin Bó Fraích, “The Driving-off of Fraech's Cattle,” has been questioned before for its anti-climatic conclusion to an epic Celtic tale. In this passage Conall Cernach is portrayed as a hero and mighty warrior who assists the protagonist Fraech in rescuing his wife and son, and in reclaiming for Fraech his cattle. The fort that Conall must penetrate is guarded by a mighty serpent. The supposed anti-climax of this tale is when the fearsome serpent, instead of attacking Conall, darts to Conall’s waist and girdles him as a belt. Rather than killing the serpent, Conall allows it to live, and then proceeds to attack and rob the fort of its great treasures the serpent previously protected.\n\nCernunnos, as the Gaulish manifestation of the Roman Dis Pater, is considered to share the latter’s attributes of ruling over the hidden treasures of the underworld. Subterranean treasures were commonly linked in Medieval Bestiaries to the serpent, the occupant of the underground, or otherworld, and the keeper of its treasures and mysteries. This aspect of Cernunnos is depicted on a stone statue from a well in Sommerécourt, Haute-Marne, France, and on a bronze figurine from Autun. Both statue and figurine portray Cernunnos with the two ram-headed serpents encircling his waist. This is more than just a small similarity to the instance of the serpent that guarded the treasure of the fort in Táin Bó Fraích surrendering to Conall Cernach and becoming his belt. Cernunno’s connections to the deity Mars serve to underline Conall’s role as hero-warrior in the tale.\n\nThe anti-climatic nature of the eighth-century Irish tale then gains significant clarity in the light of the relationship between a horned or antler-bearing deity, warrior, or progenitor, and the chthonic dwelling, treasure-guarding serpent that encircled the waist of the one it chose to protect. This universal Celtic concept comes down to us as a mere echo of its ancient self through centuries of the Christianization of Ireland. The Gaelic Cernunnos may now possibly only be found in the slight similarity of a name and the peculiarity of a single passage from a Middle Ages Irish epic.\n\nSome see the qualities of Cernunnos subsumed into the \"life\" of Saint Ciarán of Saighir, one of the Twelve Apostles of Ireland. When he was building his first tiny cell, as his hagiograph goes, his first disciple and monk was a boar that had been rendered gentle by God. This was followed by a fox, a badger, a wolf and a stag.\n\nIn Wicca and other forms of Neopaganism a Horned God is revered; this divinity syncretises a number of horned or antlered gods from various cultures, including Cernunnos. The Horned God reflects the seasons of the year in an annual cycle of life, death and rebirth.\n\nIn the tradition of Gardnerian Wicca, the Horned God is sometimes specifically referred to as Cernunnos, or sometimes also as Kernunno.\n\n\n\n", "id": "7811", "title": "Cernunnos"}
{"url": "https://en.wikipedia.org/wiki?curid=7816", "text": "Click consonant\n\nClicks are speech sounds that occur as consonants in many languages of Southern Africa and in three languages of East Africa. Examples familiar to English-speakers are the \"tsk! tsk!\" (American spelling) or \"tut-tut\" (British spelling) used to express disapproval or pity, the \"tchick!\" used to spur on a horse, and the \"clip-clop!\" sound children make with their tongue to imitate a horse trotting.\n\nTechnically, clicks are obstruents articulated with two closures (points of contact) in the mouth, one forward and one at the back. The enclosed pocket of air is rarefied by a sucking action of the tongue (in technical terminology, clicks have a lingual ingressive airstream mechanism). The forward closure is then released, producing what may be the loudest consonants in the language, but in some languages such as Hadza and Sandawe, clicks can be more subtle and may even be mistaken for ejectives.\n\nClick consonants occur at five principal places of articulation. IPA represents a click by placing the assigned symbol for the place of click articulation adjacent to a symbol for a non-click sound at the rear place of articulation. The IPA symbols are used in writing most Khoisan languages, but Bantu languages such as Zulu typically use Latin , and for dental, lateral, and alveolar clicks respectively.\n\nThe above clicks sound like affricates, in that they involve a lot of friction. The other two families are more abrupt sounds that do not have this friction.\n\nClicks occur in all three Khoisan language families of southern Africa, where they may be the most numerous consonants. To a lesser extent they occur in three neighbouring groups of Bantu languages—which borrowed them, directly or indirectly, from Khoisan. In the southeast, in eastern South Africa, Swaziland, Lesotho, Zimbabwe, and southern Mozambique, they were adopted from a Tuu language or languages by the languages of the Nguni cluster (especially Zulu, Xhosa, and Phuthi, but also to a lesser extent Swazi and Ndebele), and spread from them in a reduced fashion to the Zulu-based pidgin Fanagalo, Sesotho, Tsonga, Ronga, the Mzimba dialect of Tumbuka, and more recently to Ndau and urban varieties of Pedi, where the spread of clicks continues. The second point of transfer was near the Caprivi Strip and the Okavango River where, apparently, the Yeyi language borrowed the clicks from a West Kalahari Khoe language; a separate development led to a smaller click inventory in the neighboring Mbukushu, Kwangali, Gciriku, Kuhane, and Fwe languages in Angola, Namibia, Botswana, and Zambia. These sounds occur not only in borrowed vocabulary, but have spread to native Bantu words as well, in the case of Nguni at least partially due to a type of word taboo called hlonipha. Some creolized varieties of Afrikaans, such as Oorlams, retain clicks in Khoekhoe words.\n\nThree languages in East Africa use clicks: Sandawe and Hadza of Tanzania, and Dahalo, an endangered South Cushitic language of Kenya that has clicks in only a few dozen words. It is thought the latter may remain from an episode of language shift.\n\nThe only non-African language known to have clicks as regular speech sounds is Damin, a ritual code used by speakers of Lardil in Australia. One of the clicks in Damin is actually an egressive click, using the tongue to compress the air in the mouth for an outward (egressive) \"spurt\".\n\nFor the most part, the Southern African Khoisan languages only utilize root-initial clicks. Hadza, Sandawe, and several Bantu languages also allow syllable-initial clicks within roots, but in no language does a click close a syllable or end a word. Once clicks are borrowed into a language as regular speech sounds, they may spread to native words, as has happened due to \"hlonipa\" word-taboo in the Nguni languages. In Gciriku, for example, the European loanword \"tomate\" (tomato) appears as \"cumáte\" with a click \"c\", though it begins with a \"t\" in all neighboring languages.\n\nScattered clicks are found in ideophones in other languages, such as Kongo , Mijikenda , and Hadza (Hadza does not otherwise have labial clicks). Ideophones often utilize phonemic distinctions not found in normal vocabulary.\n\nEnglish and many other languages may use bare clicks in interjections, without the accompaniment of vowels, such as the dental \"tsk-tsk\" sound used to express disapproval, or the lateral \"tchick\" used with horses. In Ningdu Chinese (a variety of Hakka), flapped nasal clicks are used in nursery rhymes. In Bulgarian, Greek, Levantine Arabic, Maltese, Persian, Turkish, as well as southern Italian languages such as Sicilian, a bare dental click accompanied by tipping the head upwards signifies \"no\". Libyan Arabic apparently has three such sounds.\n\nClicks occasionally turn up elsewhere, as in the special registers twins sometimes develop with each other. In West Africa, clicks have been reported allophonically, and similarly in German, faint clicks have been recorded in rapid speech where the consonants and overlap between words.\n\nOccasionally other languages are said to have click sounds. This is usually a misnomer for ejective consonants, which are found across much of the world.\n\nThe essence of a click is a lingual ingressive airstream mechanism. However, in nasal clicks the nasalization involves a separate nasal airstream, generally pulmonic egressive but occasionally pulmonic ingressive. Similarly, voiced clicks require a simultaneous pulmonic egressive airstream to make the voicing possible.\n\nThe front articulation may be coronal or, rarely, labial. In the languages in which it has been investigated, the articulations of the front and rear occlusions are interdependent, with the rear contact being uvular or pharyngeal depending on the shape of the front of the tongue.\n\nThe rear articulation had been thought to be velar, with a few languages contrasting a uvular place of articulation. However, recent investigations of languages with very complex click systems such as Nǁng have revealed that the supposed velar–uvular contrast is actually a contrast of a simple clicks versus click–plosive airstream contours (or consonant clusters, depending on analysis). Even in languages without such a distinction, such as Xhosa, experiments have shown that when the click release is removed from a recording, the resulting sound is judged to be uvular, not velar. In related Zulu, though nasal assimilation is velar, that only indicates that the onset of the rear articulation is velar; the release is still uvular. Therefore, although not all languages have been investigated on this point, phoneticians have recently come to use the term \"lingual\" (made with the tongue) as being more accurate for this airstream mechanism than \"velaric\" (made at the velum).\n\nLike other consonants, clicks can be described using four parameters: place of articulation, manner of articulation, phonation (including glottalization), and airstream mechanism. As noted above, clicks necessarily involve at least two closures, which in some cases operate partially independently: an anterior articulation traditionally represented by the special click symbol in the IPA—and a posterior articulation traditionally described as oral or nasal, voiced or voiceless, etc. The literature also describes a contrast between velar and uvular rear articulations for some languages.\n\nHowever, recent work shows that in languages that make this distinction, all clicks have a uvular, or even pharyngeal, rear closure—and the clicks explicitly described as uvular are in fact clusters/contours of a click plus a pulmonic or ejective component, in which the cluster/contour has two release bursts, the forward (click) and then the rearward (uvular) component. \"Velar\" clicks in these languages have only a single release burst, that of the forward click release, and the release of the rear articulation isn't separately audible (Miller 2011).\n\nNonetheless, in most of the literature the stated place of the click is the anterior articulation (called the \"release\" or \"influx),\" whereas the manner is ascribed to the posterior articulation (called the \"accompaniment\" or \"efflux).\" The anterior articulation defines the \"click type\" and is written with the IPA letter for the click (dental , alveolar , etc.), whereas the traditional term 'accompaniment' conflates the categories of manner (nasal, affricated), phonation (voiced, aspirated, breathy voiced, glottalized), as well as any change in the airstream with the release of the posterior articulation (pulmonic, ejective), all of which are transcribed with additional letters or diacritics, as in the \"nasal alveolar click\", or or—to take an extreme example—the \"voiced (uvular) ejective alveolar click\", .\n\nThe size of click inventories ranges from as few as three (in Sesotho) or four (in Dahalo), to dozens in the Kx'a and Tuu (Northern and Southern Khoisan) languages. Taa, the last vibrant language in the latter family, has 45 to 115 click phonemes, depending on analysis (clusters vs. contours), and over 70% of words in the dictionary of this language begin with a click.\n\nClicks appear more stop-like (sharp/abrupt) or affricate-like (noisy) depending on their place of articulation: In southern Africa, clicks involving an apical alveolar or laminal postalveolar closure are acoustically abrupt and sharp, like stops, whereas labial, dental, and lateral clicks typically have longer and acoustically noisier releases that are superficially more like affricates. In East Africa, however, the alveolar clicks tend to be flapped, whereas the lateral clicks tend to be more sharp.\n\nThe five click releases with dedicated symbols in the International Phonetic Alphabet (IPA) are labial , dental , palato-alveolar or \"palatal\" , (post)alveolar or \"retroflex\" , and lateral . In most languages, the retroflex and palatal releases are \"abrupt\"; that is, they are sharp popping sounds with little frication (turbulent airflow). The labial, dental, and lateral releases, on the other hand, are typically \"noisy\": they are longer, lip- or tooth-sucking sounds with turbulent airflow, and are sometimes called affricates. (This applies to the forward articulation; both may also have either an affricate or non-affricate rear articulation as well.) The apical releases, and , are sometimes called \"grave\", because their pitch is dominated by low frequencies; whereas the laminal releases, and , are sometimes called \"acute\", because they are dominated by high frequencies. (At least in the Nǁng language and Juǀʼhoan, this is associated with a difference in the placement of the rear articulation: \"grave\" clicks are uvular, whereas \"acute\" clicks are pharyngeal.) Thus the alveolar click sounds something like a cork pulled from a bottle (a low-pitch pop), at least in Xhosa; whereas the dental click is like English \"tsk! tsk!,\" a high-pitched sucking on the incisors. The lateral clicks are pronounced by sucking on the molars of one or both sides. The labial click is different from what many people associate with a kiss: the lips are pressed more-or-less flat together, as they are for a or an , not rounded as they are for a .\n\nThe most populous languages with clicks, Zulu and Xhosa, use the letters \"c, q, x,\" by themselves and in digraphs, to write click consonants. Most Khoisan languages, on the other hand (with the notable exceptions of Naro and Sandawe), use a more iconic system based on the pipe . (The exclamation point for the \"retroflex\" click was originally a pipe with a subscript dot, along the lines of \"ṭ, ḍ, ṇ\" used to transcribe the retroflex consonants of India.) There are also two main conventions for the second letter of the digraph as well: voicing may be written with \"g\" and uvular affrication with \"x\", or voicing with \"d\" and affrication with \"g\" (a convention of Afrikaans). In two orthographies of Juǀ’hoan, for example, is written \"g!\" or \"dq\", and \"!x\" or \"qg\". In languages without , such as Zulu, may be written \"gq\".\n\nThere are a few less-well-attested articulations. A reported subapical retroflex release in Grootfontein !Kung turns out to be alveolar with lateral release, ; Ekoka !Kung has a fricated alveolar click with an s-like release, provisionally transcribed ; and Hadza and Sandawe have a \"slapped\" alveolar click, provisionally transcribed (in turn, the lateral clicks in Hadza and Sandawe are more abrupt and less noisy than in southern Africa). However, the Khoisan languages are poorly attested, and it is quite possible that, as they become better described, more click releases will be found.\n\nFormerly when a click consonant was transcribed, two symbols were used, one for each articulation, and connected with a tie bar. This is because a click such as was analyzed as a nasal velar rear articulation pronounced simultaneously with the forward ingressive release . The symbols may be written in either order, depending on the analysis: or . However, a tie bar was not often used in practice, and when the manner is tenuis (a simple ), it was often omitted as well. That is, = = = = . Regardless, elements that do not overlap with the release are always written according to their temporal order: Prenasalization is always written first ( = = ), and the non-lingual part of a contour is always written second ( = = ).\n\nHowever, it has become standard to analyze clicks as simplex segments, as research has shown that the front and rear articulations are not independent, and to use click symbols to cover the rear articulation as well, with diacritics rather than digraphs for the accompaniments. At first this tended to be for , based on the belief that the rear articulation was velar; but as it has become clear that the rear articulation of both \"velar\" and \"uvular\" clicks is actually uvular or even pharyngeal, voicing and nasalization diacritics more in keeping with the IPA have started to appear: for .\n\nIn practical orthography, the voicing or nasalization is sometimes given the anterior place of articulation: \"dc\" for and \"mʘ\" for , for example.\n\nKirshenbaum transcription uses a very different convention: clicks are denoted by (always ) added to the letter for the stop homorganic to the release, but with the manner of the accompaniment. For example, is a voiceless dental click, and is a nasal bilabial click. This convention is used in the literature on Damin, where the clicks are transcribed as .\n\nPlaces of articulation are often called click \"types, releases,\" or \"influxes.\" There are seven or eight known releases, not counting slapped or egressive clicks. These are \"(bi)labial affricated\" , or \"(bi)labial\"; \"laminal denti-alveolar affricated\" , or \"dental\"; \"apical (post)alveolar plosive\" , or \"alveolar\"; \"laminal postalveolar (palato-alveolar) plosive\" , or \"palatal\"; \"laminal postalveolar (palato-alveolar) affricated\" (known only from Ekoka !Kung); \"subapical postalveolar (retroflex)\" (only known from Central !Kung and Damin); and \"apical postalveolar lateral\" . \n\nLanguages illustrating each of these articulations are listed below. Given the poor state of documentation of Khoisan languages, it is quite possible that additional releases will turn up. No language is known to contrast more than five places of articulation, though one publication has reconstructed Proto-Kx'a with six.\n\nExtra-linguistically, Coatlán Zapotec of Mexico uses a linguolabial click, , as mimesis for a pig drinking water, and several languages, such as Wolof, use a velar click , long judged to be physically impossible, for backchanneling and to express approval. A sublingual click (\"sucking-teeth\") is found across West Africa, the Caribbean, and into the United States. \n\nThe terms for the click releases were originally developed by Bleek in 1911. Since then there has been some conflicting variation. Here are the terms used in some of the main references.\n\nThe dental, lateral, and bilabial clicks are rarely confused. However, the palatal and alveolar clicks frequently have the opposite names in older literature, and they were not distinguished in the IPA until 1989. However, since Ladefoged & Traill (1984) clarified the places of articulation, the terms in the left column above have become standard.\n\nIn several languages, including Nama and Juǀ'hoan, the alveolar click types and only occur, or preferentially occur, before back vowels, whereas the dental and palatal clicks occur before any vowel. The effect is most noticeable with the high front vowel . In Nama, for example, the diphthong is common but is rare after alveolar clicks, whereas the opposite is true after dental and palatal clicks. This is a common effect of uvular or uvularized consonants on vowels in both click and non-click languages. In Taa, for example, the back-vowel constraint is triggered by both alveolar clicks and uvular stops, but not by palatal clicks or velar stops: sequences such as and are rare to non-existent, whereas sequences such as and are common. It is also triggered by labial clicks, though not by labial stops. Clicks subject to this constraint involve a sharp retraction of the tongue during release.\n\nMiller and colleagues (2003) used ultrasound imaging to show that the rear articulation of the alveolar clicks () in Nama is substantially different from that of palatal and dental clicks. Specifically, the shape of the body of the tongue in palatal clicks is very similar to that of the vowel , and involves the same tongue muscles, so that sequences such as involved a simple and quick transition. The rear articulation of the alveolar clicks, however, is several centimeters further back, and involves a different set of muscles in the uvular region. The part of the tongue required to approach the palate for the vowel is deeply retracted in , as it lies at the bottom of the air pocket used to create the vacuum required for click airstream. This makes the transition required for much more complex and the timing more difficult than the shallower and more forward tongue position of the palatal clicks. Consequently, takes 50 ms longer to pronounce than , the same amount of time required to pronounce .\n\nLanguages do not all behave alike. In Nǀuu, the simple clicks trigger the and allophones of and , whereas do not. All of the affricated contour clicks, such as , do as well, as do the uvular stops . However, the occlusive contour clicks pattern like the simple clicks, and does not trigger the back-vowel constraint. This is because they involve tongue-root raising rather than tongue-root retraction in the uvular-pharyngeal region. However, in Gǀwi, which is otherwise largely similar, both and trigger the back-vowel constraint (Miller 2009).\n\nClick manners are often called click \"accompaniments\" or \"effluxes\", but both terms have met with objections on theoretical grounds.\n\nThere is a great variety of click manners, both simplex and complex, the latter variously analysed as consonant clusters or contours. With so few click languages, and so little study of them, it is also unclear to what extent clicks in different languages are equivalent. For example, the of Khoekhoe, of Sandawe, and of Hadza may be essentially the same phone; no language distinguishes them, and the differences in transcription may have more to do with the approach of the linguist than with actual differences in the sounds. Such suspected allophones/allographs are listed on a common row in the table below.\n\nSome Khoisan languages are typologically unusual in allowing mixed voicing in non-click consonant clusters/contours, such as , so it is not surprising that they would allow mixed voicing in clicks as well. This may be an effect of epiglottalized voiced consonants, because voicing is incompatible with epiglottalization.\n\nAs do other consonants, clicks vary in phonation. Oral clicks are attested with four phonations: tenuis, aspirated, voiced, and breathy voiced (murmured). Nasal clicks may also vary, with plain voiced, breathy voiced / murmured nasal, aspirated, and unaspirated voiceless clicks attested (the last only in Taa). The aspirated nasal clicks are often said to have 'delayed aspiration'; there is nasal airflow throughout the click, which may become voiced between vowels, though the aspiration itself is voiceless. A few languages also have pre-glottalized nasal clicks, which have very brief prenasalization but have not been phonetically analyzed to the extent that other types of clicks have.\n\nAll languages have nasal clicks, and all but Dahalo and Damin also have oral clicks. All languages but Damin have at least one phonation contrast as well.\n\nClicks may be pronounced with a third place of articulation, glottal. A glottal stop is made during the hold of the click; the (necessarily voiceless) click is released, and then the glottal hold is released into the vowel. Glottalized clicks are very common, and they are generally nasalized as well. The nasalization cannot be heard during the click release, as there is no pulmonic airflow, and generally not at all when the click occurs at the beginning of an utterance, but it has the effect of nasalizing preceding vowels, to the extent that the glottalized clicks of Sandawe and Hadza are often described as prenasalized when in medial position. Two languages, Gǀwi and Yeyi, contrast plain and nasal glottalized clicks, but in languages without such a contrast, the glottalized click is nasal. Miller (2011) analyses the glottalization as phonation, and so considers these to be simple clicks.\n\nVarious languages also have prenasalized clicks, which may be analyzed as consonant sequences. Sotho, for example, allows a syllabic nasal before its three clicks, as in \"nnqane\" 'the other side' (prenasalized nasal) and \"seqhenqha\" 'hunk'.\n\nThere is ongoing discussion as to how the distinction between what were historically described as 'velar' and 'uvular' clicks is best described. The 'uvular' clicks are only found in some languages, and have an extended pronunciation that suggests that they are more complex than the simple ('velar') clicks, which are found in all. Nakagawa (1996) describes the extended clicks in Gǀwi as consonant clusters, sequences equivalent to English \"st\" or \"pl\", whereas Miller (2011) analyses similar sounds in several languages as click–non-click contours, where a click transitions into a pulmonic or ejective articulation within a single segment, analogous to how English \"ch\" and \"j\" transition from occlusive to fricative but still behave as unitary sounds. With ejective clicks, for example, Miller finds that although the ejective release follows the click release, it is the rear closure of the click that is ejective, not an independently articulated consonant. That is, in a simple click, the release of the rear articulation is not audible, whereas in a contour click, the rear (uvular) articulation is audibly released after the front (click) articulation, resulting in a double release.\n\nThese contour clicks may be \"linguo-pulmonic\", that is, they may transition from a click (lingual) articulation to a normal pulmonic consonant like (e.g. ); or \"linguo-glottalic\" and transition from lingual to an ejective consonant like (e.g. ): that is, a sequence of ingressive (lingual) release + egressive (pulmonic or glottalic) release. In some cases there is a shift in place of articulation as well, and instead of a uvular release, the uvular click transitions to a velar or epigottal release (depending on the description, or ). Although homorganic does not contrast with heterorganic in any known language, they are phonetically quite distinct (Miller 2011).\n\nApart from Dahalo, Damin, and many of the Bantu languages (Yeyi and Xhosa being exceptions), 'click' languages have glottalized clicks. Contour clicks are restricted to southern Africa, but are very common there: they are found in all members of the Tuu, Kx'a, and Khoe families, as well as in the Bantu language Yeyi.\n\nIn a comparative study of clicks across various languages, using her own field work as well as phonetic descriptions and data by other field researchers, Miller (2011) posits 21 types of clicks that contrast in manner or airstream. The homorganic and heterorganic affricated ejective clicks do not contrast in any known language, but are judged dissimilar enough to keep separate. Miller's conclusions differ from those of the primary researcher of a language; see the individual languages for details.\n(all spoken primarily in South Africa, Namibia and Botswana; Khoekhoe is like Korana except it has lost ejective )\n(Zulu is like Xhosa apart from not having )\n\nEach language below is illustrated with alveolar clicks, apart from Dahalo, which only has dental. Under each language are the orthography (in italics, with old forms in parentheses), the researchers' transcription (in ), or allophonic variation (in [brackets]). Some languages also have labialized or prenasalized clicks.\nYeyi also has prenasalized . The original researchers believe that and are allophones.\n\nA DoBeS (2008) study of the Western !Xoo dialect of Taa found several new manners: creaky voiced (the voiced equivalent of glottalized oral), breathy-voiced nasal, prenasalized ɡlottalized (the voiced equivalent of glottalized), and a (pre)voiced ejective. These extra voiced clicks reflect Western !Xoo morphology, where many nouns form their plural by voicing their initial consonant. DoBeS analyses most Taa clicks as clusters, leaving nine basic manners (marked with asterisks in the table). This comes close to Miller's distinction between simple and contour clicks, shaded light and medium grey in the table.\n\nClicks are often portrayed as a primordial feature of human language, a romantic reflection of the primordial lifestyle imagined of the speakers of Khoisan languages. One genetic study concluded that clicks, which occur in the languages of the genetically divergent populations Hadza and !Kung, may be an ancient element of human language. However, this conclusion relies on several dubious assumptions (see Hadza language), and most linguists assume that clicks, being quite complex consonants, arose relatively late in human history. How they arose is not known, but it is generally assumed that they developed from sequences of non-click consonants, as they are found allophonically for doubly articulated consonants in West Africa (Ladefoged 1968), where sequences overlap at word boundaries in German (Fuchs 2007), and for the sequence in Ndau and Tonga. Such developments have also been posited in historical reconstruction. For example, the Sandawe word for 'horn', , with a lateral affricate, may be a cognate with the root found throughout the Khoe family, which has a lateral click. This and other words suggests that at least some Khoe clicks may have formed from consonant clusters when the first vowel of a word was lost; in this instance * > * > .\n\nOn the other side of the equation, several non-endangered languages in vigorous use demonstrate click loss. For example, the East Kalahari languages have lost clicks from a large percentage of their vocabulary, presumably due to Bantu influence. As a rule, a click is replaced by a consonant with close to the manner of articulation of the click and the place of articulation of the forward release: alveolar click releases (the family) tend to mutate into a velar stop or affricate, such as ; palatal clicks ( \"etc.\") tend to mutate into a palatal stop such as , or a post-alveolar affricate ; and dental clicks ( \"etc.\") tend to mutate into an alveolar affricate .\n\nClicks are often presented as difficult sounds to articulate within words. However, children acquire them readily; a two-year-old, for example, may be able to pronounce a word with a lateral click with no problem, but still be unable to pronounce . Lucy Lloyd reported that after long contact with the Khoi and San, it was difficult for her to refrain from using clicks when speaking English.\n\n\n\n", "id": "7816", "title": "Click consonant"}
{"url": "https://en.wikipedia.org/wiki?curid=7817", "text": "The Cider House Rules\n\nThe Cider House Rules is a 1985 novel by John Irving. It is Irving's sixth published novel, and has been adapted into a film of the same name and a stage play by Peter Parnell.\n\nHomer Wells grows up in an orphanage where he spends his childhood \"being of use\" as a medical assistant to the director, Dr. Wilbur Larch, whose history is told in flashbacks: After a traumatic misadventure with a prostitute as a young man, Wilbur turns his back on sex and love, choosing instead to help women with unwanted pregnancies give birth and then keeping the babies in an orphanage. He makes a point of maintaining an emotional distance from the orphans, so that they can more easily make the transition into an adoptive family, but when it becomes clear that Homer is going to spend his entire childhood at the orphanage, Wilbur trains the orphan as an obstetrician and then comes to love him like a son.\n\nWilbur's and Homer's lives are complicated by Wilbur also secretly being an abortionist. Wilbur came to this work reluctantly, but he is driven by having seen the horrors of back-alley operations. Homer, upon learning Wilbur's secret, considers it morally wrong.\n\nAs a young man, Homer befriends a young couple, Candy Kendall and Wally Worthington, who come to St. Cloud's for an abortion. Homer leaves the orphanage, and returns with them to Ocean View Orchards (Wally's family's orchard) in Heart's Rock, near the Maine coast. Wally and Homer become best friends and Homer develops a secret love for Candy. Wally goes off to serve in the Second World War and his plane is shot down over Burma. He is presumed missing by the military, but Homer and Candy both believe he is dead and move on with their lives, which includes beginning a romantic relationship. When Candy becomes pregnant, they go back to St. Cloud's Orphanage, where their son is born and named Angel.\n\nSubsequently, Wally is found in Burma and returns home, paralyzed from the waist down. He is still able to have sexual intercourse but is sterile due to an infection received in Burma. They lie to the family about Angel's parentage, claiming that Homer decided to adopt him. Wally and Candy marry shortly afterward, but Candy and Homer maintain a secret affair that lasts some 15 years.\n\nMany years later, teenaged Angel falls in love with Rose. Rose, the daughter of the head migrant worker at the apple orchard, becomes pregnant by her father, and Homer performs an abortion on her. Homer decides to return to the orphanage after the death of Wilbur, to work as the new director. Though he maintains his distaste for abortions, he continues Dr. Larch's legacy of honoring the choice of his patients, and he dreams of the day when abortions are free, legal, and safe, so he'll no longer feel obliged to offer them.\n\nA subplot follows the character Melony, who grew up alongside Homer in the orphanage. She was Homer's first girlfriend in a relationship of circumstances. After Homer leaves the orphanage, so does she in an effort to find him. She eventually becomes an electrician and takes a female lover, Lorna. Melony is an extremely stoic woman, who refuses to press charges against a man who brutally broke her nose and arm so that she can later take revenge herself. She is the catalyst that transforms Homer from his comfortable but not entirely admirable position at the apple orchard to becoming Dr. Larch's replacement at the orphanage.\n\nThe story about Wally being shot down over Burma was based in part on that of Irving's biological father (whom he never met), who had been shot down over Burma and survived.\n", "id": "7817", "title": "The Cider House Rules"}
{"url": "https://en.wikipedia.org/wiki?curid=7818", "text": "Consumer\n\nA consumer is a person or organization that uses economic services or commodities.\n\nIn economic systems \"consumers\" are utilities expressed in the decision to trade or not.\n\nThe \"consumer\" is the one who pays to consume goods and services produced. As such, \"consumers\" play a vital role in the economic system of a nation. Without consumer demand, producers would lack one of the key motivations to produce: to sell to consumers. The \"consumer\" also forms part of the chain of distribution.\n\nRecently in marketing instead of marketers generating broad demographic profiles and Fisio-graphic profiles of market segments, marketers have started to engage in personalized marketing, permission marketing, and mass customization.\n\nLargely due the rise of the Internet, consumers are shifting more and more towards becoming \"prosumers\", consumers that are also producers (often of information and media on the social web) or influence the products created (e.g. by customization, crowdfunding or publishing their preferences) or actively participate in the production process or use interactive products.\n\nThe law primarily uses the notion of the consumer in relation to consumer protection laws, and the definition of consumer is often restricted to living persons (i.e. not corporations or businesses) and excludes commercial users. A typical legal rationale for protecting the consumer is based on the notion of policing market failures and inefficiencies, such as inequalities of bargaining power between a consumer and a business. As of all potential voters are also consumers, consumer protection takes on a clear political significance.\n\nConcern over the interests of consumers has also spawned activism, as well as incorporation of consumer education into school curricula. There are also various non-profit publications, such as \"Which?\", \"Consumer Reports\" and \"Choice Magazine\", dedicated to assist in consumer education and decision making.\n\nIn India, the Consumer Protection Act 1986 differentiates the consummation of a commodity or service for personal use or to earn a livelihood. Only consumers are protected per this act and any person, entity or organization purchasing a commodity for commercial reasons are exempted from any benefits of this act.\n\n", "id": "7818", "title": "Consumer"}
{"url": "https://en.wikipedia.org/wiki?curid=7819", "text": "Cactus\n\nA cactus (plural: \"cacti\", \"cactuses\", or \"cactus\") is a member of the plant family Cactaceae, a family comprising about 127 genera with some 1750 known species of the order Caryophyllales. The word \"cactus\" derives, through Latin, from the Ancient Greek , \"kaktos\", a name originally used by Theophrastus for a spiny plant whose identity is not certain. Cacti occur in a wide range of shapes and sizes. Most cacti live in habitats subject to at least some drought. Many live in extremely dry environments, even being found in the Atacama Desert, one of the driest places on earth. Cacti show many adaptations to conserve water. Almost all cacti are succulents, meaning they have thickened, fleshy parts adapted to store water. Unlike many other succulents, the stem is the only part of most cacti where this vital process takes place. Most species of cacti have lost true leaves, retaining only spines, which are highly modified leaves. As well as defending against herbivores, spines help prevent water loss by reducing air flow close to the cactus and providing some shade. In the absence of leaves, enlarged stems carry out photosynthesis. Cacti are native to the Americas, ranging from Patagonia in the south to parts of western Canada in the north—except for \"Rhipsalis baccifera\", which also grows in Africa and Sri Lanka.\n\nCactus spines are produced from specialized structures called areoles, a kind of highly reduced branch. Areoles are an identifying feature of cacti. As well as spines, areoles give rise to flowers, which are usually tubular and multipetaled. Many cacti have short growing seasons and long dormancies, and are able to react quickly to any rainfall, helped by an extensive but relatively shallow root system that quickly absorb any water reaching the ground surface. Cactus stems are often ribbed or fluted, which allows them to expand and contract easily for quick water absorption after rain, followed by long drought periods. Like other succulent plants, most cacti employ a special mechanism called \"crassulacean acid metabolism\" (CAM) as part of photosynthesis. Transpiration, during which carbon dioxide enters the plant and water escapes, does not take place during the day at the same time as photosynthesis, but instead occurs at night. The plant stores the carbon dioxide it takes in as malic acid, retaining it until daylight returns, and only then using it in photosynthesis. Because transpiration takes place during the cooler, more humid night hours, water loss is significantly reduced.\n\nMany smaller cacti have globe-shaped stems, combining the highest possible volume for water storage, with the lowest possible surface area for water loss from transpiration. The tallest free-standing cactus is \"Pachycereus pringlei\", with a maximum recorded height of , and the smallest is \"Blossfeldia liliputiana\", only about in diameter at maturity. A fully grown saguaro (\"Carnegiea gigantea\") is said to be able to absorb as much as of water during a rainstorm. A few species differ significantly in appearance from most of the family. At least superficially, plants of the genus \"Pereskia\" resemble other trees and shrubs growing around them. They have persistent leaves, and when older, bark-covered stems. Their areoles identify them as cacti, and in spite of their appearance, they, too, have many adaptations for water conservation. \"Pereskia\" is considered close to the ancestral species from which all cacti evolved. In tropical regions, other cacti grow as forest climbers and epiphytes (plants that grow on trees). Their stems are typically flattened, almost leaf-like in appearance, with fewer or even no spines, such as the well-known Christmas cactus or Thanksgiving cactus (in the genus \"Schlumbergera\").\n\nCacti have a variety of uses: many species are used as ornamental plants, others are grown for fodder or forage, and others for food (particularly their fruit). Cochineal is the product of an insect that lives on some cacti.\n\nMany succulent plants in both the Old and New World, such as some Euphorbiaceae (euphorbias), bear a striking resemblance to cacti, and may incorrectly be called \"cactus\" in common usage.\n\nThe 1,500 to 1,800 species of cacti mostly fall into one of two groups of \"core cacti\": opuntias (subfamily Opuntioideae) and \"cactoids\" (subfamily Cactoideae). Most members of these two groups are easily recognizable as cacti. They have fleshy succulent stems that are major organs of photosynthesis. They have absent, small, or transient leaves. They have flowers with ovaries that lie below the sepals and petals, often deeply sunken into a fleshy receptacle (the part of the stem from which the flower parts grow). All cacti have areoles—highly specialized short shoots with extremely short internodes that produce spines, normal shoots, and flowers.\n\nThe remaining cacti fall into only two genera, \"Pereskia\" and \"Maihuenia\", and are rather different, which means any description of cacti as a whole must frequently make exceptions for them. \"Pereskia\" species superficially resemble other tropical forest trees. When mature, they have woody stems that may be covered with bark and long-lasting leaves that provide the main means of photosynthesis. Their flowers may have superior ovaries (i.e., above the points of attachment of the sepals and petals), and areoles that produce further leaves. The two species of \"Maihuenia\" have small, globe-shaped bodies with prominent leaves at the top.\n\nCacti show a wide variety of growth habits, which are difficult to divide into clear, simple categories. They can be tree-like (arborescent), meaning they typically have a single more-or-less woody trunk topped by several to many branches. In the genus \"Pereskia\", the branches are covered with leaves, so the species of this genus may not be recognized as cacti. In most other cacti, the branches are more typically cactus-like, bare of leaves and bark, and covered with spines, as in \"Pachycereus pringlei\" or the larger opuntias. Some cacti may become tree-sized but without branches, such as larger specimens of \"Echinocactus platyacanthus\". Cacti may also be described as shrubby, with several stems coming from the ground or from branches very low down, such as in \"Stenocereus thurberi\".\n\nSmaller cacti may be described as columnar. They consist of erect, cylinder-shaped stems, which may or may not branch, without a very clear division into trunk and branches. The boundary between columnar forms and tree-like or shrubby forms is difficult to define. Smaller and younger specimens of \"Cephalocereus senilis\", for example, are columnar, whereas older and larger specimens may become tree-like. In some cases, the \"columns\" may be horizontal rather than vertical. Thus, \"Stenocereus eruca\" has stems growing along the ground, rooting at intervals.\n\nCacti whose stems are even smaller may be described as globular (or globose). They consist of shorter, more ball-shaped stems than columnar cacti. Globular cacti may be solitary, such as \"Ferocactus latispinus\", or their stems may form clusters that can create large mounds. All or some stems in a cluster may share a common root.\n\nOther cacti have a quite different appearance. In tropical regions, some grow as forest climbers and epiphytes. Their stems are typically flattened, almost leaf-like in appearance, with fewer or even no spines. Climbing cacti can be very large; a specimen of \"Hylocereus\" was reported as long from root to the most distant stem. Epiphytic cacti, such as species of \"Rhipsalis\" or \"Schlumbergera\", often hang downwards, forming dense clumps where they grow in trees high above the ground.\n\nThe leafless, spiny stem is the characteristic feature of the majority of cacti (and all of those belonging to the largest subfamily, the Cactoideae). The stem is typically succulent, meaning it is adapted to store water. The surface of the stem may be smooth (as in some species of \"Opuntia\") or covered with protuberances of various kinds, which are usually called tubercles. These vary from small \"bumps\" to prominent, nipple-like shapes in the genus \"Mammillaria\" and outgrowths almost like leaves in \"Ariocarpus\" species. The stem may also be ribbed or fluted in shape. The prominence of these ribs depends on how much water the stem is storing: when full (up to 90% of the mass of a cactus may be water), the ribs may be almost invisible on the swollen stem, whereas when the cactus is short of water and the stems shrink, the ribs may be very visible.\n\nThe stems of most cacti are some shade of green, often bluish or brownish green. Such stems contain chlorophyll and are able to carry out photosynthesis; they also have stomata (small structures that can open and close to allow passage of gases). Cactus stems are often visibly waxy.\n\nAreoles are structures unique to cacti. Although variable, they typically appear as woolly or hairy areas on the stems from which spines emerge. Flowers are also produced from areoles. In the genus \"Pereskia\", believed similar to the ancestor of all cacti, the areoles occur in the axils of leaves (i.e. in the angle between the leaf stalk and the stem). In leafless cacti, areoles are often borne on raised areas on the stem where leaf bases would have been.\n\nAreoles are highly specialized and very condensed shoots or branches. In a normal shoot, nodes bearing leaves or flowers would be separated by lengths of stem (internodes). In an areole, the nodes are so close together, they form a single structure. The areole may be circular, elongated into an oval shape, or even separated into two parts; the two parts may be visibly connected in some way (e.g. by a groove in the stem) or appear entirely separate (a dimorphic areole). The part nearer the top of the stem then produces flowers, the other part spines. Areoles often have multicellular hairs (trichomes) that give the areole a hairy or woolly appearance, sometimes of a distinct color such as yellow or brown.\n\nIn most cacti, the areoles produce new spines or flowers only for a few years, and then become inactive. This results in a relatively fixed number of spines, with flowers being produced only from the ends of stems, which are still growing and forming new areoles. In \"Pereskia\", a genus close to the ancestor of cacti, areoles remain active for much longer; this is also the case in \"Opuntia\" and \"Neoraimondia\".\n\nThe great majority of cacti have no visible leaves; photosynthesis takes place in the stems (which may be flattened and leaflike in some species). Exceptions occur in three groups of cacti. All the species of \"Pereskia\" are superficially like normal trees or shrubs and have numerous leaves. Many cacti in the opuntia group (subfamily Opuntioideae, opuntioids) also have visible leaves, which may be long-lasting (as in \"Pereskiopsis\" species) or be produced only during the growing season and then be lost (as in many species of \"Opuntia\"). The small genus \"Maihuenia\" also relies on leaves for photosynthesis. The structure of the leaves varies somewhat between these groups. \"Pereskia\" species have \"normal\" leaves, with a midrib and a flattened blade (lamina) on either side. Opuntioids and \"Maihuenia\" have leaves that appear to consist only of a midrib.\n\nEven those cacti without visible photosynthetic leaves do usually have very small leaves, less than long in about half of the species studied and almost always less than long. The function of such leaves cannot be photosynthesis; a role in the production of plant hormones, such as auxin, and in defining axillary buds has been suggested.\n\nBotanically, \"spines\" are distinguished from \"thorns\": spines are modified leaves, and thorns are modified branches. Cacti produce spines, always from areoles as noted above. Spines are present even in those cacti with leaves, such as \"Pereskia\", \"Pereskiopsis\" and \"Maihuenia\", so they clearly evolved before complete leaflessness. Some cacti only have spines when young, possibly only when seedlings. This is particularly true of tree-living cacti, such as \"Rhipsalis\" or \"Schlumbergera\", but some ground-living cacti, such as \"Ariocarpus\", also lack spines when mature.\n\nThe spines of cacti are often useful in identification, since they vary greatly between species in number, color, size, shape and hardness, as well as in whether all the spines produced by an areole are similar or whether they are of distinct kinds. Most spines are straight or at most slightly curved, and are described as hair-like, bristle-like, needle-like or awl-like, depending on their length and thickness. Some cacti have flattened spines (e.g. \"Schlerocactus papyracanthus\"). Other cacti have hooked spines. Sometimes, one or more central spines are hooked, while outer spines are straight (e.g., \"Mammillaria rekoi\").\n\nIn addition to normal-length spines, members of the subfamily Opuntioideae have relatively short spines, called glochids that are barbed along their length and easily shed. These enter the skin and are difficult to remove, causing long-lasting irritation.\n\nMost ground-living cacti have only fine roots, which spread out around the base of the plant for varying distances, close to the surface. Some cacti have taproots; in genera such as \"Copiapoa\", these are considerably larger and of a greater volume than the body. Taproots may aid in stabilizing the larger columnar cacti. Climbing, creeping and epiphytic cacti may have only adventitious roots, produced along the stems where these come into contact with a rooting medium.\n\nLike their spines, cactus flowers are variable. Typically, the ovary is surrounded by material derived from stem or receptacle tissue, forming a structure called a pericarpel. Tissue derived from the petals and sepals continues the pericarpel, forming a composite tube—the whole may be called a floral tube, although strictly speaking only the part furthest from the base is floral in origin. The outside of the tubular structure often has areoles that produce wool and spines. Typically, the tube also has small scale-like bracts, which gradually change into sepal-like and then petal-like structures, so the sepals and petals cannot be clearly differentiated (and hence are often called \"tepals\"). Some cacti produce floral tubes without wool or spines (e.g. \"Gymnocalycium\") or completely devoid of any external structures (e.g. \"Mammillaria\"). Unlike the flowers of other cacti, \"Pereskia\" flowers may be borne in clusters.\n\nCactus flowers usually have many stamens, but only a single style, which may branch at the end into more than one stigma. The stamens usually arise from all over the inner surface of the upper part of the floral tube, although in some cacti, the stamens are produced in one or more distinct \"series\" in more specific areas of the inside of the floral tube.\n\nThe flower as a whole is usually radially symmetrical (actinomorphic), but may be bilaterally symmetrical (zygomorphic) in some species. Flower colors range from white through yellow and red to magenta.\n\nAll cacti have some adaptations to promote efficient water use. Most cacti—opuntias and cactoids—specialize in surviving in hot and dry environments (i.e. they are xerophytes), but the first ancestors of modern cacti were already adapted to periods of intermittent drought. A small number of cactus species in the tribes Hylocereeae and Rhipsalideae have become adapted to life as climbers or epiphytes, often in tropical forests, where water conservation is less important.\n\nThe absence of visible leaves is one of the most striking features of most cacti. \"Pereskia\" (which is close to the ancestral species from which all cacti evolved) does have long-lasting leaves, which are, however, thickened and succulent in many species. Other species of cactus with long-lasting leaves, such as the opuntioid \"Pereskiopsis\", also have succulent leaves. A key issue in retaining water is the ratio of surface area to volume. Water loss is proportional to surface area, whereas the amount of water present is proportional to volume. Structures with a high surface area-to-volume ratio, such as thin leaves, necessarily lose water at a higher rate than structures with a low area-to-volume ratio, such as thickened stems.\n\nSpines, which are modified leaves, are present on even those cacti with true leaves, showing the evolution of spines preceded the loss of leaves. Although spines have a high surface area-to-volume ratio, at maturity they contain little or no water, being composed of fibers made up of dead cells. Spines provide protection from herbivores and camouflage in some species, and assist in water conservation in several ways. They trap air near the surface of the cactus, creating a moister layer that reduces evaporation and transpiration. They can provide some shade, which lowers the temperature of the surface of the cactus, also reducing water loss. When sufficiently moist air is present, such as during fog or early morning mist, spines can condense moisture, which then drips onto the ground and is absorbed by the roots.\n\nThe majority of cacti are stem succulents, i.e., plants in which the stem is the main organ used to store water. Water may form up to 90% of the total mass of a cactus. Stem shapes vary considerably among cacti. The cylindrical shape of columnar cacti and the spherical shape of globular cacti produce a low surface area-to-volume ratio, thus reducing water loss, as well as minimizing the heating effects of sunlight. The ribbed or fluted stems of many cacti allow the stem to shrink during periods of drought and then swell as it fills with water during periods of availability. A mature saguaro (\"Carnegiea gigantea\") is said to be able to absorb as much as of water during a rainstorm. The outer layer of the stem usually has a tough cuticle, reinforced with waxy layers, which reduce water loss. These layers are responsible for the grayish or bluish tinge to the stem color of many cacti.\n\nThe stems of most cacti have adaptations to allow them to conduct photosynthesis in the absence of leaves. This is discussed further below under Metabolism.\n\nMany cacti have roots that spread out widely, but only penetrate a short distance into the soil. In one case, a young saguaro only tall had a root system with a diameter of , but no more than deep. Cacti can also form new roots quickly when rain falls after a drought. The concentration of salts in the root cells of cacti is relatively high. All these adaptations enable cacti to absorb water rapidly during periods of brief or light rainfall. Thus, \"Ferocactus cylindraceus\" reportedly can take up a significant amount of water within 12 hours of as little as of rainfall, becoming fully hydrated in a few days.\n\nAlthough in most cacti, the stem acts as the main organ for storing water, some cacti have in addition large taproots. These may be several times the length of the above-ground body in the case of species such as \"Copiapoa atacamensis\", which grows in one of the driest places in the world, the Atacama Desert in northern Chile.\nPhotosynthesis requires plants to take in carbon dioxide gas (). As they do so, they lose water through transpiration. Like other types of succulents, cacti reduce this water loss by the way in which they carry out photosynthesis. \"Normal\" leafy plants use the C mechanism: during daylight hours, is continually drawn out of the air present in spaces inside leaves and converted first into a compound containing three carbon atoms (3-phosphoglycerate) and then into products such as carbohydrates. The access of air to internal spaces within a plant is controlled by stomata, which are able to open and close. The need for a continuous supply of during photosynthesis means the stomata must be open, so water vapor is continuously being lost. Plants using the C mechanism lose as much as 97% of the water taken up through their roots in this way. A further problem is that as temperatures rise, the enzyme that captures starts to capture more and more oxygen instead, reducing the efficiency of photosynthesis by up to 25%.\n\nCrassulacean acid metabolism (CAM) is a mechanism adopted by cacti and other succulents to avoid the problems of the C mechanism. In full CAM, the stomata open only at night, when temperatures and water loss are lowest. enters the plant and is captured in the form of organic acids stored inside cells (in vacuoles). The stomata remain closed throughout the day, and photosynthesis uses only this stored . CAM uses water much more efficiently at the price of limiting the amount of carbon fixed from the atmosphere and thus available for growth. CAM-cycling is a less efficient system whereby stomata open in the day, just as in plants using the C mechanism. At night, or when the plant is short of water, the stomata close and the CAM mechanism is used to store produced by respiration for use later in photosynthesis. CAM-cycling is present in \"Pereskia\" species.\n\nBy studying the ratio of C to C incorporated into a plant—its isotopic signature—it is possible to deduce how much is taken up at night and how much in the daytime. Using this approach, most of the \"Pereskia\" species investigated exhibit some degree of CAM-cycling, suggesting this ability was present in the ancestor of all cacti. \"Pereskia\" leaves are claimed to only have the C mechanism with CAM restricted to stems. More recent studies show that \"it is highly unlikely that significant carbon assimilation occurs in the stem\"; \"Pereskia\" species are described as having \"C with inducible CAM.\" Leafless cacti carry out all their photosynthesis in the stem, using full CAM. , it is not clear whether stem-based CAM evolved once only in the core cacti, or separately in the opuntias and cactoids; CAM is known to have evolved convergently many times.\n\nTo carry out photosynthesis, cactus stems have undergone many adaptations. Early in their evolutionary history, the ancestors of modern cacti (other than one group of \"Pereskia\" species) developed stomata on their stems and began to delay developing bark. However, this alone was not sufficient; cacti with only these adaptations appear to do very little photosynthesis in their stems. Stems needed to develop structures similar to those normally found only in leaves. Immediately below the outer epidermis, a hypodermal layer developed made up of cells with thickened walls, offering mechanical support. Air spaces were needed between the cells to allow carbon dioxide to diffuse inwards. The center of the stem, the cortex, developed \"chlorenchyma\" – a plant tissue made up of relatively unspecialized cells containing chloroplasts, arranged into a \"spongy layer\" and a \"palisade layer\" where most of the photosynthesis occurs.\n\nNaming and classifying cacti has been both difficult and controversial since the first cacti were discovered for science. The difficulties began with Carl Linnaeus. In 1737, he placed the cacti he knew into two genera, \"Cactus\" and \"Pereskia\". However, when he published \"Species Plantarum\" in 1753—the starting point for modern botanical nomenclature—he relegated them all to one genus, \"Cactus\". The word \"cactus\" is derived through Latin from the Ancient Greek (\"kaktos\"), a name used by Theophrastus for a spiny plant, which may have been the cardoon (\"Cynara cardunculus\").\n\nLater botanists, such as Philip Miller in 1754, divided cacti into several genera, which, in 1789, Antoine Laurent de Jussieu placed in his newly created family Cactaceae. By the early 20th century, botanists came to feel Linnaeus's name \"Cactus\" had become so confused as to its meaning (was it the genus or the family?) that it should not be used as a genus name. The 1905 Vienna botanical congress rejected the name \"Cactus\" and instead declared \"Mammillaria\" was the type genus of the family Cactaceae. It did, however, conserve the name Cactaceae, leading to the unusual situation in which the family Cactaceae no longer contains the genus after which it was named.\n\nThe difficulties continued, partly because giving plants scientific names relies on \"type specimens\". Ultimately, if botanists want to know whether a particular plant is an example of, say, \"Mammillaria mammillaris\", they should be able to compare it with the type specimen to which this name is permanently attached. Type specimens are normally prepared by compression and drying, after which they are stored in herbaria to act as definitive references. However, cacti are very difficult to preserve in this way; they have evolved to resist drying and their bodies do not easily compress. A further difficulty is that many cacti were given names by growers and horticulturalists rather than botanists; as a result, the provisions of the \"International Code of Nomenclature for algae, fungi, and plants\" (which governs the names of cacti, as well as other plants) were often ignored. Curt Backeberg, in particular, is said to have named or renamed 1,200 species without one of his names ever being attached to a specimen, which, according to David Hunt, ensured he \"left a trail of nomenclatural chaos that will probably vex cactus taxonomists for centuries.\"\n\nIn 1984, it was decided that the Cactaceae Section of the International Organization for Succulent Plant Study should set up a working party, now called the International Cactaceae Systematics Group (ICSG), to produce consensus classifications down to the level of genera. Their system has been used as the basis of subsequent classifications. Detailed treatments published in the 21st century have divided the family into around 125–130 genera and 1,400–1,500 species, which are then arranged into a number of tribes and subfamilies. The ICSG classification of the cactus family recognizes four subfamilies, the largest of which is divided into nine tribes. The subfamilies are:\n\n\nMolecular phylogenetic studies have supported the monophyly of three of these subfamilies (not Pereskioideae), but have not supported all of the tribes or even genera below this level; indeed, a 2011 study found only 39% of the genera in the subfamily Cactoideae sampled in the research were monophyletic. Classification of the cacti currently remains uncertain and is likely to change.\n\nA 2005 study suggested the genus \"Pereskia\" was basal within the Cactaceae, but confirmed earlier suggestions it was not monophyletic, i.e., did not include all the descendants of a common ancestor. The Bayesian consensus cladogram from this study is shown below.\n\nA more recent 2011 study using fewer genes but more species also found that \"Pereskia\" was divided into these two clades, but was unable to resolve the members of the \"core cacti\" clade. It was accepted that the relationships shown above are \"the most robust to date.\"\n\nThe two clades of \"Pereskia\" differ in their geographical distribution; with one exception, clade A is found around the Gulf of Mexico and the Caribbean Sea, whereas clade B occurs south of the Amazon Basin. Species of \"Pereskia\" within clade A always lack two key features of the stem present in most of the remaining \"caulocacti\": like most non-cacti, their stems begin to form bark early in the plants' life and also lack stomata—structures that control admission of air into a plant and hence control photosynthesis. By contrast, caulocacti, including species of \"Pereskia\" clade B, typically delay forming bark and have stomata on their stems, thus giving the stem the potential to become a major organ for photosynthesis. (The two highly specialized species of \"Maihuenia\" are something of an exception.)\n\nThe first cacti are thought to have been only slightly succulent shrubs or small trees whose leaves carried out photosynthesis. They lived in tropical areas that experienced periodic drought. If \"Pereskia\" clade A is a good model of these early cacti, then, although they would have appeared superficially similar to other trees growing nearby, they had already evolved strategies to conserve water (some of which are present in members of other families in the order Caryophyllales). These strategies included being able to respond rapidly to periods of rain, and keeping transpiration low by using water very efficiently during photosynthesis. The latter was achieved by tightly controlling the opening of stomata. Like \"Pereskia\" species today, early ancestors may have been able to switch from the normal C mechanism, where carbon dioxide is used continuously in photosynthesis, to CAM cycling, in which when the stomata are closed, carbon dioxide produced by respiration is stored for later use in photosynthesis.\n\n\"Pereskia\" clade B marks the beginnings of an evolutionary switch to using stems as photosynthetic organs. Stems have stomata and the formation of bark takes place later than in normal trees. The \"core cacti\" show a steady increase in both stem succulence and photosynthesis accompanied by multiple losses of leaves, more-or-less complete in the Cactoideae. One evolutionary question at present unanswered is whether the switch to full CAM photosynthesis in stems occurred only once in the core cacti, in which case it has been lost in \"Maihuenia\", or separately in Opuntioideae and Cactoideae, in which case it never evolved in \"Maihuenia\".\n\nUnderstanding evolution within the core cacti clade is difficult , since phylogenetic relationships are still uncertain and not well related to current classifications. Thus, a 2011 study found \"an extraordinarily high proportion of genera\" were not monophyletic, so were not all descendants of a single common ancestor. For example, of the 36 genera in the subfamily Cactoideae sampled in the research, 22 (61%) were found not monophyletic. Nine tribes are recognized within Cactoideae in the International Cactaceae Systematics Group (ICSG) classification; one, Calymmantheae, comprises a single genus, \"Calymmanthium\". Only two of the remaining eight, Cacteae and Rhipsalideae, were shown to be monophyletic in a 2011 study by Hernández-Hernández et al. For a more detailed discussion of the phylogeny of the cacti, see Classification of the Cactaceae.\n\nNo known fossils of cacti exist to throw light on their evolutionary history. However, the geographical distribution of cacti offers some evidence. Except for a relatively recent spread of \"Rhipsalis baccifera\" to parts of the Old World, cacti are plants of South America and mainly southern regions of North America. This suggests the family must have evolved after the ancient continent of Gondwana split into South America and Africa, which occurred during the Early Cretaceous, around . Precisely when after this split cacti evolved is less clear. Older sources suggest an early origin around 90 – 66 million years ago, during the Late Cretaceous. More recent molecular studies suggest a much younger origin, perhaps in very Late Eocene to early Oligocene periods, around 35–30 million years ago. Based on the phylogeny of the cacti, the earliest diverging group (\"Pereskia\" clade A) may have originated in Central America and northern South America, whereas the caulocacti, those with more-or-less succulent stems, evolved later in the southern part of South America, and then moved northwards. Core cacti, those with strongly succulent stems, are estimated to have evolved around 25 million years ago. A possible stimulus to their evolution may have been uplifting in the central Andes, some 25–20 million years ago, which was associated with increasing and varying aridity. However, the current species diversity of cacti is thought to have arisen only in the last 10–5 million years (from the late Miocene into the Pliocene). Other succulent plants, such as the Aizoaceae in South Africa, the Didiereaceae in Madagascar and the genus \"Agave\" in the Americas, appear to have diversified at the same time, which coincided with a global expansion of arid environments.\n\nCacti inhabit diverse regions, from coastal plains to high mountain areas. With one exception, they are native to the Americas, where their range extends from Patagonia to British Columbia and Alberta in western Canada. A number of centers of diversity exist. For cacti adapted to drought, the three main centers are Mexico and the southwestern United States; the southwestern Andes, where they are found in Peru, Bolivia, Chile and Argentina; and eastern Brazil, away from the Amazon Basin. Tree-living epiphytic and climbing cacti necessarily have different centers of diversity, as they require moister environments. They are mainly found in the coastal mountains and Atlantic forests of southeastern Brazil; in Bolivia, which is the center of diversity for the subfamily Rhipsalideae; and in forested regions of Central America, where the climbing Hylocereeae are most diverse.\n\n\"Rhipsalis baccifera\" is the exception; it is native to both the Americas and the Old World, where it is found in tropical Africa, Madagascar, and Sri Lanka. One theory is it was spread by being carried as seeds in the digestive tracts of migratory birds; the seeds of \"Rhipsalis\" are adapted for bird distribution. Old World populations are polyploid, and regarded as distinct subspecies, supporting the idea that the spread was not recent. The alternative theory is the species initially crossed the Atlantic on European ships trading between South America and Africa, after which birds may have spread it more widely.\n\nMany other species have become naturalized outside the Americas after having been introduced by people, especially in Australia, Hawaii, and the Mediterranean region. In Australia, species of \"Opuntia\", particularly \"Opuntia stricta\", were introduced in the 19th century for use as natural agricultural fences and in an attempt to establish a cochineal industry. They rapidly became a major weed problem, but are now controlled by biological agents, particularly the moth \"Cactoblastis cactorum\". The weed potential of Opuntia species in Australia continues however, leading to all opuntioid cacti except \"O. ficus-indica\" being declared Weeds of National Significance by the Australian Weeds Committee in April 2012.\n\nCactus flowers are pollinated by insects, birds and bats. None are known to be wind-pollinated and self-pollination occurs in only a very few species; for example the flowers of some species of \"Frailea\" do not open (cleistogamy). The need to attract pollinators has led to the evolution of pollination syndromes, which are defined as groups of \"floral traits, including rewards, associated with the attraction and utilization of a specific group of animals as pollinators.\"\n\nBees are the most common pollinators of cacti; bee-pollination is considered to have been the first to evolve. Day-flying butterflies and nocturnal moths are associated with different pollination syndromes. Butterfly-pollinated flowers are usually brightly colored, opening during the day, whereas moth-pollinated flowers are often white or pale in color, opening only in the evening and at night. As an example, \"Pachycereus schottii\" is pollinated by a particular species of moth, \"Upiga virescens\", which also lays its eggs among the developing seeds its caterpillars later consume. The flowers of this cactus are funnel-shaped, white to deep pink, up to long, and open at night.\n\nHummingbirds are significant pollinators of cacti. Species showing the typical hummingbird-pollination syndrome have flowers with colors towards the red end of the spectrum, anthers and stamens that protrude from the flower, and a shape that is not radially symmetrical, with a lower lip that bends downwards; they produce large amounts of nectar with a relatively low sugar content. \"Schlumbergera\" species, such as \"S. truncata\", have flowers that correspond closely to this syndrome. Other hummingbird-pollinated genera include \"Cleistocactus\" and \"Disocactus\".\n\nBat-pollination is relatively uncommon in flowering plants, but about a quarter of the genera of cacti are known to be pollinated by bats—an unusually high proportion, exceeded among eudicots by only two other families, both with very few genera. Columnar cacti growing in semidesert areas are among those most likely to be bat-pollinated; this may be because bats are able to travel considerable distances, so are effective pollinators of plants growing widely separated from one another. The pollination syndrome associated with bats includes a tendency for flowers to open in the evening and at night, when bats are active. Other features include a relatively dull color, often white or green; a radially symmetrical shape, often tubular; a smell described as \"musty\"; and the production of a large amount of sugar-rich nectar. \"Carnegiea gigantea\" is an example of a bat-pollinated cactus, as are many species of \"Pachycereus\" and \"Pilosocereus\".\n\nThe fruits produced by cacti after the flowers have been fertilized vary considerably; many are fleshy, although some are dry. All contain a large number of seeds. Fleshy, colorful and sweet-tasting fruits are associated with seed dispersal by birds. The seeds pass through their digestive systems and are deposited in their droppings. Fruit that falls to the ground may be eaten by other animals; giant tortoises are reported to distribute \"Opuntia\" seeds in the Galápagos Islands. Ants appear to disperse the seeds of a few genera, such as \"Blossfeldia\". Drier spiny fruits may cling to the fur of mammals or be moved around by the wind.\n\n, there is still controversy as to the precise dates when humans first entered those areas of the New World where cacti are commonly found, and hence when they might first have used them. An archaeological site in Chile has been dated to around 15,000 years ago, suggesting cacti would have been encountered before then. Early evidence of the use of cacti includes cave paintings in the Serra da Capivara in Brazil, and seeds found in ancient middens (waste dumps) in Mexico and Peru, with dates estimated at 12,000–9,000 years ago. Hunter-gatherers likely collected cactus fruits in the wild and brought them back to their camps.\n\nIt is not known when cacti were first cultivated. Opuntias (prickly pears) were used for a variety of purposes by the Aztecs, whose empire, lasting from the 14th to the 16th century, had a complex system of horticulture. Their capital from the 15th century was Tenochtitlan (now Mexico City); one explanation for the origin of the name is that it includes the Nahuatl word \"nōchtli\", referring to the fruit of an opuntia. The coat of arms of Mexico shows an eagle perched on a cactus while holding a snake, an image at the center of the myth of the founding of Tenochtitlan. The Aztecs symbolically linked the ripe red fruits of an opuntia to human hearts; just as the fruit quenches thirst, so offering human hearts to the sun god ensured the sun would keep moving.\n\nEuropeans first encountered cacti when they arrived in the New World late in the 15th century. Their first landfalls were in the West Indies, where relatively few cactus genera are found; one of the most common is the genus \"Melocactus\". Thus, melocacti were possibly among the first cacti seen by Europeans. \"Melocactus\" species were present in English collections of cacti before the end of the 16th century (by 1570 according to one source,) where they were called \"Echinomelocactus\", later shortened to \"Melocactus\" by Joseph Pitton de Tourneville in the early 18th century. Cacti, both purely ornamental species and those with edible fruit, continued to arrive in Europe, so Carl Linnaeus was able to name 22 species by 1753. One of these, his \"Cactus opuntia\" (now part of \"Opuntia ficus-indica\"), was described as \"\"\"\" (with larger fruit ... now in Spain and Portugal), indicative of its early use in Europe.\n\nThe plant now known as \"Opuntia ficus-indica\", or the Indian fig cactus, has long been an important source of food. The original species is thought to have come from central Mexico, although this is now obscure because the indigenous people of southern North America developed and distributed a range of horticultural varieties (cultivars), including forms of the species and hybrids with other opuntias. Both the fruit and pads are eaten, the former often under the Spanish name \"tuna\", the latter under the name \"nopal\". Cultivated forms are often significantly less spiny or even spineless. The nopal industry in Mexico was said to be worth US$150 million in 2007. The Indian fig cactus was probably already present in the Caribbean when the Spanish arrived, and was soon after brought to Europe. It spread rapidly in the Mediterranean area, both naturally and by being introduced—so much so, early botanists assumed it was native to the area. Outside the Americas, the Indian fig cactus is an important commercial crop in Sicily, Algeria and other North African countries. Fruits of other opuntias are also eaten, generally under the same name, \"tuna\". Flower buds, particularly of \"Cylindropuntia\" species, are also consumed.\n\nAlmost any fleshy cactus fruit is edible. The word \"pitaya\" or \"pitahaya\" (usually considered to have been taken into Spanish from Haitian creole) can be applied to a range of \"scaly fruit\", particularly those of columnar cacti. The fruit of the saguaro (\"Carnegiea gigantea\") has long been important to the indigenous peoples of northwestern Mexico and the southwestern United States, including the Sonoran Desert. It can be preserved by boiling to produce syrup and by drying. The syrup can also be fermented to produce an alcoholic drink. Fruits of \"Stenocereus\" species have also been important food sources in similar parts of North America; \"Stenocereus queretaroensis\" is cultivated for its fruit. In more tropical southern areas, the climber \"Hylocereus undatus\" provides \"pitahaya orejona\", now widely grown in Asia under the name dragon fruit. Other cacti providing edible fruit include species of \"Echinocereus\", \"Ferocactus\", \"Mammillaria\", \"Myrtillocactus\", \"Pachycereus\", \"Peniocereus\" and \"Selenicereus\". The bodies of cacti other than opuntias are less often eaten, although Anderson reported that \"Neowerdermannia vorwerkii\" is prepared and eaten like potatoes in upland Bolivia.\n\nA number of species of cacti have been shown to contain psychoactive agents, chemical compounds that can cause changes in mood, perception and cognition through their effects on the brain. Two species have a long history of use by the indigenous peoples of the Americas: peyote, \"Lophophora williamsii\", in North America, and the San Pedro cactus, \"Echinopsis pachanoi\", in South America. Both contain mescaline.\n\n\"L. williamsii\" is native to northern Mexico and southern Texas. Individual stems are about high with a diameter of , and may be found in clumps up to wide. A large part of the stem is usually below ground. Mescaline is concentrated in the photosynthetic portion of the stem above ground. The center of the stem, which contains the growing point (the apical meristem), is sunken. Experienced collectors of peyote remove a thin slice from the top of the plant, leaving the growing point intact, thus allowing the plant to regenerate. Evidence indicates peyote was in use more than 5,500 years ago; dried peyote buttons presumed to be from a site on the Rio Grande, Texas, were radiocarbon dated to around 3780–3660 BC. Peyote is perceived as a means of accessing the spirit world. Attempts by the Roman Catholic church to suppress its use after the Spanish conquest were largely unsuccessful, and by the middle of the 20th century, peyote was more widely used than ever by indigenous peoples as far north as Canada. It is now used formally by the Native American Church.\n\n\"Echinopsis pachanoi\" is native to Ecuador and Peru. It is very different in appearance from \"L. williamsii\". It has tall stems, up to high, with a diameter of , which branch from the base, giving the whole plant a shrubby or tree-like appearance. Archaeological evidence of the use of this cactus appears to date back to 2,000–2,300 years ago, with carvings and ceramic objects showing columnar cacti. Although church authorities under the Spanish attempted to suppress its use, this failed, as shown by the Christian element in the common name \"San Pedro cactus\"—Saint Peter cactus. Anderson attributes the name to the belief that just as St Peter holds the keys to heaven, the effects of the cactus allow users \"to reach heaven while still on earth.\" It continues to be used for its psychoactive effects, both for spiritual and for healing purposes, often combined with other psychoactive agents, such as \"Datura ferox\" and tobacco. Several other species of \"Echinopsis\", including \"E. peruviana\", also contain mescaline.\n\nCacti were cultivated as ornamental plants from the time they were first brought from the New World. By the early 1800s, enthusiasts in Europe had large collections (often including other succulents alongside cacti). Rare plants were sold for very high prices. Suppliers of cacti and other succulents employed collectors to obtain plants from the wild, in addition to growing their own. In the late 1800s, collectors turned to orchids, and cacti became less popular, although never disappearing from cultivation.\n\nCacti are often grown in greenhouses, particularly in regions unsuited to the cultivation of cacti outdoors, such the northern parts of Europe and North America. Here, they may be kept in pots or grown in the ground. Cacti are also grown as houseplants, many being tolerant of the often dry atmosphere. Cacti in pots may be placed outside in the summer to ornament gardens or patios, and then kept under cover during the winter. Less drought-resistant epiphytes, such as epiphyllum hybrids, \"Schlumbergera\" (the Thanksgiving or Christmas cactus) and \"Hatiora\" (the Easter cactus), are widely cultivated as houseplants.\n\nCacti may also be planted outdoors in regions with suitable climates. Concern for water conservation in arid regions has led to the promotion of gardens requiring less watering (xeriscaping). For example, in California, the East Bay Municipal Utility District sponsored the publication of a book on plants and landscapes for summer-dry climates. Cacti are one group of drought-resistant plants recommended for dry landscape gardening.\n\nCacti have many other uses. They are used for human food and as fodder for animals, usually after burning off their spines. In addition to their use as psychoactive agents, some cacti are employed in herbal medicine. The practice of using various species of \"Opuntia\" in this way has spread from the Americas, where they naturally occur, to other regions where they grow, such as India.\n\nCochineal is a red dye produced by a scale insect that lives on species of \"Opuntia\". Long used by the peoples of Central and North America, demand fell rapidly when European manufacturers began to produce synthetic dyes in the middle of the 19th century. Commercial production has now increased following a rise in demand for natural dyes.\n\nCacti are used as construction materials. Living cactus fences are employed as barricades. The woody parts of cacti, such as \"Cereus repandus\" and \"Echinopsis atacamensis\", are used in buildings and in furniture. The frames of wattle and daub houses built by the Seri people of Mexico may use parts of \"Carnegiea gigantea\". The very fine spines and hairs (trichomes) of some cacti were used as a source of fiber for filling pillows and in weaving.\n\nAll cacti are included in Appendix II of the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES), which \"lists species that are not necessarily now threatened with extinction but that may become so unless trade is closely controlled.\" Control is exercised by making international trade in most specimens of cacti illegal unless permits have been issued, at least for exports. Some exceptions are allowed, e.g., for \"naturalized or artificially propagated plants\". Some cacti, such as all \"Ariocarpus\" and \"Discocactus\" species, are included in the more restrictive Appendix I, used for the \"most endangered\" species. These may only be moved between countries for scientific purposes, and only then when accompanied by both export and import permits.\n\nThe three main threats to cacti in the wild are development, grazing and over-collection. Development takes many forms. The construction of a dam near Zimapan, Mexico, caused the destruction of a large part of the natural habitat of \"Echinocactus grusonii\". Urban development and highways have destroyed cactus habitats in parts of Mexico, New Mexico and Arizona, including the Sonoran Desert. The conversion of land to agriculture has affected populations of \"Ariocarpus kotschoubeyanus\" in Mexico, where dry plains were plowed for maize cultivation, and of \"Copiapoa\" and \"Eulychnia\" in Chile, where valley slopes were planted with vines. Grazing, in many areas by introduced animals, such as goats, has caused serious damage to populations of cacti (as well as other plants); two examples cited by Anderson are the Galápagos Islands generally and the effect on \"Browningia candelaris\" in Peru. Over-collection of cacti for sale has greatly affected some species. For example, the type locality of \"Pelecyphora strobiliformis\" near Miquihuana, Mexico, was virtually denuded of plants, which were dug up for sale in Europe. Illegal collecting of cacti from the wild continues to pose a threat.\n\nConservation of cacti can be \"in situ\" or \"ex situ\". \"In situ\" conservation involves preserving habits through enforcement of legal protection and the creation of specially protected areas such as national parks and reserves. Examples of such protected areas in the United States include Big Bend National Park, Texas; Joshua Tree National Park, California; and Saguaro National Park, Arizona. Latin American examples include Parque Nacional del Pinacate, Sonora, Mexico and Pan de Azúcar National Park, Chile. \"Ex situ\" conservation aims to preserve plants and seeds outside their natural habitats, often with the intention of later reintroduction. Botanical gardens play an important role in \"ex situ\" conservation; for example, seeds of cacti and other succulents are kept in long-term storage at the Desert Botanical Garden, Arizona.\n\nThe popularity of cacti means many books are devoted to their cultivation. Cacti naturally occur in a wide range of habitats and are then grown in many countries with different climates, so precisely replicating the conditions in which a species normally grows is usually not practical. A broad distinction can be made between semidesert cacti and epiphytic cacti, which need different conditions and are best grown separately. This section is primarily concerned with the cultivation of semidesert cacti in containers and under protection, such as in a greenhouse or in the home, rather than cultivation outside in the ground in those climates that permit it. For the cultivation of epiphytic cacti, see Cultivation of \"Schlumbergera\" (Christmas or Thanksgiving cacti), and Cultivation of epiphyllum hybrids.\n\nThe purpose of the growing medium is to provide support and to store water, oxygen and dissolved minerals to feed the plant. In the case of cacti, there is general agreement that an open medium with a high air content is important. When cacti are grown in containers, recommendations as to how this should be achieved vary greatly; Miles Anderson says that if asked to describe a perfect growing medium, \"ten growers would give 20 different answers\". Roger Brown suggests a mixture of two parts commercial soilless growing medium, one part hydroponic clay and one part coarse pumice or perlite, with the addition of soil from earthworm castings. The general recommendation of 25–75% organic-based material, the rest being inorganic such as pumice, perlite or grit, is supported by other sources. However, the use of organic material is rejected altogether by others; Hecht says that cacti (other than epiphytes) \"want soil that is low in or free of humus\", and recommends coarse sand as the basis of a growing medium.\n\nSemi-desert cacti need careful watering. General advice is hard to give, since the frequency of watering required depends on where the cacti are being grown, the nature of the growing medium, and the original habitat of the cacti. Brown says that more cacti are lost through the \"untimely application of water than for any other reason\" and that even during the dormant winter season, cacti need some water. Other sources say that water can be withheld during winter (November to March in the Northern Hemisphere). Another issue is the hardness of the water; where it is necessary to use hard water, regular re-potting is recommended to avoid the build up of salts. The general advice given is that during the growing season, cacti should be allowed to dry out between thorough waterings. A water meter can help in determining when the soil is dry.\n\nAlthough semi-desert cacti may be exposed to high light levels in the wild, they may still need some shading when subjected to the higher light levels and temperatures of a greenhouse in summer. Allowing the temperature to rise above is not recommended. The minimum winter temperature required depends very much on the species of cactus involved. For a mixed collection, a minimum temperature of between and is often suggested, except for cold-sensitive genera such as \"Melocactus\" and \"Discocactus\". Some cacti, particularly those from the high Andes, are fully frost-hardy when kept dry (e.g. \"Rebutia minuscula\" survives temperatures down to in cultivation) and may flower better when exposed to a period of cold.\n\nCacti can be propagated by seed, cuttings or grafting. Seed sown early in the year produces seedlings that benefit from a longer growing period. Seed is sown in a moist growing medium and then kept in a covered environment, until 7–10 days after germination, to avoid drying out. A very wet growing medium can cause both seeds and seedlings to rot. A temperature range of is suggested for germination; soil temperatures of around promote the best root growth. Low light levels are sufficient during germination, but afterwards semi-desert cacti need higher light levels to produce strong growth, although acclimatization is needed to conditions in a greenhouse, such as higher temperatures and strong sunlight.\n\nReproduction by cuttings makes use of parts of a plant that can grow roots. Some cacti produce \"pads\" or \"joints\" that can be detached or cleanly cut off. Other cacti produce offsets that can be removed. Otherwise, stem cuttings can be made, ideally from relatively new growth. It is recommended that any cut surfaces be allowed to dry for a period of several days to several weeks until a callus forms over the cut surface. Rooting can then take place in an appropriate growing medium at a temperature of around .\n\nGrafting is used for species difficult to grow well in cultivation or that cannot grow independently, such as some chlorophyll-free forms with white, yellow or red bodies, or some forms that show abnormal growth (e.g., cristate or monstrose forms). For the host plant—the \"stock\"—growers choose one that grows strongly in cultivation and is compatible with the plant to be propagated—the \"scion.\" The grower makes cuts on both stock and scion and joins the two, binding them together while they unite. Various kinds of graft are used—flat grafts, where both scion and stock are of similar diameters, and cleft grafts, where a smaller scion is inserted into a cleft made in the stock.\n\nCommercially, huge numbers of cacti are produced annually. For example, in 2002 in Korea alone, 49 million plants were propagated, with a value of almost US$9 million. Most of them, 31 million plants, were propagated by grafting.\n\nA range of pests attack cacti in cultivation. Those that feed on sap include: mealybugs, living on both stems and roots; scale insects, generally only found on stems; whiteflies, which are said to be an \"infrequent\" pest of cacti; red spider mites, which are very small but can occur in large numbers, constructing a fine web around themselves and badly marking the cactus via their sap sucking, even if they do not kill it; and thrips, which particularly attack flowers. Some of these pests are resistant to many insecticides, although there are biological controls available. Roots of cacti can be eaten by the larvae of sciarid flies and fungus gnats. Slugs and snails also eat cacti.\n\nFungi, bacteria and viruses attack cacti, the first two particularly when plants are over-watered. Fusarium rot can gain entry through a wound and cause rotting accompanied by red-violet mold. \"Helminosporium rot\" is caused by \"Bipolaris cactivora\" (syn. \"Helminosporium cactivorum\"); \"Phytophthora\" species also cause similar rotting in cacti. Fungicides may be of limited value in combating these diseases. Several viruses have been found in cacti, including cactus virus X. These appear to cause only limited visible symptoms, such as chlorotic (pale green) spots and mosaic effects (streaks and patches of paler color). However, in an \"Agave\" species, cactus virus X has been shown to reduce growth, particularly when the roots are dry. There are no treatments for virus diseases.\n\n\n", "id": "7819", "title": "Cactus"}
{"url": "https://en.wikipedia.org/wiki?curid=7820", "text": "CCC\n\nCCC may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "7820", "title": "CCC"}
{"url": "https://en.wikipedia.org/wiki?curid=7821", "text": "Civilian Conservation Corps\n\nThe civilian Conservation Corps (\"CCC\") was a public work relief program that operated from 1933 to 1942 in the United States for unemployed, unmarried men from relief families as part of the New Deal. Originally for young men ages 18–25, it was eventually expanded to young men ages 17–28. Robert Fechner was the head of the agency. It was a major part of President Franklin D. Roosevelt's New Deal that provided unskilled manual labor jobs related to the conservation and development of natural resources in rural lands owned by federal, state and local governments. The CCC was designed to provide jobs for young men, and to relieve families who had difficulty finding jobs during the Great Depression in the United States. At the same time, it implemented a general natural resource conservation program in every state and territory. Maximum enrollment at any one time was 300,000. Over the course of its nine years in operation, 3 million young men participated in the CCC, which provided them with shelter, clothing, and food, together with a small wage of $30 (about $547 in 2015) a month ($25 of which had to be sent home to their families).\n\nThe American public made the CCC the most popular of all the New Deal programs. Principal benefits of an individual's enrollment in the CCC included improved physical condition, heightened morale, and increased employability. The CCC also led to a greater public awareness and appreciation of the outdoors and the nation's natural resources, and the continued need for a carefully planned, comprehensive national program for the protection and development of natural resources.\n\nDuring the time of the CCC, enrollees planted nearly 3 billion trees to help reforest America, constructed trails, lodges and related facilities in more than 800 parks nationwide and upgraded most state parks, updated forest fire fighting methods, and built a network of service buildings and public roadways in remote areas.\n\nThe CCC operated separate programs for veterans and Native Americans. Approximately 15,000 Native Americans participated in the program, helping them weather the Great Depression.\n\nDespite its popular support, the CCC was never a permanent agency. It depended on emergency and temporary Congressional legislation and funding to operate. By 1942, with World War II and the draft in operation, the need for work relief declined, and Congress voted to close the program.\n\nAs governor of New York, Roosevelt had run a similar program on a much smaller scale. Long interested in conservation, as president, he proposed to Congress a full-scale national program on March 21, 1933:\nHe promised this law would provide 250,000 young men with meals, housing, uniforms, and medical care for working in the national forests and other government properties. The Emergency Conservation Work (ECW) Act was introduced to Congress the same day and enacted by voice vote on March 31. Roosevelt issued Executive Order 6101 on April 5, 1933 which established the CCC organization and appointed a director, Robert Fechner, a former labor union official who served until 1939. The organization and administration of the CCC was a new experiment in operations for a federal government agency. The order indicated that the program was to be supervised jointly by four government departments: Labor, which recruited the young men, War, which operated the camps, and Agriculture and Interior, which organized and supervised the work projects. A CCC Advisory Council was composed of a representative from each of the supervising departments. In addition, the Office of Education and Veterans Administration participated in the program. To end the opposition from labor unions (which wanted no training programs started when so many of their men were unemployed) Roosevelt chose Robert Fechner, vice president of the American Machinists Union, as director of the corps. William Green, head of the American Federation of Labor, was taken to the first camp to demonstrate that there would be no job training involved beyond simple manual labor.\n\nReserve officers from the U.S. Army were in charge of the camps, but there was no military training. General Douglas MacArthur was placed in charge of the program but said that the number of Army officers and soldiers assigned to the camps was affecting the readiness of the Regular Army. But the Army also found numerous benefits in the program. When the draft began in 1940, the policy was to make CCC alumni corporals and sergeants. CCC also provided command experience to Organized Reserve Corps officers. Through the CCC, the Regular Army could assess the leadership performance of both Regular and Reserve Officers. The CCC provided lessons which the Army used in developing its wartime and mobilization plans for training camps.\n\nThe legislation and mobilization of the program occurred quite rapidly. Roosevelt made his request to Congress on March 21, 1933; the legislation was submitted to Congress the same day; Congress passed it by voice vote on March 31; Roosevelt signed it the same day, then issued an executive order on April 5 creating the agency, appointing its director (Fechner), and assigning War Department corps area commanders to begin enrollment. The first CCC enrollee was selected April 8 and subsequent lists of unemployed men were supplied by state and local welfare and relief agencies for immediate enrollment. On April 17 the first camp, NF-1, Camp Roosevelt, was established at George Washington National Forest near Luray, Virginia. On June 18, the first of 161 soil erosion control camps was opened, in Clayton, Alabama. By July 1, 1933 there were 1,463 working camps with 250,000 junior enrollees (18–25 years of age); 28,000 veterans; 14,000 American Indians; and 25,000 Locally Enrolled (or Experienced) Men (LEM).\n\nThe typical CCC enrollee was a U.S. citizen, unmarried, unemployed male, 18–25 years of age. Normally his family was on local relief. Each enrollee volunteered and, upon passing a physical exam and/or a period of conditioning, was required to serve a minimum six-month period, with the option to serve as many as four periods, or up to two years, if employment outside the Corps was not possible. Enrollees worked 40 hours a week over five days, sometimes including Saturdays if poor weather dictated. In return they received $30 a month with a compulsory allotment of $22–25 sent to a family dependent, as well as food, clothing, and medical care.\n\nFollowing the second Bonus Army march on Washington D.C., President Roosevelt issued Executive Order 6129 (May 11, 1933) to amend the CCC program, to include work opportunities for veterans. Veteran qualifications differed from the junior enrollee; one needed to be certified by the Veterans Administration by application. They could be any age, and married or single as long as they were in need of work. Veterans were generally assigned to entire veteran camps. Enrollees were eligible for the following \"rated\" positions to help with camp administration: senior leader, mess steward, store keeper and two cooks; assistant leader, company clerk, assistant educational advisor and three second cooks. These men received additional pay ranging from $36 to $45 per month depending on their rating.\n\nEach CCC camp was located in the area of particular conservation work to be performed, and organized around a complement of up to 200 civilian enrollees in a designated numbered \"company\" unit. The CCC camp was a temporary community in itself, structured to have barracks (initially Army tents) for 50 enrollees each, officer/technical staff quarters, medical dispensary, mess hall, recreation hall, educational building, lavatory and showers, technical/administrative offices, tool room/blacksmith shop and motor pool garages.\n\nThe company organization of each camp had a dual-authority supervisory staff: firstly, Department of War personnel or Reserve officers (until July 1, 1939), a \"company commander\" and junior officer, who were responsible for overall camp operation, logistics, education and training; and secondly, ten to fourteen technical service civilians, including a camp \"superintendent\" and \"foreman\", employed by either the Departments of Interior or Agriculture, responsible for the particular field work. Also included in camp operation were several non-technical supervisor LEMs, who provided knowledge of the work at hand, \"lay of the land,\" and paternal guidance for inexperienced enrollees. Enrollees were organized into work detail units called \"sections\" of 25 men each, according to the barracks they resided in. Each section had an enrollee \"senior leader\" and \"assistant leader\" who were accountable for the men at work and in the barracks.\n\nThe CCC performed 300 types of work projects within ten approved general classifications:\n\nThe responses to this seven-month experimental conservation program were enthusiastic. On October 1, 1933 Director Fechner was directed to arrange for a second period of enrollment. By January 1934, 300,000 men were enrolled. In July 1934 this cap was increased by 50,000 to include men from Midwest states that had been affected by drought. The temporary tent camps had also developed to include wooden barracks. An education program had been established, emphasizing job training and literacy.\n\nApproximately 55% of enrollees were from rural communities, a majority of which were non-farm; 45% came from urban areas. Level of education for the enrollee averaged 3% illiterate, 38% less than eight years of school, 48% did not complete high school, 11% were high school graduates. At the time of entry, 70% of enrollees were malnourished and poorly clothed. Few had work experience beyond occasional odd jobs. Peace was maintained by the threat of \"dishonorable discharge\". \"This is a training station; we're going to leave morally and physically fit to lick 'Old Man Depression,'\" boasted the newsletter, \"Happy Days,\" of a North Carolina camp.\n\nBecause of the power of the Solid South white Democrats in Congress, who insisted on racial segregation, the New Deal programs were racially segregated; blacks and whites rarely worked alongside each other. At this time, all the states of the South had passed legislation imposing racial segregation and, since the turn of the century, laws and constitutional provisions that disenfranchised most blacks; they were excluded from formal politics. Because of discrimination by white officials at the local and state levels, blacks in the South did not receive as many benefits as whites from New Deal programs.\n\nIn the first few weeks of operation, CCC camps in the North were integrated. By July 1935, however, all the camps in the United States were segregated.\n\nA total of 200,000 blacks were enrolled in the CCC; they served in 143 all-black camps and received equal pay and housing. Black leaders lobbied to secure leadership roles. Adult white men held the major leadership roles in all the camps. Director Robert Fechner refused to appoint black adults to any supervisory positions except that of education director in the all-black camps.\n\nThe CCC operated an entirely separate division for members of federally recognized tribes: the \"Indian Emergency Conservation Work\" (IECW or CCC-ID). Native men from reservations worked on roads, bridges, clinics, shelters, and other public works near their reservations. Although they were organized as groups classified as camps, no permanent camps were established for Native Americans. Instead, organized groups moved with their families from project to project, and were provided with an additional rental allowance in their pay. The CCC often provided the only paid work, as many reservations were in remote rural areas. Enrollees had to be between the ages of 17 and 35.\n\nDuring 1933, about half the male heads of households on the Sioux reservations in South Dakota were employed by the CCC-ID. With grants from the Public Works Administration (PWA), the Indian Division built schools and conducted an extensive road-building program in and around many reservations to improve infrastructure. The mission was to reduce erosion and improve the value of Indian lands. Crews built dams of many types on creeks, then sowed grass on the eroded areas from which the damming materials had been taken. They built roads and planted shelter-belts on federal lands. The steady income helped participants regain self-respect and many used the funds to improve their lives. John Collier the federal Commissioner of Indian Affairs and Daniel Murphy, the director of the CCC-ID, both based the program on Indian self-rule and the restoration of tribal lands, governments, and cultures. The next year, Congress passed the Indian Reorganization Act of 1934, which ended allotments and helped preserve tribal lands, and encouraged tribes to re-establish self-government.\n\nCollier said of the CCC-Indian Division, \"no previous undertaking in Indian Service has so largely been the Indians' own undertaking\". Education programs also trained participants in gardening, stock raising, safety, native arts, and some academic subjects. IECW differed from other CCC activities in that it explicitly trained men in skills to be carpenters, truck drivers, radio operators, mechanics, surveyors, and technicians. With the passage of the National Defense Vocational Training Act of 1941, enrollees began participating in defense-oriented training. The government paid for the classes and, after individuals completed courses and passed a competency test, guaranteed automatic employment in the defense work. A total of 85,000 Native Americans were enrolled in this training. This proved valuable social capital for the 24,000 alumni who later served in the military and the 40,000 who left the reservations for city jobs supporting the war effort.\n\nResponding to favorable public opinion to alleviate unemployment, Congress approved the Emergency Relief Appropriation Act of 1935, on April 8, 1935, which included continued funding for the CCC program through March 31, 1937. The age limit was expanded to 18-28 to include more men. From April 1, 1935 to March 31, 1936 was the period of greatest activity and work accomplished by the CCC program. Enrollment had peaked at 505,782 in about 2,900 camps by August 31, 1935, followed by a reduction to 350,000 enrollees in 2,019 camps by June 30, 1936. During this period the public response to the CCC program was overwhelmingly popular. A Gallup poll of April 18, 1936 asked \"Are you in favor of the CCC camps?\"; 82% of respondents said yes, including 92% of Democrats and 67% of Republicans.\n\nOn June 28, 1937 the Civilian Conservation Corps was legally established, transferred from its original designation as the Emergency Conservation Work program. Funding was extended for three more years by Public Law No. 163, 75th Congress, effective July 1, 1937. Congress changed the age limits to 17–23 years old, and changed the requirement that enrollees be on relief to \"not regularly in attendance at school, or possessing full time employment.\" The 1937 law mandated the inclusion of vocational and academic training for a minimum of 10 hours per week. Students in school were allowed to enroll during (summer) vacation. During this period, the CCC forces contributed to disaster relief following 1937 floods in New York, Vermont, and the Ohio and Mississippi river valleys, and response and clean-up after the 1938 hurricane in New England.\n\nIn 1939 Congress ended the independent status of the CCC, transferring it to the control of the Federal Security Agency. The National Youth Administration, U.S. Employment Service, the Office of Education, and the Works Progress Administration also had some responsibilities. About 5,000 Reserve officers for the camps were affected, as they were transferred to federal Civil Service, and military ranks and titles were eliminated. Despite the loss of overt military leadership in the camps by July 1940, with war underway in Europe and Asia, the government directed an increasing number of CCC projects on resources for national defense. It developed infrastructure for military training facilities and forest protection. By 1940 the CCC was no longer wholly a relief agency, was rapidly losing its non-military character, and it was becoming a system for work-training, as its ranks had become increasingly younger, with little experience.\n\nAlthough the CCC was probably the most popular New Deal program, it never was authorized as a permanent agency. The program was reduced in scale as the Depression waned and employment opportunities improved. After conscription (the draft) began in 1940, fewer eligible young men were available. Following the attack on Pearl Harbor in December 1941, the Roosevelt administration directed all federal programs to emphasize the war effort. Most CCC work, except for wildland firefighting, was shifted onto U.S. military bases to help with construction.\n\nThe CCC disbanded one year earlier than planned, as the 77th United States Congress ceased funding it. Operations were formally concluded at the end of the federal fiscal year on June 30, 1942. The end of the CCC program and closing of the camps involved arrangements to leave the incomplete work projects in the best possible state, the separation of about 1,800 appointed employees, the transfer of CCC property to the War and Navy Departments and other agencies, and the preparation of final accountability records. Liquidation of the CCC was ordered by Congress by the Labor-Federal Security Appropriation Act (56 Stat. 569) on July 2, 1942; and virtually completed on June 30, 1943. Liquidation appropriations for the CCC continued through April 20, 1948.\n\nSome former CCC sites in good condition were reactivated from 1941 to 1947 as Civilian Public Service camps where conscientious objectors performed \"work of national importance\" as an alternative to military service. Other camps were used to hold Japanese, German and Italian Americans interned under the Western Defense Command's Enemy Alien Control Program, as well as Axis prisoners of war. Most of the Japanese American internment camps were built by the people held there. After the CCC disbanded, the federal agencies responsible for public lands organized their own seasonal fire crews, modeled after the CCC. These have performed a firefighting function formerly done by the CCC and provided the same sort of outdoor work experience for young people. Approximately 47 young men have died while in this line of duty.\n\n\n\nIn several cities where CCC workers worked, statues were erected to commemorate them.\n\n\nThe CCC program was never officially terminated. Congress provided funding for closing the remaining camps in 1942 with the equipment being reallocated. It became a model for conservation programs that were implemented in the period after World War II. Present-day corps are national, state and local programs that engage primarily youth and young adults (ages 16–25) in community service, training and educational activities. The nation's approximate 113 corps programs operate in 41 states and the District of Columbia. During 2004, they enrolled more than 23,000 young people. The Corps Network, known originally as the National Association of Service and Conservation Corps (NASCC), works to expand and enhance corps-type programs throughout the country. The Corps Network began in 1985, when the nation's first 24 Corps directors banded together to secure an advocate at the federal level and a repository of information on how best to start and manage a corps. Early financial assistance from the Ford, Hewlett and Mott Foundations was critical to establishing the association.\n\nAnother similar program is the National Civilian Community Corps, part of the AmeriCorps program, a team-based national service program in which 18- to 24-year-olds spend 10 months working for non-profit and government organizations.\n\nThe CCC program became a model for the creation of team-based national service youth conservation programs such as the Student Conservation Association (SCA). The SCA, founded in 1959, is a nonprofit organization that offers conservation internships and summer trail crew opportunities to more than 4,000 people each year. The SCA mission is to build a new generation of conservation managers by inspiring lifelong stewardship of the environment and communities by engaging high school and college-age volunteers in hands-on service to the land. SCA program is active nationwide in the USA, including national and state parks, forests, wildlife refuges, seashores and historic sites. SCA National Headquarters is located in Charlestown, New Hampshire with regional offices across the country.\n\nIn 1976, Governor of California Jerry Brown established the California Conservation Corps. This new program had many similar characteristics - residential centers, high expectations for participation, emphasis on hard work on public lands. Young adults from different backgrounds were recruited for a term of one-year. Corps members attended a training session called the Corpsmember Orientation Motivation Education and Training (COMET) program before being assigned to one of the various centers. \nLife at CCC centers is rigorous, starting with early morning exercises, breakfast, roll call and a full day's work. After hours include education, life skills workshops, community meetings, volunteerism. Project work is also similar to the original CCC of the 1930s - work on public forests, state and federal parks.\n\nEstablished in 1995, Environmental Corps, now Texas Conservation Corps (TxCC), is an American YouthWorks program which allows youth, ages 17 to 28, to contribute to the restoration and preservation of parks and public lands in Texas. The only conservation corps in Texas, TxcC is a 501(c)3 non profit corporation based in Austin, Texas, which serves the entire state. Their work ranges from disaster relief to trail building to habitat restoration. TxCC has done projects in national, state, and city parks.\n\nThe Montana Conservation Corps (MCC) is a registered 501(c)3 non-profit organization with a mission to equip young people with the skills and values to be vigorous citizens who improve their communities and environment. Collectively, MCC crews contribute more than 90,000 work hours each year. The MCC was established in 1991 by Montana's Human Resource Development Councils in Billings, Bozeman and Kalispell. Originally, it was a summer program for disadvantaged youth, although it has grown into an AmeriCorps-sponsored non-profit organization with six regional offices that serve Montana, Idaho, Wyoming, and North and South Dakota. All regions also offer MontanaYES (Youth Engaged in Service) summer programs for teenagers who are 14 to 16 years old.\n\nThe Washington Conservation Corps (WCC) is a sub-agency of the Washington State Department of Ecology. It employs men and women 18 to 25 years old in a program to protect and enhance Washington's natural resources. WCC is a part of the AmeriCorps program.\n\nConservation Corps Minnesota & Iowa provides environmental stewardship and service-learning opportunities to youth and young adults while accomplishing conservation, natural resource management projects and emergency response work through its Young Adult Program and the Summer Youth Program. These programs emphasize the development of job and life skills by conservation and community service work.\n\nThe Vermont Youth Conservation Corps (VYCC) is a non-profit, youth service and education organization that hires Corps Members, aged 16–24, to work on high-priority conservation projects in Vermont. Through these work projects, Corps Members develop a strong work ethic, strengthen their leadership skills, and learn how to take personal responsibility for their actions. VYCC Crews work at VT State Parks, U.S. Forest Service Campgrounds, in local communities, and throughout the state's back country. The VYCC has also given aid to a similar program in North Carolina, which is currently in its infancy.\n\nConservation Legacy is a non-profit employment, job training, and education organization with locations across the United States including Arizona Conservation Corps in Tucson and Flagstaff, AZ, Southwest Conservation Corps in Durango and Salida, Colorado, and Southeast Conservation Corps in Chattanooga, TN. Conservation Legacy also operates an AmeriCorps VISTA team serving to improve the environment and economies of historic mining communities in the American West and Appalachia. Conservation Legacy also hosts the Environmental Stewards Program - providing internships with federal, state, municipal and NGO land management agencies nationwide. Conservation Legacy formed as a merger of the Southwest Youth Corps, San Luis Valley Youth Corps, The Youth Corps of Southern Arizona, and Coconino Rural Environmental Corps.\n\nConservation Legacy engages young adults ages 14 to 26 and US military veterans of all ages in personal and professional development experiences involving conservation projects on public lands. Corp members live, work, and learn in teams of six to eight for terms of service ranging from 3 months to 1 year.\n\n\n\n\n\n\n\n\n\n", "id": "7821", "title": "Civilian Conservation Corps"}
