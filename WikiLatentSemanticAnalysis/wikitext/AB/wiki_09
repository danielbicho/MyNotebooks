{"url": "https://en.wikipedia.org/wiki?curid=14148", "text": "Home run\n\nIn baseball, a home run (abbreviated HR, also \"homer\", \"dinger\", \"dong\", \"bomb\", \"blast\", \"goner\" , \"shot\", or \"four-bagger\") is scored when the ball is hit in such a way that the batter is able to circle the bases and reach home \nsafely in one play without any errors being committed by the defensive team in the process. In modern baseball, the feat is typically achieved by hitting the ball over the outfield fence between the foul poles (or making contact with either foul pole) without first touching the ground, resulting in an automatic home run. There is also the \"inside-the-park\" home run, increasingly rare in modern baseball, where the batter reaches home safely while the baseball is in play on the field. When a home run is scored, the batter is also credited with a hit and a run scored, and an RBI for each runner that scores, including himself. Likewise, the pitcher is recorded as having given up a hit, and a run for each runner that scores including the batter.\n\nHomeruns are among the most popular aspects of baseball and, as a result, prolific home run hitters are usually the most popular among fans and consequently the highest paid by teams—hence the old saying, variously attributed to slugger Ralph Kiner, or to a teammate talking about Kiner, \"Home run hitters drive Cadillacs, and singles hitters drive Fords.\"\n\nIn modern times a home run is most often scored when the ball is hit over the outfield wall between the foul poles (in fair territory) before it touches the ground (in flight), and without being caught or deflected back onto the field by a fielder. A batted ball is also a home run if it touches either foul pole or its attached screen before touching the ground, as the foul poles are by definition in fair territory. Furthermore, many major-league ballparks have ground rules stating that a batted ball striking specific architectural features while in flight is a home run due to their location beyond the outfield wall. \n\nA batted ball that goes over the outfield wall \"after\" touching the ground is not a home run, but an automatic double for the batter (often colloqually called a \"ground rule double\").\n\nA fielder is allowed to reach over the wall to attempt to catch the ball as long as his feet are on or over the field during the attempt. If the fielder successfully catches the ball while it is in flight the batter is out, even if the ball has already passed the vertical plane of the wall. However, since the fielder is not part of the field, a ball that bounces off a fielder and over the wall without touching the ground is still a home run. A fielder may not deliberately throw his glove, cap, or any other equipment or apparel to stop or deflect a fair ball, and an umpire may award a home run to the batter if a fielder does so on a ball that, in the umpire's judgment, would have otherwise been a home run (this is rare in modern professional baseball).\n\nA home run accomplished in any of the above manners is an automatic home run. The ball is dead, even if it rebounds onto the field from striking a foul pole or an object beyond the outfield wall, and the batter and any preceding runners cannot be put out at any time while running the bases. However, if one or more runners fail to touch a base or one runner passes another before reaching home plate, that runner or runners can be called out on appeal, though in the case of not touching a base a runner can go back and touch it if doing so won't cause them to be passed by another preceding runner and they have not yet touched the next base (or home plate in the case of missing third base). This stipulation is in Approved Ruling (2) of Rule 7.10(b).\n\nAn inside-the-park home run occurs when a batter hits the ball into play and is able to circle the bases before the fielders can put him out. Unlike with an outside-the-park home run, the batter-runner and all preceding runners are liable to be put out by the defensive team at any time while running the bases. This can only happen if the ball does not leave the ballfield.\n\nIn the early days of baseball, outfields were relatively much more spacious, reducing the likelihood of an over-the-fence home run, while increasing the likelihood of an inside-the-park home run, as a ball getting past an outfielder had more distance that it could roll before a fielder could track it down.\n\nWith outfields much less spacious and more uniformly designed than in the game's early days, inside-the-park home runs are now a rarity. They are usually the result of a ball being hit by a very fast runner, coupled with an outfielder either misjudging the flight of the ball (e.g., diving and missing) or the ball taking an unexpected bounce. Either way, this sends the ball into open space in the outfield and thereby allows the batter-runner to circle the bases before the defensive team can put him out. The speed of the runner is crucial as even triples are relatively rare in most modern ballparks.\n\nIf any defensive play on an inside-the-park home run is labeled an error by the official scorer, a home run is not scored; instead, it is scored as a single, double, or triple, and the batter-runner and any applicable preceding runners are said to have taken all additional bases on error. All runs scored on such a play, however, still count.\n\nAn example of an unexpected bounce occurred during the 2007 Major League Baseball All-Star Game at AT&T Park in San Francisco on July 10, 2007. Ichiro Suzuki of the American League team hit a fly ball off the right-center field wall, which caromed in the opposite direction from where National League right fielder Ken Griffey, Jr. was expecting it to go. By the time the ball was relayed, Ichiro had already crossed the plate standing up. This was the first inside-the-park home run in All-Star Game history, and led to Suzuki being named the game's Most Valuable Player.\n\nHome runs are often characterized by the number of runners on base at the time. A home run hit with the bases empty is seldom called a \"one-run homer\", but rather a solo home run, solo homer, or \"solo shot\". With one runner on base, two runs are scored (the baserunner and the batter) and thus the home run is often called a two-run homer or two-run shot. Similarly, a home runs with two runners on base is a three-run homer or three-run shot.\n\nThe term \"four-run homer\" is seldom used; instead, it is nearly always called a \"grand slam\". Hitting a grand slam is the best possible result for the batter's turn at bat and the worst possible result for the pitcher and his team.\n\nA grand slam occurs when the bases are \"loaded\" (that is, there are base runners standing at first, second, and third base) and the batter hits a home run. According to \"The Dickson Baseball Dictionary\", the term originated in the card game of contract bridge. An inside-the-park grand slam is a grand slam that is also an inside-the-park home run, a home run without the ball leaving the field, and it is very rare, due to the relative rarity of loading the bases along with the significant rarity (nowadays) of inside-the-park home runs.\n\nOn July 25, 1956, Roberto Clemente became the only MLB player to have ever scored a walk-off inside-the-park grand slam in a 9–8 Pittsburgh Pirates win over the Chicago Cubs, at Forbes Field.\n\nOn April 23, 1999, Fernando Tatís made history by hitting two grand slams in one inning, both against Chan Ho Park of the Los Angeles Dodgers. With this feat, Tatís also set a Major League record with 8 RBI in one inning.\n\nOn July 29, 2003 against the Texas Rangers, Bill Mueller of the Boston Red Sox became the only player in major league history to hit two grand slams in one game from opposite sides of the plate. In fact, he hit three home runs in that game, and his two grand slams were in consecutive at-bats.\n\nOn August 25, 2011 the New York Yankees became the first team to hit three grand slams in one game vs the Oakland A's. The Yankees eventually went on to win the game 22–9, after trailing 7–1.\n\nThese types of home runs are characterized by the specific game situation in which they occur, and can theoretically occur on either an outside-the-park or inside-the-park home run.\n\nA walk-off home run is a home run hit by the home team in the bottom of the ninth inning, any extra inning, or other scheduled final inning, which gives the home team the lead and thereby ends the game. The term is attributed to Hall of Fame relief pitcher Dennis Eckersley, so named because after the run is scored, the losing team has to \"walk off\" the field.\n\nTwo World Series have ended via the \"walk-off\" home run. The first was the 1960 World Series when Bill Mazeroski of the Pittsburgh Pirates hit a 9th inning solo home run in the 7th game of the series off New York Yankees pitcher Ralph Terry to give the Pirates the World Championship. The second time was the 1993 World Series when Joe Carter of the Toronto Blue Jays hit a 9th inning 3-run home run off Philadelphia Phillies pitcher Mitch Williams in Game 6 of the series, to help the Toronto Blue Jays capture their second World Series Championship in a row.\n\nSuch a home run can also be called a \"sudden death\" or \"sudden victory\" home run. That usage has lessened as \"walk-off home run\" has gained favor. Along with Mazeroski's 1960 shot, the most famous walk-off or sudden-death homer would probably be the \"Shot Heard 'Round the World\" hit by Bobby Thomson to win the 1951 National League pennant for the New York Giants.\n\nA walk-off home run over the fence is an exception to baseball's one-run rule. Normally if the home team is tied or behind in the ninth or extra innings the game ends as soon as the home team scores enough runs to achieve a lead. If the home team has two outs in the inning, and the game is tied, the game will officially end either the moment the batter successfully reaches 1st base or the moment the runner touches home plate—whichever happens last. However, this is superseded by the \"ground rule\", which provides automatic doubles (when a ball-in-play hits the ground first then leaves the playing field) and home runs (when a ball-in-play leaves the playing field without ever touching the ground). In the latter case, all base runners including the batter are allowed to cross the plate.\n\nA lead-off home run is a home run hit by the first batter of a team, the leadoff hitter of the first inning of the game. In MLB, Rickey Henderson holds the record with 81 lead-off home runs. Craig Biggio holds the National League record with 53, second overall to Henderson.\n\nIn 1996, Brady Anderson set a Major League record by hitting a lead-off home run in four consecutive games.\n\nWhen two consecutive batters each hit a home run, this is described as back-to-back home runs. It is still considered back-to-back even if both batters hit their home runs off different pitchers. A third batter hitting a home run is commonly referred to as back-to-back-to-back, the most recent occurrence on opening day, April 4, 2016 when Denard Span, Joe Panik and Buster Posey of the San Francisco Giants hit back-to-back-to-back home runs off Ariel Pena of the Milwaukee Brewers.\n\nFour home runs in a row by consecutive batters has only occurred eight times in the history of Major League Baseball. Following convention, this is called back-to-back-to-back-to-back. The most recent occurrence was on August 11, 2010, when the Arizona Diamondbacks hit four in a row against the Milwaukee Brewers in Miller Park as Adam LaRoche, Miguel Montero, Mark Reynolds and Stephen Drew homered off pitcher Dave Bush. Bush became the third pitcher to surrender back-to-back-to-back-to-back home runs, following Paul Foytack on July 31, 1963 and Chase Wright on April 22, 2007.\n\nOn August 14, 2008, the Chicago White Sox defeated the Kansas City Royals 9-2. In this game, Jim Thome, Paul Konerko, Alexei Ramírez, and Juan Uribe hit back-to-back-to-back-to-back home runs in that order. Thome, Konerko, and Ramirez blasted their homers off of Joel Peralta, while Uribe did it off of Rob Tejeda. The next batter, veteran backstop Toby Hall, tried aimlessly to hit the ball as far as possible, but his effort resulted in a strike out.\n\nOn April 22, 2007 the Boston Red Sox were trailing the New York Yankees 3–0 when Manny Ramirez, J. D. Drew, Mike Lowell and Jason Varitek hit back-to-back-to-back-to-back home runs to put them up 4–3. They eventually went on to win the game 7–6 after a three-run home run by Mike Lowell in the bottom of the 7th inning. On September 18, 2006 trailing 9–5 to the San Diego Padres in the 9th inning, Jeff Kent, J. D. Drew, Russell Martin, and Marlon Anderson of the Los Angeles Dodgers hit back-to-back-to-back-to-back home runs to tie the game. After giving up a run in the top of the 10th, the Dodgers won the game in the bottom of the 10th, on a walk-off two run home run by Nomar Garciaparra. J. D. Drew has been part of two different sets of back-to-back-to-back-to-back home runs. In both occurrences, his homer was the second of the four.\n\nOn September 30, 1997, in the sixth inning of Game One of the American League Division Series between the New York Yankees and Cleveland Indians, Tim Raines, Derek Jeter and Paul O'Neill hit back-to-back-to-back home runs for the Yankees. Raines' home run tied the game. New York went on to win 8–6. This was the first occurrence of three home runs in a row ever in postseason play. The Boston Red Sox repeated the feat in Game Four of the 2007 American League Championship Series, also against the Indians.\n\nTwice in MLB history have two brothers hit back-to-back home runs. On April 23, 2013, brothers Melvin Upton, Jr. (formerly B.J. Upton) and Justin Upton hit back-to-back home runs. The first time was on September 15, 1938, when Lloyd Waner and Paul Waner performed the feat.\n\nSimple back-to-back home runs are a relatively frequent occurrence. If a pitcher gives up a homer, he might have his concentration broken and might alter his normal approach in an attempt to \"make up for it\" by striking out the next batter with some fastballs. Sometimes the next batter will be expecting that and will capitalize on it. A notable back-to-back home run of that type in World Series play involved \"Babe Ruth's called shot\" in 1932, which was accompanied by various Ruthian theatrics, yet the pitcher, Charlie Root, was allowed to stay in the game. He delivered just one more pitch, which Lou Gehrig drilled out of the park for a back-to-back shot, after which Root was removed from the game.\n\nIn Game 3 of the 1976 NLCS, George Foster and Johnny Bench hit back-to-back homers in the last of the ninth off Ron Reed to tie the game. The Series-winning run was scored later in the inning.\n\nAnother notable pair of back-to-back home runs occurred on September 14, 1990, when Ken Griffey, Sr. and Ken Griffey, Jr. hit back-to-back home runs, off Kirk McCaskill, the only father-and-son duo to do so in Major League history.\n\nOn May 2, 2002, Bret Boone and Mike Cameron of the Seattle Mariners hit back-to-back home runs off of starter Jon Rauch in the first inning of a game against the Chicago White Sox. The Mariners batted around in the inning, and Boone and Cameron came up to bat against reliever Jim Parque with two outs, again hitting back-to-back home runs and becoming the only pair of teammates to hit back-to-back home runs twice in the same inning.\n\nOn June 19, 2012, José Bautista and Colby Rasmus hit back-to-back home runs and back-to-back-to-back home runs with Edwin Encarnación for a lead change in each instance.\n\nThe occurrence of individuals hitting home runs in consecutive at-bats is not unusual, but three or more is rare. If a player hits three home runs in a game, it is termed a hat-trick (which is also used for a player getting three strikeouts in a game). The record for consecutive home runs by a batter under any circumstances is 4. Of the sixteen players (through 2012) who have hit 4 in one game, six have hit them consecutively. Twenty-eight other batters have hit four consecutive across two games.\n\nBases on balls do not count as at-bats, and Ted Williams holds the record for consecutive home runs across the most games, 4 in four games played, during September 17–22, 1957, for the Red Sox. Williams hit a pinch-hit homer on the 17th; walked as a pinch-hitter on the 18th; there was no game on the 19th; hit another pinch-homer on the 20th; homered and then was lifted for a pinch-runner after at least one walk, on the 21st; and homered after at least one walk on the 22nd. All in all, he had 4 walks interspersed among his 4 homers.\n\nIn World Series play, Reggie Jackson hit a record three in one Series game, the final game (Game 6) in 1977. But those three were a part of a much more impressive feat. He walked on four pitches in the second inning of game 6. Then he hit his three home runs on the first pitch of his next three at bats, off of three different pitchers (4th inning- Hooten, 5th inning- Sosa, 8th inning- Hough). He had also hit one in his last at bat of the previous game, giving him four home runs on four consecutive swings. (His home run in game 5 was also hit on the first pitch, although this did not add to any significant streak.) The four in a row set the record for consecutive homers across two Series games.\n\nIn Game 3 of the World Series in 2011, Albert Pujols hit three home runs to tie the record with Babe Ruth and Reggie Jackson. The St. Louis Cardinals went on to win the World Series in Game 7 at Busch Stadium. In Game 1 of the World Series in 2012, Pablo Sandoval of the San Francisco Giants hit three home runs on his first three at-bats of the Series, also tying the record with Pujols, Jackson, and Ruth.\n\nNomar Garciaparra holds the record for consecutive home runs in the shortest time in terms of innings: 3 homers in 2 innings, on July 23, 2002, for the Boston Red Sox.\n\nAn offshoot of hitting for the cycle, a \"home run cycle\" is where a player hits a solo, 2-run, 3-run, and grand slam home run all in one game. This is an extremely rare feat, as it requires the batter to not only hit four home runs in a game (which itself has only occurred 16 times in the Major Leagues), but also to hit those home runs with the specific number of runners already on base. Although it is a rare accomplishment, it is largely dependent on circumstances outside the player's control, such as his preceding teammates' ability to get on base, as well as the order in which he comes to bat in any particular inning.\n\nAnother variant of the home run cycle would be the \"natural home run cycle\", which would require a batter to hit a solo, 2-run, 3-run, and grand slam home run in that order.\n\nThough multiple home run cycles have been recorded in collegiate baseball, the only home run cycle in a professional baseball game belongs to Tyrone Horne, who stroked four long balls for the minor league, Double-A Arkansas Travelers in a game against the San Antonio Missions on July 27, 1998.\n\nOn May 20, 1998, the Triple-A Indianapolis Indians performed a feat possibly never before duplicated in professional baseball. In the fifth inning of a game against the Pawtucket Red Sox, Indianapolis players hit for a \"Homer Cycle\". Pete Rose, Jr. opened the inning with a solo home run, Jason Williams connected for a 3-run shot, Glenn Murray slugged a grand slam, and Guillermo Garcia finished the scoring with a 2-run blast. The Indians won the game 11–4.\n\nA major league player has come close to hitting for the home run cycle several times. Recent examples include:\n\nIn the early days of the game, when the ball was less lively and the ballparks generally had very large outfields, most home runs were of the inside-the-park variety. The first home run ever hit in the National League was by Ross Barnes of the Chicago White Stockings (now known as the Chicago Cubs), in 1876. The home \"run\" was literally descriptive. Home runs over the fence were rare, and only in ballparks where a fence was fairly close. Hitters were discouraged from trying to hit home runs, with the conventional wisdom being that if they tried to do so they would simply fly out. This was a serious concern in the 19th century, because in baseball's early days a ball caught after one bounce was still an out. The emphasis was on place-hitting and what is now called \"manufacturing runs\" or \"small ball\".\n\nThe home run's place in baseball changed dramatically when the live-ball era began after World War I. First, the materials and manufacturing processes improved significantly, making the now-mass-produced, cork-centered ball somewhat more lively. Batters such as Babe Ruth and Rogers Hornsby took full advantage of rules changes that were instituted during the 1920s, particularly prohibition of the spitball, and the requirement that balls be replaced when worn or dirty. These changes resulted in the baseball being easier to see and hit, and easier to hit out of the park. Meanwhile, as the game's popularity boomed, more outfield seating was built, shrinking the size of the outfield and increasing the chances of a long fly ball resulting in a home run. The teams with the sluggers, typified by the New York Yankees, became the championship teams, and other teams had to change their focus from the \"inside game\" to the \"power game\" in order to keep up.\n\nBefore 1931, a ball that bounced over an outfield fence during a major league game was considered a home run. The rule was changed to require the ball to clear the fence on the fly, and balls that reached the seats on a bounce became ground rule doubles in most parks. A carryover of the old rule is that if a player deflects a ball over the outfield fence without it touching the ground, it is a home run.\n\nAlso, until approximately that time, the ball had to not only go over the fence in fair territory, but to land in the bleachers in fair territory or to still be visibly fair when disappearing behind a wall. The rule stipulated \"fair when last seen\" by the umpires. Photos from that era in ballparks, such as the Polo Grounds and Yankee Stadium, show ropes strung from the foul poles to the back of the bleachers, or a second \"foul pole\" at the back of the bleachers, in a straight line with the foul line, as a visual aid for the umpire. Ballparks still use a visual aid much like the ropes; a net or screen attached to the foul poles on the fair side has replaced ropes. As with American football, where a touchdown once required a literal \"touch down\" of the ball in the end zone but now only requires the \"breaking of the [vertical] plane\" of the goal line, in baseball the ball need only \"break the plane\" of the fence in fair territory (unless the ball is caught by a player who is in play, in which case the batter is called out).\n\nBabe Ruth's 60th home run in 1927 was somewhat controversial, because it landed barely in fair territory in the stands down the right field line. Ruth lost a number of home runs in his career due to the when-last-seen rule. Bill Jenkinson, in \"The Year Babe Ruth Hit 104 Home Runs\", estimates that Ruth lost at least 50 and as many as 78 in his career due to this rule.\n\nFurther, the rules once stipulated that an over-the-fence home run in a sudden-victory situation would only count for as many bases as was necessary to \"force\" the winning run home. For example, if a team trailed by two runs with the bases loaded, and the batter hit a fair ball over the fence, it only counted as a triple, because the runner immediately ahead of him had technically already scored the game-winning run. That rule was changed in the 1920s as home runs became increasingly frequent and popular. Babe Ruth's career total of 714 would have been one higher had that rule not been in effect in the early part of his career.\n\nMajor League Baseball keeps running totals of all-time home runs by team, including teams no longer active (prior to 1900) as well as by individual players. Gary Sheffield hit the 250,000th home run in MLB history with a grand slam on September 8, 2008. Sheffield had hit MLB's 249,999th home run against Gio Gonzalez in his previous at-bat.\n\nThe all-time, verified professional baseball record for career home runs for one player, excluding the U. S. Negro Leagues during the era of segregation, is held by Sadaharu Oh. Oh spent his entire career playing for the Yomiuri Giants in Japan's Nippon Professional Baseball, later managing the Giants, the Fukuoka SoftBank Hawks and the 2006 World Baseball Classic Japanese team. Oh holds the all-time home run world record, having hit 868 home runs in his career.\n\nIn Major League Baseball, the career record is 762, held by Barry Bonds, who broke Hank Aaron's record on August 7, 2007, when he hit his 756th home run at AT&T Park off pitcher Mike Bacsik. Only seven other major league players have hit as many as 600: Hank Aaron (755), Babe Ruth (714), Alex Rodriguez (696), Willie Mays (660), Ken Griffey, Jr. (630), Jim Thome (612) and Sammy Sosa (609).\n\nThe single season record is 73, set by Barry Bonds in 2001. Other notable single season records were achieved by Babe Ruth who hit 60 in 1927, Roger Maris, with 61 home runs in 1961, and Mark McGwire, who hit 70 in 1998.\n\nNegro League slugger Josh Gibson's Baseball Hall of Fame plaque says he hit \"almost 800\" home runs in his career. The \"Guinness Book of World Records\" lists Gibson's lifetime home run total at 800. Ken Burns' award-winning series, Baseball, states that his actual total may have been as high as 950. Gibson's true total is not known, in part due to inconsistent record keeping in the Negro Leagues. The 1993 edition of the MacMillan \"Baseball Encyclopedia\" attempted to compile a set of Negro League records, and subsequent work has expanded on that effort. Those records demonstrate that Gibson and Ruth were of comparable power. The 1993 book had Gibson hitting 146 home runs in the 501 \"official\" Negro League games they were able to account for in his 17-year career, about 1 homer every 3.4 games. Babe Ruth, in 22 seasons (several of them in the dead-ball era), hit 714 in 2503 games, or 1 homer every 3.5 games. The large gap in the numbers for Gibson reflect the fact that Negro League clubs played relatively far fewer league games and many more \"barnstorming\" or exhibition games during the course of a season, than did the major league clubs of that era.\n\nOther legendary home run hitters include Jimmie Foxx, Mel Ott, Ted Williams, Mickey Mantle (who on September 10, 1960, mythically hit \"the longest home run ever\" at an estimated distance of , although this was measured after the ball stopped rolling), Reggie Jackson, Harmon Killebrew, Ernie Banks, Mike Schmidt, Dave Kingman, Sammy Sosa (who hit 60 or more home runs in a season 3 times), Ken Griffey, Jr. and Eddie Mathews. In 1987, Joey Meyer of the Denver Zephyrs hit the longest verifiable home run in professional baseball history. The home run was measured at a distance of and was hit inside Denver's Mile High Stadium. Major League Baseball's longest verifiable home run distance is about , by Babe Ruth, to straightaway center field at Tiger Stadium (then called Navin Field and before the double-deck), which landed nearly across the intersection of Trumbull and Cherry.\n\nThe location of where Hank Aaron's record 755th home run landed has been monumented in Milwaukee. The hallowed spot sits outside Miller Park, where the Milwaukee Brewers currently play. Similarly, the point where Aaron's 715th homer landed, upon breaking Ruth's career record in 1974, is marked in the Turner Field parking lot. A red-painted seat in Fenway Park marks the landing place of the 502-ft home run Ted Williams hit in 1946, the longest measured homer in Fenway's history; a red stadium seat mounted on the wall of the Mall of America in Bloomington, Minnesota, marks the landing spot of Harmon Killebrew's record 520-foot shot in old Metropolitan Stadium.\n\nReplays \"to get the call right\" have been used extremely sporadically in the past, but the use of instant replay to determine \"boundary calls\"—home runs and foul balls—was not officially allowed until 2008.\n\nIn a game on May 31, 1999, involving the St. Louis Cardinals and Florida Marlins, a hit by Cliff Floyd of the Marlins was initially ruled a double, then a home run, then was changed back to a double when umpire Frank Pulli decided to review video of the play. The Marlins protested that video replay was not allowed, but while the National League office agreed that replay was not to be used in future games, it declined the protest on the grounds it was a judgment call, and the play stood.\n\nIn November 2007, the general managers of Major League Baseball voted in favor of implementing instant replay reviews on boundary home run calls. The proposal limited the use of instant replay to determining whether a boundary/home run call is:\n\nOn August 28, 2008, instant replay review became available in MLB for reviewing calls in accordance with the above proposal. It was first utilized on September 3, 2008 in a game between the New York Yankees and the Tampa Bay Rays at Tropicana Field. Alex Rodriguez of the Yankees hit what appeared to be a home run, but the ball hit a catwalk behind the foul pole. It was at first called a home run, until Tampa Bay manager Joe Maddon argued the call, and the umpires decided to review the play. After 2 minutes and 15 seconds, the umpires came back and ruled it a home run.\n\nAbout two weeks later, on September 19, also at Tropicana Field, a boundary call was overturned for the first time. In this case, Carlos Peña of the Rays was given a ground rule double in a game against the Minnesota Twins after an umpire believed a fan reached into the field of play to catch a fly ball in right field. The umpires reviewed the play, determined the fan did not reach over the fence, and reversed the call, awarding Peña a home run.\n\nAside from the two aforementioned reviews at Tampa Bay, replay was used four more times in the 2008 MLB regular season: twice at Houston, once at Seattle, and once at San Francisco. The San Francisco incident is perhaps the most unusual. Bengie Molina, the Giants' catcher, hit what was first called a single. Molina then was replaced in the game by Emmanuel Burriss, a pinch-runner, before the umpires re-evaluated the call and ruled it a home run. In this instance though, Molina was not allowed to return to the game to complete the run, as he had already been replaced. Molina was credited with the home run, and two RBIs, but not for the run scored which went to Burriss instead.\n\nOn October 31, 2009, in the fourth inning of Game 3 of the World Series, Alex Rodriguez hit a long fly ball that appeared to hit a camera protruding over the wall and into the field of play in deep left field. The ball ricocheted off the camera and re-entered the field, initially ruled a double. However, after the umpires consulted with each other after watching the instant replay, the hit was ruled a home run, marking the first time an instant replay home run was hit in a playoff game.\n\n\nCareer achievements\n\n", "id": "14148", "title": "Home run"}
{"url": "https://en.wikipedia.org/wiki?curid=14149", "text": "Harappa\n\nHarappa (; Urdu/) is an archaeological site in Punjab, Pakistan, about west of Sahiwal. The site takes its name from a modern village located near the former course of the Ravi River. The current village of Harappa is from the ancient site. Although modern Harappa has a legacy railway station from the period of the British Raj, it is today just a small crossroads town of population 15,000.\n\nThe site of the ancient city contains the ruins of a Bronze Age fortified city, which was part of the Cemetery H culture and the Indus Valley Civilization, centered in Sindh and the Punjab. The city is believed to have had as many as 23,500 residents and occupied about with clay sculptured houses at its greatest extent during the Mature Harappan phase (2600–1900 BC), which is considered large for its time. Per archaeological convention of naming a previously unknown civilization by its first excavated site, the Indus Valley Civilization is also called the Harappan Civilization.\n\nThe ancient city of Harappa was heavily damaged under British rule, when bricks from the ruins were used as track ballast in the construction of the Lahore-Multan Railway. In 2005, a controversial amusement park scheme at the site was abandoned when builders unearthed many archaeological artifacts during the early stages of building work. A plea from the Pakistani archaeologist Ahmad Hasan Dani to the Ministry of Culture resulted in a restoration of the site.\n\nThe Indus Valley Civilization (also known as the Harappan culture) has its earliest roots in cultures such as that of Mehrgarh, approximately 6000 BCE. The two greatest cities, Mohenjo-daro and Harappa, emerged circa 2600 BCE along the Indus River valley in Punjab and Sindh. The civilization, with a possible writing system, urban centers, and diversified social and economic system, was rediscovered in the 1920s after excavations at Mohenjo-daro in Sindh near Larkana, and Harappa, in west Punjab south of Lahore. A number of other sites stretching from the Himalayan foothills in east Punjab, India in the north, to Gujarat in the south and east, and to Pakistani Balochistan in the west have also been discovered and studied. Although the archaeological site at Harappa was damaged in 1857 when engineers constructing the Lahore-Multan railroad (as part of the Sind and Punjab Railway), used brick from the Harappa ruins for track ballast, an abundance of artifacts has nevertheless been found. The bricks discovered were made of red sand, clay, stones and were baked at very high temperature. As early as 1826 Harappa located in west Punjab attracted the attention of a British officer in India, gets credit for preliminary excavations in Harappa.\n\nIndus Valley civilization was mainly an urban culture sustained by surplus agricultural production and commerce, the latter including trade with Sumer in southern Mesopotamia. Both Mohenjo-Daro and Harappa are generally characterized as having \"differentiated living quarters, flat-roofed brick houses, and fortified administrative or religious centers.\" Although such similarities have given rise to arguments for the existence of a standardized system of urban layout and planning, the similarities are largely due to the presence of a semi-orthogonal type of civic layout, and a comparison of the layouts of Mohenjo-Daro and Harappa shows that they are in fact, arranged in a quite dissimilar fashion.\n\nThe weights and measures of the Indus Valley Civilization, on the other hand, were highly standardized, and conform to a set scale of gradations. Distinctive seals were used, among other applications, perhaps for identification of property and shipment of goods. Although copper and bronze were in use, iron was not yet employed. \"Cotton was woven and dyed for clothing; wheat, rice, and a variety of vegetables and fruits were cultivated; and a number of animals, including the humped bull, were domesticated,\" as well as \"fowl for fighting\". Wheel-made pottery—some of it adorned with animal and geometric motifs—has been found in profusion at all the major Indus sites. A centralized administration for each city, though not the whole civilization, has been inferred from the revealed cultural uniformity; however, it remains uncertain whether authority lay with a commercial oligarchy. Harappans had many trade routes along the Indus River that went as far as the Persian Gulf, Mesopotamia, and Egypt. Some of the most valuable things traded were carnelian and lapis lazuli.\n\nWhat is clear is that Harappan society was not entirely peaceful, with the human skeletal remains demonstrating some of the highest rates of injury (15.5%) found in South Asian prehistory. Paleopathological analysis demonstrated that leprosy and tuberculosis were present at Harappa, with the highest prevalence of both disease and trauma present in the skeletons from Area G (an ossuary located south-east of the city walls). Furthermore, rates of cranio-facial trauma and infection increased through time demonstrating that the civilization collapsed amid illness and injury. The bioarchaeologists who examined the remains have suggested that the combined evidence for differences in mortuary treatment and epidemiology indicate that some individuals and communities at Harappa were excluded from access to basic resources like health and safety, a basic feature of hierarchical societies world-wide.\n\nThe excavators of the site have proposed the following chronology of Harappa's occupation:\n\nBy far the most exquisite and obscure artifacts unearthed to date are the small, square steatite (soapstone) seals engraved with human or animal motifs. A large number of seals have been found at such sites as Mohenjo-Daro and Harappa. Many bear pictographic inscriptions generally thought to be a form of writing or script. Despite the efforts of philologists from all parts of the world, and despite the use of modern cryptographic analysis, the signs remain undeciphered. It is also unknown if they reflect proto-Dravidian or other non-Vedic language(s). The ascription of Indus Valley Civilization iconography and epigraphy to historically known cultures is extremely problematic, in part due to the rather tenuous archaeological evidence of such claims, as well as the projection of modern South Asian political concerns onto the archaeological record of the area. This is especially evident in the radically varying interpretations of Harappan material culture as seen from both Pakistan- and India-based scholars.\nIn February 2006 a school teacher in the village of Sembian-Kandiyur in Tamil Nadu discovered a stone celt (tool) with an inscription estimated to be up to 3,500 years old.\n\nClay and stone tablets unearthed at Harappa, which were carbon dated 3300–3200 BCE., contain trident-shaped and plant-like markings. \"It is a big question as to if we can call what we have found true writing, but we have found symbols that have similarities to what became Indus script\" said Dr. Richard Meadow of Harvard University, Director of the Harappa Archeological Research Project. This primitive writing is placed slightly earlier than primitive writings of the Sumerians of Mesopotamia, dated c.3100 BCE. These markings have similarities to what later became Indus Script.\n\n\n\n", "id": "14149", "title": "Harappa"}
{"url": "https://en.wikipedia.org/wiki?curid=14153", "text": "Hendecasyllable\n\nThe hendecasyllable is a line of eleven syllables, used in Ancient Greek and Latin quantitative verse as well as in medieval and modern European poetry.\n\nThe classical hendecasyllable is a quantitative meter used in Ancient Greece in Aeolic verse and in scolia, and later by the Roman poets Catullus and Martial. Each line has eleven syllables; hence the name, which comes from the Greek word for eleven. The heart of the line is the choriamb (- u u -). There are three different versions.\nThe pattern of the Phalaecian (Latin: \"hendecasyllabus phalaecius\") is as follows (using \"-\" for a long syllable, \"u\" for a short and \"x\" for an \"anceps\" or variable syllable):\n\nAnother form of hendecasyllabic verse is the \"Alcaic\" (Latin: \"hendecasyllabus alcaicus\"; used in the Alcaic stanza), which has the pattern:\n\nThe third form of hendecasyllabic verse is the \"Sapphic\" (Latin: \"hendecasyllabus sapphicus\"; so named for its use in the Sapphic stanza), with the pattern:\n\nForty-three of Catullus's poems are hendecasyllabic; for an example, see Catullus 1.\n\nThe metre has been imitated in English, notably by Alfred Tennyson, Swinburne, and Robert Frost, cf. \"For Once Then Something.\" Contemporary American poets Annie Finch (\"Lucid Waking\") and Patricia Smith (\"The Reemergence of the Noose\") have published recent examples. Poets wanting to capture the hendecasyllabic rhythm in English have simply transposed the pattern into its accentual-syllabic equivalent: /u|/u|/uu/u|/u|, or trochee/trochee/dactyl/trochee/trochee, so that the long/short pattern becomes a stress/unstress pattern. Tennyson, however, maintained the quantitative features of the metre:\n\nThe hendecasyllable () is the principal metre in Italian poetry. Its defining feature is a constant stress on the tenth syllable, so that the number of syllables in the verse may vary, equaling eleven in the usual case where the final word is stressed on the penultimate syllable. The verse also has a stress preceding the caesura, on either the fourth or sixth syllable. The first case is called \"endecasillabo a minore\", or lesser hendecasyllable, and has the first hemistich equivalent to a \"quinario\"; the second is called \"endecasillabo a maiore\", or greater hendecasyllable, and has a \"settenario\" as the first hemistich.\nThe most usual stress schemes for the Italian hendecasyllable are stresses on sixth and tenth syllables (for example, \"\"Nel mezzo del cammin di nostra vita\",\" Dante Alighieri, first line of \"The Divine Comedy),\" and on the fourth, seventh and tenth syllables (\"\"Un incalzar di cavalli accorrenti\",\" Ugo Foscolo, \"Dei sepolcri\").\n\nMost classical Italian poems are composed in hendecasyllables, including the major works of Dante, Francesco Petrarca, Ludovico Ariosto, and Torquato Tasso. The rhyme system varies from terza rima to ottava, from sonnet to canzone. From the early 16th century, hendecasyllables are often used without a strict system, with few or no rhymes, both in poetry and in drama. An early example is \"Le Api\" (\"the bees\") by Giovanni di Bernardo Rucellai, written around 1517 and published in 1525, which begins:\n\nLike other early Italian-language tragedies, the \"Sophonisba\" of Gian Giorgio Trissino (1515) is in blank hendecasyllables. Later examples can be found in the \"Canti\" of Giacomo Leopardi, where hendecasyllables are alternated with \"settenari\". The effect of \"endecasillabi sciolti\" (\"untied\" hendecasyllables) may be considered similar to that of English blank verse.\n\nIt has a role in Italian poetry, and a formal structure, comparable to the alexandrine in French.\n\nThe term \"hendecasyllable\" is sometimes used in English poetry to describe a line of iambic pentameter with an extra short syllable at the end, as in the first line of John Keats's \"Endymion:\" \"A thing of beauty is a joy for ever.\"\n\nThe 11-syllable metre was very popular in Polish poetry, especially in th 16. and 17. centuries, because of strong Italian literary influence. It was used by Jan Kochanowski, Piotr Kochanowski, who translated Jeruselem delivered by Torquato Tasso, Sebastian Grabowiecki, Wespazjan Kochowski and Stanisław Herakliusz Lubomirski. The greatest Polish romantic poet, Adam Mickiewicz put his poem Grażyna into this measure. The Polish hendecasyllable is widely used in translation form English blank verse.\nAlmost always 11-syllable line is divided by caesura into 5+6. Only rarely it is fully iambic. \nA popular form in Polish literature is Sapphic stanza 11/11/11/5. \nPolish hendecasyllable is often combined with 8-syllable line: 11a/8b/11a/8b. Such a stanza was used by the afore-mentioned Adam Mickiewicz in his ballads:\n\nIn Polish language hendecasyllable is called \"jedenastozgłoskowiec\".\n\n\n", "id": "14153", "title": "Hendecasyllable"}
{"url": "https://en.wikipedia.org/wiki?curid=14155", "text": "Hebrides\n\nThe Hebrides (; Scottish Gaelic: \"Innse Gall\" (); Old Norse: \"Suðreyjar\") comprise a widespread and diverse archipelago off the west coast of mainland Scotland. There are two main groups: the Inner and Outer Hebrides. These islands have a long history of occupation dating back to the Mesolithic, and the culture of the residents has been affected by the successive influences of Celtic, Norse, and English-speaking peoples. This diversity is reflected in the names given to the islands, which are derived from the languages that have been spoken there in historic and perhaps prehistoric times.\n\nThe Hebrides are the source of much of Scottish Gaelic literature and Gaelic music. Today the economy of the islands is dependent on crofting, fishing, tourism, the oil industry, and renewable energy. The Hebrides have lower biodiversity than mainland Britain, but there is a significant presence of seals and seabirds.\n\nThe Hebrides have a diverse geology ranging in age from Precambrian strata that are amongst the oldest rocks in Europe to Paleogene igneous intrusions.\n\nThe Hebrides can be divided into two main groups, separated from one another by the Minch to the north and the Sea of the Hebrides to the south. The Inner Hebrides lie closer to mainland Scotland and include Islay, Jura, Skye, Mull, Raasay, Staffa and the Small Isles. There are 36 inhabited islands in this group. The Outer Hebrides are a chain of more than 100 islands and small skerries located about west of mainland Scotland. There are 15 inhabited islands in this archipelago. The main islands include Barra, Benbecula, Berneray, Harris, Lewis, North Uist, South Uist, and St Kilda. In total, the islands have an area of approximately and a population of 44,759.\n\nA complication is that there are various descriptions of the scope of the Hebrides. The \"Collins Encyclopedia of Scotland\" describes the Inner Hebrides as lying \"east of the Minch\", which would include any and all offshore islands. There are various islands that lie in the sea lochs such as Eilean Bàn and Eilean Donan that might not ordinarily be described as \"Hebridean\", but no formal definitions exist.\n\nIn the past, the Outer Hebrides were often referred to as the \"Long Isle\" (). Today, they are also known as the \"Western Isles\", although this phrase can also be used to refer to the Hebrides in general.\n\nThe Hebrides have a cool temperate climate that is remarkably mild and steady for such a northerly latitude, due to the influence of the Gulf Stream. In the Outer Hebrides the average temperature for the year is 6 °C (44 °F) in January and 14 °C (57 °F) in summer. The average annual rainfall in Lewis is and sunshine hours range from 1,100 – 1,200 \"per annum\". The summer days are relatively long, and May to August is the driest period.\n\nThe Hebrides were settled during the Mesolithic era around 6500 BC or earlier, after the climatic conditions improved enough to sustain human settlement. Occupation at a site on Rùm is dated to 8590 ±95 uncorrected radiocarbon years BP, which is amongst the oldest evidence of occupation in Scotland. There are many examples of structures from the Neolithic period, the finest example being the standing stones at Callanish, dating to the 3rd millennium BC. Cladh Hallan, a Bronze Age settlement on South Uist is the only site in the UK where prehistoric mummies have been found.\n\nIn 55 BC, the Greek historian Diodorus Siculus wrote that there was an island called \"Hyperborea\" (which means \"beyond the North Wind\"), where a round temple stood from which the moon appeared only a little distance above the earth every 19 years. This may have been a reference to the stone circle at Callanish.\n\nA traveller called Demetrius of Tarsus related to Plutarch the tale of an expedition to the west coast of Scotland in or shortly before AD 83. He stated it was a gloomy journey amongst uninhabited islands, but he had visited one which was the retreat of holy men. He mentioned neither the druids nor the name of the island.\n\nThe first written records of native life begin in the 6th century AD, when the founding of the kingdom of Dál Riata took place. This encompassed roughly what is now Argyll and Bute and Lochaber in Scotland and County Antrim in Ireland. The figure of Columba looms large in any history of Dál Riata, and his founding of a monastery on Iona ensured that the kingdom would be of great importance in the spread of Christianity in northern Britain. However, Iona was far from unique. Lismore in the territory of the Cenél Loairn, was sufficiently important for the death of its abbots to be recorded with some frequency and many smaller sites, such as on Eigg, Hinba, and Tiree, are known from the annals.\n\nNorth of Dál Riata, the Inner and Outer Hebrides were nominally under Pictish control, although the historical record is sparse. Hunter (2000) states that in relation to King Bridei I of the Picts in the sixth century: \"As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.”\n\nViking raids began on Scottish shores towards the end of the 8th century and the Hebrides came under Norse control and settlement during the ensuing decades, especially following the success of Harald Fairhair at the Battle of Hafrsfjord in 872. In the Western Isles Ketill Flatnose may have been the dominant figure of the mid 9th century, by which time he had amassed a substantial island realm and made a variety of alliances with other Norse leaders. These princelings nominally owed allegiance to the Norwegian crown, although in practice the latter's control was fairly limited. Norse control of the Hebrides was formalised in 1098 when Edgar of Scotland formally signed the islands over to Magnus III of Norway. The Scottish acceptance of Magnus III as King of the Isles came after the Norwegian king had conquered Orkney, the Hebrides and the Isle of Man in a swift campaign earlier the same year, directed against the local Norwegian leaders of the various island petty kingdoms. By capturing the islands Magnus imposed a more direct royal control, although at a price. His skald Bjorn Cripplehand recorded that in Lewis \"fire played high in the heaven\" as \"flame spouted from the houses\" and that in the Uists \"the king dyed his sword red in blood\".\n\nThe Hebrides were now part of the Kingdom of the Isles, whose rulers were themselves vassals of the Kings of Norway. This situation lasted until the partitioning of the Western Isles in 1156, at which time the Outer Hebrides remained under Norwegian control while the Inner Hebrides broke out under Somerled, the Norse-Celtic kinsman of the Manx royal house.\n\nFollowing the ill-fated 1263 expedition of Haakon IV of Norway, the Outer Hebrides and the Isle of Man were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth. Although their contribution to the islands can still be found in personal and place names, the archaeological record of the Norse period is very limited. The best known find is the Lewis chessmen, which date from the mid 12th century.\n\nAs the Norse era drew to a close, the Norse-speaking princes were gradually replaced by Gaelic-speaking clan chiefs including the MacLeods of Lewis and Harris, Clan Donald and MacNeil of Barra. This transition did little to relieve the islands of internecine strife although by the early 14th century the MacDonald Lords of the Isles, based on Islay, were in theory these chiefs' feudal superiors and managed to exert some control.\n\nThe Lords of the Isles ruled the Inner Hebrides as well as part of the Western Highlands as subjects of the King of Scots until John MacDonald, fourth Lord of the Isles, squandered the family's powerful position. A rebellion by his nephew, Alexander of Lochalsh provoked an exasperated James IV to forfeit the family's lands in 1493.\n\nIn 1598, King James VI authorised some \"Gentleman Adventurers\" from Fife to civilise the \"most barbarous Isle of Lewis\". Initially successful, the colonists were driven out by local forces commanded by Murdoch and Neil MacLeod, who based their forces on Bearasaigh in Loch Ròg. The colonists tried again in 1605 with the same result, but a third attempt in 1607 was more successful and in due course Stornoway became a Burgh of Barony. By this time, Lewis was held by the Mackenzies of Kintail (later the Earls of Seaforth), who pursued a more enlightened approach, investing in fishing in particular. The Seaforths' royalist inclinations led to Lewis becoming garrisoned during the Wars of the Three Kingdoms by Cromwell's troops, who destroyed the old castle in Stornoway.\n\nWith the implementation of the Treaty of Union in 1707, the Hebrides became part of the new Kingdom of Great Britain, but the clans' loyalties to a distant monarch were not strong. A considerable number of islesmen \"came out\" in support of the Jacobite Earl of Mar in the \"15\" and again in the 1745 rising including Macleod of Dunvegan and MacLea of Lismore. The aftermath of the decisive Battle of Culloden, which effectively ended Jacobite hopes of a Stuart restoration, was widely felt. The British government's strategy was to estrange the clan chiefs from their kinsmen and turn their descendants into English-speaking landlords whose main concern was the revenues their estates brought rather than the welfare of those who lived on them. This may have brought peace to the islands, but in the following century it came at a terrible price. In the wake of the rebellion, the clan system was broken up and islands of the Hebrides became a series of landed estates.\n\nThe early 19th century was a time of improvement and population growth. Roads and quays were built; the slate industry became a significant employer on Easdale and surrounding islands; and the construction of the Crinan and Caledonian canals and other engineering works such as Telford's \"Bridge across the Atlantic\" improved transport and access. However, in the mid-19th century, the inhabitants of many parts of the Hebrides were devastated by the clearances, which destroyed communities throughout the Highlands and Islands as the human populations were evicted and replaced with sheep farms. The position was exacerbated by the failure of the islands' kelp industry that thrived from the 18th century until the end of the Napoleonic Wars in 1815 and large scale emigration became endemic. \n\nAs Iain mac Fhearchair, a Gaelic poet from Uibhist a Deas, wrote for his countrymen who were obliged to leave the Hebrides in the late eighteenth century, emigration was the only alternative to \"sinking into slavery\" as the Gaels had been unfairly dispossessed by rapacious landlords. In the 1880s, the \"Battle of the Braes\" involved a demonstration against unfair land regulation and eviction, stimulating the calling of the Napier Commission. Disturbances continued until the passing of the 1886 Crofters' Act.\n\nFor those who remained, new economic opportunities emerged through the export of cattle, commercial fishing and tourism. Nonetheless emigration and military service became the choice of many and the archipelago's populations continued to dwindle throughout the late 19th century and for much of the 20th century. Lengthy periods of continuous occupation notwithstanding, many of the smaller islands were abandoned.\n\nThere were, however, continuing gradual economic improvements, among the most visible of which was the replacement of the traditional thatched blackhouse with accommodation of a more modern design and with the assistance of Highlands and Islands Enterprise many of the islands' populations have begun to increase after decades of decline. The discovery of substantial deposits of North Sea oil in 1965 and the renewables sector have contributed to a degree of economic stability in recent decades. For example, the Arnish yard has had a chequered history but has been a significant employer in both the oil and renewables industries.\n\nMany contemporary Gaelic musicians have roots in the Hebrides, including Julie Fowlis (North Uist), Catherine-Ann MacPhee (Barra), Kathleen MacInnes (South Uist), and Ishbel MacAskill (Lewis). All of these singers have repertoire based on the Hebridean tradition, such as \"puirt à beul\" and òrain luaidh (waulking songs). This tradition includes many songs composed by little-known or anonymous poets before 1800, such as \"Fear a' bhàta,\" \"Ailein duinn,\" and \"Alasdair mhic Cholla Ghasda.\" Several of Runrig's songs are inspired by the archipelago; Calum and Ruaraidh Dòmhnallach were raised on North Uist and Donnie Munro on Skye. \n\nThe Gaelic poet Alasdair mac Mhaighstir Alasdair spent much of his life in the Hebrides and made frequent references to them in his poetry, including in \"An Airce\" and \"Birlinn Chlann Raghnaill\". The best known Gaelic poet of her era, Màiri Mhòr nan Óran (Mary MacPherson, 1821–98), embodied the spirit of the land agitation of the 1870s and 1880s. This, and her powerful evocation of the Hebrides—she was from Skye—made her among the most enduring Gaelic poets. Allan MacDonald (1859–1905), who spent his adult life on Eriskay and South Uist, composed hymns and verse in honour of the Blessed Virgin, the Christ Child, and the Eucharist. In his secular poetry, MacDonald praised the beauty of Eriskay and its people. In his verse drama, \"Parlamaid nan Cailleach\" (\"The Old Wives' Parliament\"), he lampooned the gossiping of his female parishioners and local marriage customs. \n\nIn the twentieth century, Murdo Macfarlane of Lewis wrote \"Cànan nan Gàidheal,\" a well-known poem about the Gaelic revival in the Outer Hebrides.\nSorley MacLean was born and raised on Raasay, where he set his best known poem, \"Hallaig\", about the devastating effect of the Highland Clearances.\n\nThe area around the Inaccessible Pinnacle of Sgurr Dearg of Skye provided the setting for the Scottish Gaelic feature film \"\" (2006). The script was written by the actor, novelist, and poet Aonghas Phàdraig Chaimbeul, originally from South Uist. Caimbeul also starred in the movie.\n\nForeign artists who spent time in the Hebrides include the novelist Compton Mackenzie and George Orwell, who wrote \"1984\" whilst living on Jura. J.M. Barrie's \"Marie Rose\" contains references to Harris inspired by a holiday visit to Amhuinnsuidhe Castle and he wrote a screenplay for the 1924 film adaptation of \"Peter Pan\" whilst on Eilean Shona. \"The Hebrides\", also known as \"Fingal's Cave\", is a famous overture composed by Felix Mendelssohn while residing on these islands, while Granville Bantock composed the \"Hebridean Symphony\". Enya's song \"Ebudæ\" from \"Shepherd Moons\" is named for the Hebrides (see below). The 1973 British horror film \"The Wicker Man\" is set on the fictional Hebridean island of Summerisle. The experimental first-person adventure video game \"Dear Esther\" takes place on an unnamed Hebridean island. The 2011 British romantic comedy \"The Decoy Bride\" is set on the fictional Hebrides island of Hegg.\n\nThe residents of the Hebrides have spoken a variety of different languages during the long period of human occupation.\n\nIt is assumed that Pictish must once have predominated in the northern Inner Hebrides and Outer Hebrides. The Scottish Gaelic language arrived via Ireland due to the growing influence of the kingdom of Dál Riata from the 6th century onwards and became the dominant language of the southern Hebrides at that time. For a few centuries, the military might of the \"Gall-Ghàidhils\" meant that Old Norse was prevalent in the Hebrides. North of Ardnamurchan, the place names that existed prior to the 9th century have been all but obliterated. The Old Norse name for the Hebrides during the Viking occupation was \"Suðreyjar\", which means \"Southern Isles\". It was given in contrast to the \"Norðreyjar\", or the \"Northern Isles\" of Orkney and Shetland.\n\nSouth of Ardnamurchan Gaelic place names are more common and after the 13th century Gaelic became the main language of the entire Hebridean archipelago. Due to Scots and English being favored in government and the educational system, the Hebrides have been in a state of diglossia since at least the seventeenth century. The Highland Clearances of the nineteenth century accelerated the language shift away from Scottish Gaelic, as did increased migration and the continuing lower status of Gaelic speakers. Nevertheless, as late as the end of the nineteenth century, there were significant populations of monolingual Gaelic speakers, and the Hebrides still contain the largest concentration of Gaelic speakers in Scotland. This is especially true of the Outer Hebrides, where the majority of people speak the language. The Scottish Gaelic college, Sabhal Mòr Ostaig, is based on Skye and Islay.\n\nIronically, given the status of the Western Isles as the last Gàidhlig-speaking stronghold in Scotland, the Gaelic language name for the islands – \"Innse Gall\" – means \"isles of the foreigners\" which has roots in the time when they were under Norse colonisation.\n\nThe earliest written references that have survived relating to the islands were made by Pliny the Elder in his \"Natural History\", where he states that there are 30 \"Hebudes\", and makes a separate reference to \"Dumna\", which Watson (1926) concludes is unequivocally the Outer Hebrides. Writing about 80 years later, in 140-150 AD, Ptolemy, drawing on the earlier naval expeditions of Agricola, writes that there are five \"Ebudes\" (possibly meaning the Inner Hebrides) and \"Dumna\". Later texts in classical Latin, by writers such as Solinus, use the forms \"Hebudes\" and \"Hæbudes\".\n\nThe name \"Ebudes\" recorded by Ptolemy may be pre-Celtic. Islay is Ptolemy's \"Epidion\", the use of the \"p\" hinting at a Brythonic or Pictish tribal name, Epidii, although the root is not Gaelic. Woolf (2012) has suggested that \"Ebudes\" may be \"an Irish attempt to reproduce the word \"Epidii\" phonetically rather than by translating it\" and that the tribe's name may come from the root \"epos\" meaning \"horse\". Watson (1926) also notes the possible relationship between \"Ebudes\" and the ancient Irish Ulaid tribal name \"Ibdaig\" and the personal name of a king Iubdán recorded in the \"Silva Gadelica\".\n\nThe names of other individual islands reflect their complex linguistic history. The majority are Norse or Gaelic but the roots of several other Hebrides may have a pre-Celtic origin. Adomnán, the 7th century abbot of Iona, records Colonsay as \"Colosus\" and Tiree as \"Ethica\", both of which may be pre-Celtic names. The etymology of Skye is complex and may also include a pre-Celtic root. Lewis is \"Ljoðhús\" in Old Norse and although various suggestions have been made as to a Norse meaning (such as \"song house\") the name is not of Gaelic origin and the Norse credentials are questionable.\n\nThe earliest comprehensive written list of Hebridean island names was undertaken by Donald Monro in 1549, which in some cases also provides the earliest written form of the island name. The derivations of all of the inhabited islands of the Hebrides and some of the larger uninhabited ones are listed below.\n\nLewis and Harris is the largest island in Scotland and the third largest in the British Isles, after Great Britain and Ireland. It incorporates Lewis in the north and Harris in the south, both of which are frequently referred to as individual islands, although they are joined by a land border. Remarkably, the island does not have a common name in either English or Gaelic and is referred to as \"Lewis and Harris\", \"Lewis with Harris\", \"Harris with Lewis\" etc. For this reason it is treated as two separate islands below. The derivation of Lewis may be pre-Celtic (see above) and the origin of Harris is no less problematic. In the Ravenna Cosmography, \"Erimon\" may refer to Harris (or possibly the Outer Hebrides as a whole). This word may derive from the Ancient Greek \"erimos\" meaning \"desert\". The origin of Uist (Old Norse: \"Ívist\") is similarly unclear.\n\nThere are various examples of Inner Hebridean island names that were originally Gaelic but have become completely replaced. For example, Adomnán records \"Sainea\", \"Elena\", \"Ommon\" and \"Oideacha\" in the Inner Hebrides, which names must have passed out of usage in the Norse era and whose locations are not clear. One of the complexities is that an island may have had a Celtic name, that was replaced by a similar-sounding Norse name, but then reverted to an essentially Gaelic name with a Norse \"øy\" or \"ey\" ending. See for example Rona below.\n\nThe names of uninhabited islands follow the same general patterns as the inhabited islands. The following are the ten largest in the Hebrides and their outliers.\n\nThe etymology of St Kilda, a small archipelago west of the Outer Hebrides, and its main island Hirta is very complex. No saint is known by the name of Kilda and various theories have been proposed for the word's origin, which dates from the late 16th century. Haswell-Smith (2004) notes that the full name \"St Kilda\" first appears on a Dutch map dated 1666, and that it may have been derived from Norse \"sunt kelda\" (\"sweet wellwater\") or from a mistaken Dutch assumption that the spring \"Tobar Childa\" was dedicated to a saint. (\"Tobar Childa\" is a tautological placename, consisting of the Gaelic and Norse words for \"well\", i.e. \"well well\"). The origin of the Gaelic for \"Hirta\"—\"Hiort\", \"Hirt\", or \"Irt\"—which long pre-dates the use of \"St Kilda\", is similarly open to interpretation. Watson (1926) offers the Old Irish \"hirt\", a word meaning \"death\", possibly relating to the dangerous seas. Maclean (1977), drawing on an Icelandic saga describing an early 13th-century voyage to Ireland that mentions a visit to the islands of \"Hirtir\", speculates that the shape of Hirta resembles a stag, \"hirtir\" being \"stags\" in Norse.\n\nThe etymology of small islands may be no less complex. In relation to Dubh Artach, R. L. Stevenson believed that \"black and dismal\" was a translation of the name, noting that \"as usual, in Gaelic, it is not the only one.\"\n\nIn some respects the Hebrides lack biodiversity in comparison to mainland Britain; for example, there are only half as many mammalian species. However, these islands provide breeding grounds for many important seabird species including the world's largest colony of northern gannets. Avian life includes the corncrake, red-throated diver, rock dove, kittiwake, tystie, Atlantic puffin, goldeneye, golden eagle and white-tailed sea eagle. The last named was re-introduced to Rùm in 1975 and has successfully spread to various neighbouring islands, including Mull. There is a small population of red-billed chough concentrated on the islands of Islay and Colonsay.\n\nRed deer are common on the hills and the grey seal and common seal are present around the coasts of Scotland. Colonies of seals are found on Oronsay and the Treshnish Isles. The rich freshwater streams contain brown trout, Atlantic salmon and water shrew. Offshore, minke whales, Killer whales, basking sharks, porpoises and dolphins are among the sealife that can be seen. \n\nHeather moor containing ling, bell heather, cross-leaved heath, bog myrtle and fescues is abundant and there is a diversity of Arctic and alpine plants including Alpine pearlwort and mossy cyphal.\n\nLoch Druidibeg on South Uist is a national nature reserve owned and managed by Scottish Natural Heritage. The reserve covers 1,677 hectares across the whole range of local habitats. Over 200 species of flowering plants have been recorded on the reserve, some of which are nationally scarce. South Uist is considered the best place in the UK for the aquatic plant slender naiad, which is a European Protected Species.\n\nHedgehogs are not native to the Outer Hebrides—they were introduced in the 1970s to reduce garden pests—and their spread poses a threat to the eggs of ground nesting wading birds. In 2003, Scottish Natural Heritage undertook culls of hedgehogs in the area although these were halted in 2007 due to protests; trapped animals were instead relocated to the mainland.\n\n\n\n", "id": "14155", "title": "Hebrides"}
{"url": "https://en.wikipedia.org/wiki?curid=14158", "text": "HMS Dreadnought\n\nSeveral ships and one submarine of the Royal Navy have borne the name HMS \"Dreadnought\" in the expectation that they would \"dread nought\", i.e. \"fear nothing, but God\". The 1906 ship was one of the Royal Navy's most famous vessels; battleships built after her were referred to as 'dreadnoughts', and earlier battleships became known as pre-dreadnoughts.\n\n\nAlso\n\nCitations\n\nReferences\n\n", "id": "14158", "title": "HMS Dreadnought"}
{"url": "https://en.wikipedia.org/wiki?curid=14159", "text": "Hartmann Schedel\n\nHartmann Schedel (13 February 1440 – 28 November 1514) was a German physician, humanist, historian, and one of the first cartographers to use the printing press. He was born and died in Nuremberg. Matheolus Perusinus served as his tutor. \n\nSchedel is best known for his writing the text for the \"Nuremberg Chronicle\", known as \"Schedelsche Weltchronik\" (English: \"Schedel's World Chronicle\"), published in 1493 in Nuremberg. It was commissioned by Sebald Schreyer (1446 – 1520) and Sebastian Kammermeister (1446 – 1503). Maps in the \"Chronicle\" were the first ever illustrations of many cities and countries.\n\nWith the invention of the printing press by Johannes Gutenberg in 1447, it became feasible to print books and maps for a larger customer basis. Because they had to be handwritten, books were previously rare and very expensive.\n\nSchedel was also a notable collector of books, art and old master prints. An album he had bound in 1504, which once contained five engravings by Jacopo de' Barbari, provides important evidence for dating de' Barbari's work.\n\n\n\n", "id": "14159", "title": "Hartmann Schedel"}
{"url": "https://en.wikipedia.org/wiki?curid=14160", "text": "Hexameter\n\nHexameter is a metrical line of verses consisting of six feet. It was the standard epic metre in classical Greek and Latin literature, such as in the \"Iliad\", \"Odyssey\" and \"Aeneid\". Its use in other genres of composition include Horace's satires, Ovid's \"Metamorphoses,\" and the Hymns of Orpheus. According to Greek mythology, hexameter was invented by the god Hermes.\n\nIn classical hexameter, the six feet follow these rules:\n\nA short syllable (υ) is a syllable with a short vowel and no consonant at the end. A long syllable (–) is a syllable that either has a long vowel, one or more consonants at the end (or a long consonant), or both. Spaces between words are not counted in syllabification, so for instance \"cat\" is a long syllable in isolation, but \"cat attack\" would be syllabified as short-short-long: \"ca\", \"ta\", \"tack\" (υ υ –).\n\nVariations of the sequence from line to line, as well as the use of caesura (logical full stops within the line) are essential in avoiding what may otherwise be a monotonous sing-song effect.\n\nAlthough the rules seem simple, it is hard to use classical hexameter in English, because English is a stress-timed language that condenses vowels and consonants between stressed syllables, while hexameter relies on the regular timing of the phonetic sounds. Languages having the latter properties (i.e., languages that are not stress-timed) include Ancient Greek, Latin, Lithuanian and Hungarian.\n\nWhile the above classical hexameter has never enjoyed much popularity in English, where the standard metre is iambic pentameter, English poems have frequently been written in iambic hexameter. There are numerous examples from the 16th century and a few from the 17th; the most prominent of these is Michael Drayton's \"Poly-Olbion\" (1612) in couplets of iambic hexameter. An example from Drayton (marking the feet):\n\nIn the 17th century the iambic hexameter, also called alexandrine, was used as a substitution in the heroic couplet, and as one of the types of permissible lines in lyrical stanzas and the Pindaric odes of Cowley and Dryden.\n\nSeveral attempts were made in the 19th century to naturalise the dactylic hexameter to English, by Henry Wadsworth Longfellow, Arthur Hugh Clough and others, none of them particularly successful. Gerard Manley Hopkins wrote many of his poems in six-foot iambic and sprung rhythm lines. In the 20th century a loose ballad-like six-foot line with a strong medial pause was used by William Butler Yeats. The iambic six-foot line has also been used occasionally, and an accentual six-foot line has been used by translators from the Latin and many poets.\n\nIn the late 18th century the hexameter was adapted to the Lithuanian language by Kristijonas Donelaitis. His poem \"\"Metai\" (The Seasons)\" is considered the most successful hexameter text in Lithuanian as yet.\n\n\n\n", "id": "14160", "title": "Hexameter"}
{"url": "https://en.wikipedia.org/wiki?curid=14162", "text": "Timeline of Polish history\n\nThis is a timeline of Polish history, comprising important legal and territorial changes and political events in Poland and its predecessor states. To read about the background to these events, see History of Poland. See also the list of Polish monarchs and list of Prime Ministers of Poland.\n\n\n\n", "id": "14162", "title": "Timeline of Polish history"}
{"url": "https://en.wikipedia.org/wiki?curid=14168", "text": "Himalia\n\nHimalia may refer to:\n", "id": "14168", "title": "Himalia"}
{"url": "https://en.wikipedia.org/wiki?curid=14169", "text": "Heracleidae\n\nIn Greek mythology, the Heracleidae (; ) or Heraclids were the numerous descendants of Heracles (Hercules), especially applied in a narrower sense to the descendants of Hyllus, the eldest of his four sons by Deianira (Hyllus was also sometimes thought of as Heracles' son by Melite) Other Heracleidae included Macaria, Lamos, Manto, Bianor, Tlepolemus, and Telephus. These Heraclids were a group of Dorian kings who conquered the Peloponnesian kingdoms of Mycenae, Sparta and Argos; according to the literary tradition in Greek mythology, they claimed a right to rule through their ancestor. Since Karl Otfried Müller's \"Die Dorier\" (1830, English translation 1839), I. ch. 3, their rise to dominance has been associated with a \"Dorian invasion\".\n\nThough details of genealogy differ from one ancient author to another, the cultural significance of the mythic theme, that the descendants of Heracles, exiled after his death, returned some generations later to reclaim land that their ancestors had held in Mycenaean Greece, was to assert the primal legitimacy of a traditional ruling clan that traced its origin, thus its legitimacy, to Heracles.\n\nHeracles, whom Zeus had originally intended to be ruler of Argos, Lacedaemon and Messenian Pylos, had been supplanted by the cunning of Hera, and his intended possessions had fallen into the hands of Eurystheus, king of Mycenae. After the death of Heracles, his children, after many wanderings, found refuge from Eurystheus at Athens. Eurystheus, on his demand for their surrender being refused, attacked Athens, but was defeated and slain. Hyllus and his brothers then invaded Peloponnesus, but after a year's stay were forced by a pestilence to quit. They withdrew to Thessaly, where Aegimius, the mythical ancestor of the Dorians, whom Heracles had assisted in war against the Lapithae, adopted Hyllus and made over to him a third part of his territory.\n\nAfter the death of Aegimius, his two sons, Pamphylus and Dymas, voluntarily submitted to Hyllus (who was, according to the Dorian tradition in Herodotus V. 72, really an Achaean), who thus became ruler of the Dorians, the three branches of that race being named after these three heroes. Desiring to reconquer his paternal inheritance, Hyllus consulted the Delphic oracle, which told him to wait for \"the third fruit\", (or \"the third crop\") and then enter Peloponnesus by \"a narrow passage by sea\". Accordingly, after three years, Hyllus marched across the isthmus of Corinth to attack Atreus, the successor of Eurystheus, but was slain in single combat by Echemus, king of Tegea. This second attempt was followed by a third under Cleodaeus and a fourth under Aristomachus, both unsuccessful.\n\nAt last, Temenus, Cresphontes and Aristodemus, the sons of Aristomachus, complained to the oracle that its instructions had proved fatal to those who had followed them. They received the answer that by the \"third fruit\" the \"third generation\" was meant, and that the \"narrow passage\" was not the isthmus of Corinth, but the straits of Rhium. They accordingly built a fleet at Naupactus, but before they set sail, Aristodemus was struck by lightning (or shot by Apollo) and the fleet destroyed, because one of the Heracleidae had slain an Acarnanian soothsayer.\n\nThe oracle, being again consulted by Temenus, bade him offer an expiatory sacrifice and banish the murderer for ten years, and look out for a man with three eyes to act as guide. On his way back to Naupactus, Temenus fell in with Oxylus, an Aetolian, who had lost one eye, riding on a horse (thus making up the three eyes) and immediately pressed him into his service. According to another account, a mule on which Oxylus rode had lost an eye. The Heracleidae repaired their ships, sailed from Naupactus to Antirrhium, and thence to Rhium in Peloponnesus. A decisive battle was fought with Tisamenus, son of Orestes, the chief ruler in the peninsula, who was defeated and slain. This conquest was traditionally dated eighty years after the Trojan War.\n\nThe Heracleidae, who thus became practically masters of Peloponnesus, proceeded to distribute its territory among themselves by lot. Argos fell to Temenus, Lacedaemon to Procles and Eurysthenes, the twin sons of Aristodemus; and Messenia to Cresphontes (tradition maintains that Cresphontes cheated in order to obtain Messenia, which had the best land of all.) The fertile district of Elis had been reserved by agreement for Oxylus. The Heracleidae ruled in Lacedaemon until 221 BCE, but disappeared much earlier in the other countries.\n\nThis conquest of Peloponnesus by the Dorians, commonly called the \"Dorian invasion\" or the \"Return of the Heraclidae\", is represented as the recovery by the descendants of Heracles of the rightful inheritance of their hero ancestor and his sons. The Dorians followed the custom of other Greek tribes in claiming as ancestor for their ruling families one of the legendary heroes, but the traditions must not on that account be regarded as entirely mythical. They represent a joint invasion of Peloponnesus by Aetolians and Dorians, the latter having been driven southward from their original northern home under pressure from the Thessalians. It is noticeable that there is no mention of these Heraclidae or their invasion in Homer or Hesiod. Herodotus (vi. 52) speaks of poets who had celebrated their deeds, but these were limited to events immediately succeeding the death of Heracles.\n\nAt Sparta, the Heraclids formed two dynasties ruling jointly: the Agiads and the Eurypontids.\n\nAt Corinth the Heraclids ruled as the Bacchiadae dynasty before the aristocratic revolution, which brought a Bacchiad aristocracy into power. The kings were as follows:\n\nThe story was first amplified by the Greek tragedians, who probably drew their inspiration from local legends, which glorified the services rendered by Athens to the rulers of Peloponnesus.\n\nThe Heracleidae are the main subject of Euripides' play, \"Heracleidae\". J. A. Spranger found the political subtext of \"Heracleidae\", never far to seek, so particularly apt in Athens towards the end of the peace of Nicias, in 419 BCE, that he suggested the date as its first performance.\n\nIn the tragedy, Iolaus, Heracles' old comrade, and his children, Macaria and her brothers and sisters have hidden from Eurystheus in Athens, which was ruled by King Demophon; as the first scene makes clear, their expectation is that the blood relationship of the kings with Heracles and their father's past indebtedness to Theseus, will finally provide them sanctuary. As Eurysttheus prepared to attack, an oracle told Demophon that he would win if and only if a noble woman was sacrificed to Persephone. Macaria volunteered for the sacrifice and a spring was named the Macarian spring in her honor.\n\n\n", "id": "14169", "title": "Heracleidae"}
{"url": "https://en.wikipedia.org/wiki?curid=14170", "text": "HIV\n\nThe human immunodeficiency virus (HIV) is a lentivirus (a subgroup of retrovirus) that causes HIV infection and over time acquired immunodeficiency syndrome (AIDS). AIDS is a condition in humans in which progressive failure of the immune system allows life-threatening opportunistic infections and cancers to thrive. Without treatment, average survival time after infection with HIV is estimated to be 9 to 11 years, depending on the HIV subtype. Infection with HIV occurs by the transfer of blood, semen, vaginal fluid, pre-ejaculate, or breast milk. Within these bodily fluids, HIV is present as both free virus particles and virus within infected immune cells.\n\nHIV infects vital cells in the human immune system such as helper T cells (specifically CD4 T cells), macrophages, and dendritic cells. HIV infection leads to low levels of CD4 T cells through a number of mechanisms, including pyroptosis of abortively infected T cells, apoptosis of uninfected bystander cells, direct viral killing of infected cells, and killing of infected CD4 T cells by CD8 cytotoxic lymphocytes that recognize infected cells. When CD4 T cell numbers decline below a critical level, cell-mediated immunity is lost, and the body becomes progressively more susceptible to opportunistic infections.\n\nHIV is a member of the genus \"Lentivirus\", part of the family \"Retroviridae\". Lentiviruses have many morphologies and biological properties in common. Many species are infected by lentiviruses, which are characteristically responsible for long-duration illnesses with a long incubation period. Lentiviruses are transmitted as single-stranded, positive-sense, enveloped RNA viruses. Upon entry into the target cell, the viral RNA genome is converted (reverse transcribed) into double-stranded DNA by a virally encoded reverse transcriptase that is transported along with the viral genome in the virus particle. The resulting viral DNA is then imported into the cell nucleus and integrated into the cellular DNA by a virally encoded integrase and host co-factors. Once integrated, the virus may become latent, allowing the virus and its host cell to avoid detection by the immune system. Alternatively, the virus may be transcribed, producing new RNA genomes and viral proteins that are packaged and released from the cell as new virus particles that begin the replication cycle anew.\n\nTwo types of HIV have been characterized: HIV-1 and HIV-2. HIV-1 is the virus that was initially discovered and termed both LAV and HTLV-III. It is more virulent, more infective, and is the cause of the majority of HIV infections globally. The lower infectivity of HIV-2 compared to HIV-1 implies that fewer of those exposed to HIV-2 will be infected per exposure. Because of its relatively poor capacity for transmission, HIV-2 is largely confined to West Africa.\n\nHIV is different in structure from other retroviruses. It is roughly spherical with a diameter of about 120 nm, around 60 times smaller than a red blood cell. It is composed of two copies of positive single-stranded RNA that codes for the virus's nine genes enclosed by a conical capsid composed of 2,000 copies of the viral protein p24. The single-stranded RNA is tightly bound to nucleocapsid proteins, p7, and enzymes needed for the development of the virion such as reverse transcriptase, proteases, ribonuclease and integrase. A matrix composed of the viral protein p17 surrounds the capsid ensuring the integrity of the virion particle.\n\nThis is, in turn, surrounded by the viral envelope, that is composed of the lipid bilayer taken from the membrane of a human cell when the newly formed virus particle buds from the cell. The viral envelope contains proteins from the host cell and relatively few copies of the HIV Envelope protein, which consists of a cap made of three molecules known as glycoprotein (gp) 120, and a stem consisting of three gp41 molecules which anchor the structure into the viral envelope. The Envelope protein, encoded by the HIV \"env\" gene, allows the virus to attach to target cells and fuse the viral envelope with the target cell membrane releasing the viral contents into the cell and initiating the infectious cycle.\nAs the sole viral protein on the surface of the virus, the Envelope protein is a major target for HIV vaccine efforts. Over half of the mass of the trimeric envelope spike is N-linked glycans. The density is high as the glycans shield the underlying viral protein from neutralisation by antibodies. This is one of the most densely glycosylated molecules known and the density is sufficiently high to prevent the normal maturation process of glycans during biogenesis in the endoplasmic and Golgi apparatus. The majority of the glycans are therefore stalled as immature 'high-mannose' glycans not normally present on secreted or cell surface human glycoproteins. The unusual processing and high density means that almost all broadly neutralising antibodies that have so far been identified (from a subset of patients that have been infected for many months to years) bind to or, are adapted to cope with, these envelope glycans.\n\nThe molecular structure of the viral spike has now been determined by X-ray crystallography and cryo-electron microscopy. These advances in structural biology were made possible due to the development of stable recombinant forms of the viral spike by the introduction of an intersubunit disulphide bond and an isoleucine to proline mutation in gp41. The so-called SOSIP trimers not only reproduce the antigenic properties of the native viral spike but also display the same degree of immature glycans as presented on the native virus. Recombinant trimeric viral spikes are promising vaccine candidates as they display less non-neutralising epitopes than recombinant monomeric gp120 which act to suppress the immune response to target epitopes. \n\nThe RNA genome consists of at least seven structural landmarks (LTR, TAR, RRE, PE, SLIP, CRS, and INS), and nine genes (\"gag\", \"pol\", and \"env\", \"tat\", \"rev\", \"nef\", \"vif\", \"vpr\", \"vpu\", and sometimes a tenth \"tev\", which is a fusion of tat env and rev), encoding 19 proteins. Three of these genes, \"gag\", \"pol\", and \"env\", contain information needed to make the structural proteins for new virus particles. For example, \"env\" codes for a protein called gp160 that is cut in two by a cellular protease to form gp120 and gp41. The six remaining genes, \"tat\", \"rev\", \"nef\", \"vif\", \"vpr\", and \"vpu\" (or \"vpx\" in the case of HIV-2), are regulatory genes for proteins that control the ability of HIV to infect cells, produce new copies of virus (replicate), or cause disease.\n\nThe two Tat proteins (p16 and p14) are transcriptional transactivators for the LTR promoter acting by binding the TAR RNA element. The TAR may also be processed into microRNAs that regulate the apoptosis genes ERCC1 and IER3. The Rev protein (p19) is involved in shuttling RNAs from the nucleus and the cytoplasm by binding to the RRE RNA element. The Vif protein (p23) prevents the action of APOBEC3G (a cellular protein that deaminates Cytidine to Uridine in the single stranded viral DNA and/or interferes with reverse transcription). The Vpr protein (p14) arrests cell division at G2/M. The Nef protein (p27) down-regulates CD4 (the major viral receptor), as well as the MHC class I and class II molecules.\n\nNef also interacts with SH3 domains. The Vpu protein (p16) influences the release of new virus particles from infected cells. The ends of each strand of HIV RNA contain an RNA sequence called the long terminal repeat (LTR). Regions in the LTR act as switches to control production of new viruses and can be triggered by proteins from either HIV or the host cell. The Psi element is involved in viral genome packaging and recognized by Gag and Rev proteins. The SLIP element (TTTTTT) is involved in the frameshift in the Gag-Pol reading frame required to make functional Pol.\n\nThe term viral tropism refers to the cell types a virus infects. HIV can infect a variety of immune cells such as CD4 T cells, macrophages, and microglial cells. HIV-1 entry to macrophages and CD4 T cells is mediated through interaction of the virion envelope glycoproteins (gp120) with the CD4 molecule on the target cells and also with chemokine coreceptors.\n\nMacrophage (M-tropic) strains of HIV-1, or non-syncytia-inducing strains (NSI; now called R5 viruses ) use the \"β\"-chemokine receptor CCR5 for entry and are, thus, able to replicate in macrophages and CD4 T cells. This CCR5 coreceptor is used by almost all primary HIV-1 isolates regardless of viral genetic subtype. Indeed, macrophages play a key role in several critical aspects of HIV infection. They appear to be the first cells infected by HIV and perhaps the source of HIV production when CD4 cells become depleted in the patient. Macrophages and microglial cells are the cells infected by HIV in the central nervous system. In tonsils and adenoids of HIV-infected patients, macrophages fuse into multinucleated giant cells that produce huge amounts of virus.\n\nT-tropic isolates, or syncytia-inducing (SI; now called X4 viruses ) strains replicate in primary CD4 T cells as well as in macrophages and use the \"α\"-chemokine receptor, CXCR4, for entry. Dual-tropic HIV-1 strains are thought to be transitional strains of HIV-1 and thus are able to use both CCR5 and CXCR4 as co-receptors for viral entry.\n\nThe \"α\"-chemokine SDF-1, a ligand for CXCR4, suppresses replication of T-tropic HIV-1 isolates. It does this by down-regulating the expression of CXCR4 on the surface of these cells. HIV that use only the CCR5 receptor are termed R5; those that use only CXCR4 are termed X4, and those that use both, X4R5. However, the use of coreceptor alone does not explain viral tropism, as not all R5 viruses are able to use CCR5 on macrophages for a productive infection and HIV can also infect a subtype of myeloid dendritic cells, which probably constitute a reservoir that maintains infection when CD4 T cell numbers have declined to extremely low levels.\n\nSome people are resistant to certain strains of HIV. For example, people with the CCR5-Δ32 mutation are resistant to infection with R5 virus, as the mutation stops HIV from binding to this coreceptor, reducing its ability to infect target cells.\n\nSexual intercourse is the major mode of HIV transmission. Both X4 and R5 HIV are present in the seminal fluid, which is passed from a male to his sexual partner. The virions can then infect numerous cellular targets and disseminate into the whole organism. However, a selection process leads to a predominant transmission of the R5 virus through this pathway. How this selective process works is still under investigation, but one model is that spermatozoa may selectively carry R5 HIV as they possess both CCR3 and CCR5 but not CXCR4 on their surface and that genital epithelial cells preferentially sequester X4 virus. In patients infected with subtype B HIV-1, there is often a co-receptor switch in late-stage disease and T-tropic variants appear that can infect a variety of T cells through CXCR4. These variants then replicate more aggressively with heightened virulence that causes rapid T cell depletion, immune system collapse, and opportunistic infections that mark the advent of AIDS. Thus, during the course of infection, viral adaptation to the use of CXCR4 instead of CCR5 may be a key step in the progression to AIDS. A number of studies with subtype B-infected individuals have determined that between 40 and 50 percent of AIDS patients can harbour viruses of the SI and, it is presumed, the X4 phenotypes.\n\nHIV-2 is much less pathogenic than HIV-1 and is restricted in its worldwide distribution. The adoption of \"accessory genes\" by HIV-2 and its more promiscuous pattern of coreceptor usage (including CD4-independence) may assist the virus in its adaptation to avoid innate restriction factors present in host cells. Adaptation to use normal cellular machinery to enable transmission and productive infection has also aided the establishment of HIV-2 replication in humans. A survival strategy for any infectious agent is not to kill its host but ultimately become a commensal organism. Having achieved a low pathogenicity, over time, variants more successful at transmission will be selected.\n\nThe HIV virion enters macrophages and CD4 T cells by the adsorption of glycoproteins on its surface to receptors on the target cell followed by fusion of the viral envelope with the cell membrane and the release of the HIV capsid into the cell.\n\nEntry to the cell begins through interaction of the trimeric envelope complex (gp160 spike) and both CD4 and a chemokine receptor (generally either CCR5 or CXCR4, but others are known to interact) on the cell surface. gp120 binds to integrin αβ activating LFA-1 the central integrin involved in the establishment of virological synapses, which facilitate efficient cell-to-cell spreading of HIV-1. The gp160 spike contains binding domains for both CD4 and chemokine receptors.\n\nThe first step in fusion involves the high-affinity attachment of the CD4 binding domains of gp120 to CD4. Once gp120 is bound with the CD4 protein, the envelope complex undergoes a structural change, exposing the chemokine binding domains of gp120 and allowing them to interact with the target chemokine receptor. This allows for a more stable two-pronged attachment, which allows the N-terminal fusion peptide gp41 to penetrate the cell membrane. Repeat sequences in gp41, HR1, and HR2 then interact, causing the collapse of the extracellular portion of gp41 into a hairpin. This loop structure brings the virus and cell membranes close together, allowing fusion of the membranes and subsequent entry of the viral capsid.\n\nAfter HIV has bound to the target cell, the HIV RNA and various enzymes, including reverse transcriptase, integrase, ribonuclease, and protease, are injected into the cell. During the microtubule-based transport to the nucleus, the viral single-strand RNA genome is transcribed into double-strand DNA, which is then integrated into a host chromosome.\n\nHIV can infect dendritic cells (DCs) by this CD4-CCR5 route, but another route using mannose-specific C-type lectin receptors such as DC-SIGN can also be used. DCs are one of the first cells encountered by the virus during sexual transmission. They are currently thought to play an important role by transmitting HIV to T-cells when the virus is captured in the mucosa by DCs. The presence of FEZ-1, which occurs naturally in neurons, is believed to prevent the infection of cells by HIV.\nHIV-1 entry, as well as entry of many other retroviruses, has long been believed to occur exclusively at the plasma membrane. More recently, however, productive infection by pH-independent, clathrin-dependent endocytosis of HIV-1 has also been reported and was recently suggested to constitute the only route of productive entry.\n\nShortly after the viral capsid enters the cell, an enzyme called \"reverse transcriptase\" liberates the single-stranded (+)RNA genome from the attached viral proteins and copies it into a complementary DNA (cDNA) molecule. The process of reverse transcription is extremely error-prone, and the resulting mutations may cause drug resistance or allow the virus to evade the body's immune system. The reverse transcriptase also has ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that creates a sense DNA from the \"antisense\" cDNA. Together, the cDNA and its complement form a double-stranded viral DNA that is then transported into the cell nucleus. The integration of the viral DNA into the host cell's genome is carried out by another viral enzyme called \"integrase\".\n\nThis integrated viral DNA may then lie dormant, in the latent stage of HIV infection. To actively produce the virus, certain cellular transcription factors need to be present, the most important of which is NF-\"κ\"B (NF kappa B), which is upregulated when T-cells become activated. This means that those cells most likely to be killed by HIV are those currently fighting infection.\n\nDuring viral replication, the integrated DNA provirus is transcribed into RNA, some of which then undergo RNA splicing to produce mature mRNAs. These mRNAs are exported from the nucleus into the cytoplasm, where they are translated into the regulatory proteins Tat (which encourages new virus production) and Rev. As the newly produced Rev protein accumulates in the nucleus, it binds to full-length, unspliced copies of virus RNAs and allows them to leave the nucleus. Some of these full-length RNAs function as new copies of the virus genome, while others function as mRNAs that are translated to produce the structural proteins Gag and Env. Gag proteins bind to copies of the virus RNA genome to package them into new virus particles.\n\nHIV-1 and HIV-2 appear to package their RNA differently. HIV-1 will bind to any appropriate RNA. HIV-2 will preferentially bind to the mRNA that was used to create the Gag protein itself.\n\nTwo RNA genomes are encapsidated in each HIV-1 particle (see Structure and genome of HIV). Upon infection and replication catalyzed by reverse transcriptase, recombination between the two genomes can occur. Recombination occurs as the single-strand (+)RNA genomes are reverse transcribed to form DNA. During reverse transcription the nascent DNA can switch multiple times between the two copies of the viral RNA. This form of recombination is known as copy-choice. Recombination events may occur throughout the genome. From 2 to 20 events per genome may occur at each replication cycle, and these events can rapidly shuffle the genetic information that is transmitted from parental to progeny genomes.\n\nViral recombination produces genetic variation that likely contributes to the evolution of resistance to anti-retroviral therapy. Recombination may also contribute, in principle, to overcoming the immune defenses of the host. Yet, for the adaptive advantages of genetic variation to be realized, the two viral genomes packaged in individual infecting virus particles need to have arisen from separate progenitor parental viruses of differing genetic constitution. It is unknown how often such mixed packaging occurs under natural conditions.\n\nBonhoeffer et al. suggested that template switching by the reverse transcriptase acts as a repair process to deal with breaks in the ssRNA genome. In addition, Hu and Temin suggested that recombination is an adaptation for repair of damage in the RNA genomes. Strand switching (copy-choice recombination) by reverse transcriptase could generate an undamaged copy of genomic DNA from two damaged ssRNA genome copies. This view of the adaptive benefit of recombination in HIV could explain why each HIV particle contains two complete genomes, rather than one. Furthermore, the view that recombination is a repair process implies that the benefit of repair can occur at each replication cycle, and that this benefit can be realized whether or not the two genomes differ genetically. On the view that that recombination in HIV is a repair process, the generation of recombinational variation would be a consequence, but not the cause of, the evolution of template switching.\n\nHIV-1 infection causes chronic ongoing inflammation and production of reactive oxygen species. Thus, the HIV genome may be vulnerable to oxidative damages, including breaks in the single-stranded RNA. For HIV, as well as for viruses generally, successful infection depends on overcoming host defensive strategies that often include production of genome-damaging reactive oxygen. Thus, Michod et al. suggested that recombination by viruses is an adaptation for repair of genome damages, and that recombinational variation is a byproduct that may provide a separate benefit.\n\nThe final step of the viral cycle, assembly of new HIV-1 virions, begins at the plasma membrane of the host cell. The Env polyprotein (gp160) goes through the endoplasmic reticulum and is transported to the Golgi complex where it is cleaved by furin resulting in the two HIV envelope glycoproteins, gp41 and gp120. These are transported to the plasma membrane of the host cell where gp41 anchors gp120 to the membrane of the infected cell. The Gag (p55) and Gag-Pol (p160) polyproteins also associate with the inner surface of the plasma membrane along with the HIV genomic RNA as the forming virion begins to bud from the host cell. The budded virion is still immature as the gag polyproteins still need to be cleaved into the actual matrix, capsid and nucleocapsid proteins. This cleavage is mediated by the also packaged viral protease and can be inhibited by antiretroviral drugs of the protease inhibitor class. The various structural components then assemble to produce a mature HIV virion. Only mature virions are then able to infect another cell.\n\nHIV is now known to spread between CD4+ T cells by two parallel routes: cell-free spread and cell-to-cell spread, i.e. it employs hybrid spreading mechanisms. In the cell-free spread, virus particles bud from an infected T cell, enter the blood/extracellular fluid and then infect another T cell following a chance encounter. HIV can also disseminate by direct transmission from one cell to another by a process of cell-to-cell spread. Two pathways of cell-to-cell transmission have been reported. Firstly, an infected T cell can transmit virus directly to a target T cell via a virological synapse. Secondly, an antigen presenting cell (APC) can also transmit HIV to T cells by a process that either involves productive infection (in the case of macrophages) or capture and transfer of virions \"in trans\" (in the case of dendritic cells). Whichever pathway is used, infection by cell-to-cell transfer is reported to be much more efficient than cell-free virus spread. A number of factors contribute to this increased efficiency, including polarised virus budding towards the site of cell-to-cell contact, close apposition of cells which minimizes fluid-phase diffusion of virions, and clustering of HIV entry receptors on the target cell to the contact zone. Cell-to-cell spread is thought to be particularly important in lymphoid tissues where CD4+ T lymphocytes are densely packed and likely to frequently interact. Intravital imaging studies have supported the concept of the HIV virological synapse \"in vivo\". The hybrid spreading mechanisms of HIV contribute to the virus's ongoing replication against antiretroviral therapies.\n\nHIV differs from many viruses in that it has very high genetic variability. This diversity is a result of its fast replication cycle, with the generation of about 10 virions every day, coupled with a high mutation rate of approximately 3 x 10 per nucleotide base per cycle of replication and recombinogenic properties of reverse transcriptase.\n\nThis complex scenario leads to the generation of many variants of HIV in a single infected patient in the course of one day. This variability is compounded when a single cell is simultaneously infected by two or more different strains of HIV. When simultaneous infection occurs, the genome of progeny virions may be composed of RNA strands from two different strains. This hybrid virion then infects a new cell where it undergoes replication. As this happens, the reverse transcriptase, by jumping back and forth between the two different RNA templates, will generate a newly synthesized retroviral DNA sequence that is a recombinant between the two parental genomes. This recombination is most obvious when it occurs between subtypes.\n\nThe closely related simian immunodeficiency virus (SIV) has evolved into many strains, classified by the natural host species. SIV strains of the African green monkey (SIVagm) and sooty mangabey (SIVsmm) are thought to have a long evolutionary history with their hosts. These hosts have adapted to the presence of the virus, which is present at high levels in the host's blood but evokes only a mild immune response, does not cause the development of simian AIDS, and does not undergo the extensive mutation and recombination typical of HIV infection in humans.\n\nIn contrast, when these strains infect species that have not adapted to SIV (\"heterologous\" hosts such as rhesus or cynomologus macaques), the animals develop AIDS and the virus generates genetic diversity similar to what is seen in human HIV infection. Chimpanzee SIV (SIVcpz), the closest genetic relative of HIV-1, is associated with increased mortality and AIDS-like symptoms in its natural host. SIVcpz appears to have been transmitted relatively recently to chimpanzee and human populations, so their hosts have not yet adapted to the virus. This virus has also lost a function of the Nef gene that is present in most SIVs. For non-pathogenic SIV variants, Nef suppresses T-cell activation through the CD3 marker. Nef’s function in non-pathogenic forms of SIV is to downregulate expression of inflammatory cytokines, MHC-1, and signals that affect T cell trafficking. In HIV-1 and SIVcpz, Nef does not inhibit T-cell activation and it has lost this function. Without this function, T cell depletion is more likely, leading to immunodeficiency.\n\nThree groups of HIV-1 have been identified on the basis of differences in the envelope (\"env\") region: M, N, and O. Group M is the most prevalent and is subdivided into eight subtypes (or clades), based on the whole genome, which are geographically distinct. The most prevalent are subtypes B (found mainly in North America and Europe), A and D (found mainly in Africa), and C (found mainly in Africa and Asia); these subtypes form branches in the phylogenetic tree representing the lineage of the M group of HIV-1. Coinfection with distinct subtypes gives rise to circulating recombinant forms (CRFs). In 2000, the last year in which an analysis of global subtype prevalence was made, 47.2% of infections worldwide were of subtype C, 26.7% were of subtype A/CRF02_AG, 12.3% were of subtype B, 5.3% were of subtype D, 3.2% were of CRF_AE, and the remaining 5.3% were composed of other subtypes and CRFs. Most HIV-1 research is focused on subtype B; few laboratories focus on the other subtypes. The existence of a fourth group, \"P\", has been hypothesised based on a virus isolated in 2009. The strain is apparently derived from gorilla SIV (SIVgor), first isolated from western lowland gorillas in 2006.\n\nHIV-2’s closest relative is SIVsm, a strain of SIV found in sooty mangabees. Since HIV-1 is derived from SIVcpz, and HIV-2 from SIVsm, the genetic sequence of HIV-2 is only partially homologous to HIV-1 and more closely resembles that of SIVsm.\n\nMany HIV-positive people are unaware that they are infected with the virus. For example, in 2001 less than 1% of the sexually active urban population in Africa had been tested, and this proportion is even lower in rural populations. Furthermore, in 2001 only 0.5% of pregnant women attending urban health facilities were counselled, tested or receive their test results. Again, this proportion is even lower in rural health facilities. Since donors may therefore be unaware of their infection, donor blood and blood products used in medicine and medical research are routinely screened for HIV.\n\nHIV-1 testing is initially by an enzyme-linked immunosorbent assay (ELISA) to detect antibodies to HIV-1. Specimens with a nonreactive result from the initial ELISA are considered HIV-negative unless new exposure to an infected partner or partner of unknown HIV status has occurred. Specimens with a reactive ELISA result are retested in duplicate. If the result of either duplicate test is reactive, the specimen is reported as repeatedly reactive and undergoes confirmatory testing with a more specific supplemental test (e.g., western blot or, less commonly, an immunofluorescence assay (IFA)). Only specimens that are repeatedly reactive by ELISA and positive by IFA or reactive by western blot are considered HIV-positive and indicative of HIV infection. Specimens that are repeatedly ELISA-reactive occasionally provide an indeterminate western blot result, which may be either an incomplete antibody response to HIV in an infected person or nonspecific reactions in an uninfected person.\n\nAlthough IFA can be used to confirm infection in these ambiguous cases, this assay is not widely used. In general, a second specimen should be collected more than a month later and retested for persons with indeterminate western blot results. Although much less commonly available, nucleic acid testing (e.g., viral RNA or proviral DNA amplification method) can also help diagnosis in certain situations. In addition, a few tested specimens might provide inconclusive results because of a low quantity specimen. In these situations, a second specimen is collected and tested for HIV infection.\n\nModern HIV testing is extremely accurate. A single screening test is correct more than 99% of the time. The chance of a false-positive result in standard two-step testing protocol is estimated to be about 1 in 250,000 in a low risk population. Testing post exposure is recommended initially and at six weeks, three months, and six months.\n\nThe latest recommendations of the CDC show that HIV testing must start with an immunoassay combination test for HIV-1 and HIV-2 antibodies and p24 antigen. A negative result rules out HIV exposure, while a positive one must be followed by an HIV-1/2 antibody differentiation immunoassay to detect which is present. This gives rise to four possible scenarios:\n\n\nAn updated algorithm published by the CDC in June 2014 recommends that diagnosis starts with the p24 antigen test. A negative result rules out infection, while a positive one must be followed by an HIV-1/2 antibody differentiation immunoassay. A positive differentiation test confirms diagnosis, while a negative or indeterminate result must be followed by nucleic acid test (NAT). A positive NAT result confirms HIV-1 infection whereas a negative result rules out infection (false positive p24).\n\nHIV/AIDS research includes all medical research that attempts to prevent, treat, or cure HIV/AIDS, as well as fundamental research about the nature of HIV as an infectious agent and AIDS as the disease caused by HIV.\n\nMany governments and research institutions participate in HIV/AIDS research. This research includes behavioral health interventions, such as research into sex education, and drug development, such as research into microbicides for sexually transmitted diseases, HIV vaccines, and antiretroviral drugs. Other medical research areas include the topics of pre-exposure prophylaxis, post-exposure prophylaxis, circumcision and HIV, and accelerated aging effects.\n\nAfter many years of research an untested HIV vaccine has been created. Bi-specific antibodies that target both the surface of T-cells and viral epitopes can prevent entry of the virus into somatic cells. Another group has utilised the same technology to develop a bi-specific antibody that neutralises viral particles by cross linking of envelope glycoproteins. \n\nAIDS was first clinically observed in 1981 in the United States. The initial cases were a cluster of injection drug users and gay men with no known cause of impaired immunity who showed symptoms of \"Pneumocystis carinii\" pneumonia (PCP), a rare opportunistic infection that was known to occur in people with very compromised immune systems. Soon thereafter, additional gay men developed a previously rare skin cancer called Kaposi's sarcoma (KS). Many more cases of PCP and KS emerged, alerting U.S. Centers for Disease Control and Prevention (CDC) and a CDC task force was formed to monitor the outbreak. The earliest retrospectively described case of AIDS is believed to have been in Norway beginning in 1966.\n\nIn the beginning, the CDC did not have an official name for the disease, often referring to it by way of the diseases that were associated with it, for example, lymphadenopathy, the disease after which the discoverers of HIV originally named the virus. They also used \"Kaposi's Sarcoma and Opportunistic Infections\", the name by which a task force had been set up in 1981. In the general press, the term \"GRID\", which stood for gay-related immune deficiency, had been coined. The CDC, in search of a name, and looking at the infected communities coined \"the 4H disease,\" as it seemed to single out homosexuals, heroin users, hemophiliacs, and Haitians. However, after determining that AIDS was not isolated to the gay community, it was realized that the term GRID was misleading and \"AIDS\" was introduced at a meeting in July 1982. By September 1982 the CDC started using the name AIDS.\nIn 1983, two separate research groups led by Robert Gallo and Luc Montagnier independently declared that a novel retrovirus may have been infecting AIDS patients, and published their findings in the same issue of the journal \"Science\". Gallo claimed that a virus his group had isolated from a person with AIDS was strikingly similar in shape to other human T-lymphotropic viruses (HTLVs) his group had been the first to isolate. Gallo's group called their newly isolated virus HTLV-III. At the same time, Montagnier's group isolated a virus from a patient presenting with swelling of the lymph nodes of the neck and physical weakness, two classic symptoms of AIDS. Contradicting the report from Gallo's group, Montagnier and his colleagues showed that core proteins of this virus were immunologically different from those of HTLV-I. Montagnier's group named their isolated virus lymphadenopathy-associated virus (LAV). As these two viruses turned out to be the same, in 1986, LAV and HTLV-III were renamed HIV.\n\nAnother group working contemporaneously as the Montagnier and Gallo groups is that of Dr. Jay Levy at the University of California in San Francisco. He independently discovered the AIDS virus in 1983 and named it the AIDS associated retrovirus (ARV). This virus was very different from the virus reported by the Montagnier and Gallo groups. The ARV strains indicated, for the first time, the heterogeneity of HIV isolates and several of these remain classic examples of the AIDS virus found in the United States. \n\nBoth HIV-1 and HIV-2 are believed to have originated in non-human primates in West-central Africa, and are believed to have transferred to humans (a process known as zoonosis) in the early 20th century.\n\nHIV-1 appears to have originated in southern Cameroon through the evolution of SIV(cpz), a simian immunodeficiency virus (SIV) that infects wild chimpanzees (HIV-1 descends from the SIV(cpz) endemic in the chimpanzee subspecies \"Pan troglodytes troglodytes\"). The closest relative of HIV-2 is SIV (smm), a virus of the sooty mangabey (\"Cercocebus atys atys\"), an Old World monkey living in litoral West Africa (from southern Senegal to western Côte d'Ivoire). New World monkeys such as the owl monkey are resistant to HIV-1 infection, possibly because of a genomic fusion of two viral resistance genes.\nHIV-1 is thought to have jumped the species barrier on at least three separate occasions, giving rise to the three groups of the virus, M, N, and O.\nThere is evidence that humans who participate in bushmeat activities, either as hunters or as bushmeat vendors, commonly acquire SIV. However, SIV is a weak virus, and it is typically suppressed by the human immune system within weeks of infection. It is thought that several transmissions of the virus from individual to individual in quick succession are necessary to allow it enough time to mutate into HIV. Furthermore, due to its relatively low person-to-person transmission rate, it can only spread throughout the population in the presence of one or more high-risk transmission channels, which are thought to have been absent in Africa prior to the 20th century.\n\nSpecific proposed high-risk transmission channels, allowing the virus to adapt to humans and spread throughout the society, depend on the proposed timing of the animal-to-human crossing. Genetic studies of the virus suggest that the most recent common ancestor of the HIV-1 M group dates back to circa 1910. Proponents of this dating link the HIV epidemic with the emergence of colonialism and growth of large colonial African cities, leading to social changes, including a higher degree of sexual promiscuity, the spread of prostitution, and the concomitant high frequency of genital ulcer diseases (such as syphilis) in nascent colonial cities. While transmission rates of HIV during vaginal intercourse are typically low, they are increased many fold if one of the partners suffers from a sexually transmitted infection resulting in genital ulcers. Early 1900s colonial cities were notable due to their high prevalence of prostitution and genital ulcers to the degree that as of 1928 as many as 45% of female residents of eastern Leopoldville were thought to have been prostitutes and as of 1933 around 15% of all residents of the same city were infected by one of the forms of syphilis.\n\nAn alternative view holds that unsafe medical practices in Africa during years following World War II, such as unsterile reuse of single use syringes during mass vaccination, antibiotic, and anti-malaria treatment campaigns, were the initial vector that allowed the virus to adapt to humans and spread.\n\nThe earliest well documented case of HIV in a human dates back to 1959 in the Belgian Congo. The virus may have been present in the United States as early as the mid-to-late 1950s, as a sixteen-year-old male presented with symptoms in 1966 died in 1969.\n\n\n", "id": "14170", "title": "HIV"}
{"url": "https://en.wikipedia.org/wiki?curid=14173", "text": "HOL\n\nHol or HOL may refer to:\n\n\n\n\n\n\n", "id": "14173", "title": "HOL"}
{"url": "https://en.wikipedia.org/wiki?curid=14174", "text": "Hostile witness\n\nA hostile witness, otherwise known as an adverse witness or an unfavorable witness, is a witness at trial whose testimony on direct examination is either openly antagonistic or appears to be contrary to the legal position of the party who called the witness. \n\nDuring direct examination, if the examining attorney who called the witness finds that their testimony is antagonistic or contrary to the legal position of their client, the attorney may request that the judge declare the witness hostile. If the request is granted, the attorney may proceed to ask the witness leading questions. Leading questions either suggest the answer (\"You saw my client sign the contract, correct?\") or challenge (impeach) the witness' testimony. As a rule, leading questions are generally only allowed during cross-examination, but a hostile witness is an exception to this rule. \n\nIn cross-examination conducted by the opposing party's attorney, a witness is presumed to be hostile and the examining attorney is not required to seek the judge's permission before asking leading questions. Attorneys can influence a hostile witness' responses by using Gestalt psychology to influence the way the witness perceives the situation, and utility theory to understand their likely responses. The attorney will integrate a hostile witness' expected responses into the larger case strategy through pretrial planning and through adapting as necessary during the course of the trial.\n\n\n", "id": "14174", "title": "Hostile witness"}
{"url": "https://en.wikipedia.org/wiki?curid=14179", "text": "Henry I of England\n\nHenry I (c. 1068 – 1 December 1135), also known as Henry Beauclerc, was King of England from 1100 to his death. Henry was the fourth son of William the Conqueror and was educated in Latin and the liberal arts. On William's death in 1087, Henry's elder brothers Robert Curthose and William Rufus inherited Normandy and England, respectively, but Henry was left landless. Henry purchased the County of Cotentin in western Normandy from Robert, but William and Robert deposed him in 1091. Henry gradually rebuilt his power base in the Cotentin and allied himself with William against Robert. Henry was present when William died in a hunting accident in 1100, and he seized the English throne, promising at his coronation to correct many of William's less popular policies. Henry married Matilda of Scotland but continued to have a large number of mistresses, by whom he had many illegitimate children.\n\nRobert, who invaded in 1101, disputed Henry's control of England; this military campaign ended in a negotiated settlement that confirmed Henry as king. The peace was short-lived, and Henry invaded the Duchy of Normandy in 1105 and 1106, finally defeating Robert at the Battle of Tinchebray. Henry kept Robert imprisoned for the rest of his life. Henry's control of Normandy was challenged by Louis VI of France, Baldwin VII of Flanders and Fulk V of Anjou, who promoted the rival claims of Robert's son, William Clito, and supported a major rebellion in the Duchy between 1116 and 1119. Following Henry's victory at the Battle of Brémule, a favourable peace settlement was agreed with Louis in 1120.\n\nConsidered by contemporaries to be a harsh but effective ruler, Henry skilfully manipulated the barons in England and Normandy. In England, he drew on the existing Anglo-Saxon system of justice, local government and taxation, but also strengthened it with additional institutions, including the royal exchequer and itinerant justices. Normandy was also governed through a growing system of justices and an exchequer. Many of the officials who ran Henry's system were \"new men\" of obscure backgrounds rather than from families of high status, who rose through the ranks as administrators. Henry encouraged ecclesiastical reform, but became embroiled in a serious dispute in 1101 with Archbishop Anselm of Canterbury, which was resolved through a compromise solution in 1105. He supported the Cluniac order and played a major role in the selection of the senior clergy in England and Normandy.\n\nHenry's only legitimate son and heir, William Adelin, drowned in the \"White Ship\" disaster of 1120, throwing the royal succession into doubt. Henry took a second wife, Adeliza, in the hope of having another son, but their marriage was childless. In response to this, Henry declared his daughter, Matilda, his heir and married her to Geoffrey of Anjou. The relationship between Henry and the couple became strained, and fighting broke out along the border with Anjou. Henry died on 1 December 1135 after a week of illness. Despite his plans for Matilda, the King was succeeded by his nephew, Stephen of Blois, resulting in a period of civil war known as the Anarchy.\nHenry was probably born in England in 1068, in either the summer or the last weeks of the year, possibly in the town of Selby in Yorkshire. His father was William the Conqueror, who had originally been the Duke of Normandy and then, following the invasion of 1066, became the King of England, with lands stretching into Wales. The invasion had created an Anglo-Norman elite, many with estates spread across both sides of the English Channel. These Anglo-Norman barons typically had close links to the kingdom of France, which was then a loose collection of counties and smaller polities, under only the minimal control of the king. Henry's mother, Matilda of Flanders, was the granddaughter of Robert II of France, and she probably named Henry after her uncle, King Henry I of France.\n\nHenry was the youngest of William and Matilda's four sons. Physically he resembled his older brothers Robert Curthose, Richard and William Rufus, being, as historian David Carpenter describes, \"short, stocky and barrel-chested,\" with black hair. As a result of their age differences and Richard's early death, Henry would have probably seen relatively little of his older brothers. He probably knew his sister, Adela, well, as the two were close in age. There is little documentary evidence for his early years; historians Warren Hollister and Kathleen Thompson suggest he was brought up predominantly in England, while Judith Green argues he was initially brought up in the Duchy. He was probably educated by the Church, possibly by Bishop Osmund, the King's chancellor, at Salisbury Cathedral; it is uncertain if this indicated an intent by his parents for Henry to become a member of the clergy. It is also uncertain how far Henry's education extended, but he was probably able to read Latin and had some background in the liberal arts. He was given military training by an instructor called Robert Achard, and Henry was knighted by his father on 24 May 1086.\n\nIn 1087, William was fatally injured during a campaign in the Vexin. Henry joined his dying father near Rouen in September, where the King partitioned his possessions among his sons. The rules of succession in western Europe at the time were uncertain; in some parts of France, primogeniture, in which the eldest son would inherit a title, was growing in popularity. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands – usually considered to be the most valuable – and younger sons given smaller, or more recently acquired, partitions or estates.\n\nIn dividing his lands, William appears to have followed the Norman tradition, distinguishing between Normandy, which he had inherited, and England, which he had acquired through war. William's second son, Richard, had died in a hunting accident, leaving Henry and his two brothers to inherit William's estate. Robert, the eldest, despite being in armed rebellion against his father at the time of his death, received Normandy. England was given to William Rufus, who was in favour with the dying king. Henry was given a large sum of money, usually reported as £5,000, with the expectation that he would also be given his mother's modest set of lands in Buckinghamshire and Gloucestershire. William's funeral at Caen was marred by angry complaints from a local man, and Henry may have been responsible for resolving the dispute by buying off the protester with silver.\n\nRobert returned to Normandy, expecting to have been given both the Duchy and England, to find that William Rufus had crossed the Channel and been crowned king, as William II. The two brothers disagreed fundamentally over the inheritance, and Robert soon began to plan an invasion of England to seize the kingdom, helped by a rebellion by some of the leading nobles against William Rufus. Henry remained in Normandy and took up a role within Robert's court, possibly either because he was unwilling to openly side with William Rufus, or because Robert might have taken the opportunity to confiscate Henry's inherited money if he had tried to leave. William Rufus sequestered Henry's new estates in England, leaving Henry landless.\n\nIn 1088, Robert's plans for the invasion of England began to falter, and he turned to Henry, proposing that his brother lend him some of his inheritance, which Henry refused. Henry and Robert then came to an alternative arrangement, in which Robert would make Henry the count of western Normandy, in exchange for £3,000. Henry's lands were a new countship based around a delegation of the ducal authority in the Cotentin, but it extended across the Avranchin, with control over the bishoprics of both. This also gave Henry influence over two major Norman leaders, Hugh d'Avranches and Richard de Redvers, and the abbey of Mont Saint-Michel, whose lands spread out further across the Duchy. Robert's invasion force failed to leave Normandy, leaving William Rufus secure in England.\n\nHenry quickly established himself as count, building up a network of followers from western Normandy and eastern Brittany, whom historian John Le Patourel has characterised as \"Henry's gang\". His early supporters included Roger of Mandeville, Richard of Redvers, Richard d'Avranches and Robert Fitzhamon, along with the churchman Roger of Salisbury. Robert attempted to go back on his deal with Henry and re-appropriate the county, but Henry's grip was already sufficiently firm to prevent this. Robert's rule of the Duchy was chaotic, and parts of Henry's lands became almost independent of central control from Rouen.\n\nDuring this period, neither William nor Robert seems to have trusted Henry. Waiting until the rebellion against William Rufus was safely over, Henry returned to England in July 1088. He met with the King but was unable to persuade him to grant him their mother's estates, and travelled back to Normandy in the autumn. While he had been away, however, Odo, the Bishop of Bayeux, who regarded Henry as a potential competitor, had convinced Robert that Henry was conspiring against the duke with William Rufus. On landing, Odo seized Henry and imprisoned him in Neuilly-la-Forêt, and Robert took back the county of the Cotentin. Henry was held there over the winter, but in the spring of 1089 the senior elements of the Normandy nobility prevailed upon Robert to release him.\n\nAlthough no longer formally the Count of Cotentin, Henry continued to control the west of Normandy. The struggle between Henry's brothers continued. William Rufus continued to put down resistance to his rule in England, but began to build a number of alliances against Robert with barons in Normandy and neighbouring Ponthieu. Robert allied himself with Philip I of France. In late 1090 William Rufus encouraged Conan Pilatus, a powerful burgher in Rouen, to rebel against Robert; Conan was supported by most of Rouen and made appeals to the neighbouring ducal garrisons to switch allegiance as well.\n\nRobert issued an appeal for help to his barons, and Henry was the first to arrive in Rouen in November. Violence broke out, leading to savage, confused street fighting as both sides attempted to take control of the city. Robert and Henry left the castle to join the battle, but Robert then retreated, leaving Henry to continue the fighting. The battle turned in favour of the ducal forces and Henry took Conan prisoner. Henry was angry that Conan had turned against his feudal lord. He had him taken to the top of Rouen Castle and then, despite Conan's offers to pay a huge ransom, threw him off the top of the castle to his death. Contemporaries considered Henry to have acted appropriately in making an example of Conan, and Henry became famous for his exploits in the battle.\n\nIn the aftermath, Robert forced Henry to leave Rouen, probably because Henry's role in the fighting had been more prominent than his own, and possibly because Henry had asked to be formally reinstated as the count of the Cotentin. In early 1091, William Rufus invaded Normandy with a sufficiently large army to bring Robert to the negotiating table. The two brothers signed a treaty at Rouen, granting William Rufus a range of lands and castles in Normandy. In return, William Rufus promised to support Robert's attempts to regain control of the neighbouring county of Maine, once under Norman control, and help in regaining control over the Duchy, including Henry's lands. They nominated each other as heirs to England and Normandy, excluding Henry from any succession while either one of them lived.\n\nWar now broke out between Henry and his brothers. Henry mobilised a mercenary army in the west of Normandy, but as William Rufus and Robert's forces advanced, his network of baronial support melted away. Henry focused his remaining forces at Mont Saint-Michel, where he was besieged, probably in March 1091. The site was easy to defend, but lacked fresh water. The chronicler William of Malmesbury suggested that when Henry's water ran short, Robert allowed his brother fresh supplies, leading to remonstrations between Robert and William Rufus. The events of the final days of the siege are unclear: the besiegers had begun to argue about the future strategy for the campaign, but Henry then abandoned Mont Saint-Michel, probably as part of a negotiated surrender. He left for Brittany and crossed over into France.\n\nHenry's next steps are not well documented; one chronicler, Orderic Vitalis, suggests that he travelled in the French Vexin, along the Normandy border, for over a year with a small band of followers. By the end of the year, Robert and William Rufus had fallen out once again, and the Treaty of Rouen had been abandoned. In 1092, Henry and his followers seized the Normandy town of Domfront. Domfront had previously been controlled by Robert of Bellême, but the inhabitants disliked his rule and invited Henry to take over the town, which he did in a bloodless coup. Over the next two years, Henry re-established his network of supporters across western Normandy, forming what Judith Green terms a \"court in waiting\". By 1094, he was allocating lands and castles to his followers as if he were the Duke of Normandy. William Rufus began to support Henry with money, encouraging his campaign against Robert, and Henry used some of this to construct a substantial castle at Domfront.\n\nWilliam Rufus crossed into Normandy to take the war to Robert in 1094, and when progress stalled, called upon Henry for assistance. Henry responded, but travelled to London instead of joining the main campaign further east in Normandy, possibly at the request of the King, who in any event abandoned the campaign and returned to England. Over the next few years, Henry appears to have strengthened his power base in western Normandy, visiting England occasionally to attend at William Rufus's court. In 1095 Pope Urban II called the First Crusade, encouraging knights from across Europe to join. Robert joined the Crusade, borrowing money from William Rufus to do so, and granting the King temporary custody of his part of the Duchy in exchange. The King appeared confident of regaining the remainder of Normandy from Robert, and Henry appeared ever closer to William Rufus, the pair campaigning together in the Norman Vexin between 1097 and 1098.\n\nHenry became King of England following the death of William Rufus, who had been shot while hunting. On the afternoon of 2 August 1100, the King had gone hunting in the New Forest, accompanied by a team of huntsmen and a number of the Norman nobility, including Henry. An arrow was fired, possibly by the baron Walter Tirel, which hit and killed William Rufus. Numerous conspiracy theories have been put forward suggesting that the King was killed deliberately; most modern historians reject these, as hunting was a risky activity, and such accidents were common. Chaos broke out, and Tirel fled the scene for France, either because he had fired the fatal shot, or because he had been incorrectly accused and feared that he would be made a scapegoat for the King's death.\n\nHenry rode to Winchester, where an argument ensued as to who now had the best claim to the throne. William of Breteuil championed the rights of Robert, who was still abroad, returning from the Crusade, and to whom Henry and the barons had given homage in previous years. Henry argued that, unlike Robert, he had been born to a reigning king and queen, thereby giving him a claim under the right of porphyrogeniture. Tempers flared, but Henry, supported by Henry de Beaumont and Robert of Meulan, held sway and persuaded the barons to follow him. He occupied Winchester Castle and seized the royal treasury.\n\nHenry was hastily crowned king in Westminster Abbey on 5 August by Maurice, the Bishop of London, as Anselm, the Archbishop of Canterbury, had been exiled by William Rufus, and Thomas, the Archbishop of York, was in the north of England at Ripon. In accordance with English tradition and in a bid to legitimise his rule, Henry issued a coronation charter laying out various commitments. The new king presented himself as having restored order to a trouble-torn country. He announced that he would abandon William Rufus's policies towards the Church, which had been seen as oppressive by the clergy; he promised to prevent royal abuses of the barons' property rights, and assured a return to the gentler customs of Edward the Confessor; he asserted that he would \"establish a firm peace\" across England and ordered \"that this peace shall henceforth be kept\".\n\nIn addition to his existing circle of supporters, many of whom were richly rewarded with new lands, Henry quickly co-opted many of the existing administration into his new royal household. William Giffard, William Rufus's chancellor, was made the Bishop of Winchester, and the prominent sheriffs Urse d'Abetot, Haimo Dapifer and Robert Fitzhamon continued to play a senior role in government. By contrast, the unpopular Ranulf Flambard, the Bishop of Durham and a key member of the previous regime, was imprisoned in the Tower of London and charged with corruption. The late king had left many church positions unfilled, and Henry set about nominating candidates to these, in an effort to build further support for his new government. The appointments needed to be consecrated, and Henry wrote to Anselm, apologising for having been crowned while the Archbishop was still in France and asking him to return at once.\n\nOn 11 November 1100 Henry married Matilda, the daughter of Malcolm III of Scotland. Henry was now around 31 years old, but late marriages for noblemen were not unusual in the 11th century. The pair had probably first met earlier the previous decade, possibly being introduced through Bishop Osmund of Salisbury. Historian Warren Hollister argues that Henry and Matilda were emotionally close, but their union was also certainly politically motivated. Matilda had originally been named Edith, an Anglo-Saxon name, and was a member of the West Saxon royal family, being the niece of Edgar the Ætheling, the great-granddaughter of Edmund Ironside and a descendant of Alfred the Great. For Henry, marrying Matilda gave his reign increased legitimacy, and for Matilda, an ambitious woman, it was an opportunity for high status and power in England.\n\nMatilda had been educated in a sequence of convents, however, and may well have taken the vows to formally become a nun, which formed an obstacle to the marriage progressing. She did not wish to be a nun and appealed to Anselm for permission to marry Henry, and the Archbishop established a council at Lambeth Palace to judge the issue. Despite some dissenting voices, the council concluded that although Matilda had lived in a convent, she had not actually become a nun and was therefore free to marry, a judgement that Anselm then affirmed, allowing the marriage to proceed. Matilda proved an effective queen for Henry, acting as a regent in England on occasion, addressing and presiding over councils, and extensively supporting the arts. The couple soon had two children, Matilda, born in 1102, and William Adelin, born in 1103; it is possible that they also had a second son, Richard, who died young. Following the birth of these children, Matilda preferred to remain based in Westminster while Henry travelled across England and Normandy, either for religious reasons or because she enjoyed being involved in the machinery of royal governance.\n\nHenry had a considerable sexual appetite and enjoyed a substantial number of sexual partners, resulting in a large number of illegitimate children, at least nine sons and 13 daughters, many of whom he appears to have recognised and supported. It was normal for unmarried Anglo-Norman noblemen to have sexual relations with prostitutes and local women, and kings were also expected to have mistresses. Some of these relationships occurred before Henry was married, but many others took place after his marriage to Matilda. Henry had a wide range of mistresses from a range of backgrounds, and the relationships appear to have been conducted relatively openly. He may have chosen some of his noble mistresses for political purposes, but the evidence to support this theory is limited.\n\nBy early 1101, Henry's new regime was established and functioning, but many of the Anglo-Norman elite still supported Robert, or would be prepared to switch sides if Henry's elder brother appeared likely to gain power in England. In February, Flambard escaped from the Tower of London and crossed the Channel to Normandy, where he injected fresh direction and energy to Robert's attempts to mobilise an invasion force. By July, Robert had formed an army and a fleet, ready to move against Henry in England. Raising the stakes in the conflict, Henry seized Flambard's lands and, with the support of Anselm, Flambard was removed from his position as bishop. Henry held court in April and June, where the nobility renewed their oaths of allegiance to him, but their support still appeared partial and shaky.\n\nWith the invasion imminent, Henry mobilised his forces and fleet outside Pevensey, close to Robert's anticipated landing site, training some of them personally in how to counter cavalry charges. Despite English levies and knights owing military service to the Church arriving in considerable numbers, many of his barons did not appear. Anselm intervened with some of the doubters, emphasising the religious importance of their loyalty to Henry. Robert unexpectedly landed further up the coast at Portsmouth on 20 July with a modest force of a few hundred men, but these were quickly joined by many of the barons in England. However, instead of marching into nearby Winchester and seizing Henry's treasury, Robert paused, giving Henry time to march west and intercept the invasion force.\n\nThe two armies met at Alton where peace negotiations began, possibly initiated by either Henry or Robert, and probably supported by Flambard. The brothers then agreed to the Treaty of Alton, under which Robert released Henry from his oath of homage and recognised him as king; Henry renounced his claims on western Normandy, except for Domfront, and agreed to pay Robert £2,000 a year for life; if either brother died without a male heir, the other would inherit his lands; the barons whose lands had been seized by either the King or the Duke for supporting his rival would have them returned, and Flambard would be reinstated as bishop; the two brothers would campaign together to defend their territories in Normandy. Robert remained in England for a few months more with Henry before returning to Normandy.\n\nDespite the treaty, Henry set about inflicting severe penalties on the barons who had stood against him during the invasion. William de Warenne, the Earl of Surrey, was accused of fresh crimes, which were not covered by the Alton amnesty, and was banished from England. In 1102 Henry then turned against Robert of Bellême and his brothers, the most powerful of the barons, accusing him of 45 different offences. Robert escaped and took up arms against Henry. Henry besieged Robert's castles at Arundel, Tickhill and Shrewsbury, pushing down into the south-west to attack Bridgnorth. His power base in England broken, Robert accepted Henry's offer of banishment and left the country for Normandy.\n\nHenry's network of allies in Normandy became stronger during 1103. Henry married Juliana, one of his illegitimate daughters, to Eustace of Breteuil, and another illegitimate daughter, Matilda, to Rotrou, the Count of Perche, on the Normandy border. Henry attempted to win over other members of the Normandy nobility and gave other English estates and lucrative offers to key Norman lords. Duke Robert continued to fight Robert of Bellême, but the Duke's position worsened, until by 1104, he had to ally himself formally with Bellême to survive. Arguing that Duke Robert had broken the terms of their treaty, Henry crossed over the Channel to Domfront, where he met with senior barons from across Normandy, eager to ally themselves with the King. Henry confronted his brother and accused him of siding with his enemies, before returning to England.\n\nNormandy continued to disintegrate into chaos. In 1105, Henry sent his friend Robert Fitzhamon and a force of knights into the Duchy, apparently to provoke a confrontation with Duke Robert. Fitzhamon was captured, and Henry used this as an excuse to invade, promising to restore peace and order. Henry had the support of most of the neighbouring counts around Normandy's borders, and King Philip of France was persuaded to remain neutral. Henry occupied western Normandy, and advanced east on Bayeux, where Fitzhamon was held. The city refused to surrender, and Henry besieged it, burning it to the ground. Terrified of meeting the same fate, the town of Caen switched sides and surrendered, allowing Henry to advance on Falaise, which he took with some casualties. Henry's campaign stalled, and the King instead began peace discussions with Robert. The negotiations were inconclusive and the fighting dragged on until Christmas, when Henry returned to England.\n\nHenry invaded again in July 1106, hoping to provoke a decisive battle. After some initial tactical successes, he turned south-west towards the castle of Tinchebray. He besieged the castle and Duke Robert, supported by Robert of Bellême, advanced from Falaise to relieve it. After attempts at negotiation failed, the Battle of Tinchebray took place, probably on 28 September. The battle lasted around an hour, and began with a charge by Duke Robert's cavalry; the infantry and dismounted knights of both sides then joined the battle. Henry's reserves, led by Elias, the Count of Maine and Alan, the Duke of Brittany, attacked the enemy's flanks, routing first Bellême's troops and then the bulk of the ducal forces. Duke Robert was taken prisoner, but Bellême escaped.\n\nHenry mopped up the remaining resistance in Normandy, and Robert ordered his last garrisons to surrender. Reaching Rouen, Henry reaffirmed the laws and customs of Normandy and took homage from the leading barons and citizens. The lesser prisoners taken at Tinchebray were released, but Robert and several other leading nobles were imprisoned indefinitely. Henry's nephew, Robert's son William Clito, was only three years old and was released to the care of Helias of Saint-Saens, a Norman baron. Henry reconciled himself with Robert of Bellême, who gave up the ducal lands he had seized and rejoined the royal court. Henry had no way of legally removing the Duchy from his brother Robert, and initially Henry avoided using the title \"duke\" at all, emphasising that, as the King of England, he was only acting as the guardian of the troubled Duchy.\n\nHenry inherited the kingdom of England from William Rufus, giving him a claim of suzerainty over Wales and Scotland, and acquired the Duchy of Normandy, a complex entity with troubled borders. The borders between England and Scotland were still uncertain during Henry's reign, with Anglo-Norman influence pushing northwards through Cumbria, but Henry's relationship with King David I of Scotland was generally good, partially due to Henry's marriage to his sister. In Wales, Henry used his power to coerce and charm the indigenous Welsh princes, while Norman Marcher Lords pushed across the valleys of South Wales. Normandy was controlled via various interlocking networks of ducal, ecclesiastical and family contacts, backed by a growing string of important ducal castles along the borders. Alliances and relationships with neighbouring counties along the Norman border were particularly important to maintaining the stability of the Duchy.\n\nHenry ruled through the various barons and lords in England and Normandy, whom he manipulated skilfully for political effect. Political friendships, termed \"amicitia\" in Latin, were important during the 12th century, and Henry maintained a wide range of these, mediating between his friends in various factions across his realm when necessary, and rewarding those who were loyal to him. Henry also had a reputation for punishing those barons who stood against him, and he maintained an effective network of informers and spies who reported to him on events. Henry was a harsh, firm ruler, but not excessively so by the standards of the day. Over time, he increased the degree of his control over the barons, removing his enemies and bolstering his friends until the \"reconstructed baronage\", as historian Warren Hollister describes it, was predominantly loyal and dependent on the King.\n\nHenry's itinerant royal court comprised various parts. At the heart was Henry's domestic household, called the \"domus\"; a wider grouping was termed the \"familia regis\", and formal gatherings of the court were termed \"curia\". The \"domus\" was divided into several parts. The chapel, headed by the chancellor, looked after the royal documents, the chamber dealt with financial affairs and the master-marshal was responsible for travel and accommodation. The \"familia regis\" included Henry's mounted household troops, up to several hundred strong, who came from a wider range of social backgrounds, and could be deployed across England and Normandy as required. Initially Henry continued his father's practice of regular crown-wearing ceremonies at his \"curia\", but they became less frequent as the years passed. Henry's court was grand and ostentatious, financing the construction of large new buildings and castles with a range of precious gifts on display, including the King's private menagerie of exotic animals, which he kept at Woodstock Palace. Despite being a lively community, Henry's court was more tightly controlled than those of previous kings. Strict rules controlled personal behaviour and prohibited members of the court from pillaging neighbouring villages, as had been the norm under William Rufus.\n\nHenry was responsible for a substantial expansion of the royal justice system. In England, Henry drew on the existing Anglo-Saxon system of justice, local government and taxes, but strengthened it with additional central governmental institutions. Roger of Salisbury began to develop the royal exchequer after 1110, using it to collect and audit revenues from the King's sheriffs in the shires. Itinerant justices began to emerge under Henry, travelling around the country managing eyre courts, and many more laws were formally recorded. Henry gathered increasing revenue from the expansion of royal justice, both from fines and from fees. The first Pipe Roll that is known to have survived dates from 1130, recording royal expenditures. Henry reformed the coinage in 1107, 1108 and in 1125, inflicting harsh corporal punishments to English coiners who had been found guilty of debasing the currency. In Normandy, Henry restored law and order after 1106, operating through a body of Norman justices and an exchequer system similar to that in England. Norman institutions grew in scale and scope under Henry, although less quickly than in England. Many of the officials that ran Henry's system were termed \"new men\", relatively low-born individuals who rose through the ranks as administrators, managing justice or the royal revenues.\n\nHenry's ability to govern was intimately bound up with the Church, which formed the key to the administration of both England and Normandy, and this relationship changed considerably over the course of his reign. William the Conqueror had reformed the English Church with the support of his Archbishop of Canterbury, Lanfranc, who became a close colleague and advisor to the King. Under William Rufus this arrangement had collapsed, the King and Archbishop Anselm had become estranged and Anselm had gone into exile. Henry also believed in Church reform, but on taking power in England he became embroiled in the investiture controversy.\n\nThe argument concerned who should invest a new bishop with his staff and ring: traditionally, this had been carried out by the king in a symbolic demonstration of royal power, but Pope Urban II had condemned this practice in 1099, arguing that only the papacy could carry out this task, and declaring that the clergy should not give homage to their local temporal rulers. Anselm returned to England from exile in 1100 having heard Urban's pronouncement, and informed Henry that he would be complying with the Pope's wishes. Henry was in a difficult position. On one hand, the symbolism and homage was important to him; on the other hand, he needed Anselm's support in his struggle with his brother Duke Robert.\n\nAnselm stuck firmly to the letter of the papal decree, despite Henry's attempts to persuade him to give way in return for a vague assurance of a future royal compromise. Matters escalated, with Anselm going back into exile and Henry confiscating the revenues of his estates. Anselm threatened excommunication, and in July 1105 the two men finally negotiated a solution. A distinction was drawn between the secular and ecclesiastical powers of the prelates, under which Henry gave up his right to invest his clergy, but retained the custom of requiring them to come and do homage for the temporalities, the landed properties they held in England. Despite this argument, the pair worked closely together, combining to deal with Duke Robert's invasion of 1101, for example, and holding major reforming councils in 1102 and 1108.\n\nA long-running dispute between the Archbishops of Canterbury and York flared up under Anselm's successor, Ralph d'Escures. Canterbury, traditionally the senior of the two establishments, had long argued that the Archbishop of York should formally promise to obey their Archbishop, but York argued that the two episcopates were independent within the English Church and that no such promise was necessary. Henry supported the primacy of Canterbury, to ensure that England remained under a single ecclesiastical administration, but the Pope preferred the case of York. The matter was complicated by Henry's personal friendship with Thurstan, the Archbishop of York, and the King's desire that the case should not end up in a papal court, beyond royal control. Henry badly needed the support of the Papacy in his struggle with Louis of France, however, and therefore allowed Thurstan to attend the Council of Rheims in 1119, where Thurstan was then consecrated by the Pope with no mention of any duty towards Canterbury. Henry believed that this went against assurances Thurstan had previously made and exiled him from England until the King and Archbishop came to a negotiated solution the following year.\n\nEven after the investiture dispute, the King continued to play a major role in the selection of new English and Norman bishops and archbishops. Henry appointed many of his officials to bishoprics and, as historian Martin Brett suggests, \"some of his officers could look forward to a mitre with all but absolute confidence\". Henry's chancellors, and those of his queens, became bishops of Durham, Hereford, London, Lincoln, Winchester and Salisbury. Henry increasingly drew on a wider range of these bishops as advisors – particularly Roger of Salisbury – breaking with the earlier tradition of relying primarily on the Archbishop of Canterbury. The result was a cohesive body of administrators through which Henry could exercise careful influence, holding general councils to discuss key matters of policy. This stability shifted slightly after 1125, when Henry began to inject a wider range of candidates into the senior positions of the Church, often with more reformist views, and the impact of this generation would be felt in the years after Henry's death.\n\nLike other rulers of the period, Henry donated to the Church and patronised various religious communities, but contemporary chroniclers did not consider him an unusually pious king. His personal beliefs and piety may, however, have developed during the course of his life. Henry had always taken an interest in religion, but in his later years he may have become much more concerned about spiritual affairs. If so, the major shifts in his thinking would appear to have occurred after 1120, when his son William Adelin died, and 1129, when his daughter's marriage teetered on the verge of collapse.\n\nAs a proponent of religious reform, Henry gave extensively to reformist groups within the Church. He was a keen supporter of the Cluniac order, probably for intellectual reasons. He donated money to the abbey at Cluny itself, and after 1120 gave generously to Reading Abbey, a Cluniac establishment. Construction on Reading began in 1121, and Henry endowed it with rich lands and extensive privileges, making it a symbol of his dynastic lines. He also focused effort on promoting the conversion of communities of clerks into Augustinian canons, the foundation of leper hospitals, expanding the provision of nunneries, and the charismatic orders of the Savigniacs and Tironensians. He was an avid collector of relics, sending an embassy to Constantinople in 1118 to collect Byzantine items, some of which were donated to Reading Abbey.\n\nNormandy faced an increased threat from France, Anjou and Flanders after 1108. Louis VI succeeded to the French throne in 1108 and began to reassert central royal power. Louis demanded Henry give homage to him and that two disputed castles along the Normandy border be placed into the control of neutral castellans. Henry refused, and Louis responded by mobilising an army. After some arguments, the two kings negotiated a truce and retreated without fighting, leaving the underlying issues unresolved. Fulk V assumed power in Anjou in 1109 and began to rebuild Angevin authority. Fulk also inherited the county of Maine, but refused to recognise Henry as his feudal lord and instead allied himself with Louis. Robert II of Flanders also briefly joined the alliance, before his death in 1111.\n\nIn 1108, Henry betrothed his eight-year-old daughter, Matilda, to Henry V, the future Holy Roman Emperor. For King Henry, this was a prestigious match; for Henry V, it was an opportunity to restore his financial situation and fund an expedition to Italy, as he received a dowry of £6,666 from England and Normandy. Raising this money proved challenging, and required the implementation of a special \"aid\", or tax, in England. Matilda was crowned Henry V's queen in 1110.\n\nHenry responded to the French and Angevin threat by expanding his own network of supporters beyond the Norman borders. Some Norman barons deemed unreliable were arrested or dispossessed, and Henry used their forfeited estates to bribe his potential allies in the neighbouring territories, in particular Maine. Around 1110, Henry attempted to arrest the young William Clito, but William's mentors moved him to the safety of Flanders before he could be taken. At about this time, Henry probably began to style himself as the Duke of Normandy. Robert of Bellême turned against Henry once again, and when he appeared at Henry's court in 1112 in a new role as a French ambassador, he was arrested and imprisoned.\n\nRebellions broke out in France and Anjou between 1111 and 1113, and Henry crossed into Normandy to support his nephew, Count Theobald of Blois, who had sided against Louis in the uprising. In a bid to diplomatically isolate the French King, Henry betrothed his young son, William Adelin, to Fulk's daughter Matilda, and married his illegitimate daughter Matilda to Conan III, the Duke of Brittany, creating alliances with Anjou and Brittany respectively. Louis backed down and in March 1113 met with Henry near Gisors to agree a peace settlement, giving Henry the disputed fortresses and confirming Henry's overlordship of Maine, Bellême and Brittany.\n\nMeanwhile, the situation in Wales was deteriorating. Henry had conducted a campaign in South Wales in 1108, pushing out royal power in the region and colonising the area around Pembroke with Flemings. By 1114, some of the resident Norman lords were under attack, while in Mid-Wales, Owain ap Cadwgan blinded one of the political hostages he was holding, and in North Wales Gruffudd ap Cynan threatened the power of the Earl of Chester. Henry sent three armies into Wales that year, with Gilbert Fitz Richard leading a force from the south, Alexander, King of Scotland, pressing from the north and Henry himself advancing into Mid-Wales. Owain and Gruffudd sued for peace, and Henry accepted a political compromise. Henry reinforced the Welsh Marches with his own appointees, strengthening the border territories.\n\nConcerned about the succession, Henry sought to persuade Louis VI to accept his son, William Adelin, as the legitimate future Duke of Normandy, in exchange for his son's homage. Henry crossed into Normandy in 1115 and assembled the Norman barons to swear loyalty; he also almost successfully negotiated a settlement with King Louis, affirming William's right to the Duchy in exchange for a large sum of money, but the deal fell through and Louis, backed by his ally Baldwin of Flanders, instead declared that he considered William Clito the legitimate heir to the Duchy.\n\nWar broke out after Henry returned to Normandy with an army to support Theobald of Blois, who was under attack from Louis. Henry and Louis raided each other's towns along the border, and a wider conflict then broke out, probably in 1116. Henry was pushed onto the defensive as French, Flemish and Angevin forces began to pillage the Normandy countryside. Amaury III of Montfort and many other barons rose up against Henry, and there was an assassination plot from within his own household. Henry's wife, Matilda, died in early 1118, but the situation in Normandy was sufficiently pressing that Henry was unable to return to England for her funeral.\n\nHenry responded by mounting campaigns against the rebel barons and deepening his alliance with Theobald. Baldwin of Flanders was wounded in battle and died in September 1118, easing the pressure on Normandy from the north-east. Henry attempted to crush a revolt in the city of Alençon, but was defeated by Fulk and the Angevin army. Forced to retreat from Alençon, Henry's position deteriorated alarmingly, as his resources became overstretched and more barons abandoned his cause. Early in 1119, Eustace of Breteuil and Henry's daughter, Juliana, threatened to join the baronial revolt. Hostages were exchanged in a bid to avoid conflict, but relations broke down and both sides mutilated their captives. Henry attacked and took the town of Breteuil, despite Juliana's attempt to kill her father with a crossbow. In the aftermath, Henry dispossessed the couple of almost all of their lands in Normandy.\n\nHenry's situation improved in May 1119 when he enticed Fulk to switch sides by finally agreeing to marry William Adelin to Fulk's daughter, Matilda, and paying Fulk a large sum of money. Fulk left for the Levant, leaving the County of Maine in Henry's care, and the King was free to focus on crushing his remaining enemies. During the summer Henry advanced into the Norman Vexin, where he encountered Louis's army, resulting in the Battle of Brémule. Henry appears to have deployed scouts and then organised his troops into several carefully formed lines of dismounted knights. Unlike Henry's forces, the French knights remained mounted; they hastily charged the Anglo-Norman positions, breaking through the first rank of the defences but then becoming entangled in Henry's second line of knights. Surrounded, the French army began to collapse. In the melee, Henry was hit by a sword blow, but his armour protected him. Louis and William Clito escaped from the battle, leaving Henry to return to Rouen in triumph.\n\nThe war slowly petered out after this battle, and Louis took the dispute over Normandy to Pope Callixtus II's council in Reims that October. Henry faced a number of French complaints concerning his acquisition and subsequent management of Normandy, and despite being defended by Geoffrey, the Archbishop of Rouen, Henry's case was shouted down by the pro-French elements of the council. Callixtus declined to support Louis, however, and merely advised the two rulers to seek peace. Amaury de Montfort came to terms with Henry, but Henry and William Clito failed to find a mutually satisfactory compromise. In June 1120, Henry and Louis formally made peace on terms advantageous to the English King: William Adelin gave homage to Louis, and in return Louis confirmed William's rights to the Duchy.\n\nHenry's succession plans were thrown into chaos by the sinking of the \"White Ship\" on 25 November 1120. Henry had left the port of Barfleur for England in the early evening, leaving William Adelin and many of the younger members of the court to follow on that night in a separate vessel, the \"White Ship\". Both the crew and passengers were drunk and, just outside the harbour, the ship hit a submerged rock. The ship sank, killing as many as 300 people, with only one survivor, a butcher from Rouen. Henry's court was initially too scared to report William's death to the King. When he was finally told, he collapsed with grief.\n\nThe disaster left Henry with no legitimate son, his various nephews now the closest male heirs. Henry announced he would take a new wife, Adeliza of Louvain, opening up the prospect of a new royal son, and the two were married at Windsor Castle in January 1121. Henry appears to have chosen her because she was attractive and came from a prestigious noble line. Adela seems to have been fond of Henry and joined him in his travels, probably to maximise the chances of her conceiving a child. The \"White Ship\" disaster initiated fresh conflict in Wales, where the drowning of Richard, Earl of Chester, encouraged a rebellion led by Maredudd ap Bleddyn. Henry intervened in North Wales that summer with an army and, although the King was hit by a Welsh arrow, the campaign reaffirmed royal power across the region.\n\nWith William dead, Henry's alliance with Anjou – which had been based on his son marrying Fulk's daughter – began to disintegrate. Fulk returned from the Levant and demanded that Henry return Matilda and her dowry, a range of estates and fortifications in Maine. Matilda left for Anjou, but Henry argued that the dowry had in fact originally belonged to him before it came into the possession of Fulk, and so declined to hand the estates back to Anjou. Fulk married his daughter Sibylla to William Clito, and granted them Maine. Once again, conflict broke out, as Amaury de Montfort allied himself with Fulk and led a revolt along the Norman-Anjou border in 1123. Amaury was joined by several other Norman barons, headed by Waleran de Beaumont, one of the sons of Henry's old ally, Robert of Meulan.\n\nHenry dispatched Robert of Gloucester and Ranulf le Meschin to Normandy and then intervened himself in late 1123. Henry began the process of besieging the rebel castles, before wintering in the Duchy. In the spring, campaigning began again. Ranulf received intelligence that the rebels were returning to one of their bases at Vatteville, allowing him to ambush them en route at Rougemontiers; Waleran charged the royal forces, but his knights were cut down by Ranulf's archers and the rebels were quickly overwhelmed. Waleran was captured, but Amaury escaped. Henry mopped up the remainder of the rebellion, blinding some of the rebel leaders – considered, at the time, a more merciful punishment than execution – and recovering the last rebel castles. Henry paid Pope Callixtus a large amount of money, in exchange for the Papacy annulling the marriage of William Clito and Sibylla on the grounds of consanguinity.\n\nHenry and his new wife did not conceive any children, generating prurient speculation as to the possible explanation, and the future of the dynasty appeared at risk. Henry may have begun to look among his nephews for a possible heir. He may have considered Stephen of Blois as a possible option and, perhaps in preparation for this, he arranged a beneficial marriage for Stephen to a wealthy heiress, Matilda. Theobald of Blois, his close ally, may have also felt that he was in favour with Henry. William Clito, who was King Louis's preferred choice, remained opposed to Henry and was therefore unsuitable. Henry may have also considered his own illegitimate son, Robert of Gloucester, as a possible candidate, but English tradition and custom would have looked unfavourably on this.\n\nHenry's plans shifted when the Empress Matilda's husband, the Emperor Henry, died in 1125. King Henry recalled his daughter to England the next year and declared that, should he die without a male heir, she was to be his rightful successor. The Anglo-Norman barons were gathered together at Westminster on Christmas 1126, where they swore to recognise Matilda and any future legitimate heir she might have. Putting forward a woman as a potential heir in this way was unusual: opposition to Matilda continued to exist within the English court, and Louis was vehemently opposed to her candidacy.\n\nFresh conflict broke out in 1127, when Charles, the childless Count of Flanders, was murdered, creating a local succession crisis. Backed by King Louis, William Clito was chosen by the Flemings to become their new ruler. This development potentially threatened Normandy, and Henry began to finance a proxy war in Flanders, promoting the claims of William's Flemish rivals. In an effort to disrupt the French alliance with William, Henry mounted an attack into France in 1128, forcing Louis to cut his aid to William. William died unexpectedly in July, removing the last major challenger to Henry's rule and bringing the war in Flanders to a halt. Without William, the baronial opposition in Normandy lacked a leader. A fresh peace was made with France, and the King was finally able to release the remaining prisoners from the revolt of 1123, including Waleran of Meulan, who was rehabilitated into the royal court.\n\nMeanwhile, Henry rebuilt his alliance with Fulk of Anjou, this time by marrying Matilda to Fulk's eldest son, Geoffrey. The pair were betrothed in 1127 and married the following year. It is unknown whether Henry intended Geoffrey to have any future claim on England or Normandy, and he was probably keeping his son-in-law's status deliberately uncertain. Similarly, although Matilda was granted a number of Normandy castles as part of her dowry, it was not specified when the couple would actually take possession of them. Fulk left Anjou for Jerusalem in 1129, declaring Geoffrey the Count of Anjou and Maine. The marriage proved difficult, as the couple did not particularly like each other and the disputed castles proved a point of contention, resulting in Matilda returning to Normandy later that year. Henry appears to have blamed Geoffrey for the separation, but in 1131 the couple were reconciled. Much to the pleasure and relief of Henry, Matilda then gave birth to a sequence of two sons, Henry and Geoffrey, in 1133 and 1134.\n\nRelations between Henry, Matilda, and Geoffrey became increasingly strained during the King's final years. Matilda and Geoffrey suspected that they lacked genuine support in England. In 1135 they urged Henry to hand over the royal castles in Normandy to Matilda whilst he was still alive, and insisted that the Norman nobility swear immediate allegiance to her, thereby giving the couple a more powerful position after Henry's death. Henry angrily declined to do so, probably out of concern that Geoffrey would try to seize power in Normandy. A fresh rebellion broke out amongst the barons in southern Normandy, led by William, the Count of Ponthieu, whereupon Geoffrey and Matilda intervened in support of the rebels.\n\nHenry campaigned throughout the autumn, strengthening the southern frontier, and then travelled to Lyons-la-Forêt in November to enjoy some hunting, still apparently healthy. There Henry fell ill – according to the chronicler Henry of Huntingdon, he ate a number of lampreys against his physician's advice – and his condition worsened over the course of a week. Once the condition appeared terminal, Henry gave confession and summoned Archbishop Hugh of Amiens, who was joined by Robert of Gloucester and other members of the court. In accordance with custom, preparations were made to settle Henry's outstanding debts and to revoke outstanding sentences of forfeiture. The King died on 1 December 1135, and his corpse was taken to Rouen accompanied by the barons, where it was embalmed; his entrails were buried locally at Port-du-Salut Abbey, and the preserved body was taken on to England, where it was interred at Reading Abbey.\n\nDespite Henry's efforts, the succession was disputed. When news began to spread of the King's death, Geoffrey and Matilda were in Anjou supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late king was properly buried, which prevented them from returning to England. The Norman nobility discussed declaring Theobald of Blois king. Theobald's younger brother, Stephen of Blois, quickly crossed from Boulogne to England, however, accompanied by his military household. With the help of his brother, Henry of Blois, he seized power in England and was crowned king on 22 December. The Empress Matilda did not give up her claim to England and Normandy, leading to the prolonged civil war known as the Anarchy between 1135 and 1153.\n\nHistorians have drawn on a range of sources on Henry, including the accounts of chroniclers; other documentary evidence, including early pipe rolls; and surviving buildings and architecture. The three main chroniclers to describe the events of Henry's life were William of Malmesbury, Orderic Vitalis, and Henry of Huntingdon, but each incorporated extensive social and moral commentary into their accounts and borrowed a range of literary devices and stereotypical events from other popular works. Other chroniclers include Eadmer, Hugh the Chanter, Abbot Suger, and the authors of the Welsh \"Brut\". Not all royal documents from the period have survived, but there are a number of royal acts, charters, writs, and letters, along with some early financial records. Some of these have since been discovered to be forgeries, and others had been subsequently amended or tampered with.\n\nLate medieval historians seized on the accounts of selected chroniclers regarding Henry's education and gave him the title of Henry \"Beauclerc\", a theme echoed in the analysis of Victorian and Edwardian historians such as Francis Palgrave and Henry Davis. The historian Charles David dismissed this argument in 1929, showing the more extreme claims for Henry's education to be without foundation. Modern histories of Henry commenced with Richard Southern's work in the early 1960s, followed by extensive research during the rest of the 20th century into a wide number of themes from his reign in England, and a much more limited number of studies of his rule in Normandy. Only two major, modern biographies of Henry have been produced, Warren Hollister's posthumous volume in 2001, and Judith Green's 2006 work.\n\nInterpretation of Henry's personality by historians has altered over time. Earlier historians such as Austin Poole and Richard Southern considered Henry as a cruel, draconian ruler. More recent historians, such as Hollister and Green, view his implementation of justice much more sympathetically, particularly when set against the standards of the day, but even Green has noted that Henry was \"in many respects highly unpleasant\", and Alan Cooper has observed that many contemporary chroniclers were probably too scared of the King to voice much criticism. Historians have also debated the extent to which Henry's administrative reforms genuinely constituted an introduction of what Hollister and John Baldwin have termed systematic, \"administrative kingship\", or whether his outlook remained fundamentally traditional.\n\nHenry's burial at Reading Abbey is marked by a local cross, but Reading Abbey was slowly demolished during the Dissolution of the Monasteries in the 16th century. The exact location is uncertain, but the most likely location of the tomb itself is now in a built-up area of central Reading, on the site of the former abbey choir. A plan to locate his remains was announced in March 2015, with support from English Heritage and Philippa Langley, who aided with the successful exhumation of Richard III.\n\nHenry and his first wife, Matilda, had at least two legitimate children:\n\n\nHenry and his second wife, Adeliza, had no children.\n\nHenry had a number of illegitimate children by various mistresses.\n\n\n\n", "id": "14179", "title": "Henry I of England"}
{"url": "https://en.wikipedia.org/wiki?curid=14183", "text": "Hentai\n\nIn Japanese, the term describes any type of perverse or bizarre sexual desire or act; it does not represent a genre of work. Internationally, hentai is a catch-all term to describe a genre of anime and manga pornography. English adopts and uses hentai as a genre of pornography by the commercial sale and marketing of explicit works under this label.\n\nThe word's narrow Japanese-language usage and broad international usage are often incompatible. \"Weather Report Girl\" is considered yuri hentai in English usage for its depiction of lesbian sex, but in Japan it is just yuri. The definition clash also appears with the Japanese definition of yuri as any lesbian relationship, as opposed to its sexually explicit definition in English usage.\n\n \"\" is a kanji compound of 変 (\"hen\"; \"change\", \"weird\", or \"strange\") and 態 (\"tai\"; \"appearance\" or \"condition\"). It also means \"perversion\" or \"abnormality\", especially when used as an adjective. It is the shortened form of the phrase which means \"sexual perversion\". The character \"hen\" is catch-all for queerness as a peculiarity—it does not carry an explicit sexual reference. While the term has expanded in use to cover a range of publications including homosexual publications, it remains primarily a heterosexual term, as terms indicating homosexuality entered Japan as foreign words. Japanese pornographic works are often simply tagged as , meaning \"prohibited to those not yet 18 years old\", and . Less official terms also in use include , , and the English initialism AV (for \"adult video\"). Usage of the term hentai does not define a genre in Japan.\n\nHentai is defined differently in English. The \"Oxford Dictionary Online\" defines hentai as \"a subgenre of the Japanese genres of manga and anime, characterized by overtly sexualized characters and sexually explicit images and plots.\" The origin of the word in English is unknown, but AnimeNation's John Oppliger points to the early 1990s, when a \"Dirty Pair\" erotic \"doujinshi\" (self-published work) titled \"H-Bomb\" was released, and when many websites sold access to images culled from Japanese erotic visual novels and games. The earliest English use of the term traces back to the rec.arts.anime boards; with a 1990 post concerning Happosai of \"Ranma ½\" and the first discussion of the meaning in 1991. A 1995 Glossary on the rec.arts.anime boards contained reference to the Japanese usage and the evolving definition of hentai as \"pervert\" or \"perverted sex\". \"The Anime Movie Guide\", published in 1997, defines as the initial sound of hentai (i.e., the name of the letter \"H\", as pronounced in Japanese); it included that ecchi was \"milder than hentai\". A year later it was defined as a genre in \"Good Vibrations Guide to Sex\". At the beginning of 2000, \"hentai\" was listed as the 41st most popular search term of the internet, while \"anime\" ranked 99th. The attribution has been applied retroactively to works such as \"Urotsukidōji\", \"La Blue Girl\", and \"Cool Devices\". \"Urotsukidōji\" had previously been described with terms such as \"Japornimation\", and \"erotic grotesque\", prior to being identified as hentai.\n\nThe history of word \"hentai\" has its origins in science and psychology. By the middle of the Meiji era, the term appeared in publications to describe unusual or abnormal traits, including paranormal abilities and psychological disorders. A translation of German sexologist Richard von Krafft-Ebing's text \"Psychopathia Sexualis\" originated the concept of \"hentai seiyoku\", as a \"perverse or abnormal sexual desire\". Though it was popularized outside psychology, as in the case of Mori Ōgai's 1909 novel \"Vita Sexualis\". Continued interest in \"hentai seiyoku\", resulted in numerous journals and publications on sexual advice which circulated in the public, served to establish the sexual connotation of 'hentai' as perverse. Any perverse or abnormal act could be hentai, such as committing \"shinjū\" (love suicide). It was Nakamura Kokyo's journal \"Abnormal Psychology\" which started the popular sexology boom in Japan which would see the rise of other popular journals like \"Sexuality and Human Nature\", \"Sex Research\" and \"Sex\". Originally, Tanaka Kogai wrote articles for \"Abnormal Psychology\", but it would be Tanaka's own journal \"Modern Sexuality\" which would become one of the most popular sources of scholarly information about erotic and neurotic expression. \"Modern Sexuality\" was created to promote fetishism, S&M, and necrophilia as a facet of modern life. The ero-guro movement and depiction of perverse, abnormal and often erotic undertones were a response to interest in \"hentai seiyoku\".\n\nFollowing the end of World War II, Japan took a new interest in sexualization and public sexuality. Mark McLelland puts forth the observation that the term \"hentai\" found itself shortened to \"H\" and that the English pronunciation was \"etchi\", referring to lewdness and which did not carry the stronger connotation of abnormality or perversion. By the 1950s, the \"hentai seiyoku\" publications became their own genre and included fetish and homosexual topics. By the 1960s, the homosexual content was dropped in favor of subjects like sadomasochism and stories of lesbianism targeted to male readers. The late 1960s brought a sexual revolution which expanded and solidified the normalizing the terms identity in Japan that continues to exist today through publications such as Bessatsu Takarajima's \"Hentai-san ga iku\" series.\n\nWith the usage of hentai as any erotic depiction, the history of these depictions are split into its media. Japanese artwork and comics serve as the first example of hentai material, coming to represent the iconic style after the publication of Azuma Hideo's \"Cybele\" in 1979. Japanese animation (anime) had its first hentai, in both definitions, with the 1984 release of Wonderkid's \"Lolita Anime\", overlooking the erotic and sexual depictions in 1969's \"One Thousand and One Arabian Nights\" and the bare breasted Cleopatra in 1970's \"Cleopatra\" film. Erotic games, another area of contention, has the iconic art style first depicted in sexual acts in 1985's \"Tenshitachi no Gogo\". The history of each medium itself, complicated based on the broad definition and usage.\n\nDepictions of sex and abnormal sex can be traced back through the ages, predating the term \"hentai\". , a Japanese term for erotic art, is thought to have and existed in some form since Heian period. From the 16th to the 19th century, Shunga works were suppressed by shoguns. A well-known example is \"The Dream of the Fisherman's Wife\" which depicts a woman being pleasured by two octopi. Shunga production fell with the rise of pornographic photographs in the late 19th century.\n\nTo define erotic manga, a definition for manga is needed. While the \"Hokusai Manga\" uses the term \"manga\" in its title, it does not depict the story-telling aspect common to modern manga, as the images are unrelated. Due to the influence of pornographic photographs in the 19th and 20th centuries, the manga artwork was depicted by realistic characters. However, Osamu Tezuka has helped define the modern look and form of manga, and was later proclaimed as the \"God of Manga\". His debut work \"New Treasure Island\" was released in 1947 as a comic book through Ikuei Publishing and sold over 400,000 copies, though it was the popularity of Tezuka's \"Astro Boy\", \"Metropolis\", and \"Jungle Emperor\" manga that would come to define the media. This story-driven manga style is distinctly unique from comic strips like \"Sazae-san\", and story-driven works are now dominating shoujo and shonen magazines.\n\nMature themes in manga have existed since the 1940s, but these depictions were more realistic than the cartoon-cute characters popularized by Tezuka. Early well-known \"ero-gekiga\" releases were \"Ero Mangatropa\" (1973), \"Erogenica\" (1975), and \"Alice\" (1977). The distinct shift in the style of Japanese pornographic comics from realistic to cartoon-cute characters is accredited to Azuma Hideo, \"The Father of Lolicon\". In 1979, he penned \"Cybele\" which offered the first commentary on unrealistic depictions of sexual acts between Tezuka-style characters. This would start a pornographic manga movement. The lolicon boom of the 1980s saw the rise of magazines such as the \"Lemon People\" and \"Petit Apple Pie\" anthologies.\n\nThe publication of erotic materials in America can be traced back to at least 1990, when IANVS Publications printed its first \"Anime Shower Special\". In March 1994, Antarctic Press released \"Bondage Fairies\", an English translation of \"Insect Hunter\".\n\nBecause there are fewer animation productions, most erotic works are retroactively tagged as hentai since the coining of the term in English. Hentai is typically defined as consisting of excessive nudity, and graphic sexual intercourse whether or not it is perverse. The term \"ecchi\" is typically related to fanservice, with no sexual intercourse being depicted.\n\nTwo early works escape being defined as hentai, but contain erotic themes. This is likely due to the obscurity and unfamiliarity of the works, arriving in America and fading from public focus a full twenty years before importation and surging interests coined the Americanized term of hentai. The first is the 1969 film \"One Thousand and One Arabian Nights\" which faithfully includes erotic elements of the original story. In 1970, \"\", was the first animated film to carry an X rating, but it was mislabeled as erotica in America.\n\nThe term typically identifies the \"Lolita Anime\" series as the first erotic anime and original video animation (OVA); it was released in 1984 by Wonder Kids. Containing 8 episodes, the series focused on underage sex and rape and included one episode containing BDSM bondage. Several sub-series were released in response, including a second \"Lolita Anime\" series released by Nikkatsu. It has not been officially licensed or distributed outside of its original release.\n\nThe \"Cream Lemon\" franchise of works ran from 1984 to 2005, with a number of them entering the American market in various forms. \"The Brothers Grime\" series released by Excalibur Films contained \"Cream Lemon\" works as early as 1986. However, they were not billed as anime and were introduced during the same time that the first underground distribution of erotic works began.\n\nThe American release of licensed erotic anime was first attempted in 1991 by Central Park Media, with \"I Give My All\", but it never occurred. In December 1992, Devil Hunter Yohko was the first risque (ecchi) title was released by A.D. Vision. While it contains no sexual intercourse it pushes the limits of the ecchi category with sexual dialogue, nudity and one scene in which the heroine is about to be raped.\n\nIt was Central Park Media's 1993 release of \"Urotsukidoji\" which brought the first hentai film to American viewers. Often cited for creating the hentai and tentacle rape genres, it contains extreme depictions of violence and monster sex. It is notable for being the first depiction of tentacle sex on screen. When the movie premiered in America it was described as being \"drenched in graphic scenes of perverse sex and ultra-violence\".\n\nFollowing this release, a wealth of pornographic content began to arrive in America, with companies such as A.D. Vision, Central Park Media and Media Blasters releasing licensed titles under various labels. A.D. Vision's label SoftCel Pictures released 19 titles in 1995 alone. Another label, Critical Mass, was created in 1996 to release an unedited edition of \"Violence Jack\". When A.D. Vision's hentai label SoftCel Pictures shut down in 2005, most of its titles were acquired by Critical Mass. Following the bankruptcy of Central Park Media in 2009, the licenses for all Anime 18-related products and movies were transferred to Critical Mass.\n\nThe term eroge (erotic game) literally defines any erotic game, but has become synonymous with video games depicting the artistic styles of anime and manga. The origins of eroge began in the early 1980s, while the computer industry in Japan was struggling to define a computer standard with makers like NEC, Sharp, and Fujitsu competing against one another. The PC98 series, despite lacking in processing power, CD drives and limited graphics, came to dominate the market, with the popularity of eroge games contributing to their success.\n\nDue to the vague definitions of any erotic game, depending on its classification, citing the first erotic game is a subjective one. If the definition applies to adult themes, the first game was \"Softporn Adventure\". Released in America in 1981 for the Apple II, this was a text-based comedic game from On-Line Systems. If eroge is defined as the first graphical depictions and/or Japanese adult themes, it would be Koei's 1982 release of \"Night Life\". Sexual intercourse is depicted through simple graphic outlines. Notably, \"Night Life\" was not intended to be erotic so much as an instructional guide \"to support married life\". A series of \"undressing\" games appeared as early as 1983, such as \"Strip Mahjong\". The first anime-styled erotic game was Tenshitachi no Gogo, released in 1985 by JAST. In 1988, ASCII released the first erotic role-playing game, \"Chaos Angel\". In 1989, AliceSoft released the turn-based RPG \"Rance\" and ELF released \"Dragon Knight\".\n\nIn the late 1980s, eroge began to stagnate under high prices and the majority of games containing uninteresting plots and mindless sex. ELF's 1992 release of \"Dokyusei\" came as customer frustration with eroge was mounting and spawned a new genre of games called dating sims. \"Dokyusei\" was unique because it had no defined plot and required the player to build a relationship with different girls in order to advance the story. Each girl had her own story, but the prospect of consummating a relationship required the girl growing to love the player; there was no easy sex.\n\nThe term \"visual novel\" is vague, with Japanese and English definitions classifying the genre as a type of interactive fiction game driven by narration and limited player interaction. While the term is often retroactively applied to many games, it was Leaf that coined the term with their \"Leaf Visual Novel Series\" (LVNS) with the 1996 release of \"Shizuku\" and \"Kizuato\". The success of these two dark eroge games would be followed by the third and final installment of the LVNS, the 1997 romantic eroge \"To Heart\". Eroge visual novels took a new emotional turn with Tactics' 1998 release \"\". Key's 1999 release of \"Kanon\" proved to be a major success and would go on to have numerous console ports, two manga series and two anime series.\n\nJapanese laws have impacted depictions of works since the Meiji Restoration, but these predate the common definition of hentai material. Since becoming law in 1907, Article 175 of the Criminal Code of Japan forbids the publication of obscene materials. Specifically, depictions of male-female sexual intercourse and pubic hair are considered obscene, but bare genitalia is not. As censorship is required for published works, the most common representations are the blurring dots on pornographic videos and \"bars\" or \"lights\" on still images. In 1986, Toshio Maeda sought to get past censorship on depictions of sexual intercourse, by creating tentacle sex. This led to the large number of works containing sexual intercourse with monsters, demons, robots, and aliens, whose genitals look different from men's. While western views attribute hentai to any explicit work, it was the products of this censorship which became not only the first titles legally imported to America and Europe, but the first successful ones. While uncut for American release, the United Kingdom's release of \"Urotsukidoji\" removed many scenes of the violence and tentacle rape scenes.\n\nIt was also because of this law that the artists began to depict the characters with a minimum of anatomical details and without pubic hair, by law, prior to 1991. Part of the ban was lifted when Nagisa Oshima prevailed over the obscenity charges at his trial for his film \"In the Realm of the Senses\". Though not enforced, the lifting of this ban did not apply to anime and manga as they were not deemed artistic exceptions.\n\nHowever, alterations of material or censorship and even banning of works are common. The U.S. release of the \"La Blue Girl\" altered the age of the heroine from 16 to 18 and removed sex scenes with a dwarf ninja named Nin-nin, and removed the Japanese censoring blurring dots. \"La Blue Girl\" was outright rejected by UK censors who refused to classify it and prohibited its distribution. In 2011 the Liberal Democratic Party of Japan sought a ban on the subgenre lolicon.\n\nThe most prolific consumers of hentai are men. Eroge games in particular combine three favored media, cartoons, pornography and gaming, into an experience. The hentai genre engages a wide audience that expands yearly, and desires better quality and storylines, or works which push the creative envelope. The unusual and extreme depictions in hentai are not about perversion so much as they are an example of the profit-oriented industry. Anime depicting normal sexual situations enjoy less market success than those that break social norms, such as sex at schools or bondage.\n\nAccording to Dr. Megha Hazuria Gorem, a clinical psychologist, \"Because toons are a kind of final fantasy, you can make the person look the way you want him or her to look. Every fetish can be fulfilled.\" Dr. Narayan Reddy, a sexologist, commented on the eroge games, \"Animators make new games because there is a demand for them, and because they depict things that the gamers do not have the courage to do in real life, or that might just be illegal, these games are an outlet for suppressed desire.\"\n\nThe hentai genre can be divided into numerous subgenres, the broadest of which encompasses heterosexual and homosexual acts. Hentai that features mainly heterosexual interactions occur in both male-targeted (\"ero\") and female-targeted (\"ladies' comics\") form. Those that feature mainly homosexual interactions are known as yaoi (male-male) and yuri (female-female). Both yaoi and, to a lesser extent, yuri, are generally aimed at members of the opposite sex from the persons depicted. While yaoi and yuri are not always explicit, their pornographic history and association remain. Yaoi's pornographic usage has remained strong in textual form through fanfiction. The definition of yuri has begun to be replaced by the broader definitions of \"lesbian-themed animation or comics\".\n\nHentai is perceived as \"dwelling\" on sexual fetishes. These include dozens of fetish and paraphilia related subgenres, which can be further classified with additional terms, such as heterosexual or homosexual types.\n\nMany works are focused on depicting the mundane and the impossible across every conceivable act and situation no matter how fantastical. The largest subgenre of hentai is \"futanari\" (hermaphroditism), which most often features a female with a penis or penis-like appendage in place of, or in addition to normal female genitals. Futanari characters are primarily depicted as having sex with other women and will almost always be submissive with a male; exceptions include Yonekura Kengo's work, which features female empowerment and domination over males.\n\n\n", "id": "14183", "title": "Hentai"}
{"url": "https://en.wikipedia.org/wiki?curid=14186", "text": "Henry VII of England\n\nHenry VII (; 28 January 1457 – 21 April 1509) was King of England from seizing the crown on 22 August 1485 until his death on 21 April 1509, and the first monarch of the House of Tudor. He ruled the Principality of Wales until 29 November 1489 and was Lord of Ireland. \n\nHenry won the throne when his forces defeated King Richard III at the Battle of Bosworth Field, the culmination of the Wars of the Roses. Henry was the last king of England to win his throne on the field of battle. He cemented his claim by marrying Elizabeth of York, daughter of Edward IV and niece of Richard III. Henry was successful in restoring the power and stability of the English monarchy after the civil war, and after a reign of nearly 24 years, he was peacefully succeeded by his son, Henry VIII.\n\nHenry can also be credited with a number of commendable administrative, economic and diplomatic initiatives, though the latter part of his reign was characterised by financial greed stretching the bounds of legality.\nPerhaps most impactful to posterity was his establishment of the Pound Avoirdupois as a weights and measures standard—with several adjustments this became part of the Imperial System and today's units. \nHis supportive stance of the islands' wool industry and stand-off with the Low Countries had long lasting benefits to all the British Isles economy. However, the capriciousness and lack of due process that indebted many would tarnish his legacy and fortunately, were soon ended upon Henry VII's death, after a commission revealed widespread abuses. According to the contemporary historian Polydore Vergil, simple \"greed\" underscored the means by which royal control was over-asserted in Henry's final years.\n\nHenry VII was born at Pembroke Castle on 28 January 1457 to Margaret Beaufort, Countess of Richmond. His father, Edmund Tudor, 1st Earl of Richmond, died three months before his birth.\n\nHenry's paternal grandfather, Owen Tudor, originally from the Tudors of Penmynydd, Isle of Anglesey in Wales, had been a page in the court of Henry V. He rose to become one of the \"Squires to the Body to the King\" after military service at the Battle of Agincourt. Owen is said to have secretly married the widow of Henry V, Catherine of Valois. One of their sons was Edmund Tudor, father of Henry VII. Edmund was created Earl of Richmond in 1452, and \"formally declared legitimate by Parliament\".\n\nHenry's main claim to the English throne derived from his mother through the House of Beaufort. Henry's mother, Lady Margaret Beaufort, was a great-granddaughter of John of Gaunt, Duke of Lancaster, fourth son of Edward III, and his third wife Katherine Swynford. Katherine was Gaunt's mistress for about 25 years; when they married in 1396, they already had four children, including Henry's great-grandfather John Beaufort. Thus Henry's claim was somewhat tenuous: it was from a woman, and by illegitimate descent. In theory, the Portuguese and Castilian royal families had a better claim (as far as \"legitimacy\" is concerned) as descendants of Catherine of Lancaster, the daughter of John of Gaunt and his second wife Constance of Castile.\n\nGaunt's nephew Richard II legitimised Gaunt's children by Katherine Swynford by Letters Patent in 1397. In 1407, Henry IV, who was Gaunt's son by his first wife, issued new Letters Patent confirming the legitimacy of his half-siblings, but also declaring them ineligible for the throne. Henry IV's action was of doubtful legality, as the Beauforts were previously legitimised by an Act of Parliament, but it further weakened Henry's claim.\n\nNonetheless, by 1483 Henry was the senior male Lancastrian claimant remaining, after the deaths in battle or by murder or execution of Henry VI, his son Edward of Westminster, Prince of Wales, and the other Beaufort line of descent through Lady Margaret's uncle, the 2nd Duke of Somerset.\n\nHenry also made some political capital out of his Welsh ancestry, for example in attracting military support and safeguarding his army's passage through Wales on its way to the Battle of Bosworth. He came from an old, established Anglesey family that claimed descent from Cadwaladr (in legend, the last ancient British king), and on occasion Henry displayed the red dragon of Cadwaladr. He took it, as well as the standard of St George, on his procession through London after the victory at Bosworth. A contemporary writer and Henry's biographer, Bernard André, also made much of Henry's Welsh descent.\n\nIn reality, however, his hereditary connections to Welsh aristocracy were not strong. He was descended by the paternal line, through several generations, from Ednyfed Fychan, the seneschal (steward) of Gwynedd and through this seneschal's wife from Rhys ap Tewdwr, the King of Deheubarth in South Wales. His more immediate ancestor, Tudur ap Goronwy, had aristocratic land rights, but his sons, who were first cousins to Owain Glyndŵr, sided with Owain in his revolt. One son was executed and the family land was forfeited. Another son, Henry's great-grandfather, became a butler to the Bishop of Bangor. Owen Tudor, the son of the butler, like the children of other rebels, was provided for by Henry V, a circumstance that precipitated his access to Queen Catherine of Valois.\n\nNotwithstanding this lineage, to the bards of Wales, Henry was a candidate for Y Mab Darogan – \"The Son of Prophecy\" who would free the Welsh from oppression.\n\nIn 1456, Henry's father Edmund Tudor was captured while fighting for Henry VI in South Wales against the Yorkists. He died in Carmarthen Castle, three months before Henry was born. Henry's uncle Jasper Tudor, the Earl of Pembroke and Edmund's younger brother, undertook to protect the young widow, who was 13 years old when she gave birth to Henry. When Edward IV became King in 1461, Jasper Tudor went into exile abroad. Pembroke Castle, and later the Earldom of Pembroke, were granted to the Yorkist William Herbert, who also assumed the guardianship of Margaret Beaufort and the young Henry.\n\nHenry lived in the Herbert household until 1469, when Richard Neville, Earl of Warwick (the \"Kingmaker\"), went over to the Lancastrians. Herbert was captured fighting for the Yorkists and executed by Warwick. When Warwick restored Henry VI in 1470, Jasper Tudor returned from exile and brought Henry to court. When the Yorkist Edward IV regained the throne in 1471, Henry fled with other Lancastrians to Brittany, where he spent most of the next 14 years under the protection of Francis II, Duke of Brittany. In November 1476, Henry's protector fell ill and his principal advisers were more amenable to negotiating with the English king. Henry was handed over and escorted to the Breton port of Saint-Malo. While there, he feigned stomach cramps and in the confusion fled into a monastery. As at Tewkesbury Abbey after 1471 battle, Edward IV prepared to order his extraction and probable execution. The townspeople took exception to his behaviour, however, and Francis recovered from his illness. Thus a small band of scouts rescued Henry.\n\nBy 1483, Henry's mother was actively promoting him as an alternative to Richard III, despite her being married to a Yorkist, Lord Stanley. At Rennes Cathedral on Christmas Day 1483, Henry pledged to marry the eldest daughter of Edward IV, Elizabeth of York, who was also Edward's heir since the presumed death of her brothers, the Princes in the Tower (King Edward V and his brother Richard of Shrewsbury, Duke of York). Henry then received the homage of his supporters.\n\nWith money and supplies borrowed from his host, Francis II, Duke of Brittany, Henry tried to land in England, but his conspiracy unravelled, resulting in the execution of his primary co-conspirator, the Duke of Buckingham. Now supported by Francis II's prime-minister, Pierre Landais, Richard III attempted to extradite Henry from Brittany, but Henry escaped to France. He was welcomed by the French, who readily supplied him with troops and equipment for a second invasion.\n\nHenry gained the support of the Woodvilles, in-laws of the late Edward IV, and sailed with a small French and Scottish force, landing in Mill Bay, Pembrokeshire, close to his birthplace. He marched towards England accompanied by his uncle Jasper and the Earl of Oxford. Wales was traditionally a Lancastrian stronghold, and Henry owed the support he gathered to his Welsh birth and ancestry, being directly descended, through his father, from Rhys ap Gruffydd. He amassed an army of around 5,000 soldiers.\n\nHenry was aware that his best chance to seize the throne was to engage Richard quickly and defeat him immediately, as Richard had reinforcements in Nottingham and Leicester. Richard only needed to avoid being killed to keep his throne. Though outnumbered, Henry's Lancastrian forces decisively defeated Richard's Yorkist army at the Battle of Bosworth Field on 22 August 1485. Several of Richard's key allies, such as the Earl of Northumberland and William and Thomas Stanley, crucially switched sides or left the battlefield. Richard III's death at Bosworth Field effectively ended the Wars of the Roses, although it was not the last battle Henry had to fight.\n\nThe first concern for Henry was to secure his hold on the throne. He declared himself king \"by right of conquest\" retroactively from 21 August 1485, the day before Bosworth Field. Thus anyone who had fought for Richard against him would be guilty of treason, and Henry could legally confiscate his lands and property of Richard III while restoring his own. However, he spared Richard's nephew and designated heir, the Earl of Lincoln, and he made Margaret Plantagenet, a Yorkist heiress, Countess of Salisbury sui juris. He took great care not to address the baronage, or summon Parliament, until after his coronation, which took place in Westminster Abbey on 30 October 1485. Almost immediately afterwards, he issued an edict that any gentleman who swore fealty to him would, notwithstanding any previous attainder, be secure in his property and person.\n\nHenry then honoured his pledge of December 1483 to marry Elizabeth of York. They were third cousins, as both were great-great-grandchildren of John of Gaunt. The marriage took place on 18 January 1486 at Westminster. The marriage unified the warring houses and gave his children a strong claim to the throne. The unification of the houses of York and Lancaster by this marriage is symbolised by the heraldic emblem of the Tudor rose, a combination of the white rose of York and the red rose of Lancaster. It also ended future discussion as to whether the descendants of the fourth son of Edward III, Edmund, Duke of York, through marriage to Philippa, heiress of the second son, Lionel, Duke of Clarence, had a superior or inferior claim to those of the third son John of Gaunt, who had held the throne for three generations.\nIn addition, Henry had Parliament repeal \"Titulus Regius\", the statute that declared Edward IV's marriage invalid and his children illegitimate, thus legitimising his wife. Amateur historians Bertram Fields and Sir Clements Markham have claimed that he may have been involved in the murder of the Princes in the Tower, as the repeal of \"Titulus Regius\" gave the Princes a stronger claim to the throne than his own. Alison Weir, however, points out that the Rennes ceremony, two years earlier, was possible only if Henry and his supporters were certain that the Princes were already dead.\n\nHenry secured his crown principally by dividing and undermining the power of the nobility, especially through the aggressive use of bonds and recognisances to secure loyalty. He also enacted laws against livery and maintenance, the great lords' practice of having large numbers of \"retainers\" who wore their lord's badge or uniform and formed a potential private army.\n\nWhile he was still in Leicester, after the battle of Bosworth Field, Henry was already taking precautions to prevent any rebellions against his reign. Before leaving Leicester to go to London, Henry dispatched Robert Willoughby to Sheriff Hutton in Yorkshire, to have the ten-year-old Edward, Earl of Warwick, arrested and taken to the Tower of London. Edward was the son of George, Duke of Clarence, and as such he presented a threat as a potential rival to the new King Henry VII for the throne of England. However, Henry was threatened by several active rebellions over the next few years. The first was the Rebellion of the Stafford brothers and Viscount Lovell of 1486, which collapsed without fighting.\n\nIn 1487, Yorkists led by Lincoln rebelled in support of Lambert Simnel, a boy who was claimed to be the Earl of Warwick, son of Edward IV's brother Clarence (who had last been seen as a prisoner in the Tower). The rebellion began in Ireland, where the traditionally Yorkist nobility, headed by the powerful Gerald FitzGerald, 8th Earl of Kildare, proclaimed Simnel King and provided troops for his invasion of England. The rebellion was defeated and Lincoln killed at the Battle of Stoke. Henry showed remarkable clemency to the surviving rebels: he pardoned Kildare and the other Irish nobles, and he made the boy, Simnel, a servant in the royal kitchen.\n\nIn 1490, a young Fleming, Perkin Warbeck, appeared and claimed to be Richard, the younger of the \"Princes in the Tower\". Warbeck won the support of Edward IV's sister Margaret of Burgundy. He led attempted invasions of Ireland in 1491 and England in 1495, and persuaded James IV of Scotland to invade England in 1496. In 1497 Warbeck landed in Cornwall with a few thousand troops, but was soon captured and executed.\n\nIn 1499, Henry had the Earl of Warwick executed. However, he spared Warwick's elder sister Margaret. She survived until 1541, when she was executed by Henry VIII.\n\nHenry married Elizabeth of York with the hope of uniting the Yorkist and Lancastrian sides of the Plantagenet dynastic disputes, and he was largely successful. However, such a level of paranoia persisted that anyone (John de la Pole, Earl of Richmond, for example) with blood ties to the Plantagenets was suspected of coveting the throne.\n\nFor most of Henry VII's reign Edward Story was Bishop of Chichester. Story's register still exists and according to the 19th century historian W.R.W Stephens \"affords some illustrations of the avaricious and parsimonious character of the king\". It seems that the king was skillful at extracting money from his subjects on many pretexts including that of war with France or war with Scotland. The money so extracted added to the king's personal fortune rather than the stated purpose.\n\nUnlike his predecessors, Henry VII came to the throne without personal experience in estate management or financial administration. Yet during his reign he became a fiscally prudent monarch who restored the fortunes of an effectively bankrupt exchequer. Henry VII introduced stability to the financial administration of England by keeping the same financial advisors throughout his reign. For instance, other than the first few months of the reign, Lord Dynham and Thomas Howard, earl of Surrey were the only two office holders in the position of Lord High Treasurer of England throughout his reign.\n\nHenry VII improved tax collection within the realm by introducing ruthlessly efficient mechanisms of taxation. He was supported in this effort by his chancellor, Archbishop John Morton, whose \"Morton's Fork\" was a catch-22 method of ensuring that nobles paid increased taxes. Morton's Fork may actually have been invented by another of Henry's supporters, Richard Foxe. However, whether it is called \"Morton's Fork\" or \"Fox's Fork\", the result was the same: Those nobles who spent little must have saved much and, thus, they could afford the increased taxes; on the other hand, those nobles who spent much obviously had the means to pay the increased taxes. Royal government was also reformed with the introduction of the King's Council that kept the nobility in check.\n\nHenry VII's policy was both to maintain peace and to create economic prosperity. Up to a point, he succeeded. He was not a military man and had no interest in trying to regain French territories lost during the reigns of his predecessors; he was therefore ready to conclude a treaty with France at Etaples that brought money into the coffers of England, and ensured the French would not support pretenders to the English throne, such as Perkin Warbeck. However, this treaty came at a slight price, as Henry mounted a minor invasion of Brittany in November 1492. Henry decided to keep Brittany out of French hands, signed an alliance with Spain to that end, and sent 7,000 troops to France. The confused, fractious nature of Breton politics undermined his efforts, which finally failed after three sizeable expeditions, at a cost of £24,000. However, as France was becoming more concerned with the Italian Wars, the French were happy to agree to the Treaty of Etaples.\nHenry had been under the financial and physical protection of the French throne or its vassals for most of his life, prior to his ascending the throne of England. To strengthen his position, however, he subsidised shipbuilding, so strengthening the navy (he commissioned Europe's first ever – and the world's oldest surviving – dry dock at Portsmouth in 1495) and improving trading opportunities.\n\nBy the time of his death, he had amassed a personal fortune of £1.25 million.\n\nHenry VII was one of the first European monarchs to recognise the importance of the newly united Spanish kingdom and concluded the Treaty of Medina del Campo, by which his son, Arthur Tudor, was married to Catherine of Aragon. He also concluded the Treaty of Perpetual Peace with Scotland (the first treaty between England and Scotland for almost two centuries), which betrothed his daughter Margaret to King James IV of Scotland. By means of this marriage, Henry VII hoped to break the Auld Alliance between Scotland and France. Though this was not achieved during his reign, the marriage eventually led to the union of the English and Scottish crowns under Margaret's great-grandson, James VI and I following the death of Henry's granddaughter Elizabeth I.\n\nHe also formed an alliance with Holy Roman Emperor Maximilian I (1493–1519) and persuaded Pope Innocent VIII to issue a papal bull of excommunication against all pretenders to Henry's throne.\n\nHenry VII was much enriched by trading alum, which was used in the wool and cloth trades for use as a chemical dye fixative when dyeing fabrics. Since alum was mined in only one area in Europe (Tolfa, Italy), it was a scarce commodity and therefore especially valuable to its land holder, the pope. With the English economy heavily invested in wool production, Henry VII became involved in the alum trade in 1486. With the assistance of the Italian merchant-banker, Lodovico della Fava and the Italian banker, Girolamo Frescobaldi, Henry VII became deeply involved in the trade by licensing ships, obtaining alum from the Ottoman Empire, and selling it to the Low Countries and in England. This trade made an expensive commodity cheaper, which raised opposition from Pope Julius II since the Tolfa mine was a part of papal territory and had given the Pope monopoly control over alum.\nHenry's most successful diplomatic achievement as regards the economy was the \"Magnus Intercursus\" (\"great agreement\") of 1496. In 1494, Henry embargoed trade (mainly in wool) with the Netherlands as retaliation for Margaret of Burgundy's support of Perkin Warbeck. The Merchant Adventurers, the company which enjoyed the monopoly of the Flemish wool trade, relocated from Antwerp to Calais. At the same time, Flemish merchants were ejected from England. The stand-off eventually paid off for Henry. Both parties realised they were mutually disadvantaged by the reduction in commerce. Its restoration by the \"Magnus Intercursus\" was very much to England's benefit in removing taxation for English merchants and significantly increasing England's wealth. In turn, Antwerp became an extremely important trade entrepot, through which, for example, goods from the Baltic, spices from the east and Italian silks were exchanged for English cloth.\n\nIn 1506, Henry extorted the Treaty of Windsor from Philip the Handsome of Burgundy. Philip had been shipwrecked on the English coast, and while Henry's guest, was bullied into an agreement so favourable to England at the expense of the Netherlands that it was dubbed the \"Malus Intercursus\" (\"evil agreement\"). France, Burgundy, the Holy Roman Empire, Spain and the Hanseatic League all rejected the treaty, which was never in force. Philip died shortly after the negotiations.\n\nHenry's principal problem was to restore royal authority in a realm recovering from the Wars of the Roses. There were too many powerful noblemen and, as a consequence of the system of so-called bastard feudalism, each had what amounted to private armies of indentured retainers (mercenaries masquerading as servants).\nHe was content to allow the nobles their regional influence if they were loyal to him. For instance, the Stanley family had control of Lancashire and Cheshire, upholding the peace on the condition that they stayed within the law. In other cases, he brought his over-powerful subjects to heel by decree. He passed laws against \"livery\" (the upper classes' flaunting of their adherents by giving them badges and emblems) and \"maintenance\" (the keeping of too many male \"servants\"). These laws were used shrewdly in levying fines upon those that he perceived as threats.\n\nHowever, his principal weapon was the Court of Star Chamber. This revived an earlier practice of using a small (and trusted) group of the Privy Council as a personal or Prerogative Court, able to cut through the cumbersome legal system and act swiftly. Serious disputes involving the use of personal power, or threats to royal authority, were thus dealt with.\n\nHenry VII used Justices of the Peace on a large, nationwide scale. They were appointed for every shire and served for a year at a time. Their chief task was to see that the laws of the country were obeyed in their area. Their powers and numbers steadily increased during the time of the Tudors, never more so than under Henry's reign. Despite this, Henry was keen to constrain their power and influence, applying the same principles to the Justices of the Peace as he did to the nobility: a similar system of bonds and recognisances to that which applied to both the gentry and the nobles who tried to exert their elevated influence over these local officials.\n\nAll Acts of Parliament were overseen by the Justices of the Peace. For example, Justices of the Peace could replace suspect jurors in accordance with the 1495 act preventing the corruption of juries. They were also in charge of various administrative duties, such as the checking of weights and measures.\n\nBy 1509, Justices of the Peace were key enforcers of law and order for Henry VII. They were unpaid, which, in comparison with modern standards, meant a lesser tax bill to pay for a police force. Local gentry saw the office as one of local influence and prestige and were therefore willing to serve. Overall, this was a successful area of policy for Henry, both in terms of efficiency and as a method of reducing the corruption endemic within the nobility of the Middle Ages.\n\nIn 1502, Henry VII's first son and heir-apparent, Arthur, Prince of Wales, died suddenly at Ludlow Castle, very likely from a viral respiratory illness known, at the time, as the \"English sweating sickness\". This made Henry, Duke of York (Henry VIII) heir-apparent to the throne. The King, normally a reserved man who rarely showed much emotion in public unless angry, surprised his courtiers by his intense grief and sobbing at his son's death, while his concern for the Queen is evidence that the marriage was a happy one, as is his reaction to the Queen's death the following year, when he shut himself away for several days, refusing to speak to anyone.\n\nHenry VII wanted to maintain the Spanish alliance. He therefore arranged a papal dispensation from Pope Julius II for Prince Henry to marry his brother's widow Catherine, a relationship that would have otherwise precluded marriage in the Roman Catholic Church. In 1503, Queen Elizabeth died in childbirth, so King Henry had the dispensation also permit him to marry Catherine himself. After obtaining the dispensation, Henry had second thoughts about the marriage of his son and Catherine. Catherine's mother Isabella I of Castile had died and Catherine's sister Joanna had succeeded her; Catherine was therefore daughter of only one reigning monarch and so less desirable as a spouse for Henry VII's heir-apparent. The marriage did not take place during his lifetime. Otherwise, at the time of his father's arranging of the marriage to Catherine of Aragon, the future Henry VIII was too young to contract the marriage according to Canon Law, and would be ineligible until age fourteen.\n\nHenry made half-hearted plans to remarry and beget more heirs, but these never came to anything. In 1505 he was sufficiently interested in a potential marriage to Joan, the recently widowed Queen of Naples, that he sent ambassadors to Naples to report on the 27-year-old's physical suitability. The wedding never took place, and curiously the physical description Henry sent with his ambassadors describing what he desired in a new wife matched the description of Elizabeth. After 1503, records show the Tower of London was never again used as a royal residence by Henry Tudor, and all royal births under Henry VIII took place in palaces. Henry VII was shattered by the loss of Elizabeth, and her death broke his heart. During his lifetime he was often jeered by the nobility for his re-centralizing of power in London, and later the 16th-century historian Francis Bacon was ruthlessly critical of the methods by which he enforced tax law, but equally true is the fact that Henry Tudor was hellbent on keeping detailed bookkeeping records of his personal finances, down to the last halfpenny; these and one account book detailing the expenses of his queen survive in the British National Archives. Until the death of his wife Elizabeth, the evidence is clear from these accounting books that Henry Tudor was a more doting father and husband than was widely known. Many of the entries in his account books show a man who loosened his purse strings generously for his wife and children, and not just on necessities: in spring 1491 he spent a great amount of gold on his daughter Mary for a lute; the following year he spent money on a lion for Queen Elizabeth's menagerie.\nWith the death of Elizabeth, the possibility for such family indulgences greatly diminished. Immediately after Elizabeth's death, Henry became very sick and nearly died himself, and only allowed Margaret Beaufort, his mother, near him: \"privily departed to a solitary place, and would that no man should resort unto him.\"\nHenry VII died at Richmond Palace on 21 April 1509 of tuberculosis and was buried at Westminster Abbey, next to his wife, Elizabeth, in the chapel he commissioned. He was succeeded by his second son, Henry VIII (reign 1509–47). His mother survived him, dying two months later on 29 June 1509.\n\nHenry is the first English king for whose appearance we have good contemporary visual records in realistic portraits that are relatively free of idealization. At twenty-seven, Henry was tall, slender, with small blue eyes, which were said to have a noticeable animation of expression, and noticeably bad teeth in a long, sallow face beneath very fair hair. Amiable and high-spirited, Henry Tudor was friendly if dignified in manner, while it was clear to everyone that he was extremely intelligent. His biographer, Professor Chrimes, credits him – even before he had become king – with possessing \"a high degree of personal magnetism, ability to inspire confidence, and a growing reputation for shrewd decisiveness\". On the debit side, he may have looked a little delicate as he suffered from poor health.\n\nHistorians have always compared Henry VII with his continental contemporaries, especially Louis XI of France and Ferdinand II of Aragon. By 1600 historians emphasised Henry's wisdom in drawing lessons in statecraft from other monarchs. By 1900 the \"New Monarchy\" interpretation stressed the common factors that in each country led to the revival of monarchical power. This approach raised puzzling questions about similarities and differences in the development of national states. In the late 20th century a model of European state formation was prominent in which Henry less resembles Louis and Ferdinand.\n\n\nHenry's full style as king was: \"Henry, by the Grace of God, King of England, France and Lord of Ireland\".\n\nUpon his succession as king, Henry became entitled to bear the arms of his kingdom. After his marriage, he used the red-and-white rose as his emblem – this continued to be his dynasty's emblem, known as the Tudor rose.\n\nHenry and Elizabeth's children are listed below.\n\nAn illegitimate son, by \"a Breton Lady\", has also been attributed to Henry:\nHenry VII's elder surviving daughter Margaret was married first to James IV of Scotland (reigned 1488–1513). Their son became James V of Scotland (reigned 1513–42), whose daughter became Mary, Queen of Scots (reigned 1542–67). Margaret Tudor's second marriage was to Archibald Douglas; their grandson, Henry Stuart, Lord Darnley married Mary, Queen of Scots. Their son, James VI of Scotland (reigned 1567–1625), inherited the throne of England as James I (reigned 1603–25) after the death of Henry's granddaughter, Elizabeth I (reigned 1558–1603). After divorcing Douglas, her third and final marriage was to Henry Stewart, with whom she had another daughter, Dorothea Stewart.\n\nHenry VII's other surviving daughter, Mary first married King Louis XII of France (reigned 1498–1515), who died after only about three months of marriage. She then married Charles Brandon without the permission of her brother, now King Henry VIII. Their daughter Frances married Henry Grey, and her children included Lady Jane Grey, in whose name her parents and in-laws tried to seize the throne after Edward VI of England (reigned 1547–53) died.\n\nThe current monarch of the United Kingdom, Elizabeth II, is a direct descendant of Henry VII. The daughter of Henry's double-great-great grandson James VI/I, Elizabeth Stuart, was the mother of Sophia of Hanover whose descendants were the monarchs of the House of Hanover and the succeeding House of Saxe-Coburg and Gotha/Windsor.\n\n\n\n", "id": "14186", "title": "Henry VII of England"}
{"url": "https://en.wikipedia.org/wiki?curid=14187", "text": "Henry VIII of England\n\nHenry VIII (28 June 1491 – 28 January 1547) was King of England from 21 April 1509 until his death. Henry was the second Tudor monarch, succeeding his father, Henry VII.\n\nHenry is best known for his six marriages and, in particular, his efforts to have his first marriage, to Catherine of Aragon, annulled. His disagreement with the Pope on the question of such an annulment led Henry to initiate the English Reformation, separating the Church of England from papal authority and appointing himself the Supreme Head of the Church of England. Despite his resulting excommunication, Henry remained a believer in core Catholic theological teachings.\n\nDomestically, Henry is known for his radical changes to the English Constitution, ushering in the theory of the divine right of kings to England. Besides asserting the sovereign's supremacy over the Church of England, he greatly expanded royal power during his reign. Charges of treason and heresy were commonly used to quash dissent, and those accused were often executed without a formal trial, by means of bills of attainder. He achieved many of his political aims through the work of his chief ministers, some of whom were banished or executed when they fell out of his favour. Thomas Wolsey, Thomas More, Thomas Cromwell, Richard Rich, and Thomas Cranmer all figured prominently in Henry's administration. He was an extravagant spender and used the proceeds from the Dissolution of the Monasteries and acts of the Reformation Parliament to convert into royal revenue the money that was formerly paid to Rome. Despite the influx of money from these sources, Henry was continually on the verge of financial ruin due to his personal extravagance as well as his numerous costly continental wars, particularly with Francis I of France and the Holy Roman Emperor Charles V, as he sought to enforce his claim to the Kingdom of France. At home, he oversaw the legal union of England and Wales with the Laws in Wales Acts 1535 and 1542 and following the Crown of Ireland Act 1542 he was the first English Monarch to rule as King of Ireland.\n\nHis contemporaries considered Henry in his prime to be an attractive, educated, and accomplished king, and he has been described as \"one of the most charismatic rulers to sit on the English throne\". He was an author and composer. As he aged, Henry became severely obese and his health suffered, contributing to his death in 1547. He is frequently characterised in his later life as a lustful, egotistical, harsh, and insecure king. He was succeeded by his son Edward VI.\n\nBorn 28June 1491 at the Palace of Placentia in Greenwich, London, Henry Tudor was the third child and second son of Henry VII and Elizabeth of York. Of the young Henry's six siblings, only three – Arthur, Prince of Wales; Margaret; and Mary – survived infancy. He was baptised by Richard Fox, the Bishop of Exeter, at a church of the Observant Franciscans close to the palace. In 1493, at the age of two, Henry was appointed Constable of Dover Castle and Lord Warden of the Cinque Ports. He was subsequently appointed Earl Marshal of England and Lord Lieutenant of Ireland at age three, and was inducted into the Order of the Bath soon after. The day after the ceremony he was created Duke of York and a month or so later made Warden of the Scottish Marches. In May 1495, he was appointed to the Order of the Garter. Henry was given a first-rate education from leading tutors, becoming fluent in Latin and French, and learning at least some Italian. Not much is known about his early life – save for his appointments – because he was not expected to become king. In November 1501, Henry also played a considerable part in the ceremonies surrounding the marriage of his brother, Prince Arthur, to Catherine of Aragon, the youngest surviving child of King Ferdinand II of Aragon and Queen Isabella I of Castile. As Duke of York, Henry used the arms of his father as king, differenced by a \"label of three points ermine\".\n\nIn 1502, Arthur died at the age of 15 of sweating sickness, just 20 weeks after his marriage to Catherine. Arthur's death thrust all his duties upon his younger brother, the 10-year-old Henry. After a little debate, Henry became the new Duke of Cornwall in October 1502, and the new Prince of Wales and Earl of Chester in February 1503. Henry VII gave the boy few tasks. Young Henry was strictly supervised and did not appear in public. As a result, the young Henry would later ascend the throne \"untrained in the exacting art of kingship\".\n\nHenry VII renewed his efforts to seal a marital alliance between England and Spain, by offering his second son in marriage to Arthur's widow Catherine. Both Isabella and Henry VII were keen on the idea, which had arisen very shortly after Arthur's death. On 23 June 1503, a treaty was signed for their marriage, and they were betrothed two days later. A papal dispensation was only needed for the \"impediment of public honesty\" if the marriage had not been consummated as Catherine and her duenna claimed, but Henry VII and the Spanish ambassador set out instead to obtain a dispensation for \"affinity\", which took account of the possibility of consummation. The young Henry's age, only eleven, prevented cohabitation. Isabella's death in 1504, and the ensuing problems of succession in Castile, complicated matters. Her father preferred her to stay in England, but Henry VII's relations with Ferdinand had deteriorated. Catherine was therefore left in limbo for some time, culminating in Prince Henry's rejection of the marriage as soon he was able, at the age of 14. Ferdinand's solution was to make his daughter ambassador, allowing her to stay in England indefinitely. Devout, she began to believe that it was God's will that she marry the prince despite his opposition.\n\nHenry VII died on 21 April 1509, and the 17-year-old Henry succeeded him as king. Soon after his father's burial on 10 May, Henry suddenly declared that he would indeed marry Catherine, leaving unresolved several issues concerning the papal dispensation and a missing part of the marriage portion. The new king maintained that it had been his father's dying wish that he marry Catherine. Whether or not this was true, it was certainly convenient. Holy Roman Emperor Maximilian I had been attempting to marry his granddaughter (and Catherine's niece) Eleanor to Henry; she had now been jilted. Henry's wedding to Catherine was kept low-key and was held at the friar's church in Greenwich on 11 June 1509. On 23 June 1509, Henry led the now 23-year-old Catherine from the Tower of London to Westminster Abbey for their coronation, which took place the following day. It was a grand affair: the king's passage was lined with tapestries and laid with fine cloth. Following the ceremony, there was a grand banquet in Westminster Hall. As Catherine wrote to her father, \"our time is spent in continuous festival\".\n\nTwo days after Henry's coronation, he arrested his father's two most unpopular ministers, Sir Richard Empson and Edmund Dudley. They were charged with high treason and were executed in 1510. Historian Ian Crofton has maintained that such executions would become Henry's primary tactic for dealing with those who stood in his way; the two executions were certainly not the last. Henry also returned to the public some of the money supposedly extorted by the two ministers. By contrast, Henry's view of the House of York – potential rival claimants for the throne – was more moderate than his father's had been. Several who had been imprisoned by his father, including the Marquess of Dorset, were pardoned. Others (most notably Edmund de la Pole) went unreconciled; de la Pole was eventually beheaded in 1513, an execution prompted by his brother Richard siding against the king.\n\nSoon after, Catherine conceived, but the child, a girl, was stillborn on 31 January 1510. About four months later, Catherine again became pregnant. On New Year's Day 1511, the child – Henry – was born. After the grief of losing their first child, the couple were pleased to have a boy and there were festivities to celebrate, including a jousting tournament. However, the child died seven weeks later. Catherine had two stillborn sons in 1514 and 1515, but gave birth in February 1516 to a girl, Mary. Relations between Henry and Catherine had been strained, but they eased slightly after Mary's birth.\n\nAlthough Henry's marriage to Catherine has since been described as \"unusually good\", it is known that Henry took mistresses. It was revealed in 1510 that Henry had been conducting an affair with one of the sisters of Edward Stafford, 3rd Duke of Buckingham, either Elizabeth or Anne Hastings, Countess of Huntingdon. The most significant mistress for about three years, starting in 1516, was Elizabeth Blount. Blount is one of only two completely undisputed mistresses, few for a virile young king. Exactly how many Henry had is disputed: David Loades believes Henry had mistresses \"only to a very limited extent\", whilst Alison Weir believes there were numerous other affairs. Catherine did not protest, and in 1518 fell pregnant again with another girl, who was also stillborn. Blount gave birth in June 1519 to Henry's illegitimate son, Henry FitzRoy. The young boy was made Duke of Richmond in June 1525 in what some thought was one step on the path to his eventual legitimisation. In 1533, FitzRoy married Mary Howard, but died childless three years later. At the time of Richmond's death in June 1536, Parliament was enacting the Second Succession Act, which could have allowed him to become king.\n\nIn 1510, France, with a fragile alliance with the Holy Roman Empire in the League of Cambrai, was winning a war against Venice. Henry renewed his father's friendship with Louis XII of France, an issue that divided his council. Certainly war with the combined might of the two powers would have been exceedingly difficult. Shortly thereafter, however, Henry also signed a pact with Ferdinand. After Pope Julius II created the anti-French Holy League in October 1511, Henry followed Ferdinand's lead and brought England into the new League. An initial joint Anglo-Spanish attack was planned for the spring to recover Aquitaine for England, the start of making Henry's dreams of ruling France a reality. The attack, however, following a formal declaration of war in April 1512, was not led by Henry personally and was a considerable failure; Ferdinand used it simply to further his own ends, and it strained the Anglo-Spanish alliance. Nevertheless, the French were pushed out of Italy soon after, and the alliance survived, with both parties keen to win further victories over the French. Henry then pulled off a diplomatic coup by convincing the Emperor to join the Holy League. Remarkably, Henry had also secured the promised title of \"Most Christian King of France\" from Julius and possibly coronation by the Pope himself in Paris, if only Louis could be defeated.\n\nOn 30 June 1513, Henry invaded France, and his troops defeated a French army at the Battle of the Spurs – a relatively minor result, but one which was seized on by the English for propaganda purposes. Soon after, the English took Thérouanne and handed it over to Maximillian; Tournai, a more significant settlement, followed. Henry had led the army personally, complete with large entourage. His absence from the country, however, had prompted his brother-in-law, James IV of Scotland, to invade England at the behest of Louis. Nevertheless, the English army, overseen by Queen Catherine, decisively defeated the Scots at the Battle of Flodden on 9 September 1513. Among the dead was the Scottish king, thus ending Scotland's brief involvement in the war. These campaigns had given Henry a taste of the military success he so desired. However, despite initial indications, he decided not to pursue a 1514 campaign. He had been supporting Ferdinand and Maximilian financially during the campaign but had received little in return; England's coffers were now empty. With the replacement of Julius by Pope Leo X, who was inclined to negotiate for peace with France, Henry signed his own treaty with Louis: his sister Mary would become Louis' wife, having previously been pledged to the younger Charles, and peace was secured for eight years, a remarkably long time.\n\nCharles V ascended the thrones of both Spain and the Holy Roman Empire following the deaths of his grandfathers, Ferdinand in 1516 and Maximilian in 1519. Francis I likewise became king of France upon the death of Louis in 1515, leaving three relatively young rulers and an opportunity for a clean slate. The careful diplomacy of Cardinal Thomas Wolsey had resulted in the Treaty of London in 1518, aimed at uniting the kingdoms of western Europe in the wake of a new Ottoman threat, and it seemed that peace might be secured. Henry met Francis I on 7 June 1520 at the Field of the Cloth of Gold near Calais for a fortnight of lavish entertainment. Both hoped for friendly relations in place of the wars of the previous decade. The strong air of competition laid to rest any hopes of a renewal of the Treaty of London, however, and conflict was inevitable. Henry had more in common with Charles, whom he met once before and once after Francis. Charles brought the Empire into war with France in 1521; Henry offered to mediate, but little was achieved and by the end of the year Henry had aligned England with Charles. He still clung to his previous aim of restoring English lands in France, but also sought to secure an alliance with Burgundy, then part of Charles' realm, and the continued support of Charles. A small English attack in the north of France made up little ground. Charles defeated and captured Francis at Pavia and could dictate peace; but he believed he owed Henry nothing. Sensing this, Henry decided to take England out of the war before his ally, signing the Treaty of the More on 30 August 1525.\n\nDuring his first marriage to Catherine of Aragon, Henry conducted an affair with Mary Boleyn, Catherine's lady-in-waiting. There has been speculation that Mary's two children, Henry and Catherine Carey, were fathered by Henry, but this has never been proved, and the King never acknowledged them as he did Henry FitzRoy. In 1525, as Henry grew more impatient with Catherine's inability to produce the male heir he desired, he became enamoured of Mary Boleyn's sister, Anne, then a charismatic young woman of 25 in the Queen's entourage. Anne, however, resisted his attempts to seduce her, and refused to become his mistress as her sister Mary Boleyn had. It was in this context that Henry considered his three options for finding a dynastic successor and hence resolving what came to be described at court as the King's \"great matter\". These options were legitimising Henry FitzRoy, which would take the intervention of the pope and would be open to challenge; marrying off Mary as soon as possible and hoping for a grandson to inherit directly, but Mary was considered unlikely to conceive before Henry's death; or somehow rejecting Catherine and marrying someone else of child-bearing age. Probably seeing the possibility of marrying Anne, the third was ultimately the most attractive possibility to the 34-year-old Henry, and it soon became the King's absorbing desire to annul his marriage to the now 40-year-old Catherine. It was a decision that would lead Henry to reject papal authority and initiate the English Reformation.\n\nHenry's precise motivations and intentions over the coming years are not widely agreed on. Henry himself, at least in the early part of his reign, was a devout and well-informed Catholic to the extent that his 1521 publication \"Assertio Septem Sacramentorum\" (\"Defence of the Seven Sacraments\") earned him the title of \"Fidei Defensor\" (Defender of the Faith) from Pope Leo X. The work represented a staunch defence of papal supremacy, albeit one couched in somewhat contingent terms. It is not clear exactly when Henry changed his mind on the issue as he grew more intent on a second marriage. Certainly, by 1527 he had convinced himself that in marrying Catherine, his brother's wife, he had acted contrary to Leviticus 20:21, an impediment the Pope had never had (he now believed) the authority to dispense with. It was this argument Henry took to Pope Clement VII in 1527 in the hope of having his marriage to Catherine annulled, forgoing at least one less openly defiant line of attack. In going public, all hope of tempting Catherine to retire to a nunnery or otherwise stay quiet were lost. Henry sent his secretary, William Knight, to appeal directly to the Holy See by way of a deceptively worded draft papal bull. Knight was unsuccessful; the Pope could not be misled so easily.\n\nOther missions concentrated on arranging an ecclesiastical court to meet in England, with a representative from Clement VII. Though Clement agreed to the creation of such a court, he never had any intention of empowering his legate, Lorenzo Campeggio, to decide in Henry's favour. This bias was perhaps the result of pressure from Charles V, Catherine's nephew, though it is not clear how far this influenced either Campeggio or the Pope. After less than two months of hearing evidence, Clement called the case back to Rome in July 1529, from which it was clear that it would never re-emerge. With the chance for an annulment lost and England's place in Europe forfeit, Cardinal Wolsey bore the blame. He was charged with \"praemunire\" in October 1529 and his fall from grace was \"sudden and total\". Briefly reconciled with Henry (and officially pardoned) in the first half of 1530, he was charged once more in November 1530, this time for treason, but died while awaiting trial. After a short period in which Henry took government upon his own shoulders, Sir Thomas More took on the role of Lord Chancellor and chief minister. Intelligent and able, but also a devout Catholic and opponent of the annulment, More initially cooperated with the king's new policy, denouncing Wolsey in Parliament.\n\nA year later, Catherine was banished from court, and her rooms were given to Anne. Anne was an unusually educated and intellectual woman for her time, and was keenly absorbed and engaged with the ideas of the Protestant Reformers, though the extent to which she herself was a committed Protestant is much debated. When Archbishop of Canterbury William Warham died, Anne's influence and the need to find a trustworthy supporter of the annulment had Thomas Cranmer appointed to the vacant position. This was approved by the Pope, unaware of the King's nascent plans for the Church.\n\nIn the winter of 1532, Henry met with Francis I at Calais and enlisted the support of the French king for his new marriage. Immediately upon returning to Dover in England, Henry, now 41, and Anne, now 32, went through a secret wedding service. She soon became pregnant, and there was a second wedding service in London on 25 January 1533. On 23 May 1533, Cranmer, sitting in judgment at a special court convened at Dunstable Priory to rule on the validity of the king's marriage to Catherine of Aragon, declared the marriage of Henry and Catherine null and void. Five days later, on 28 May 1533, Cranmer declared the marriage of Henry and Anne to be valid. Catherine was formally stripped of her title as queen, becoming instead \"princess dowager\" as the widow of Arthur. In her place, Anne was crowned queen consort on 1 June 1533. The queen gave birth to a daughter slightly prematurely on 7 September 1533. The child was christened Elizabeth, in honour of Henry's mother, Elizabeth of York.\n\nFollowing the marriage, there was a period of consolidation taking the form of a series of statutes of the Reformation Parliament aimed at finding solutions to any remaining issues, whilst protecting the new reforms from challenge, convincing the public of their legitimacy, and exposing and dealing with opponents. Although the canon law was dealt with at length by Cranmer and others, these acts were advanced by Thomas Cromwell, Thomas Audley and the Duke of Norfolk and indeed by Henry himself. With this process complete, in May 1532 More resigned as Lord Chancellor, leaving Cromwell as Henry's chief minister. With the Act of Succession 1533, Catherine's daughter, Mary, was declared illegitimate; Henry's marriage to Anne was declared legitimate; and Anne's issue was decided to be next in the line of succession. With the Acts of Supremacy in 1534, Parliament also recognised the King's status as head of the church in England and, with the Act in Restraint of Appeals in 1532, abolished the right of appeal to Rome. It was only then that Pope Clement took the step of excommunicating Henry and Thomas Cranmer, although the excommunication was not made official until some time later.\n\nThe king and queen were not pleased with married life. The royal couple enjoyed periods of calm and affection, but Anne refused to play the submissive role expected of her. The vivacity and opinionated intellect that had made her so attractive as an illicit lover made her too independent for the largely ceremonial role of a royal wife and it made her many enemies. For his part, Henry disliked Anne's constant irritability and violent temper. After a false pregnancy or miscarriage in 1534, he saw her failure to give him a son as a betrayal. As early as Christmas 1534, Henry was discussing with Cranmer and Cromwell the chances of leaving Anne without having to return to Catherine. Henry is traditionally believed to have had an affair with Margaret (\"Madge\") Shelton in 1535, although historian Antonia Fraser argues that Henry in fact had an affair with her sister Mary Shelton.\n\nOpposition to Henry's religious policies was quickly suppressed in England. A number of dissenting monks, including the first Carthusian Martyrs, were executed and many more pilloried. The most prominent resisters included John Fisher, Bishop of Rochester, and Sir Thomas More, both of whom refused to take the oath to the King. Neither Henry nor Cromwell sought to have the men executed; rather, they hoped that the two might change their minds and save themselves. Fisher openly rejected Henry as supreme head of the Church, but More was careful to avoid openly breaking the Treason Act, which (unlike later acts) did not forbid mere silence. Both men were subsequently convicted of high treason, however – More on the evidence of a single conversation with Richard Rich, the Solicitor General. Both were duly executed in the summer of 1535.\n\nThese suppressions, as well as the Dissolution of the Lesser Monasteries Act of 1536, in turn contributed to more general resistance to Henry's reforms, most notably in the Pilgrimage of Grace, a large uprising in northern England in October 1536. Some 20,000 to 40,000 rebels were led by Robert Aske, together with parts of the northern nobility. Henry VIII promised the rebels he would pardon them and thanked them for raising the issues. Aske told the rebels they had been successful and they could disperse and go home. Henry saw the rebels as traitors and did not feel obliged to keep his promises with them, so when further violence occurred after Henry's offer of a pardon he was quick to break his promise of clemency. The leaders, including Aske, were arrested and executed for treason. In total, about 200 rebels were executed, and the disturbances ended.\n\nOn 8 January 1536 news reached the king and the queen that Catherine of Aragon had died. Henry called for public displays of joy regarding Catherine's death. The queen was pregnant again, and she was aware of the consequences if she failed to give birth to a son. Later that month, the King was unhorsed in a tournament and was badly injured and it seemed for a time that his life was in danger. When news of this accident reached the queen, she was sent into shock and miscarried a male child that was about 15 weeks old, on the day of Catherine's funeral, 29 January 1536. For most observers, this personal loss was the beginning of the end of the royal marriage. Given the king's desperate desire for a son, the sequence of Anne's pregnancies has attracted much interest. Author Mike Ashley speculated that Anne had two stillborn children after Elizabeth's birth and before the birth of the male child she miscarried in 1536. Most sources attest only to the birth of Elizabeth in September 1533, a possible miscarriage in the summer of 1534, and the miscarriage of a male child, of almost four months gestation, in January 1536.\n\nAlthough the Boleyn family still held important positions on the Privy Council, Anne had many enemies, including the Duke of Suffolk. Even her own uncle, the Duke of Norfolk, had come to resent her attitude to her power. The Boleyns preferred France over the Emperor as a potential ally, but the King's favour had swung towards the latter (partly because of Cromwell), damaging the family's influence. Also opposed to Anne were supporters of reconciliation with Princess Mary (among them the former supporters of Catherine), who had reached maturity. A second annulment was now a real possibility, although it is commonly believed that it was Cromwell's anti-Boleyn influence that led opponents to look for a way of having her executed.\n\nAnne's downfall came shortly after she had recovered from her final miscarriage. Whether it was primarily the result of allegations of conspiracy, adultery, or witchcraft remains a matter of debate among historians. Early signs of a fall from grace included the King's new mistress, the 28-year-old Jane Seymour, being moved into new quarters, and Anne's brother, George Boleyn, being refused the Order of the Garter, which was instead given to Nicholas Carew. Between 30 April and 2 May, five men, including Anne's brother, were arrested on charges of treasonable adultery and accused of having sexual relationships with the queen. Anne was also arrested, accused of treasonous adultery and incest. Although the evidence against them was unconvincing, the accused were found guilty and condemned to death. George Boleyn and the other accused men were executed on 17 May 1536. At 8 am on 19 May 1536, Anne, age 36, was executed on Tower Green.\n\nThe day after Anne's execution in 1536 the 45-year-old Henry became engaged to Seymour, who had been one of the Queen's ladies-in-waiting. They were married ten days later. On 12 October 1537, Jane gave birth to a son, Prince Edward, the future Edward VI. The birth was difficult, and the queen died on 24 October 1537 from an infection and was buried in Windsor. The euphoria that had accompanied Edward's birth became sorrow, but it was only over time that Henry came to long for his wife. At the time, Henry recovered quickly from the shock. Measures were immediately put in place to find another wife for Henry, which, at the insistence of Cromwell and the court, were focused on the European continent.\n\nWith Charles V distracted by the internal politics of his many kingdoms and external threats, and Henry and Francis on relatively good terms, domestic and not foreign policy issues had been Henry's priority in the first half of the 1530s. In 1536, for example, Henry granted his assent to the Laws in Wales Act 1535, which legally annexed Wales, uniting England and Wales into a single nation. This was followed by the Second Succession Act (the Act of Succession 1536), which declared Henry's children by Jane to be next in the line of succession and declared both Mary and Elizabeth illegitimate, thus excluding them from the throne. The king was also granted the power to further determine the line of succession in his will, should he have no further issue. However, when Charles and Francis made peace in January 1539, Henry became increasingly paranoid, perhaps as a result of receiving a constant list of threats to the kingdom (real or imaginary, minor or serious) supplied by Cromwell in his role as spymaster. Enriched by the dissolution of the monasteries, Henry used some of his financial reserves to build a series of coastal defences and set some aside for use in the event of a Franco-German invasion.\n\nHaving considered the matter, Cromwell, now Earl of Essex, suggested Anne, the 25-year-old sister of the Duke of Cleves, who was seen as an important ally in case of a Roman Catholic attack on England, for the duke fell between Lutheranism and Catholicism. Hans Holbein the Younger was dispatched to Cleves to paint a portrait of Anne for the king. Despite speculation that Holbein painted her in an overly flattering light, it is more likely that the portrait was accurate; Holbein remained in favour at court. After seeing Holbein's portrait, and urged on by the complimentary description of Anne given by his courtiers, the 49-year-old king agreed to wed Anne. However, it was not long before Henry wished to annul the marriage so he could marry another. Anne did not argue, and confirmed that the marriage had never been consummated. Anne's previous betrothal to the Duke of Lorraine's son Francis provided further grounds for the annulment. The marriage was subsequently dissolved, and Anne received the title of \"The King's Sister\", two houses and a generous allowance. It was soon clear that Henry had fallen for the 17-year-old Catherine Howard, the Duke of Norfolk's niece, the politics of which worried Cromwell, for Norfolk was a political opponent.\n\nShortly after, the religious reformers (and protégés of Cromwell) Robert Barnes, William Jerome and Thomas Garret were burned as heretics. Cromwell, meanwhile, fell out of favour although it is unclear exactly why, for there is little evidence of differences of domestic or foreign policy. Despite his role, he was never formally accused of being responsible for Henry's failed marriage. Cromwell was now surrounded by enemies at court, with Norfolk also able to draw on his niece's position. Cromwell was charged with treason, selling export licences, granting passports, and drawing up commissions without permission, and may also have been blamed for the failure of the foreign policy that accompanied the attempted marriage to Anne. He was subsequently attainted and beheaded.\n\nOn 28 July 1540 (the same day Cromwell was executed), Henry married the young Catherine Howard, a first cousin and lady-in-waiting of Anne Boleyn. He was absolutely delighted with his new queen, and awarded her the lands of Cromwell and a vast array of jewellery. Soon after the marriage, however, Queen Catherine had an affair with the courtier Thomas Culpeper. She also employed Francis Dereham, who had previously been informally engaged to her and had an affair with her prior to her marriage, as her secretary. The court was informed of her affair with Dereham whilst Henry was away; they dispatched Thomas Cranmer to investigate, who brought evidence of Queen Catherine's previous affair with Dereham to the king's notice. Though Henry originally refused to believe the allegations, Dereham confessed. It took another meeting of the council, however, before Henry believed the accusations against Dereham and went into a rage, blaming the council before consoling himself in hunting. When questioned, the queen could have admitted a prior contract to marry Dereham, which would have made her subsequent marriage to Henry invalid, but she instead claimed that Dereham had forced her to enter into an adulterous relationship. Dereham, meanwhile, exposed Queen Catherine's relationship with Culpeper. Culpeper and Dereham were both executed, and Catherine too was beheaded on 13 February 1542.\n\nIn 1538, the chief minister Thomas Cromwell pursued an extensive campaign against what was termed \"idolatry\" by the followers of the old religion, culminating in September with the dismantling of the shrine of St. Thomas Becket at Canterbury. As a consequence, the king was excommunicated by the Pope Paul III on 17 December of the same year. In 1540, Henry sanctioned the complete destruction of shrines to saints. In 1542, England's remaining monasteries were all dissolved, and their property transferred to the Crown. Abbots and priors lost their seats in the House of Lords; only archbishops and bishops remained. Consequently, the Lords Spiritual – as members of the clergy with seats in the House of Lords were known – were for the first time outnumbered by the Lords Temporal.\n\nThe 1539 alliance between Francis and Charles had soured, eventually degenerating into renewed war. With Catherine of Aragon and Anne Boleyn dead, relations between Charles and Henry improved considerably, and Henry concluded a secret alliance with the Emperor and decided to enter the Italian War in favour of his new ally. An invasion of France was planned for 1543. In preparation for it, Henry moved to eliminate the potential threat of Scotland under the youthful James V. The Scots were defeated at Battle of Solway Moss on 24 November 1542, and James died on 15 December. Henry now hoped to unite the crowns of England and Scotland by marrying his son Edward to James' successor, Mary. The Scottish Regent Lord Arran agreed to the marriage in the Treaty of Greenwich on 1 July 1543, but it was rejected by the Parliament of Scotland on 11 December. The result was eight years of war between England and Scotland, a campaign later dubbed \"the Rough Wooing\". Despite several peace treaties, unrest continued in Scotland until Henry's death.\n\nDespite the early success with Scotland, Henry hesitated to invade France, annoying Charles. Henry finally went to France in June 1544 with a two-pronged attack. One force under Norfolk ineffectively besieged Montreuil. The other, under Suffolk, laid siege to Boulogne. Henry later took personal command, and Boulogne fell on 18 September 1544. However, Henry had refused Charles' request to march against Paris. Charles' own campaign fizzled, and he made peace with France that same day. Henry was left alone against France, unable to make peace. Francis attempted to invade England in the summer of 1545, but reached only the Isle of Wight before being repulsed. Out of money, France and England signed the Treaty of Camp on 7 June 1546. Henry secured Boulogne for eight years. The city was then to be returned to France for 2 million crowns (£750,000). Henry needed the money; the 1544 campaign had cost £650,000, and England was once again bankrupt.\n\nHenry married his last wife, the wealthy widow Catherine Parr, in July 1543. A reformer at heart, she argued with Henry over religion. Ultimately, Henry remained committed to an idiosyncratic mixture of Catholicism and Protestantism; the reactionary mood which had gained ground following the fall of Cromwell had neither eliminated his Protestant streak nor been overcome by it. Parr helped reconcile Henry with his daughters, Mary and Elizabeth. In 1543, an Act of Parliament put them back in the line of succession after Edward. The same act allowed Henry to determine further succession to the throne in his will.\n\nLate in life, Henry became obese, with a waist measurement of , and had to be moved about with the help of mechanical inventions. He was covered with painful, pus-filled boils and possibly suffered from gout. His obesity and other medical problems can be traced from the jousting accident in 1536, in which he suffered a leg wound. The accident re-opened and aggravated a previous injury he had sustained years earlier, to the extent that his doctors found it difficult to treat. The wound festered for the remainder of his life and became ulcerated, thus preventing him from maintaining the level of physical activity he had previously enjoyed. The jousting accident is also believed to have caused Henry's mood swings, which may have had a dramatic effect on his personality and temperament.\n\nThe theory that Henry suffered from syphilis has been dismissed by most historians. A more recent theory suggests that Henry's medical symptoms are characteristic of untreated type 2 diabetes. Alternatively, his wives' pattern of pregnancies and his mental deterioration have led some to suggest that the king may have been Kell positive and suffered from McLeod syndrome. According to another study, Henry VIII's history and body morphology may have been the result of traumatic brain injury after his 1536 jousting accident, which in turn led to a neuroendocrine cause of his obesity. This analysis identifies growth hormone deficiency (GHD) as the source for his increased adiposity but also significant behavioural changes noted in his later years, including his multiple marriages.\n\nHenry's obesity hastened his death at the age of 55, which occurred on 28 January 1547 in the Palace of Whitehall, on what would have been his father's 90th birthday. He allegedly uttered his last words: \"Monks! Monks! Monks!\" perhaps in reference to the monks he caused to be evicted during the Dissolution of the Monasteries.\n\nOn 14 February 1547 Henry's coffin lay overnight at Syon Monastery, \"en route\" for burial in St George's Chapel, Windsor Castle. Twelve years before in 1535 a Franciscan friar named William Peyto (or Peto, Petow) (died 1558 or 1559), had preached before the King at Greenwich Palace \"that God's judgements were ready to fall upon his head and that dogs would lick his blood, as they had done to Ahab\", whose infamy rests upon 1 Kings 16:33: \"And Ahab did more to provoke the Lord God of Israel to anger than all the kings of Israel that were before him\". The prophecy was said to have been fulfilled during this night at Syon, when some \"corrupted matter of a bloody colour\" fell from the coffin to the floor.\n\nHenry VIII was interred in St George's Chapel in Windsor Castle, next to Jane Seymour. Over a hundred years later, King Charles I (1625–1649) was buried in the same vault.\n\nUpon Henry's death, he was succeeded by his son Edward VI. Since Edward was then only nine years old, he could not rule directly. Instead, Henry's will designated 16 executors to serve on a council of regency until Edward reached the age of 18. The executors chose Edward Seymour, 1st Earl of Hertford, Jane Seymour's elder brother, to be Lord Protector of the Realm. If Edward died childless, the throne was to pass to Mary, Henry VIII's daughter by Catherine of Aragon, and her heirs. If Mary's issue failed, the crown was to go to Elizabeth, Henry's daughter by Anne Boleyn, and her heirs. Finally, if Elizabeth's line became extinct, the crown was to be inherited by the descendants of Henry VIII's deceased younger sister, Mary, the Greys. The descendants of Henry's sister Margaret – the Stuarts, rulers of Scotland – were thereby excluded from the succession. This final provision failed when James VI of Scotland became King of England in 1603.\n\nHenry cultivated the image of a Renaissance man, and his court was a centre of scholarly and artistic innovation and glamorous excess, epitomised by the Field of the Cloth of Gold. He scouted the country for choirboys, taking some directly from Wolsey's choir, and introduced Renaissance music into court. Musicians included Benedict de Opitiis, Richard Sampson, Ambrose Lupo, and Venetian organist Dionisio Memo.\n\nHenry himself kept a considerable collection of instruments; he was skilled on the lute, could play the organ, and was a talented player of the virginals. He could also sight read music and sing well. He was an accomplished musician, author, and poet; his best known piece of music is \"Pastime with Good Company\" (\"The Kynges Ballade\"). He is often reputed to have written \"Greensleeves\" but probably did not.\nHe was an avid gambler and dice player, and excelled at sports, especially jousting, hunting, and real tennis. He was known for his strong defence of conventional Christian piety. The King was involved in the original construction and improvement of several significant buildings, including Nonsuch Palace, King's College Chapel, Cambridge and Westminster Abbey in London. Many of the existing buildings Henry improved were properties confiscated from Wolsey, such as Christ Church, Oxford; Hampton Court Palace; the Palace of Whitehall; and Trinity College, Cambridge.\n\nHenry was an intellectual. The first English king with a modern humanist education, he read and wrote English, French and Latin, and was thoroughly at home in his well-stocked library. He personally annotated many books and wrote and published one of his own. To promote the public support for the reformation of the church, Henry had numerous pamphlets and lectures prepared. For example, Richard Sampson's \"Oratio\" (1534) was an argument for absolute obedience to the monarchy and claimed that the English church had always been independent from Rome. At the popular level, theatre and minstrel troupes funded by the crown travelled around the land to promote the new religious practices: the pope and Catholic priests and monks were mocked as foreign devils, while the glorious king was hailed as a brave and heroic defender of the true faith. Henry worked hard to present an image of unchallengeable authority and irresistible power.\n\nA large well-built athlete (over tall and strong and broad in proportion), Henry excelled at jousting and hunting. More than pastimes, they were political devices that served multiple goals, from enhancing his athletic royal image to impressing foreign emissaries and rulers, to conveying Henry's ability to suppress any rebellion. Thus he arranged a jousting tournament at Greenwich in 1517, where he wore gilded armour, gilded horse trappings, and outfits of velvet, satin and cloth of gold dripping with pearls and jewels. It suitably impressed foreign ambassadors, one of whom wrote home that, \"The wealth and civilisation of the world are here, and those who call the English barbarians appear to me to render themselves such\". Henry finally retired from jousting in 1536 after a heavy fall from his horse left him unconscious for two hours, but he continued to sponsor two lavish tournaments a year. He then started adding weight and lost the trim, athletic figure that had made him so handsome; Henry's courtiers began dressing in heavily padded clothes to emulate – and flatter – their increasingly stout monarch. Towards the end of his reign his health rapidly declined.\n\nThe power of Tudor monarchs, including Henry, was 'whole' and 'entire', ruling, as they claimed, by the grace of God alone. The crown could also rely on the exclusive use of those functions that constituted the royal prerogative. These included acts of diplomacy (including royal marriages), declarations of war, management of the coinage, the issue of royal pardons and the power to summon and dissolve parliament as and when required. Nevertheless, as evident during Henry's break with Rome, the monarch worked within established limits, whether legal or financial, that forced him to work closely with both the nobility and parliament (representing the gentry).\n\nIn practice, Tudor monarchs used patronage to maintain a royal court that included formal institutions such as the Privy Council as well as more informal advisers and confidants. Both the rise and fall of court nobles could be swift: although the often-quoted figure of 72,000 executions during his reign is inflated, Henry did undoubtedly execute at will, burning or beheading two of his wives, twenty peers, four leading public servants, six close attendants and friends, one cardinal (John Fisher) and numerous abbots. Among those who were in favour at any given point in Henry's reign, one could usually be identified as a chief minister, though one of the enduring debates in the historiography of the period has been the extent to which those chief ministers controlled Henry rather than vice versa. In particular, historian G. R. Elton has argued that one such minister, Thomas Cromwell, led a \"Tudor revolution in government\" quite independent of the king, whom Elton presented as an opportunistic, essentially lazy participant in the nitty-gritty of politics. Where Henry did intervene personally in the running of the country, Elton argued, he mostly did so to its detriment. The prominence and influence of faction in Henry's court is similarly discussed in the context of at least five episodes of Henry's reign, including the downfall of Anne Boleyn.\n\nFrom 1514 to 1529, Thomas Wolsey (1473–1530), a cardinal of the established Church, oversaw domestic and foreign policy for the young king from his position as Lord Chancellor. Wolsey centralised the national government and extended the jurisdiction of the conciliar courts, particularly the Star Chamber. The Star Chamber's overall structure remained unchanged, but Wolsey used it to provide for much-needed reform of the criminal law. The power of the court itself did not outlive Wolsey, however, since no serious administrative reform was undertaken and its role was eventually devolved to the localities. Wolsey helped fill the gap left by Henry's declining participation in government (particularly in comparison to his father) but did so mostly by imposing himself in the King's place. His use of these courts to pursue personal grievances, and particularly to treat delinquents as if mere examples of a whole class worthy of punishment, angered the rich, who were annoyed as well by his enormous wealth and ostentatious living. Following Wolsey's downfall, Henry took full control of his government, although at court numerous complex factions continued to try to ruin and destroy each other.\n\nThomas Cromwell (c. 1485–1540) also came to define Henry's government. Returning to England from the continent in 1514 or 1515, Cromwell soon entered Wolsey's service. He turned to law, also picking up a good knowledge of the Bible, and was admitted to Gray's Inn in 1524. He became Wolsey's \"man of all work\". Cromwell, driven in part by his religious beliefs, attempted to reform the body politic of the English government through discussion and consent, and through the vehicle of continuity and not outward change. He was seen by many people as the man they wanted to bring about their shared aims, including Thomas Audley. By 1531, Cromwell and those associated with him were already responsible for the drafting of much legislation. Cromwell's first office was that of the master of the King's jewels in 1532, from which he began to invigorate the government finances. By this point, Cromwell's power as an efficient administrator in a Council full of politicians exceeded what Wolsey had achieved.\n\nCromwell did much work through his many offices to remove the tasks of government from the Royal Household (and ideologically from the personal body of the King) and into a public state. He did so, however, in a haphazard fashion that left several remnants, not least because he needed to retain Henry's support, his own power, and the possibility of actually achieving the plan he set out. Cromwell made the various income streams put in place by Henry VII more formal and assigned largely autonomous bodies for their administration. The role of the King's Council was transferred to a reformed Privy Council, much smaller and more efficient than its predecessor. A difference emerged between the financial health of the king, and that of the country, although Cromwell's fall undermined much of his bureaucracy, which required his hand to keep order among the many new bodies and prevent profligate spending that strained relations as well as finances. Cromwell's reforms ground to a halt in 1539, the initiative lost, and he failed to secure the passage of an enabling act, the Proclamation by the Crown Act 1539. He too was executed, on 28 July 1540.\n\nHenry inherited a vast fortune and a prosperous economy from his father Henry VII, who had been frugal and careful with money. This fortune was estimated to £1,250,000 (£375 million by today's standards). By comparison, however, the reign of Henry was a near-disaster in financial terms. Although he further augmented his royal treasury through the seizure of church lands, Henry's heavy spending and long periods of mismanagement damaged the economy.\n\nMuch of this wealth was spent by Henry on maintaining his court and household, including many of the building works he undertook on royal palaces. Henry hung 2,000 tapestries in his palaces; by comparison, James V of Scotland hung just 200. Henry took pride in showing off his collection of weapons, which included exotic archery equipment, 2,250 pieces of land ordnance and 6,500 handguns. Tudor monarchs had to fund all the expenses of government out of their own income. This income came from the Crown lands that Henry owned as well as from customs duties like tonnage and poundage, granted by parliament to the king for life. During Henry's reign the revenues of the Crown remained constant (around £100,000), but were eroded by inflation and rising prices brought about by war. Indeed, war and Henry's dynastic ambitions in Europe exhausted the surplus he had inherited from his father by the mid-1520s.\n\nWhereas Henry VII had not involved Parliament in his affairs very much, Henry VIII had to turn to Parliament during his reign for money, in particular for grants of subsidies to fund his wars. The Dissolution of the Monasteries provided a means to replenish the treasury, and as a result the Crown took possession of monastic lands worth £120,000 (£36 million) a year. The Crown had profited a small amount in 1526 when Wolsey had put England onto a gold, rather than silver, standard, and had debased the currency slightly. Cromwell debased the currency more significantly, starting in Ireland in 1540. The English pound halved in value against the Flemish pound between 1540 and 1551 as a result. The nominal profit made was significant, helping to bring income and expenditure together, but it had a catastrophic effect on the overall economy of the country. In part, it helped to bring about a period of very high inflation from 1544 onwards.\n\nHenry is generally credited with initiating the English Reformation – the process of transforming England from a Catholic country to a Protestant one – though his progress at the elite and mass levels is disputed, and the precise narrative not widely agreed. Certainly, in 1527, Henry, until then an observant and well-informed Catholic, appealed to the Pope for an annulment of his marriage to Catherine. No annulment was immediately forthcoming, the result in part of Charles V's control of the Papacy. The traditional narrative gives this refusal as the trigger for Henry's rejection of papal supremacy (which he had previously defended), though as historian A. F. Pollard has argued, even if Henry had not needed an annulment, Henry may have come to reject papal control over the governance of England purely for political reasons.\n\nIn any case, between 1532 and 1537, Henry instituted a number of statutes that dealt with the relationship between king and pope and hence the structure of the nascent Church of England. These included the Statute in Restraint of Appeals (passed 1533), which extended the charge of \"praemunire\" against all who introduced papal bulls into England, potentially exposing them to the death penalty if found guilty. Other acts included the Supplication against the Ordinaries and the Submission of the Clergy, which recognised Royal Supremacy over the church. The Ecclesiastical Appointments Act 1534 required the clergy to elect bishops nominated by the Sovereign. The Act of Supremacy in 1534 declared that the King was \"the only Supreme Head in Earth of the Church of England\" and the Treasons Act 1534 made it high treason, punishable by death, to refuse the Oath of Supremacy acknowledging the King as such. Similarly, following the passage of the Act of Succession 1533, all adults in the Kingdom were required to acknowledge the Act's provisions (declaring Henry's marriage to Anne legitimate and his marriage to Catherine illegitimate) by oath; those who refused were subject to imprisonment for life, and any publisher or printer of any literature alleging that the marriage to Anne was invalid subject to the death penalty. Finally, the Peter's Pence Act was passed, and it reiterated that England had \"no superior under God, but only your Grace\" and that Henry's \"imperial crown\" had been diminished by \"the unreasonable and uncharitable usurpations and exactions\" of the Pope. The King had much support from the Church under Cranmer.\n\nHenry, to Thomas Cromwell's annoyance, insisted on parliamentary time to discuss questions of faith, which he achieved through the Duke of Norfolk. This led to the passing of the Act of Six Articles, whereby six major questions were all answered by asserting the religious orthodoxy, thus restraining the reform movement in England. It was followed by the beginnings of a reformed liturgy and of the Book of Common Prayer, which would take until 1549 to complete. The victory won by religious conservatives did not convert into much change in personnel, however, and Cranmer remained in his position. Overall, the rest of Henry's reign saw a subtle movement away from religious orthodoxy, helped in part by the deaths of prominent figures from before the break with Rome, especially the executions of Thomas More and John Fisher in 1535 for refusing to renounce papal authority. Henry established a new political theology of obedience to the crown that was continued for the next decade. It reflected Martin Luther's new interpretation of the fourth commandment (\"Honour thy father and mother\"), brought to England by William Tyndale. The founding of royal authority on the Ten Commandments was another important shift: reformers within the Church utilised the Commandments' emphasis on faith and the word of God, while conservatives emphasised the need for dedication to God and doing good. The reformers' efforts lay behind the publication of the \"Great Bible\" in 1539 in English. Protestant Reformers still faced persecution, particularly over objections to Henry's annulment. Many fled abroad, including the influential Tyndale, who was eventually executed and his body burned at Henry's behest.\n\nWhen taxes once payable to Rome were transferred to the Crown, Cromwell saw the need to assess the taxable value of the Church's extensive holdings as they stood in 1535. The result was an extensive compendium, the \"Valor Ecclesiasticus\". In September of the same year, Cromwell commissioned a more general visitation of religious institutions, to be undertaken by four appointee visitors. The visitation focussed almost exclusively on the country's religious houses, with largely negative conclusions. In addition to reporting back to Cromwell, the visitors made the lives of the monks more difficult by enforcing strict behavioural standards. The result was to encourage self-dissolution. In any case, the evidence gathered by Cromwell led swiftly to the beginning of the state-enforced dissolution of the monasteries with all religious houses worth less than £200 vested by statute in the crown in January 1536. After a short pause, surviving religious houses were transferred one by one to the Crown and onto new owners, and the dissolution confirmed by a further statute in 1539. By January 1540 no such houses remained: some 800 had been dissolved. The process had been efficient, with minimal resistance, and brought the crown some £90,000 a year. The extent to which the dissolution of all houses was planned from the start is debated by historians; there is some evidence that major houses were originally intended only to be reformed. Cromwell's actions transferred a fifth of England's landed wealth to new hands. The programme was designed primarily to create a landed gentry beholden to the crown, which would use the lands much more efficiently. Although little opposition to the supremacy could be found in England's religious houses, they had links to the international church and were an obstacle to further religious reform.\n\nResponse to the reforms was mixed. The religious houses had been the only support of the impoverished, and the reforms alienated much of the population outside London, helping to provoke the great northern rising of 1536–1537, known as the Pilgrimage of Grace. Elsewhere the changes were accepted and welcomed, and those who clung to Catholic rites kept quiet or moved in secrecy. They would re-emerge during the reign of Henry's daughter Mary (1553–1558).\n\nApart from permanent garrisons at Berwick, Calais, and Carlisle, England's standing army numbered only a few hundred men. This was increased only slightly by Henry. Henry's invasion force of 1513, some 30,000 men, was composed of billmen and longbowmen, at a time when the other European nations were moving to hand guns and pikemen. The difference in capability was at this stage not significant, however, and Henry's forces had new armour and weaponry. They were also supported by battlefield artillery and the war wagon, relatively new innovations, and several large and expensive siege guns. The invasion force of 1544 was similarly well-equipped and organised, although command on the battlefield was laid with the dukes of Suffolk and Norfolk, which in the case of the latter produced disastrous results at Montreuil.\n\nHenry is traditionally cited as one of the founders of the Royal Navy. Technologically, Henry invested in large cannon for his warships, an idea that had taken hold in other countries, to replace the smaller serpentines in use. He also flirted with designing ships personally – although his contribution to larger vessels, if any, is not known, it is believed that he influenced the design of rowbarges and similar galleys. Henry was also responsible for the creation of a permanent navy, with the supporting anchorages and dockyards. Tactically, Henry's reign saw the Navy move away from boarding tactics to employ gunnery instead. The Navy was enlarged up to fifty ships (the \"Mary Rose\" was one of them), and Henry was responsible for the establishment of the \"council for marine causes\" to specifically oversee all the maintenance and operation of the Navy, becoming the basis for the later Admiralty.\n\nHenry's break with Rome incurred the threat of a large-scale French or Spanish invasion. To guard against this, in 1538, he began to build a chain of expensive, state-of-the-art defences, along Britain's southern and eastern coasts from Kent to Cornwall, largely built of material gained from the demolition of the monasteries. These were known as Henry VIII's Device Forts. He also strengthened existing coastal defence fortresses such as Dover Castle and, at Dover, Moat Bulwark and Archcliffe Fort, which he personally visited for a few months to supervise. Wolsey had many years before conducted the censuses required for an overhaul of the system of militia, but no reform resulted. In 1538–39, Cromwell overhauled the shire musters, but his work mainly served to demonstrate how inadequate they were in organisation. The building works, including that at Berwick, along with the reform of the militias and musters, were eventually finished under Queen Mary.\n\nAt the beginning of Henry's reign, Ireland was effectively divided into three zones: the Pale, where English rule was unchallenged; Leinster and Munster, the so-called \"obedient land\" of Anglo-Irish peers; and the Gaelic Connaught and Ulster, with merely nominal English rule. Until 1513, Henry continued the policy of his father, to allow Irish lords to rule in the king's name and accept steep divisions between the communities. However, upon the death of the 8th Earl of Kildare, governor of Ireland, fractious Irish politics combined with a more ambitious Henry to cause trouble. When Thomas Butler, 7th Earl of Ormond died, Henry recognised one successor for Ormond's English, Welsh and Scottish lands, whilst in Ireland another took control. Kildare's successor, the 9th Earl, was replaced as Lord Lieutenant of Ireland by Thomas Howard, Earl of Surrey in 1520. Surrey's ambitious aims were costly, but ineffective; English rule became trapped between winning the Irish lords over with diplomacy, as favoured by Henry and Wolsey, and a sweeping military occupation as proposed by Surrey. Surrey was recalled in 1521, with Piers Butler – one of claimants to the Earldom of Ormond – appointed in his place. Butler proved unable to control opposition, including that of Kildare. Kildare was appointed chief governor in 1524, resuming his dispute with Butler, which had before been in a lull. Meanwhile, the Earl of Desmond, an Anglo-Irish peer, had turned his support to Richard de la Pole as pretender to the English throne; when in 1528 Kildare failed to take suitable actions against him, Kildare was once again removed from his post.\n\nThe Desmond situation was resolved on his death in 1529, which was followed by a period of uncertainty. This was effectively ended with the appointment of Henry FitzRoy, Duke of Richmond and the king's son, as lord lieutenant. Richmond had never before visited Ireland, his appointment a break with past policy. For a time it looked as if peace might be restored with the return of Kildare to Ireland to manage the tribes, but the effect was limited and the Irish parliament soon rendered ineffective. Ireland began to receive the attention of Cromwell, who had supporters of Ormond and Desmond promoted. Kildare, on the other hand, was summoned to London; after some hesitation, he departed for London in 1534, where he would face charges of treason. His son, Thomas, Lord Offaly was more forthright, denouncing the king and leading a \"Catholic crusade\" against the king, who was by this time mired in marital problems. Offaly had the Archbishop of Dublin murdered, and besieged Dublin. Offaly led a mixture of Pale gentry and Irish tribes, although he failed to secure the support of Lord Darcy, a sympathiser, or Charles V. What was effectively a civil war was ended with the intervention of 2,000 English troops – a large army by Irish standards – and the execution of Offaly (his father was already dead) and his uncles.\n\nAlthough the Offaly revolt was followed by a determination to rule Ireland more closely, Henry was wary of drawn-out conflict with the tribes, and a royal commission recommended that the only relationship with the tribes was to be promises of peace, their land protected from English expansion. The man to lead this effort was Sir Antony St Leger, as Lord Deputy of Ireland, who would remain into the post past Henry's death. Until the break with Rome, it was widely believed that Ireland was a Papal possession granted as a mere fiefdom to the English king, so in 1541 Henry asserted England's claim to the Kingdom of Ireland free from the Papal overlordship. This change did, however, also allow a policy of peaceful reconciliation and expansion: the Lords of Ireland would grant their lands to the King, before being returned as fiefdoms. The incentive to comply with Henry's request was an accompanying barony, and thus a right to sit in the Irish House of Lords, which was to run in parallel with England's. The Irish law of the tribes did not suit such an arrangement, because the chieftain did not have the required rights; this made progress tortuous, and the plan was abandoned in 1543, not to be replaced.\n\nThe complexities and sheer scale of Henry's legacy ensured that, in the words of Betteridge and Freeman, \"throughout the centuries [since his death], Henry has been praised and reviled, but he has never been ignored\". A particular focus of modern historiography has been the extent to which the events of Henry's life (including his marriages, foreign policy and religious changes) were the result of his own initiative and, if they were, whether they were the result of opportunism or of a principled undertaking by Henry. The traditional interpretation of those events was provided by historian A.F. Pollard, who in 1902 presented his own, largely positive, view of the king, \"laud[ing him] as the king and statesman who, whatever his personal failings, led England down the road to parliamentary democracy and empire\". Pollard's interpretation, which was broadly comparable to 17th century publications of Lord Herbert of Cherbury and his contemporaries, remained the dominant interpretation of Henry's life until the publication of the doctoral thesis of G. R. Elton in 1953. That thesis, entitled \"The Tudor Revolution in Government\", maintained Pollard's positive interpretation of the Henrician period as a whole, but reinterpreted Henry himself as a follower rather than a leader. For Elton, it was Cromwell and not Henry who undertook the changes in government – Henry was shrewd, but lacked the vision to follow a complex plan through. Henry was little more, in other words, than an \"ego-centric monstrosity\" whose reign \"owed its successes and virtues to better and greater men about him; most of its horrors and failures sprang more directly from [the king]\".\n\nAlthough the central tenets of Elton's thesis have since been questioned, it has consistently provided the starting point for much later work, including that of J. J. Scarisbrick, his student. Scarisbrick largely kept Elton's regard for Cromwell's abilities, but returned agency to Henry, who Scarisbrick considered to have ultimately directed and shaped policy. For Scarisbrick, Henry was a formidable, captivating man who \"wore regality with a splendid conviction\". The effect of endowing Henry with this ability, however, was largely negative in Scarisbrick's eyes: to Scarisbrick the Henrician period was one of upheaval and destruction and those in charge worthy of blame more than praise. Even among more recent biographers, including David Loades, David Starkey and John Guy, there has ultimately been little consensus on the extent to which Henry was responsible for the changes he oversaw or the correct assessment of those he did bring about.\n\nThis lack of clarity about Henry's control over events has contributed to the variation in the qualities ascribed to him: religious conservative or dangerous radical; lover of beauty or brutal destroyer of priceless artefacts; friend and patron or betrayer of those around him; chivalry incarnate or ruthless chauvinist. One traditional approach, favoured by Starkey and others, is to divide Henry's reign into two halves, the first Henry being dominated by positive qualities (politically inclusive, pious, athletic but also intellectual) who presided over a period of stability and calm, and the latter a \"hulking tyrant\" who presided over a period of dramatic, sometimes whimsical, change. Other writers have tried to merge Henry's disparate personality into a single whole; Lacey Baldwin Smith, for example, considered him an egotistical borderline neurotic given to great fits of temper and deep and dangerous suspicions, with a mechanical and conventional, but deeply held piety, and having at best a mediocre intellect.\n\nMany changes were made to the royal style during his reign. Henry originally used the style \"Henry the Eighth, by the Grace of God, King of England, France and Lord of Ireland\". In 1521, pursuant to a grant from Pope Leo X rewarding Henry for his \"Defence of the Seven Sacraments\", the royal style became \"Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith and Lord of Ireland\". Following Henry's excommunication, Pope Paul III rescinded the grant of the title \"Defender of the Faith\", but an Act of Parliament declared that it remained valid; and it continues in royal usage to the present day. Henry's motto was \"Coeur Loyal\" (\"true heart\"), and he had this embroidered on his clothes in the form of a heart symbol and with the word \"loyal\". His emblem was the Tudor rose and the Beaufort portcullis. As king, Henry's arms were the same as those used by his predecessors since Henry IV: \"Quarterly, Azure three fleurs-de-lys Or (for France) and Gules three lions passant guardant in pale Or (for England)\".\n\nIn 1535, Henry added the \"supremacy phrase\" to the royal style, which became \"Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith, Lord of Ireland and of the Church of England in Earth Supreme Head\". In 1536, the phrase \"of the Church of England\" changed to \"of the Church of England and also of Ireland\". In 1541, Henry had the Irish Parliament change the title \"Lord of Ireland\" to \"King of Ireland\" with the Crown of Ireland Act 1542, after being advised that many Irish people regarded the Pope as the true head of their country, with the Lord acting as a mere representative. The reason the Irish regarded the Pope as their overlord was that Ireland had originally been given to King Henry II of England by Pope Adrian IV in the 12th century as a feudal territory under papal overlordship. The meeting of Irish Parliament that proclaimed Henry VIII as King of Ireland was the first meeting attended by the Gaelic Irish chieftains as well as the Anglo-Irish aristocrats. The style \"Henry the Eighth, by the Grace of God, King of England, France and Ireland, Defender of the Faith and of the Church of England and also of Ireland in Earth Supreme Head\" remained in use until the end of Henry's reign.\n\n\n\n\n", "id": "14187", "title": "Henry VIII of England"}
{"url": "https://en.wikipedia.org/wiki?curid=14189", "text": "Haryana\n\nHaryana () is one of the 29 states in India, situated in North India. It was carved out of the former state of East Punjab on 1November 1966 on a linguistic basis. It stands 21st in terms of its area, which is spread about . census of India, the state is eighteenth largest by population with 25,353,081 inhabitants. The city of Chandigarh is its capital while the NCR city of Faridabad is the most populous city of the state.\n\nHaryana is one of the most economically developed regions in South Asia, and its agricultural and manufacturing industries have experienced sustained growth since the 1970s. Since 2000, the state has emerged as the largest recipient of investment per capita in India.\n\nIt is bordered by Punjab and Himachal Pradesh to the north, and by Rajasthan to the west and south. The river Yamuna defines its eastern border with Uttar Pradesh. Haryana surrounds the country's capital Delhi on three sides, forming the northern, western and southern borders of Delhi. Consequently, a large area of south Haryana is included in the National Capital Region for purposes of planning for development.\n\nThe name Haryana is found in the works of the 12th-century AD Apabhramsha writer Vibudh Shridhar (VS 1189–1230).\n\nThe name \"Haryana\" has been derived from the Sanskrit words \"Hari\" (the Hindu god Vishnu) and \"ayana\" (home), meaning \"the Abode of God\". However, scholars such as Muni Lal, Murli Chand Sharma, HA Phadke and Sukhdev Singh Chib believe that the name comes from a compound of the words \"Hari\" (Sanskrit \"Harit\", \"green\") and \"Aranya\" (forest).\n\nVedic state of Brahmavarta is claimed to be located in a new research in south Haryana, where initial Vedic scriptures were composed after the great floods some 10,000 years ago. Manusmriti, a flood time document composed by Manu and Bhrigu is now dated 10,000 years old. Rakhigarhi village in the Hisar district is home to the largest and one of the oldest ancient Indus Valley Civilization sites, dated as over 5,000 years old. Evidence of paved roads, a drainage system, a large-scale rainwater collection storage system, terracotta brick and statue production, and skilled metal working (in both bronze and precious metals) have been uncovered. According to archeologists, Rakhigarhi may be the origin of Harappan civilisation, which arose in the Ghaggar basin in Haryana and gradually and slowly moved to the Indus valley.\n\nAncient idols of Jain Tirthankara (made of bronze and stone) were found in archaeological expeditions in Badli, Bhiwani (Ranila, Charkhi Dadri, Badhara village), Dadri, Gurgaon (Ferozpur Jhirka), Hansi, Hisar (Agroha), Kasan, Nahad, Narnaul, Pehowa, Rewari, Rohad, Rohtak (Asthal-Abohar) and Sonepat in Haryana.\n\nDuring the 1398 Timurid conquests of India, Haryana was the site of countless of appalling massacres, Timur's force of 90,000 soldiers each killed 50 to 100 men, women and children in Haryana, such atrocities include the be-headings of most of the enslaved Indian women after they were used for grinding, cooking and raping by Timur's soldiers before marching onward, causing a massive depopulation of the region.\n\nThe area that is now Haryana has been ruled by major empires of India. Panipat is known for three seminal battles in the history of India. In the First Battle of Panipat (1526), Babur defeated the Lodis. In the Second Battle of Panipat (1556), Akbar defeated the local Haryanvi Hindu Emperor of Delhi, who belonged to Rewari. Hemu, had earlier won 22 battles across India from Punjab to Bengal defeating Mughals and Afghans. Hemu had defeated Akbar's forces twice at Agra and Battle of Delhi in 1556 to become last Hindu Emperor of India with formal Coronation at Purana Quila in Delhi on 7th Oct. 1556. In the Third Battle of Panipat (1761), the Afghan king Ahmad Shah Abdali defeated the Marathas.\n\nHaryana state was formed on 1November 1966. The Indian government set up the Shah Commission under the chairmanship of Justice JC Shah on 23 April 1966 to divide the existing Punjab, India and determine the boundaries of the new state of Haryana after consideration of the languages spoken by the people. The commission delivered its report on 31May 1966 whereby the then-districts of Hisar, Mahendragarh, Gurgaon, Rohtak and Karnal were to be a part of the new state of Haryana. Further, the tehsils of Jind and Narwana in the Sangrur district—along with Naraingarh, Ambala and Jagadhri—were to be included.\n\nThe commission recommended that the tehsil of Kharad, which includes Chandigarh, the state capital of Punjab, should be a part of Haryana. However, only a small portion of Kharad was given to Haryana. The city of Chandigarh was made a union territory, serving as the capital of both Punjab and Haryana.\n\nBhagwat Dayal Sharma became first Chief Minister of Haryana.\n\nHaryana is a landlocked state in northern India. It is between 27°39' to 30°35' N latitude and between 74°28' and 77°36' E longitude. The total geographical area of the state is 4.42 m ha, which is 1.4% of the geographical area of the country. The altitude of Haryana varies between 700 and 3600 ft (200 metres to 1200 metres) above sea level. As per India State of Forest Report, FSI, 2013, the Forest Cover in the state is 1586 km which is 3.59% of the state's geographical area and the Tree Cover in the state is 1282 km which is 2.90% of the geographical area. Thus the Forest and Tree Cover of the Haryana state is 6.49% of its geographical area.\n\nHaryana has four main geographical features.\n\nThe Yamuna flows along the state's eastern boundary while the ancient Sarasvati River is said to have flowed from Yamuna Nagar, but has now disappeared.\n\nHaryana's main seasonal river, the Ghaggar rises in the outer Himalayas, between the Yamuna and the Satluj and enters the state near Pinjore in the Panchkula district. Passing through Ambala and Sirsa, it reaches Bikaner in Rajasthan and runs for before disappearing into the deserts of Rajasthan. Important tributaries include the Chautang and Tangri.\n\nThe seasonal Markanda River is a stream, which in ancient times was known as the Aruna. It originates from the lower Shivalik Hills and enters Haryana west of Ambala. During monsoons, this stream swells into a raging torrent notorious for its devastating power. The surplus water is carried on to the Sanisa Lake where the Markanda joins the Saraswati and later the Ghaggar.\n\nThree other rivulets in and around the Mewat hills, the Indori, Dohan and Kasavati all flow from East to West and once were tributaries of the Drishadwati/Saraswati rivers.\n\nHaryana is extremely hot in summer at around and mild in winter. The hottest months are May and June and the coldest December and January. The climate is arid to semi-arid with average rainfall of 354.5 mm. Around 29% of rainfall is received during the months from July to September, and the remaining rainfall is received during the period from December to February.\n\nThorny, dry, deciduous forest and thorny shrubs can be found all over the state. During the monsoon, a carpet of grass covers the hills. Mulberry, eucalyptus, pine, kikar, shisham and babul are some of the trees found here. The species of fauna found in the state of Haryana include black buck, nilgai, panther, fox, mongoose, jackal and wild dog. More than 450 species of birds are found here.\n\nProtected wildlife areas\nHaryana has two national parks, eight wildlife sanctuaries, two wildlife conservation areas, four animal and bird breeding centers, one deer park and three zoos, all of which are managed by the Haryana Forest Department of the Government of Haryana.\n\nThe state is divided into four divisions for administrative purposes: Ambala, Rohtak, Gurgaon and Hisar. Within these there are 21 districts, 62 sub-divisions, 83 tehsils, 47 sub-tehsils and 126 blocks. Haryana has a total of 154 cities and towns and 6,841 villages.\n\nOn 28 December 2015, the Panchkula district of Haryana was awarded for being the top-performing district in the state under the Digital India campaign. The Common Service Centres (CSCs) have been upgraded in all districts and the number of e-services has now reached 105, which includes application of new water connection, sewer connection, electricity bill collection, ration card member registration, result of HBSE, admit cards for board examinations, online admission form for government colleges, long route booking of buses, admission forms for Kurukshetra University and HUDA plots status inquiry. Haryana has become the first state to implement Aadhaar-enabled birth registration in all the districts.\n\nHaryana Police force is the law enforcement agency of Haryana. It has a cybercrime investigation cell, in Gurgaon's Sector 51.\n\nThe judicial authority is the Punjab and Haryana High Court. It has an e-filing facility.\n\nThe economy of Haryana relies on manufacturing, business process outsourcing, agriculture and retail.\n\nThere are two Agroclimatic zones in Haryana. The North-Western part (also referred as Paddy belt) which is suitable for Rice, Wheat, Vegetables and Temperate Fruits, and the South-Western part (also referred as the Cotton belt or Dry belt) which is suitable for Cotton, Millets, coarse cereals, tropical fruits, exotic vegetables and herbal & medicinal plants. \n\nAs Kharif season cultivation depends on rainfalls & the Northern part receives ample rains, rice is extensively cultivated in this part. Punjab bordering area from Cheeka-Kaithal to Karnal-Kurukshetra is major belt of Basmati rice cultivation & most millers of Basmati rice are present in Karnal-Kurukshetra. The cotton belt which receives less rainfall grows Cotton, however farmers with irrigation still prefer growing Rice. Sirsa, Fatehabad, Hisar & Jind are among major cotton producing areas of Haryana. Southern districts of Bhiwani, Rewari, Jhajjar and Mahendragarh in Haryana which are usually arid are major producer of Millets like Bajra & Jowar. \n\nDuring Rabi season, major crops in Haryana are Wheat & Gram.\n\nSugarcane cultivation is done in parts adjoining the Yamuna river & in some internal pockets where irrigation facility is available.\nThe cultivable area is 3.7 m ha, which is 84% of the geographical area of the state. 3.64 m ha, i.e. 98% of cultivable area, is under cultivation. The gross cropped area of the state is 6.51 m ha and net cropped area is 3.64 m ha with a cropping intensity of 184.91%.\n\n\nThe Haryana and Delhi governments have constructed the international standard Delhi Faridabad Skyway, the first of its kind in North India, to connect Delhi and Faridabad. The Delhi-Agra Expressway (NH-2) that passes through Faridabad is being widened to six lanes from current four lanes. It will further boost Faridabad's connectivity with Delhi.\n\nDelhi Metro Rail Corporation connects Faridabad and Gurgaon with Delhi. Faridabad has the longest metro network in the NCR Region consisting of 9 stations and track length being 14 km.\n\nHaryana has a total road length of . There are 29 national highways with a total length of and many state highways, which have a total length of . The most remote parts of the state are linked with metaled roads. Its modern bus fleet of 3,864 buses covers a distance of 1.15 million km per day, and it was the first state in the country to introduce luxury video coaches.\n\nThe Grand Trunk Road, commonly abbreviated to GT Road, is one of South Asia's oldest and longest major roads. It passes through the districts of Sonipat, Panipat, Karnal, Kurukshetra and Ambala in north Haryana where it enters Delhi and subsequently the industrial town of Faridabad on its way. The state government proposes to construct Express highways and freeways for speedier vehicular traffic. The Kundli-Manesar-Palwal Expressway(KMP) will provide a high-speed link to northern Haryana with its southern districts such as Sonepat, Gurgaon, Jhajjar and Faridabad. The work on the project has already started and was scheduled to be completed by July 2013.\n\nHaryana State has always given high priority to the expansion of electricity infrastructure, as it is one of the most important inputs for the development of the state. Haryana was the first state in the country to achieve 100% rural electrification in 1970 as well as the first in the country to link all villages with all-weather roads and provide safe drinking water facilities throughout the state.\n\nAccording to the 2011 census, Hindus (87.45%) constitute the majority of the state's population with Sikhs (4.91%), Muslims (7.03%) (mainly Meos) being the largest minorities.\n\nMuslims are mainly found in the Mewat and Yamuna Nagar districts, while Sikhs live mostly in the districts adjoining Punjab, Hisar, Sirsa, Jind, Fatehabad, Kaithal, Kurukshetra, Ambala, Narnaul and Panchkula karnal. Haryana has the second largest Sikh population in India after the state of Punjab. In May 2014, the Haryana Government published the \"Haryana Anand Marriages Registration Rules, 2014\", allowing Sikhs to register their marriages under these rules.\n\nAgriculture and related industries have been the backbone of the local economy. Since 2001, the state has witnessed a massive\ninflux of immigrants from across the nation, primarily from Bihar, Bengal, Uttrakhand, Rajasthan, Uttar Pradesh and Nepal. Scheduled Castes form 19.3% of the population.\n\nHaryana's sex ratio (child sex ratio) crossed the mark of 900 and reached 903 in December 2015.\n\nHindi is the sole official language of Haryana and is spoken by the majority of the population (87.31%). Punjabi is given the status of additional official language.\n\nLiteracy rate in Haryana has seen an upward trend and is 76.64 percent as per 2011 population census. Male literacy stands at 85.38 percent, while female literacy is at 66.67 percent. In 2001, the literacy rate in Haryana stood at 67.91 percent of which male and female were 78.49 percent and 55.73 percent literate respectively. , Gurgaon city had the highest literacy rate in Haryana at 86.30% followed by Panchkula at 81.9 per cent and Ambala at 81.7 percent. In terms of districts, Rewari had the highest literacy rate in Haryana at 74%, higher than the national average of 59.5%: male literacy was 79%, and female 67%.\n\nHisar has three universities: Chaudhary Charan Singh Haryana Agricultural University - Asia's largest agricultural university, Guru Jambheshwar University of Science and Technology, Lala Lajpat Rai University of Veterinary & Animal Sciences); several national agricultural and veterinary research centres (National Research Centre on Equines), Central Sheep Breeding Farm, National Institute on Pig Breeding and Research, Northern Region Farm Machinery Training and Testing Institute and Central Institute for Research on Buffaloes (CIRB); and more than 20 colleges including Maharaja Agrasen Medical College, Agroha.\n\nIn 2001–02, there were 11,013 primary schools, 1,918 middle schools, 3,023 high schools and 1,301 senior secondary schools in the state. Haryana Board of School Education, established in September 1969 and shifted to Bhiwani in 1981, conducts public examinations at middle, matriculation, and senior secondary levels twice a year. Over seven lac candidates attend annual examinations in February and March; 150,000 attend supplementary examinations each November. The Board also conducts examinations for Haryana Open School at senior and senior secondary levels twice a year. The Haryana government provides free education to women up to the bachelor's degree level.\n\nUnion Minister Ravi Shankar Prasad announced on 27 February 2016 that National Institute of Electronics and Information Technology (NIELIT) would be set up in Kurukshetra to provide computer training to youth and a Software Technology Park of India (STPI) would be set up in Panchkula’s existing HSIIDC IT Park in Sector 23.\n\nThe Total Fertility Rate of Haryana is 2.3. The Infant Mortality Rate is 41 (SRS 2013) and Maternal Mortality Ratio is 146 (SRS 2010–2012).\n\nHaryana has a statewide network of telecommunication facilities. Haryana Government has its own statewide area network by which all government offices of 21 districts and 127 blocks across the state are connected with each other thus making it the first SWAN of the country. Bharat Sanchar Nigam Limited and most of the leading private sector players (such as Reliance Infocom, Tata Teleservices, Bharti Telecom, Idea Vodafone Essar, Aircel, Uninor and Videocon) have operations in the state. Important areas around Delhi are an integral part of the local Delhi Mobile Telecommunication System. This network system would easily cover major towns like Faridabad and Gurgaon.\n\nElectronic media channels include, MTV, 9XM, Star Group, SET Max, News Time, NDTV 24x7 and Zee Group. The radio stations include All India Radio and other FM stations.\n\nThe major newspapers of Haryana include \"Dainik Bhaskar\", \"Punjab Kesari\", \"Jag Bani\", \"Dainik Jagran\", \"The Tribune\", \"Amar Ujala\", \"Hindustan Times\", \"Dainik Tribune\", \"The Times of India\" and \"Hari-Bhumi\".\n\nHaryana Power Generation Corporation Ltd (HPGCL) is setting up a solar power plant at the site of a defunct thermal power plant in Faridabad. The power generator plans to set up the plant over 151.78 acres near Bata Chowk in the district that generated coal-based energy in the past.\n\nHaryana has produced some of the best Indian players in a variety of sports. The State has an old wrestling tradition, and thus some of the finest wrestlers of India hail from Haryana. These include Sushil Kumar, Yogeshwar Dutt, Sakshi Malik, Vinesh Phogat, Geeta Phogat and Babita Kumari. The non-descript town of Bhiwani in the middle of Haryana has produced several of India's best boxers, such as Vijender Singh, Jitender Kumar, Akhil Kumar and Vikas Krishan Yadav.\n\nIn the 2010 Commonwealth Games at Delhi, 22 out of 38 gold medals that India won came from Haryana. During the 33rd National Games held in Assam in 2007, Haryana stood first in the nation with a medal tally of 80, including 30 gold, 22 silver and 28 bronze medals.\n\nCricket is very popular in Haryana. The 1983 World-Cup-winning captain Kapil Dev is from Haryana. Other notable players from Haryana cricket team include Chetan Sharma, Ajay Jadeja, Amit Mishra and Mohit Sharma and Virender Sehwag. Nahar Singh Stadium was built in Faridabad in the year 1981 for international cricket. This ground has the capacity to hold around 25,000 people as spectators. Tejli Sports Complex is an Ultra-Modern sports complex in Yamuna Nagar. Tau Devi Lal Stadium in Panchkula is a multi-sport complex.\n\nChief Minister of Haryana Manohar Lal Khattar announced the \"Haryana Sports and Physical Fitness Policy\", a policy to support 26 Olympic sports, on 12 January 2015 with the words \"We will develop Haryana as the sports hub of the country.\"\n\nThere are 21 tourism hubs created by Haryana Tourism Corporation Limited, which are located in Ambala, Bhiwani, Faridabad, Fatehabad, Gurgaon, Hisar, Jhajjar, Jind, Kaithal, Karnal, Kurukshetra, Panchkula, Sirsa, Sonipat, Panipat, Rewari, Rohtak, Yamunanagar, Palwal and Mahendergarh.\n\n\n\n\n", "id": "14189", "title": "Haryana"}
{"url": "https://en.wikipedia.org/wiki?curid=14190", "text": "Himachal Pradesh\n\nHimachal Pradesh (; literally \"Snow-laden Province\") is a state of India located in Northern India. It is bordered by Jammu and Kashmir on the north, Punjab and Chandigarh on the west, Haryana on the south-west, Uttarakhand on the south-east and by the Tibet Autonomous Region on the east. The name was coined from Sanskrit \"him\" 'snow' and \"anchal\" 'lap', by Acharya Diwakar Datt Sharma, one of the state's most eminent Sanskrit scholars.\n\nHimachal Pradesh is famous for its natural beauty, hill stations, and temples. Himachal Pradesh has been ranked fifteenth in the list of the highest per capita incomes of Indian states and union territories for year 2013-14. Many perennial rivers flow in the state, and numerous hydroelectric projects set up. Himachal produces surplus hydroelectricity and sells it to other states such as Delhi, Punjab, and Rajasthan. Hydroelectric power projects, tourism, and agriculture form important parts of the state's economy.\n\nThe state has several valleys, and more than 90% of the population living in rural areas. Practically all houses have a toilet and 100% hygiene has been achieved in the state. The villages have good connectivity with roads, public health centres, and now with high-speed broadband.\n\nShimla district has maximum urban population of 25%. It has incorporated environmental protection and tourism development has been aided with a government ban on the use of polyethylene bags, reducing litter, and tobacco products, to aid people's health.\nAccording to a 2005 Transparency International survey, Himachal Pradesh was ranked the second-least corrupt state in the country, after Kerala.\n\nThe history of the area that now constitutes Himachal Pradesh dates to the Indus valley civilisation that flourished between 2250 and 1750 BCE. Tribes such as the Koili, Hali, Dagi, Dhaugri, Dasa, Khasa, Kinnar, and Kirat inhabited the region from the prehistoric era.\n\nDuring the Vedic period, several small republics known as \"Janapada\" existed which were later conquered by the Gupta Empire. After a brief period of supremacy by King Harshavardhana, the region was divided into several local powers headed by chieftains, including some Rajput principalities. These kingdoms enjoyed a large degree of independence and were invaded by Delhi Sultanate a number of times. Mahmud Ghaznavi conquered Kangra at the beginning of the 10th century. Timur and Sikander Lodi also marched through the lower hills of the state and captured a number of forts and fought many battles. Several hill states acknowledged Mughal suzerainty and paid regular tribute to the Mughals.\n\nThe Gurkha people, a martial tribe, came to power in Nepal in the year 1768. They consolidated their military power and began to expand their territory. Gradually, the Gorkhas annexed Sirmour and Shimla. Under the leadership of Amar Singh Thapa, the Gurkha laid siege to Kangra. They managed to defeat Sansar Chand Katoch, the ruler of Kangra, in 1806 with the help of many provincial chiefs. However, the Gurkha could not capture Kangra fort which came under Maharaja Ranjeet Singh in 1809. After the defeat, the Gurkha began to expand towards the south of the state. However, Raja Ram Singh, Raja of Siba State, captured the fort of Siba from the remnants of Lahore Darbar in Samvat 1846, during the First Anglo-Sikh War.\n\nThey came into direct conflict with the British along the \"tarai\" belt after which the British expelled them from the provinces of the Satluj. The British gradually emerged as the paramount power in the region. In the revolt of 1857, or first Indian war of independence, arising from a number of grievances against the British, the people of the hill states were not as politically active as were those in other parts of the country. They and their rulers, with the exception of Bushahr, remained more or less inactive. Some, including the rulers of Chamba, Bilaspur, Bhagal and Dhami, rendered help to the British government during the revolt.\nThe British territories came under the British Crown after Queen Victoria's proclamation of 1858. The states of Chamba, Mandi and Bilaspur made good progress in many fields during the British rule. During World War I, virtually all rulers of the hill states remained loyal and contributed to the British war effort, both in the form of men and materials. Among these were the states of Kangra, Jaswan, Datarpur, Guler, Nurpur, Chamba, Suket, Mandi, and Bilaspur.\n\nAfter independence, the Chief Commissioner's Province of H.P. was organized on 15 April 1948 as a result of integration of 28 petty princely states (including feudal princes and \"zaildars\") in the promontories of the western Himalaya. These were known as the Simla Hills States and four Punjab southern hill states under the Himachal Pradesh (Administration) Order, 1948 under Sections 3 and 4 of the Extra-Provincial Jurisdiction Act, 1947 (later renamed as the Foreign Jurisdiction Act, 1947 vide A.O. of 1950). The State of Bilaspur was merged into Himachal Pradesh on 1 April 1954 by the Himachal Pradesh and Bilaspur (New State) Act, 1954.\n\nHimachal became a part C state on 26 January 1950 with the implementation of the Constitution of India and the Lieutenant Governor was appointed. The Legislative Assembly was elected in 1952. Himachal Pradesh became a union territory on 1 November 1956. Some areas of Punjab State—namely Simla, Kangra, Kulu and Lahul and Spiti Districts, Nalagarh tehsil of Ambala District, Lohara, Amb and Una kanungo circles, some area of Santokhgarh kanungo circle and some other specified area of Una tehsil of Hoshiarpur District, besides some parts of Dhar Kalan Kanungo circle of Pathankot tehsil of Gurdaspur District—were merged with Himachal Pradesh on 1 November 1966 on enactment by Parliament of Punjab Reorganisation Act, 1966. On 18 December 1970, the State of Himachal Pradesh Act was passed by Parliament, and the new state came into being on 25 January 1971. Himachal was the 18th state of the Indian Union.\n\nHimachal is in the western Himalayas. Covering an area of , it is a mountainous state. Most of the state lies on the foothills of the Dhauladhar Range. At 6,816 m Reo Purgyil is the highest mountain peak in the state of Himachal Pradesh.\n\nThe drainage system of Himachal is composed both of rivers and glaciers. Himalayan rivers criss-cross the entire mountain chain.\nHimachal Pradesh provides water to both the Indus and Ganges basins. The drainage systems of the region are the Chandra Bhaga or the Chenab, the Ravi, the Beas, the Sutlej, and the Yamuna. These rivers are perennial and are fed by snow and rainfall. They are protected by an extensive cover of natural vegetation.\n\nDue to extreme variation in elevation, great variation occurs in the climatic conditions of Himachal . The climate varies from hot and subhumid tropical in the southern tracts to, with more elevation, cold, alpine, and glacial in the northern and eastern mountain ranges. The state has areas like Dharamsala that receive very heavy rainfall, as well as those like Lahaul and Spiti that are cold and almost rainless. Broadly, Himachal experiences three seasons: summer, winter, and rainy season. Summer lasts from mid-April till the end of June and most parts become very hot (except in the alpine zone which experiences a mild summer) with the average temperature ranging from . Winter lasts from late November till mid March. Snowfall is common in alpine tracts (generally above i.e. in the higher and trans-Himalayan region).\n\nAccording to 2003 Forest Survey of India report, legally defined forest areas constitute 66.52% of the area of Himachal Pradesh. Vegetation in the state is dictated by elevation and precipitation. The state endows with a high diversity of medicinal and aromatic plants. Lahaul-Spiti region of the state, being a cold desert, supports unique plants of medicinal value including Ferula jaeschkeana, Hyoscyamus Niger, Lancea tibetica, and Saussurea bracteata.\n\nHimachal is also said to be the fruit bowl of the country, with orchards being widespread. Meadows and pastures are also seen clinging to steep slopes. After the winter season, the hillsides and orchards bloom with wild flowers, while gladiolas, carnations, marigolds, roses, chrysanthemums, tulips and lilies are carefully cultivated. The state government is gearing up to make Himachal Pradesh as the flower basket of the world.\n\nHimachal Pradesh has around 463 bird 77 mammalian, 44 reptile and 80 fish species. Great Himalayan National Park, a UNESCO World Heritage Site and Pin Valley National Park are the national Parks located in the state. The state also has 30 wildlife sanctuaries and 3 conservation reserves.\n\nThe Legislative Assembly of Himachal Pradesh has no pre-Constitution history. The State itself is a post-Independence creation. It came into being as a centrally administered territory on 15 April 1948 from the integration of thirty erstwhile princely states.\n\nHimachal Pradesh is governed through a parliamentary system of representative democracy, a feature the state shares with other Indian states. Universal suffrage is granted to residents. The legislature consists of elected members and special office bearers such as the Speaker and the Deputy Speaker who are elected by the members. Assembly meetings are presided over by the Speaker or the Deputy Speaker in the Speaker's absence. The judiciary is composed of the Himachal Pradesh High Court and a system of lower courts. Executive authority is vested in the Council of Ministers headed by the , although the titular head of government is the Governor. The Governor is the head of state appointed by the President of India. The leader of the party or coalition with a majority in the Legislative Assembly is appointed as the Chief Minister by the Governor, and the Council of Ministers are appointed by the Governor on the advice of the Chief Minister. The Council of Ministers reports to the Legislative Assembly. The Assembly is unicameral with 68 Members of the Legislative Assembly (MLA). Terms of office run for 5 years, unless the Assembly is dissolved prior to the completion of the term. Auxiliary authorities known as \"panchayats\", for which local body elections are regularly held, govern local affairs.\n\nIn the assembly elections held in November 2012, the Congress secured an absolute majority. The Congress won 36 of the 68 seats while the BJP won only 26 of the 68 seats. Virbhadra Singh was sworn-in as Himachal Pradesh's Chief Minister for a record sixth term in Shimla on 25 December 2012. Virbhadra Singh who has held the top office in Himachal five times in the past, was administered the oath of office and secrecy by Governor Urmila Singh at an open ceremony at the historic Ridge Maidan in Shimla.\n\nThe state of Himachal Pradesh is divided into 12 districts which are grouped into three divisions, Shimla, Kangra and Mandi. The districts are further divided into 62 subdivisions, 78 blocks and 149 Tehsils.\n\nThe era of planning in Himachal Pradesh started in 1948 along with the rest of India. The first five-year plan allocated 52.7 million to Himachal. More than 50% of this expenditure was incurred on road construction since it was felt that without proper transport facilities, the process of planning and development could not be carried to the people, who mostly lived an isolated existence in faraway areas. Himachal now ranks fourth in per capita income among the states of the Indian Union.\n\nAgriculture contributes over 45% to the net state domestic product. It is the main source of income and employment in Himachal. Over 93% of the population in Himachal depends directly upon agriculture, which provides direct employment to 71% of its people. The main cereals grown are wheat, maize, rice and barley. Apple is the principal cash crop of the state grown principally in the districts of Shimla, Kinnaur, Kullu, Mandi, Chamba and some parts of Sirmaur and Lahaul-Spiti with an average annual production of 5 lakh tonnes and per hectare production of 8 to 10 tonnes. The apple cultivation constitute 49 per cent of the total area under fruit crops and 85% of total fruit production in the state with an estimated economy of 3500 crore. Apples from Himachal are exported to other Indian states and even other countries. In 2011-12, the total area under apple cultivation was 1.04 lakh hectares, increased from 90,347 hectares in 2000-01.\n\nHydropower is also one of the major sources of income generation for the state. The identified Hydroelectric Potential for the state is 27,436 MW in five river basins and annual hydroelectricity production is 8,418 MW.\n\nAs per the current prices, the total GDP was estimated at 254 billion as against 230 billion in the year 2004–05, showing an increase of 10.5%.\n\nLand husbandry initiatives such as the Mid-Himalayan Watershed Development Project, which includes the Himachal Pradesh Reforestation Project (HPRP), the world's largest clean development mechanism (CDM) undertaking, have improved agricultural yields and productivity, and raised rural household incomes.\n\nHimachal has a rich heritage of handicrafts. These include woolen and pashmina shawls, carpets, silver and metal ware, embroidered chappals, grass shoes, Kangra and Gompa style paintings, wood work, horse-hair bangles, wooden and metal utensils and various other house hold items. These aesthetic and tasteful handicrafts declined under competition from machine made goods and also because of lack of marketing facilities. But now the demand for handicrafts has increased within and outside the country.\n\nTourism in Himachal Pradesh is a major contributor to the state's economy and growth. The mountainous state with its diverse and beautiful Himalayan landscapes attracts tourists from all over the world. Hill stations like Shimla, Manali, Dalhousie, Chamba, Dharamsala and Kullu are popular destinations for both domestic and foreign tourists. The state has many important pilgrimage centres with prominent Hindu temples like Naina Devi Temple, Vajreshwari Devi Temple, Jwala Ji Temple, Chintpurni, Chamunda Devi Temple, Baijnath Temple, Bhimakali Temple, Bijli Mahadev, Renuka Lake and Jakhoo Temple. Like Uttarakhand, the state is also referred to as \"\"Dev Bhoomi\"\" (literally meaning \"Abode of Gods\") due to its mention in ancient holy texts and occurrence of large number of historical temples in the state.\n\nThe state is also known for its adventure tourism activities like ice skating in Shimla, paragliding in Bir-billing and Solang valley, rafting in Kullu, skiing in Manali boating in Bilaspur and trekking, horse riding and fishing in different parts in the state. Spiti Valley in Lahaul & Spiti District situated at an altitude of over 3000 metres with its picturesque landscapes is an important destination for adventure seekers. The region also has some of the oldest Buddhist Monasteries in Asia.\n\nThe state is also a famous destination for film shooting. Movies like \"Roja\", \"Henna\", \"Jab We Met\", \"Veer-Zaara\", \"Yeh Jawaani Hai Deewani\" and \"Highway\" have been filmed in Himachal Pradesh.\nHimachal hosted the first Paragliding World Cup in India from 24 October to 31 October in 2015. Venue for paragliding world cup was Bir Billing, which is 70 km from famous tourist town Macleod ganj, located in the heart of Himachal in Kangra District. Bir Billing is the centre for aero sports in Himachal and considered as best for paragliding. Buddhist monasteries, trekking to tribal villages, mountain biking are other activities to do here.\n\nHimachal has three domestic airport in Kangra, Kullu and Shimla districts. The air routes connect the state with Delhi and Chandigarh.\n\nHimachal is famous for its narrow-gauge railways. One is the Kalka-Shimla Railway, a UNESCO World Heritage Site, and another is the Pathankot-Jogindernagar Railway. The total length of these two tracks is . The Kalka-Shimla Railway passes through many tunnels, while the Pathankot–Jogindernagar meanders through a maze of hills and valleys. It also has broad-gauge railway track, which connects Amb (Una district) to Delhi. A survey is being conducted to extend this railway line to Kangra (via Nadaun). Other proposed railways in the state are Baddi-Bilaspur, Dharamsala-Palampur and Bilaspur-Manali-Leh.\n\nRoads are the major mode of transport in the hilly terrains. The state has road network of , including eight National Highways (NH) that constitute and 19 State Highways with a total length of . Some roads get closed during winter and monsoon seasons due to snow and landslides. Hamirpur has the highest road density in the state.\n\nHimachal Pradesh has a total population of 6,864,602 including 3,481,873 males and 3,382,729 females as per the final results of the Census of India 2011. This is only 0.57 per cent of India's total population, recording a growth of 12.81 per cent. The total fertility rate (TFR) per woman is 1.8, one of lowest in India.\n\nIn the census, the state is placed 21st on the population chart, followed by Tripura at 22nd place. Kangra district was top ranked with a population strength of 1,507,223 (21.98%), Mandi district 999,518 (14.58%), Shimla district 813,384 (11.86%), Solan district 576,670 (8.41%), Sirmaur district 530,164 (7.73%), Una district 521,057 (7.60%), Chamba district 518,844 (7.57%), Hamirpur district 454,293 (6.63%), Kullu district 437,474 (6.38%), Bilaspur district 382,056 (5.57%), Kinnaur district 84,298 (1.23%) and Lahaul Spiti 31,528 (0.46%).\nThe life expectancy at birth in Himachal Pradesh is 62.8 years (higher than the national average of 57.7 years) for 1986–1990. The infant mortality rate stood at 40 in 2010, and the crude birth rate has declined from 37.3 in 1971 to 16.9 in 2010, below the national average of 26.5 in 1998. The crude death rate was 6.9 in 2010. Himachal Pradesh's literacy rate almost doubled between 1981 and 2011 (see table to right).\n\nHindi is the official language of Himachal Pradesh while Himachali is spoken by the majority of the population (89.01%) in everyday conversation.English is given the status of an additional official language.\n\nHinduism is the main religion in Himachal Pradesh, which ranks first in India in terms of the proportion of Hindus present within it. More than 95% of the total population belongs to the Hindu faith, the distribution of which is evenly spread throughout the state.\nHimachal Pradesh thus has the one of the highest proportions of Hindu population in India (95.17%).\nOther religions that form a small percentage are Islam, Buddhism and Sikhism. Muslims are mainly concentrated in Sirmaur, Chamba, Kangra and Una districts where they form 1.31-6.27% of the population. The Lahaulis of Lahaul and Spiti region are mainly Buddhists. Sikhs mostly live in towns and cities and constitute 1.16% of the state population. The Buddhists, who constitute 1.15%, are mainly natives and tribals from Lahaul and Spiti, where they form a majority of 62%, and Kinnaur, where they form 21.5%.\n\nHimachal Pradesh was one of the few states that had remained largely untouched by external customs, largely due to its difficult terrain. With the technological advancements, the state has changed very rapidly. Himachal Pradesh is a multireligional, multicultural as well as multilingual state like other Indian states. Some of the most commonly spoken languages are Hindi, Punjabi, Pahari, Dogri, Mandeali, Kangri and Kinnauri. The Hindu communities residing in Himachal include the \"Brahmins\", \"Rajputs\", \"Kannets\", \"Rathis\" and \"Kolis\". There are also tribal population in the state which mainly comprise Gaddis, \"Kinnars\", Gujjars, \"Pangawals\" and \"Lahaulis\".\n\nHimachal is well known for its handicrafts. The carpets, leather works, shawls, Kangra paintings, Chamba rumals, metalware, woodwork and paintings are worth appreciating. Pashmina shawl is one of the products which is highly in demand not only in Himachal but all over the country. Himachali caps are also famous art work of the people.\n\nLocal music and dance reflects the cultural identity of the state. Through their dance and music, they entreat their gods during local festivals and other special occasions.\n\nApart from the fairs and festivals that are celebrated all over India, there are number of other fairs and festivals, including the temple fairs in nearly every region that are of great significance to Himachal Pradesh.\n\nThe day to day food of \"Himachalis\" is very similar to the rest of the north India. They too have lentil, broth, rice, vegetables and bread. As compared to other states in north India non-vegetarian cuisine is more preferred. Some of the specialities of Himachal include \"Mhanee\",\"Madhra\",\"Pateer\", \"Chouck\", \"Bhagjery\" and \"chutney\" of Til.\n\nShimla, the state capital, is home to Asia's only natural ice skating rink.\n\nProminent people associated with Himachal include:\n\nHimachal has been blessed with abundance of resources like forests, rivers and lakes. It's hydro-electric power production is still to be fully utilized.\n\nHimachal forests are known for conifurous trees. Pine, Kail, Devdar, Baan. Rich flora and fauna adds to the beauty of this land. Herbs and medicinal plants amply contribute to many local and national pharmacies. Not so famous Kangra tea is mostly organic and health booster. Himachal honey is also in great demand.\n\nHamirpur District is among the top districts in the country for literacy. Education rates among women are quite encouraging in the state. The standard of education in the state has reached a considerably high level as compared to other states in India with several reputed educational institutes for higher studies.\n\nThe Indian Institute of Technology Mandi, Himachal Pradesh University Shimla, Institute of Himalayan Bioresource Technology (IHBT, CSIR Lab), Palampur, the National Institute of Technology, Hamirpur, \nIndian Institute of Information Technology Una, the Central University Dharamshala, AP Goyal (Alakh Prakash Goyal) Shimla University, the Bahra University (Waknaghat, Solan), the Baddi University of Emerging Sciences and Technologies Baddi, IEC University, Shoolini University of Biotechnology and Management Sciences, Solan, Manav Bharti University Solan, the Jaypee University of Information Technology Waknaghat, Eternal University, Sirmaur & Chitkara University Solan are some of the pioneer universities in the state. CSK Himachal Pradesh Krishi Vishwavidyalya Palampur is one of the most renowned hill agriculture institutes in world. Dr. Yashwant Singh Parmar University of Horticulture and Forestry has earned a unique distinction in India for imparting teaching, research and extension education in horticulture, forestry and allied disciplines. Further, state-run Jawaharlal Nehru Government Engineering College started in 2006 at Sundernagar.\n\nThere are over 10,000 primary schools, 1,000 secondary schools and more than 1,300 high schools in Himachal. The state government has decided to start three major nursing colleges to develop the health system in the state. In meeting the constitutional obligation to make primary education compulsory, Himachal has become the first state in India to make elementary education accessible to every child.\n\nThe state has Indira Gandhi Medical College and Hospital, Homoeopathic Medical College & Hospital, Kumarhatti. Besides that there is Himachal Dental College which is the state's first recognised dental institute.\n\nSource: \"Department of Information and Public Relations.\"\nCensus 2011-\n\nLargest District (km²)\n(1) Lahul and Spiti 13841\n(2) Chamba 6522\n(3) Kinnaur 6401\n(4) Kangra 5739\n(5) Kullu 5503\n\nPercentage of Child\n(1) Chamba 13.55%\n(2) Sirmaur 13.14%\n(3) Solan 11.74%\n(4) Kullu 11.52%\n(5) Una 11.36%\n\nHigh Density\n(1) Hamirpur 407\n(2) Una 338\n(3) Bilaspur 327\n(4) Solan 300\n(5) Kangra 263\n\nTop Population Growth\n(1) Una 16.26%\n(2) Solan 15.93%\n(3) Sirmaur 15.54%\n(4) Kullu 14.76%\n(5) Kangra 12.77%\n\nHigh Literacy\n(1) Hamirpur 89.01%\n(2) Una 87.23%\n(3) Kangra 86.49%\n(4) Blaspur 85.87%\n(5) Solan 85.02%\n\nHigh Sex Ratio\n(1) Hamirpur 2042\n(2) Kangra 1012\n(3) Mandi 1007\n(4) Chamba 986\n(5) Bilaspur 981\n\n\n", "id": "14190", "title": "Himachal Pradesh"}
{"url": "https://en.wikipedia.org/wiki?curid=14192", "text": "Helene\n\nHelene may refer to:\n\n\n", "id": "14192", "title": "Helene"}
{"url": "https://en.wikipedia.org/wiki?curid=14193", "text": "Hyperion\n\nHyperion may refer to:\n\n\n\n\n\n\n\n\n\n\n\n", "id": "14193", "title": "Hyperion"}
{"url": "https://en.wikipedia.org/wiki?curid=14194", "text": "History of medicine\n\nThe history of medicine, as practiced by trained professionals, shows how societies have changed in their approach to illness and disease from ancient times to the present.\n\nEarly medical traditions include those of Babylon, China, Egypt and India. The Greeks went even further, introducing the concepts of medical diagnosis, prognosis, and advanced medical ethics. The Hippocratic Oath, still taken (although significantly changed from the original) by doctors up to today, was written in Greece in the 5th century BCE. In the medieval age, surgical practices inherited from the ancient masters were improved and then systematized in Rogerius's \"The Practice of Surgery\". Universities began systematic training of physicians around the years 1220 in Italy. During the Renaissance, understanding of anatomy improved, and the microscope was invented.\n\nThe germ theory of disease in the 19th century led to cures for many infectious diseases. Military doctors advanced the methods of trauma treatment and surgery. Public health measures were developed especially in the 19th century as the rapid growth of cities required systematic sanitary measures. Advanced research centers opened in the early 20th century, often connected with major hospitals. The mid-20th century was characterized by new biological treatments, such as antibiotics. These advancements, along with developments in chemistry, genetics, and lab technology (such as the x-ray) led to . Medicine was heavily professionalized in the 20th century, and new careers opened to women as nurses (from the 1870s) and as physicians (especially after 1970). The 21st century is characterized by highly advanced research involving numerous fields of science.\n\nAlthough there is no record to establish when plants were first used for medicinal purposes (herbalism), the use of plants as healing agents, as well as clays and soils is ancient. Over time through emulation of the behavior of fauna a medicinal knowledge base developed and passed between generations. As tribal culture specialized specific castes, shamans and apothecaries fulfilled the role of healer.\n\nThe first known dentistry dates to about 7,000 B.C.E. in Baluchistan, where Neolithic dentists used flint-tipped drills and bowstrings.\n\nThe first known trepanning operation was carried out about 5,000 B.C.E. in Ensisheim, France.\n\nThe earliest known surgery, an amputation was carried out about 4,900 B.C.E. in Buthiers-Bulancourt, France.\n\nAncient Egypt developed a large, varied and fruitful medical tradition. Herodotus described the Egyptians as \"the healthiest of all men, next to the Libyans\", because of the dry climate and the notable public health system that they possessed. According to him, \"the practice of medicine is so specialized among them that each physician is a healer of one disease and no more.\" Although Egyptian medicine, to a good extent, dealt with the supernatural, it eventually developed a practical use in the fields of anatomy, public health, and clinical diagnostics.\n\nMedical information in the Edwin Smith Papyrus may date to a time as early as 3000 BC. Imhotep in the 3rd dynasty is sometimes credited with being the founder of ancient Egyptian medicine and with being the original author of the \"Edwin Smith Papyrus\", detailing cures, ailments and anatomical observations. The \"Edwin Smith Papyrus\" is regarded as a copy of several earlier works and was written c. 1600 BC. It is an ancient textbook on surgery almost completely devoid of magical thinking and describes in exquisite detail the \"examination, diagnosis, treatment,\" and \"prognosis\" of numerous ailments.\n\nThe Kahun Gynaecological Papyrus treats women's complaints, including problems with conception. Thirty four cases detailing diagnosis and treatment survive, some of them fragmentarily. Dating to 1800 BCE, it is the oldest surviving medical text of any kind.\n\nMedical institutions, referred to as \"Houses of Life\" are known to have been established in ancient Egypt as early as the 1st Dynasty.\n\nThe earliest known physician is also credited to ancient Egypt: Hesy-Ra, \"Chief of Dentists and Physicians\" for King Djoser in the 27th century BCE. Also, the earliest known woman physician, Peseshet, practiced in Ancient Egypt at the time of the 4th dynasty. Her title was \"Lady Overseer of the Lady Physicians.\" In addition to her supervisory role, Peseshet trained midwives at an ancient Egyptian medical school in Sais.\n\nThe oldest Babylonian texts on medicine date back to the Old Babylonian period in the first half of the 2nd millennium BCE. The most extensive Babylonian medical text, however, is the \"Diagnostic Handbook\" written by the \"ummânū\", or chief scholar, Esagil-kin-apli of Borsippa, during the reign of the Babylonian king Adad-apla-iddina (1069–1046 BCE).\n\nAlong with the Egyptians the Babylonians introduced the practice of diagnosis, prognosis, physical examination, and remedies. In addition, the \"Diagnostic Handbook\" introduced the methods of therapy and etiology. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis.\n\nThe \"Diagnostic Handbook\" was based on a logical set of axioms and assumptions, including the modern view that through the examination and inspection of the symptoms of a patient, it is possible to determine the patient's disease, its aetiology and future development, and the chances of the patient's recovery. The symptoms and diseases of a patient were treated through therapeutic means such as bandages, herbs and creams.\n\nThere was little development after the medieval era. Major European treatises on medicine took 200 years to reach the Middle East, where local rulers might consult Western doctors to get the latest treatments. Medical works in Arabic, Turkish, and Persian as late as 1800 were based on medieval Islamic medicine.\n\nThe Atharvaveda, a sacred text of Hinduism dating from the Early Iron Age, is one of the first Indian text dealing with medicine, like the medicine of the Ancient Near East based on concepts of the exorcism of demons and magic. The Atharvaveda also contain prescriptions of herbs for various ailments. The use of herbs to treat ailments would later form a large part of Ayurveda.\n\nAyurveda, meaning the \"complete knowledge for long life\" is another medical system of India. Its two most famous texts belong to the schools of Charaka and Sushruta. The earliest foundations of Ayurveda were built on a synthesis of traditional herbal practices together with a massive addition of theoretical conceptualizations, new nosologies and new therapies dating from about 600 BCE onwards, and coming out of the communities of thinkers who included the Buddha and others.\n\nAccording to the compendium of Charaka, the Charakasamhitā, health and disease are not predetermined and life may be prolonged by human effort. The compendium of Suśruta, the Suśrutasamhitā defines the purpose of medicine to cure the diseases of the sick, protect the healthy, and to prolong life. Both these ancient compendia include details of the examination, diagnosis, treatment, and prognosis of numerous ailments. The Suśrutasamhitā is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures. Most remarkable is Sushruta's penchant for scientific classification:\nHis medical treatise consists of 184 chapters, 1,120 conditions are listed, including injuries and illnesses relating to aging and mental illness. The Sushruta Samhita describe 125 surgical instrument, 300 surgical procedures and classifies human surgery in 8 categories.\n\nThe Ayurvedic classics mention eight branches of medicine: kāyācikitsā (internal medicine), śalyacikitsā (surgery including anatomy), śālākyacikitsā (eye, ear, nose, and throat diseases), kaumārabhṛtya (pediatrics), bhūtavidyā (spirit medicine), and agada tantra (toxicology), rasāyana\n(science of rejuvenation), and vājīkaraṇa (Aphrodisiac). Apart from learning these, the student of Āyurveda was expected to know ten arts that were indispensable in the preparation and application of his medicines: distillation, operative skills, cooking, horticulture, metallurgy, sugar manufacture, pharmacy, analysis and separation of minerals, compounding of metals, and preparation of alkalis. The teaching of various subjects was done during the instruction of relevant clinical subjects. For example, teaching of anatomy was a part of the teaching of surgery, embryology was a part of training in pediatrics and obstetrics, and the knowledge of physiology and pathology was interwoven in the teaching of all the clinical disciplines.\nThe normal length of the student's training appears to have been seven years. But the physician was to continue to learn.\n\nAs an alternative form of medicine in India, Unani medicine got deep roots and royal patronage during medieval times. It progressed during Indian sultanate and mughal periods. Unani medicine is very close to Ayurveda. Both are based on theory of the presence of the elements (in Unani, they are considered to be fire, water, earth and air) in the human body. According to followers of Unani medicine, these elements are present in different fluids and their balance leads to health and their imbalance leads to illness.\n\nBy the 18th century A.D., Sanskrit medical wisdom still dominated. Muslim rulers built large hospitals in 1595 in Hyderabad, and in Delhi in 1719, and numerous commentaries on ancient texts were written.\n\nChina also developed a large body of traditional medicine. Much of the philosophy of traditional Chinese medicine derived from empirical observations of disease and illness by Taoist physicians and reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. These causative principles, whether material, essential, or mystical, correlate as the expression of the natural order of the universe.\n\nThe foundational text of Chinese medicine is the Huangdi neijing, (or \"Yellow Emperor's Inner Canon\"), written 5th century to 3rd century BCE. Near the end of the 2nd century AD, during the Han dynasty, Zhang Zhongjing, wrote a \"Treatise on Cold Damage\", which contains the earliest known reference to the \"Neijing Suwen\". The Jin Dynasty practitioner and advocate of acupuncture and moxibustion, Huangfu Mi (215-282), also quotes the Yellow Emperor in his \"Jiayi jing\", c. 265. During the Tang Dynasty, the \"Suwen\" was expanded and revised, and is now the best extant representation of the foundational roots of traditional Chinese medicine. Traditional Chinese Medicine that is based on the use of herbal medicine, acupuncture, massage and other forms of therapy has been practiced in China for thousands of years.\n\nIn the 18th century, during the Qing dynasty, there was a proliferation of popular books as well as more advanced encyclopedias on traditional medicine. Jesuit missionaries introduced Western science and medicine to the royal court, the Chinese physicians ignored them.\n\nFinally in the 19th century, Western medicine was introduced at the local level by Christian medical missionaries from the London Missionary Society (Britain), the Methodist Church (Britain) and the Presbyterian Church (USA). Benjamin Hobson (1816–1873) in 1839, set up a highly successful Wai Ai Clinic in Guangzhou, China. The Hong Kong College of Medicine for Chinese was founded in 1887 by the London Missionary Society, with its first graduate (in 1892) being Sun Yat-sen, who later led the Chinese Revolution (1911). The Hong Kong College of Medicine for Chinese was the forerunner of the School of Medicine of the University of Hong Kong, which started in 1911.\n\nBecause of the social custom that men and women should not be near to one another, the women of China were reluctant to be treated by male doctors. The missionaries sent women doctors such as Dr. Mary Hannah Fulton (1854–1927). Supported by the Foreign Missions Board of the Presbyterian Church (USA) she in 1902 founded the first medical college for women in China, the Hackett Medical College for Women, in Guangzhou.\n\nAround 800 BCE Homer in The Iliad gives descriptions of wound treatment by the two sons of Asklepios, the admirable physicians Podaleirius and Machaon and one acting doctor, Patroclus. Because Machaon is wounded and Podaleirius is in combat Eurypylus asks Patroclus to \"cut out this arrow from my thigh, wash off the blood with warm water and spread soothing ointment on the wound\". Asklepios like Imhotep becomes god of healing over time.\n\nTemples dedicated to the healer-god Asclepius, known as \"Asclepieia\" (, sing. , \"'Asclepieion\"), functioned as centers of medical advice, prognosis, and healing. At these shrines, patients would enter a dream-like state of induced sleep known as \"enkoimesis\" () not unlike anesthesia, in which they either received guidance from the deity in a dream or were cured by surgery. Asclepeia provided carefully controlled spaces conducive to healing and fulfilled several of the requirements of institutions created for healing. In the Asclepeion of Epidaurus, three large marble boards dated to 350 BCE preserve the names, case histories, complaints, and cures of about 70 patients who came to the temple with a problem and shed it there. Some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place, but with the patient in a state of enkoimesis induced with the help of soporific substances such as opium. Alcmaeon of Croton wrote on medicine between 500 and 450 BCE. He argued that channels linked the sensory organs to the brain, and it is possible that he discovered one type of channel, the optic nerves, by dissection.\n\nA towering figure in the history of medicine was the physician Hippocrates of Kos (c. 460 – c. 370 BCE), considered the \"father of Western medicine.\" The Hippocratic Corpus is a collection of around seventy early medical works from ancient Greece strongly associated with Hippocrates and his students. Most famously, Hippocrates invented the Hippocratic Oath for physicians, which is still relevant and in use today.\n\nHippocrates and his followers were first to describe many diseases and medical conditions. He is given credit for the first description of clubbing of the fingers, an important diagnostic sign in chronic suppurative lung disease, lung cancer and cyanotic heart disease. For this reason, clubbed fingers are sometimes referred to as \"Hippocratic fingers\". Hippocrates was also the first physician to describe Hippocratic face in \"Prognosis\". Shakespeare famously alludes to this description when writing of Falstaff's death in Act II, Scene iii. of \"Henry V\".\n\nHippocrates began to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, \"exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence.\"\n\nAnother of Hippocrates's major contributions may be found in his descriptions of the symptomatology, physical findings, surgical treatment and prognosis of thoracic empyema, i.e. suppuration of the lining of the chest cavity. His teachings remain relevant to present-day students of pulmonary medicine and surgery. Hippocrates was the first documented person to practise cardiothoracic surgery, and his findings are still valid.\n\nSome of the techniques and theories developed by Hippocrates are now put into practice by the fields of Environmental and Integrative Medicine. These include recognizing the importance of taking a complete history which includes environmental exposures as well as foods eaten by the patient which might play a role in his or her illness.\n\nTwo great Alexandrians laid the foundations for the scientific study of anatomy and physiology, Herophilus of Chalcedon and Erasistratus of Ceos. Other Alexandrian surgeons gave us ligature (hemostasis), lithotomy, hernia operations, ophthalmic surgery, plastic surgery, methods of reduction of dislocations and fractures, tracheotomy, and mandrake as an anaesthetic. Some of what we know of them comes from Celsus and Galen of Pergamum.\n\nHerophilus of Chalcedon, working at the medical school of Alexandria placed intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. He and his contemporary, Erasistratus of Chios, researched the role of veins and nerves, mapping their courses across the body. Erasistratus connected the increased complexity of the surface of the human brain compared to other animals to its superior intelligence. He sometimes employed experiments to further his research, at one time repeatedly weighing a caged bird, and noting its weight loss between feeding times. In Erasistratus' physiology, air enters the body, is then drawn by the lungs into the heart, where it is transformed into vital spirit, and is then pumped by the arteries throughout the body. Some of this vital spirit reaches the brain, where it is transformed into animal spirit, which is then distributed by the nerves.\n\nThe Greek Galen (129–c. 216 CE) was one of the greatest physicians of the ancient world, studying and traveling widely in ancient Rome. He dissected animals to learn about the body, and performed many audacious operations—including brain and eye surgeries— that were not tried again for almost two millennia. In \"Ars medica\" (\"Arts of Medicine\"), he explained mental properties in terms of specific mixtures of the bodily parts.\n\nGalen's medical works were regarded as authoritative until well into the Middle Ages. Galen left a physiological model of the human body that became the mainstay of the medieval physician's university anatomy curriculum, but it suffered greatly from stasis and intellectual stagnation because some of Galen's ideas were incorrect; he did not dissect a human body nor did the medieval lecturers.\n\nThe Renaissance rediscovered Galen. In 1523 Galen's \"On the Natural Faculties\" was published in London. In the 1530s Belgian anatomist and physician Andreas Vesalius launched a project to translate many of Galen's Greek texts into Latin. Vesalius's most famous work, \"De humani corporis fabrica\" was greatly influenced by Galenic writing and form.\n\nThe Romans invented numerous surgical instruments, including the first instruments unique to women, as well as the surgical uses of forceps, scalpels, cautery, cross-bladed scissors, the surgical needle, the sound, and speculas. Romans also performed cataract surgery.\n\nThe Roman army physician Dioscorides (c. 40–90 AD), was a Greek botanist and pharmacologist. He wrote the encyclopedia \"De Materia Medica\" describing over 600 herbal cures, forming an influential pharmacopoeia which was used extensively for the following 1,500 years.\n\nThe Islamic civilization rose to primacy in medical science as its physicians contributed significantly to the field of medicine, including anatomy, ophthalmology, pharmacology, pharmacy, physiology, surgery, and the pharmaceutical sciences. The Arabs were influenced by ancient Indian, Greek, Roman and Byzantine medical practices, and developed these further. Galen & Hippocrates were pre-eminent authorities. The translation of 129 of Galen's works into Arabic by the Nestorian Christian Hunayn ibn Ishaq and his assistants, and in particular Galen's insistence on a rational systematic approach to medicine, set the template for Islamic medicine, which rapidly spread throughout the Arab Empire.\n\nAfter A.D. 400, the study and practice of medicine in the Western Roman Empire went into deep decline. Medical services were provided, especially for the poor, in the thousands of monastic hospitals that sprang up across Europe, but the care was rudimentary and mainly palliative. Most of the writings of Galen and Hippocrates were lost to the West, with the summaries and compendia of St. Isidore of Seville being the primary channel for transmitting Greek medical ideas. The Carolingian renaissance brought increased contact with Byzantium and a greater awareness of ancient medicine, but only with the twelfth century renaissance and the new translations coming from Muslim and Jewish sources in Spain, and the fifteenth century flood of resources after the fall of Constantinople did the West fully recover its acquaintance with classical antiquity.\n\nWallis identifies a prestige hierarchy with university educated physicians on top, followed by learned surgeons; craft-trained surgeons; barber surgeons; itinerant specialists such as dentist and oculists; empirics; and midwives.\n\nThe first medical schools were opened in the 9th century, most notably the Schola Medica Salernitana at Salerno in southern Italy. The cosmopolitan influences from Greek, Latin, Arabic, and Hebrew sources gave it an international reputation as the Hippocratic City. Students from wealthy families came for three years of preliminary studies and five of medical studies. By the thirteenth century the medical school at Montpellier began to eclipse the Salernitan school. In the 12th century universities were founded in Italy, France and England which soon developed schools of medicine. The University of Montpellier in France and Italy's University of Padua and University of Bologna were leading schools. Nearly all the learning was from lectures and readings in Hippocrates, Galen, Avicenna and Aristotle. There was little clinical work or dissection.\n\nThe underlying principle of most medieval medicine was Galen's theory of humours. This was derived from the ancient medical works, and dominated all western medicine until the 19th century. The theory stated that within every individual there were four humours, or principal fluids - black bile, yellow bile, phlegm, and blood, these were produced by various organs in the body, and they had to be in balance for a person to remain healthy. Too much phlegm in the body, for example, caused lung problems; and the body tried to cough up the phlegm to restore a balance. The balance of humours in humans could be achieved by diet, medicines, and by blood-letting, using leeches. The four humours were also associated with the four seasons, black bile-autumn, yellow bile-summer, phlegm-winter and blood-spring.\n\nHealing included both physical and spiritual therapeutics, such as the right herbs, a suitable diet, clean bedding, and the sense that care was always at hand. Other procedures used to help patients included the Mass, prayers, relics of saints, and music used to calm a troubled mind or quickened pulse.\n\nThe Renaissance brought an intense focus on scholarship to Christian Europe. A major effort to translate the Arabic and Greek scientific works into Latin emerged. Europeans gradually became experts not only the ancient writings of the Romans and Greeks, but in the contemporary writings of Islamic scientists. During the later centuries of the Renaissance came an increase in experimental investigation, particularly in the field of dissection and body examination, thus advancing our knowledge of human anatomy.\n\nThe development of modern neurology began in the 16th century with Vesalius, who described the anatomy of the brain and other organs; he had little knowledge of the brain's function, thinking that it resided mainly in the ventricles. Over his lifetime he corrected over 200 of Galen's mistakes. Understanding of medical sciences and diagnosis improved, but with little direct benefit to health care. Few effective drugs existed, beyond opium and quinine. Folklore cures and potentially poisonous metal-based compounds were popular treatments.\nIndependently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the \"Manuscript of Paris\" in 1546, and later published in the theological work which he paid with his life in 1553. Later this was perfected by Renaldus Columbus and Andrea Cesalpino. Later William Harvey correctly described the circulatory system. The most useful tomes in medicine used both by students and expert physicians were \"De Materia Medica\" and Pharmacopoeia.\nBacteria and protists were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field of microbiology.\n\nParacelsus (1493–1541), was an erratic and abusive innovator who rejected Galen and bookish knowledge, calling for experimental research, with heavy doses of mysticism, alchemy and magic mixed in. The point is that he rejected sacred magic (miracles) under Church auspisces and looked for cures in nature. He preached but he also pioneered the use of chemicals and minerals in medicine. His hermetical views were that sickness and health in the body relied on the harmony of man (microcosm) and Nature (macrocosm). He took an approach different from those before him, using this analogy not in the manner of soul-purification but in the manner that humans must have certain balances of minerals in their bodies, and that certain illnesses of the body had chemical remedies that could cure them. Most of his influence came after his death. Paracelsus is a highly controversial figure in the history of medicine, with most experts hailing him as a Father of Modern Medicine for shaking off religious orthodoxy and inspiring many researchers; others say he was a mystic more than a scientist and downplay his importance.\n\nUniversity training of physicians began in the 13th century.\n\nThe University of Padua was founded about 1220 by walkouts from the University of Bologna, and began teaching medicine in 1222. It played a leading role in the identification and treatment of diseases and ailments, specializing in autopsies and the inner workings of the body. Starting in 1595, Padua's famous anatomical theatre drew artists and scientists studying the human body during public dissections. The intensive study of Galen led to critiques of Galen modeled on his own writing, as in the first book of Vesalius's \"De humani corporis fabrica.\" Andreas Vesalius held the chair of Surgery and Anatomy (\"explicator chirurgiae\") and in 1543 published his anatomical discoveries in De Humani Corporis Fabrica. He portrayed the human body as an interdependent system of organ groupings. The book triggered great public interest in dissections and caused many other European cities to establish anatomical theatres.\n\nAt the University of Bologna the training of physicians began in 1219. The Italian city attracted students from across Europe. Taddeo Alderotti built a tradition of medical education that established the characteristic features of Italian learned medicine and was copied by medical schools elsewhere. Turisanus (d. 1320) was his student. The curriculum was revised and strengthened in 1560–1590. A representative professor was Julius Caesar Aranzi (Arantius) (1530–89). He became Professor of Anatomy and Surgery at the University of Bologna in 1556, where he established anatomy as a major branch of medicine for the first time. Aranzi combined anatomy with a description of pathological processes, based largely on his own research, Galen, and the work of his contemporary Italians. Aranzi discovered the 'Nodules of Aranzio' in the semilunar valves of the heart and wrote the first description of the superior levator palpebral and the coracobrachialis muscles. His books (in Latin) covered surgical techniques for many conditions, including hydrocephalus, nasal polyp, goitre and tumours to phimosis, ascites, haemorrhoids, anal abscess and fistulae.\n\nCatholic women played large roles in health and healing in medieval and early modern Europe. A life as a nun was a prestigious role; wealthy families provided dowries for their daughters, and these funded the convents, while the nuns provided free nursing care for the poor.\n\nThe Catholic elites provided hospital services because of their theology of salvation that good works were the route to heaven. The Protestant reformers rejected the notion that rich men could gain God's grace through good works—and thereby escape purgatory—by providing cash endowments to charitable institutions. They also rejected the Catholic idea that the poor patients earned grace and salvation through their suffering. Protestants generally closed all the convents and most of the hospitals, sending women home to become housewives, often against their will. On the other hand, local officials recognized the public value of hospitals, and some were continued in Protestant lands, but without monks or nuns and in the control of local governments.\n\nIn London, the crown allowed two hospitals to continue their charitable work, under nonreligious control of city officials. The convents were all shut down but Harkness finds that women—some of them former nuns—were part of a new system that delivered essential medical services to people outside their family. They were employed by parishes and hospitals, as well as by private families, and provided nursing care as well as some medical, pharmaceutical, and surgical services.\n\nMeanwhile, in Catholic lands such as France, rich families continued to fund convents and monasteries, and enrolled their daughters as nuns who provided free health services to the poor. Nursing was a religious role for the nurse, and there was little call for science.\n\nDuring the Age of Enlightenment, the 18th-century, science was held in high esteem and physicians upgraded their social status by becoming more scientific. The health field was crowded with self-trained barber-surgeons, apothecaries, midwives, drug peddlers, and charlatans.\n\nAcross Europe medical schools relied primarily on lectures and readings. The final year student would have limited clinical experience by trailing the professor through the wards. Laboratory work was uncommon, and dissections were rarely done because of legal restrictions on cadavers. Most schools were small, and only Edinburgh, Scotland, with 11,000 alumni, produced large numbers of graduates.\n\nIn Britain, there were but three small hospitals after 1550. Pelling and Webster estimate that in London in the 1580 to 1600 period, out of a population of nearly 200,000 people, there were about 500 medical practitioners. Nurses and midwives are not included. There were about 50 physicians, 100 licensed surgeons, 100 apothecaries, and 250 additional unlicensed practitioners. In the last category about 25% were women. All across Britain—and indeed all of the world—the vast majority of the people in city, town or countryside depended for medical care on local amateurs with no professional training but with a reputation as wise healers who could diagnose problems and advise sick people what to do—and perhaps set broken bones, pull a tooth, give some traditional herbs or brews or perform a little magic to cure what ailed them.\n\nThe London Dispensary opened in 1696, the first clinic in the British Empire to dispense medicines to poor sick people. The innovation was slow to catch on, but new dispensaries were open in the 1770s. In the colonies, small hospitals opened in Philadelphia in 1752, New York in 1771, and Boston (Massachusetts General Hospital) in 1811.\n\nGuy's Hospital, the first great British hospital opened in 1721 in London, with funding from businessman Thomas Guy. In 1821 a bequest of £200,000 by William Hunt in 1829 funded expansion for an additional hundred beds. Samuel Sharp (1709–78), a surgeon at Guy's Hospital, from 1733 to 1757, was internationally famous; his \"A Treatise on the Operations of Surgery\" (1st ed., 1739), was the first British study focused exclusively on operative technique.\n\nEnglish physician Thomas Percival (1740–1804) wrote a comprehensive system of medical conduct, \"Medical Ethics, or a Code of Institutes and Precepts, Adapted to the Professional Conduct of Physicians and Surgeons\" (1803) that set the standard for many textbooks.\n\nIn the Spanish Empire, the viceregal capital of Mexico City was a site of medical training for physicians and the creation of hospitals. Epidemic disease had decimated indigenous populations starting with the early sixteenth-century Spanish conquest of the Aztec empire, when a black auxiliary in the armed forces of conqueror Hernán Cortés, with an active case of smallpox, set off a virgin land epidemic among indigenous peoples, Spanish allies and enemies alike. Aztec emperor Cuitlahuac died of smallpox. Disease was a significant factor in the Spanish conquest elsewhere as well.\n\nMedical education was instituted at the Royal and Pontifical University of Mexico, and their clientele was urban elites. Male and female \"curanderos\" or lay practitioners, attended to the ills of the popular classes. The Spanish crown began regulating the medical profession just a few years after the conquest, setting up the Royal Tribunal of the Protomedicato, a board for licensing medical personnel in 1527. It became more systematic after 1646, licensing physicians, druggists, surgeons, and bleeders, and requiring a license before public practice. Crown regulation of medical practice became more general in the Spanish empire.\n\nElites and the popular classes alike called on divine intervention in personal and society-wide health crises, such as the epidemic of 1737. The intervention of the Virgin of Guadalupe was depicted in a scene of dead and dying Indians, with elites on their knees praying for her aid. In the late eighteenth century, the crown began implementing secularizing policies on the Iberian peninsula and its overseas empire that sought to control disease more systematically and scientifically.\n\nThe practice of medicine changed in the face of rapid advances in science, as well as new approaches by physicians. Hospital doctors began much more systematic analysis of patients' symptoms in diagnosis. Among the more powerful new techniques were anaesthesia, and the development of both antiseptic and aseptic operating theatres. Actual cures were developed for certain endemic infectious diseases. However the decline in many of the most lethal diseases was more due to improvements in public health and nutrition than to medicine.\n\nMedicine was revolutionized in the 19th century and beyond by advances in chemistry and laboratory techniques and equipment, old ideas of infectious disease epidemiology were replaced with bacteriology and virology.\n\nIn the 1830s in Italy, Agostino Bassi traced the silkworm disease muscardine to microorganisms. Meanwhile, in Germany, Theodor Schwann led researches on alcoholic fermentation by yeast and proposed that they were alive—that is, microorganisms—a claim derided by leading chemists, such as Justus von Liebig, seeking solely physicochemical explanation, and alleging that Schwann's was regressing to vitalism. In 1847 in Vienna, Ignaz Semmelweis (1818–1865), by requiring physicians to clean their hands before attending childbirth, dramatically cut new mothers' death rate due to childbed fever, yet his principles were marginalized and attacked by professional peers.\n\nStarting in 1857 by confirming Schwann's fermentation experiments, Louis Pasteur in France placed his eminent reputation behind the belief that yeast are microorganisms, and closed his paper by indicating that such process might also explain contagious diseases. In 1860, Pasteur's report on bacterial fermention to butyric acid motivated fellow Frenchman Casimir Davaine to establish a similar species, which he called \"bacteridia\", as the pathogen of the disease anthrax, costly to the cattle industry. Yet \"bacteridia\" were found inconsistently and dismissed as a disease byproduct, not cause. British surgeon Joseph Lister, however, already took cue and introduced antisepsis to wound treatment in 1865.\n\nGerman physician Robert Koch, noting fellow German Ferdinand Cohn's report of a spore stage of a certain bacterial species, traced the life cycle of Davaine's \"bacteridia\", identified spores, inoculated laboratory animals with them, and reproduced anthrax—a breakthrough for experimental pathology and germ theory of disease. Pasteur's group added ecological investigations confirming spores' role in the natural setting, while Koch published a landmark treatise in 1878 on the bacterial pathology of wounds. In 1881, Koch reported discovery of the \"tubercle bacillus\", cementing germ theory and Koch's acclaim.\n\nUpon the outbreak of a cholera epidemic in Alexandria, Egypt, two medical missions went to investigate and attend the sick, one was sent out by Pasteur and the other actually led by Koch. Koch's group returned victorious in 1883, having discovered the cholera pathogen. In Germany, however, Koch's bacteriologists had to vie against Max von Pettenkofer, Germany's leading proponent of miasmatic theory. Pettenkofer conceded bacteria's casual involvement, but maintained that other, environmental factors were required to turn it pathogenic, and opposed water treatment as a misdirected effort amid more important ways to improve public health. The massive cholera epidemic in Hamburg in 1892 devastasted Pettenkoffer's position, and yielded German public health to \"Koch's bacteriology\".\n\nOn losing the 1883 rivalry in Alexandria, Pasteur switched research direction, and introduced his third vaccine—rabies vaccine—the first vaccine for humans since Jenner's for smallpox. From across the globe, donations poured in, funding the founding of Pasteur Institute, the globe's first biomedical institute, which opened in 1888. Along with Koch's bacteriologists, Pasteur's group—which preferred the term \"microbiology\"—led medicine into the new era of \"scientific medicine\" upon bacteriology and germ theory. Accepted from Jakob Henle, Koch's steps to confirm a species' pathogenicity became famed as \"Koch's postulates\". Although his proposed tuberculosis treatment, tuberculin, seemingly failed, it soon was used to test for infection with the involved species. In 1905, Koch was awarded the Nobel Prize in Physiology or Medicine, and remains renowned as the founder of medical microbiology.\n\nWomen had always served in ancillary roles, and as midwives and healers. The professionalization of medicine forced them increasingly to the sidelines. As hospitals multiplied they relied in Europe on orders of Roman Catholic nun-nurses, and German Protestant and Anglican deaconesses in the early 19th century. They were trained in traditional methods of physical care that involved little knowledge of medicine. The breakthrough to professionalization based on knowledge of advanced medicine was led by Florence Nightingale in England. She resolved to provide more advanced training than she saw on the Continent. At Kaiserswerth, where the first German nursing schools were founded in 1836 by Theodor Fliedner, she said, \"The nursing was nil and the hygiene horrible.\") Britain's male doctors preferred the old system, but Nightingale won out and her Nightingale Training School opened in 1860 and became a model. The Nightingale solution depended on the patronage of upper class women, and they proved eager to serve. Royalty became involved. In 1902 the wife of the British king took control of the nursing unit of the British army, became its president, and renamed it after herself as the Queen Alexandra's Royal Army Nursing Corps; when she died the next queen became president. Today its Colonel In Chief is the daughter-in-law of Queen Elizabeth. In the United States, upper middle class women who already supported hospitals promoted nursing. The new profession proved highly attractive to women of all backgrounds, and schools of nursing opened in the late 19th century. They soon a function of large hospitals, where they provided a steady stream of low-paid idealistic workers. The International Red Cross began operations in numerous countries in the late 19th century, promoting nursing as an ideal profession for middle class women.\n\nThe Nightingale model was widely copied. Linda Richards (1841 – 1930) studied in London and became the first professionally trained American nurse. She established nursing training programs in the United States and Japan, and created the first system for keeping individual medical records for hospitalized patients. The Russian Orthodox Church sponsored seven orders of nursing sisters in the late 19th century. They ran hospitals, clinics, almshouses, pharmacies, and shelters as well as training schools for nurses. In the Soviet era (1917–1991), with the aristocratic sponsors gone, nursing became a low-prestige occupation based in poorly maintained hospitals.\n\nIt was very difficult for women to become doctors in any field before the 1970s. Elizabeth Blackwell (1821–1910) became the first woman to formally study and practice medicine in the United States. She was a leader in women's medical education. While Blackwell viewed medicine as a means for social and moral reform, her student Mary Putnam Jacobi (1842–1906) focused on curing disease. At a deeper level of disagreement, Blackwell felt that women would succeed in medicine because of their humane female values, but Jacobi believed that women should participate as the equals of men in all medical specialties using identical methods, values and insights. In the Soviet Union although the majority of medical doctors were women, they were paid less than the mostly male factory workers.\n\nParis (France) and Vienna were the two leading medical centers on the Continent in the era 1750–1914.\n\nIn the 1770s-1850s Paris became a world center of medical research and teaching. The \"Paris School\" emphasized that teaching and research should be based in large hospitals and promoted the professionalization of the medical profession and the emphasis on sanitation and public health. A major reformer was Jean-Antoine Chaptal (1756–1832), a physician who was Minister of Internal Affairs. He created the Paris Hospital, health councils, and other bodies.\nLouis Pasteur (1822–1895) was one of the most important founders of medical microbiology. He is remembered for his remarkable breakthroughs in the causes and preventions of diseases. His discoveries reduced mortality from puerperal fever, and he created the first vaccines for rabies and anthrax. His experiments supported the germ theory of disease. He was best known to the general public for inventing a method to treat milk and wine in order to prevent it from causing sickness, a process that came to be called pasteurization. He is regarded as one of the three main founders of microbiology, together with Ferdinand Cohn and Robert Koch. He worked chiefly in Paris and in 1887 founded the Pasteur Institute there to perpetuate his commitment to basic research and its practical applications. As soon as his institute was created, Pasteur brought together scientists with various specialties. The first five departments were directed by Emile Duclaux (general microbiology research) and Charles Chamberland (microbe research applied to hygiene), as well as a biologist, Ilya Ilyich Mechnikov (morphological microbe research) and two physicians, Jacques-Joseph Grancher (rabies) and Emile Roux (technical microbe research). One year after the inauguration of the Institut Pasteur, Roux set up the first course of microbiology ever taught in the world, then entitled \"Cours de Microbie Technique\" (Course of microbe research techniques). It became the model for numerous research centers around the world named \"Pasteur Institutes.\"\n\nThe First Viennese School of Medicine, 1750–1800, was led by the Dutchman Gerard van Swieten (1700–1772), who aimed to put medicine on new scientific foundations - promoting unprejudiced clinical observation, botanical and chemical research, and introducing simple but powerful remedies. When the Vienna General Hospital opened in 1784, it at once became the world's largest hospital and physicians acquired a facility that gradually developed into the most important research centre. Progress ended with the Napoleonic wars and the government shutdown in 1819 of all liberal journals and schools; this caused a general return to traditionalism and eclecticism in medicine.\n\nVienna was the capital of a diverse empire and attracted not just Germans but Czechs, Hungarians, Jews, Poles and others to its world-class medical facilities. After 1820 the Second Viennese School of Medicine emerged with the contributions of physicians such as Carl Freiherr von Rokitansky, Josef Škoda, Ferdinand Ritter von Hebra, and Ignaz Philipp Semmelweis. Basic medical science expanded and specialization advanced. Furthermore, the first dermatology, eye, as well as ear, nose, and throat clinics in the world were founded in Vienna. The textbook of ophthalmologist Georg Joseph Beer (1763–1821) \"Lehre von den Augenkrankheiten\" combined practical research and philosophical speculations, and became the standard reference work for decades.\n\nAfter 1871 Berlin, the capital of the new German Empire, became a leading center for medical research. Robert Koch (1843–1910) was a representative leader. He became famous for isolating \"Bacillus anthracis\" (1877), the \"Tuberculosis bacillus\" (1882) and \"Vibrio cholerae\" (1883) and for his development of Koch's postulates. He was awarded the Nobel Prize in Physiology or Medicine in 1905 for his tuberculosis findings. Koch is one of the founders of microbiology, inspiring such major figures as Paul Ehrlich and Gerhard Domagk.\n\nIn the American Civil War (1861–65), as was typical of the 19th century, more soldiers died of disease than in battle, and even larger numbers were temporarily incapacitated by wounds, disease and accidents. Conditions were poor in the Confederacy, where doctors and medical supplies were in short supply. The war had a dramatic long-term impact on medicine in the U.S., from surgical technique to hospitals to nursing and to research facilities.\n\nThe hygiene of the training and field camps was poor, especially at the beginning of the war when men who had seldom been far from home were brought together for training with thousands of strangers. First came epidemics of the childhood diseases of chicken pox, mumps, whooping cough, and, especially, measles. Operations in the South meant a dangerous and new disease environment, bringing diarrhea, dysentery, typhoid fever, and malaria. There were no antibiotics, so the surgeons prescribed coffee, whiskey, and quinine. Harsh weather, bad water, inadequate shelter in winter quarters, poor policing of camps, and dirty camp hospitals took their toll.\n\nThis was a common scenario in wars from time immemorial, and conditions faced by the Confederate army were even worse. The Union responded by building army hospitals in every state. What was different in the Union was the emergence of skilled, well-funded medical organizers who took proactive action, especially in the much enlarged United States Army Medical Department, and the United States Sanitary Commission, a new private agency. Numerous other new agencies also targeted the medical and morale needs of soldiers, including the United States Christian Commission as well as smaller private agencies.\n\nThe U.S. Army learned many lessons and in August 1886, it established the Hospital Corps.\n\nA major breakthrough in epidemiology came with the introduction of statistical maps and graphs. They allowed careful analysis of seasonality issues in disease incidents, and the maps allowed public health officials to identifical critical loci for the dissemination of disease. John Snow in London developed the methods. In 1849, he observed that the symptoms of cholera, which had already claimed around 500 lives within a month, were vomiting and diahrrea. He concluded that the source of contamination must be through ingestion, rather than inhalation as was previously thought. It was this insight that resulted in the removal of The Pump On Broad Street, after which deaths from cholera plummeted afterwards. English nurse Florence Nightingale pioneered analysis of large amounts of statistical data, using graphs and tables, regarding the condition of thousands of patients in the Crimean War to evaluate the efficacy of hospital services. Her methods proved convincing and led to reforms in military and civilian hospitals, usually with the full support of the government.\n\nBy the late 19th and early 20th century English statisticians led by Francis Galton, Karl Pearson and Ronald Fisher developed the mathematical tools such as correlations and hypothesis tests that made possible much more sophisticated analysis of statistical data.\n\nDuring the U.S. Civil War the Sanitary Commission collected enormous amounts of statistical data, and opened up the problems of storing information for fast access and mechanically searching for data patterns. The pioneer was John Shaw Billings (1838–1913). A senior surgeon in the war, Billings built the Library of the Surgeon General's Office (now the National Library of Medicine, the centerpiece of modern medical information systems. Billings figured out how to mechanically analyze medical and demographic data by turning facts into numbers and punching the numbers onto cardboard cards that could be sorted and counted by machine. The applications were developed by his assistant Herman Hollerith; Hollerith invented the punch card and counter-sorter system that dominated statistical data manipulation until the 1970s. Hollerith's company became International Business Machines (IBM) in 1911.\n\nJohns Hopkins Hospital, founded in 1889, originated several modern medical practices, including residency and rounds.\n\nEuropean ideas of modern medicine were spread widely through the world by medical missionaries, and the dissemination of textbooks. Japanese elites enthusiastically embraced Western medicine after the Meiji Restoration of the 1860s. However they had been prepared by their knowledge of the Dutch and German medicine, for they had some contact with Europe through the Dutch. Highly influential was the 1765 edition of Hendrik van Deventer's pioneer work \"Nieuw Ligt\" (\"A New Light\") on Japanese obstetrics, especially on Katakura Kakuryo's publication in 1799 of \"Sanka Hatsumo\" (\"Enlightenment of Obstetrics\"). A cadre of Japanese physicians began to interact with Dutch doctors, who introduced smallpox vaccinations. By 1820 Japanese ranpô medical practitioners not only translated Dutch medical texts, they integrated their readings with clinical diagnoses. These men became leaders of the modernization of medicine in their country. They broke from Japanese traditions of closed medical fraternities and adopted the European approach of an open community of collaboration based on expertise in the latest scientific methods.\n\nKitasato Shibasaburō (1853–1931) studied bacteriology in Germany under Robert Koch. In 1891 he founded the Institute of Infectious Diseases in Tokyo, which introduced the study of bacteriology to Japan. He and French researcher Alexandre Yersin went to Hong Kong in 1894, where; Kitasato confirmed Yersin's discovery that the bacterium \"Yersinia pestis\" is the agent of the plague. In 1897 he isolates and described the organism that caused dysentery. He became the first dean of medicine at Keio University, and the first president of the Japan Medical Association.\n\nJapanese physicians immediately recognized the values of X-Rays. They were able to purchase the equipment locally from the Shimadzu Company, which developed, manufactured, marketed, and distributed X-Ray machines after 1900. Japan not only adopted German methods of public health in the home islands, but implemented them in its colonies, especially Korea and Taiwan, and after 1931 in Manchuria. A heavy investment in sanitation resulted in a dramatic increase of life expectancy.\n\nUntil the nineteenth century, the care of the insane was largely a communal and family responsibility rather than a medical one. The vast majority of the mentally ill were treated in domestic contexts with only the most unmanageable or burdensome likely to be institutionally confined. This situation was transformed radically from the late eighteenth century as, amid changing cultural conceptions of madness, a new-found optimism in the curability of insanity within the asylum setting emerged. Increasingly, lunacy was perceived less as a physiological condition than as a mental and moral one to which the correct response was persuasion, aimed at inculcating internal restraint, rather than external coercion. This new therapeutic sensibility, referred to as moral treatment, was epitomised in French physician Philippe Pinel's quasi-mythological unchaining of the lunatics of the Bicêtre Hospital in Paris and realised in an institutional setting with the foundation in 1796 of the Quaker-run York Retreat in England.\n\nFrom the early nineteenth century, as lay-led lunacy reform movements gained in influence, ever more state governments in the West extended their authority and responsibility over the mentally ill. Small-scale asylums, conceived as instruments to reshape both the mind and behaviour of the disturbed, proliferated across these regions. By the 1830s, moral treatment, together with the asylum itself, became increasingly medicalised and asylum doctors began to establish a distinct medical identity with the establishment in the 1840s of associations for their members in France, Germany, the United Kingdom and America, together with the founding of medico-psychological journals. Medical optimism in the capacity of the asylum to cure insanity soured by the close of the nineteenth century as the growth of the asylum population far outstripped that of the general population. Processes of long-term institutional segregation, allowing for the psychiatric conceptualisation of the natural course of mental illness, supported the perspective that the insane were a distinct population, subject to mental pathologies stemming from specific medical causes. As degeneration theory grew in influence from the mid-nineteenth century, heredity was seen as the central causal element in chronic mental illness, and, with national asylum systems overcrowded and insanity apparently undergoing an inexorable rise, the focus of psychiatric therapeutics shifted from a concern with treating the individual to maintaining the racial and biological health of national populations.\n\nEmil Kraepelin (1856–1926) introduced new medical categories of mental illness, which eventually came into psychiatric usage despite their basis in behavior rather than pathology or etiology. Shell shock among frontline soldiers exposed to heavy artillery bombardment was first diagnosed by British Army doctors in 1915. By 1916, similar symptoms were also noted in soldiers not exposed to explosive shocks, leading to questions as to whether the disorder was physical or psychiatric. In the 1920s surrealist opposition to psychiatry was expressed in a number of surrealist publications. In the 1930s several controversial medical practices were introduced including inducing seizures (by electroshock, insulin or other drugs) or cutting parts of the brain apart (leucotomy or lobotomy). Both came into widespread use by psychiatry, but there were grave concerns and much opposition on grounds of basic morality, harmful effects, or misuse.\n\nIn the 1950s new psychiatric drugs, notably the antipsychotic chlorpromazine, were designed in laboratories and slowly came into preferred use. Although often accepted as an advance in some ways, there was some opposition, due to serious adverse effects such as tardive dyskinesia. Patients often opposed psychiatry and refused or stopped taking the drugs when not subject to psychiatric control. There was also increasing opposition to the use of psychiatric hospitals, and attempts to move people back into the community on a collaborative user-led group approach (\"therapeutic communities\") not controlled by psychiatry. Campaigns against masturbation were done in the Victorian era and elsewhere. Lobotomy was used until the 1970s to treat schizophrenia. This was denounced by the anti-psychiatric movement in the 1960s and later.\n\nThe ABO blood group system was discovered in 1901, and the Rhesus blood group system in 1937, facilitating blood transfusion.\n\nDuring the 20th century, large-scale wars were attended with medics and mobile hospital units which developed advanced techniques for healing massive injuries and controlling infections rampant in battlefield conditions. During the Mexican Revolution (1910-1920), General Pancho Villa organized hospital trains for wounded soldiers. Boxcars marked \"Servicio Sanitario\" (\"sanitary service\") were re-purposed as surgical operating theaters and areas for recuperation, and staffed by up to 40 Mexican and U.S. physicians. Severely wounded soldiers were shuttled back to base hospitals. Canadian physician Norman Bethune, M.D. developed a mobile blood-transfusion service for frontline operations in the Spanish Civil War (1936-1939), but ironically, he himself died of blood poisoning.\nThousands of scarred troops provided the need for improved prosthetic limbs and expanded techniques in plastic surgery or reconstructive surgery. Those practices were combined to broaden cosmetic surgery and other forms of elective surgery.\n\nDuring the First World War, Alexis Carrel and Henry Dakin developed the Carrel-Dakin method of treating wounds with an irrigation, Dakin's solution, a germicide which helped prevent gangrene.\n\nThe Great War spurred the usage of Roentgen's X-ray, and the electrocardiograph, for the monitoring of internal bodily functions. This was followed in the inter-war period by the development of the first anti-bacterial agents such as the sulpha antibiotics.\n\nPublic health measures became particular important during the 1918 flu pandemic, which killed at least 50 million people around the world. It became an important case study in epidemiology. Bristow shows there was a gendered response of health caregivers to the pandemic in the United States. Male doctors were unable to cure the patients, and they felt like failures. Women nurses also saw their patients die, but they took pride in their success in fulfilling their professional role of caring for, ministering, comforting, and easing the last hours of their patients, and helping the families of the patients cope as well.\n\nFrom 1917 to 1923, the American Red Cross moved into Europe with a battery of long-term child health projects. It built and operated hospitals and clinics, and organized antituberculosis and antityphus campaigns. A high priority involved child health programs such as clinics, better baby shows, playgrounds, fresh air camps, and courses for women on infant hygiene. Hundreds of U.S. doctors, nurses, and welfare professionals administered these programs, which aimed to reform the health of European youth and to reshape European public health and welfare along American lines.\n\nThe advances in medicine made a dramatic difference for Allied troops, while the Germans and especially the Japanese and Chinese suffered from a severe lack of newer medicines, techniques and facilities. Harrison finds that the chances of recovery for a badly wounded British infantryman were as much as 25 times better than in the First World War. The reason was that:\n\nUnethical human subject research, and killing of patients with disabilities, peaked during the Nazi era, with Nazi human experimentation and Aktion T4 during the Holocaust as the most significant examples. Many of the details of these and related events were the focus of the Doctors' Trial. Subsequently, principles of medical ethics, such as the Nuremberg Code, were introduced to prevent a recurrence of such atrocities. After 1937, the Japanese Army established programs of biological warfare in China. In Unit 731, Japanese doctors and research scientists conducted large numbers of vivisections and experiments on human beings, mostly Chinese victims.\n\nStarting in World War II, DDT was used as insecticide to combat insect vectors carrying malaria, which was endemic in most tropical regions of the world. The first goal was to protect soldiers, but it was widely adopted as a public health device. In Liberia, for example, the United States had large military operations during the war and the U.S. Public Health Service began the use of DDT for indoor residual spraying (IRS) and as a larvicide, with the goal of controlling malaria in Monrovia, the Liberian capital. In the early 1950s, the project was expanded to nearby villages. In 1953, the World Health Organization (WHO) launched an antimalaria program in parts of Liberia as a pilot project to determine the feasibility of malaria eradication in tropical Africa. However these projects encountered a spate of difficulties that foreshadowed the general retreat from malaria eradication efforts across tropical Africa by the mid-1960s.\n\nThe World Health Organization was founded in 1948 as a United Nations agency to improve global health. In most of the world, life expectancy has improved since then, and was about 67 years as of 2010, and well above 80 years in some countries. Eradication of infectious diseases is an international effort, and several new vaccines have been developed during the post-war years, against infections such as measles, mumps, several strains of influenza and human papilloma virus. The long-known vaccine against Smallpox finally eradicated the disease in the 1970s, and Rinderpest was wiped out in 2011. Eradication of polio is underway. Tissue culture is important for development of vaccines. Though the early success of antiviral vaccines and antibacterial drugs, antiviral drugs were not introduced until the 1970s. Through the WHO, the international community has developed a response protocol against epidemics, displayed during the SARS epidemic in 2003, the Influenza A virus subtype H5N1 from 2004, the Ebola virus epidemic in West Africa and onwards.\n\nAs infectious diseases have become less lethal, and the most common causes of death in developed countries are now tumors and cardiovascular diseases, these conditions have received increased attention in medical research. Tobacco smoking as a cause of lung cancer was first researched in the 1920s, but was not widely supported by publications until the 1950s. Cancer treatment has been developed with radiotherapy, chemotherapy and surgical oncology.\n\nOral rehydration therapy has been extensively used since the 1970s to treat cholera and other diarrhea-inducing infections.\n\nThe sexual revolution included taboo-breaking research in human sexuality such as the 1948 and 1953 Kinsey reports, invention of hormonal contraception, and the normalization of abortion and homosexuality in many countries. Family planning has promoted a demographic transition in most of the world. With threatening sexually transmitted infections, not least HIV, use of barrier contraception has become imperative. The struggle against HIV has improved antiretroviral treatments, and in the late 2000s (decade), male circumcision was cited to diminish infection risk (see circumcision and HIV).\n\nX-ray imaging was the first kind of medical imaging, and later ultrasonic imaging, CT scanning, MR scanning and other imaging methods became available.\n\nGenetics have advanced with the discovery of the DNA molecule, genetic mapping and gene therapy. Stem cell research took off in the 2000s (decade), with stem cell therapy as a promising method.\n\nEvidence-based medicine is a modern concept, not introduced to literature until the 1990s.\n\nProsthetics have improved. In 1958, Arne Larsson in Sweden became the first patient to depend on an artificial cardiac pacemaker. He died in 2001 at age 86, having outlived its inventor, the surgeon, and 26 pacemakers. Lightweight materials as well as neural prosthetics emerged in the end of the 20th century.\n\nCardiac surgery was revolutionized in the 1948 as open-heart surgery was introduced for the first time since 1925.\n\nIn 1954 Joseph Murray, J. Hartwell Harrison and others accomplished the first kidney transplantation. Transplantations of other organs, such as heart, liver and pancreas, were also introduced during the later 20th century. The first partial face transplant Cwas performed in 2005, and the first full one in 2010. By the end of the 20th century, microtechnology had been used to create tiny robotic devices to assist microsurgery using micro-video and fiber-optic cameras to view internal tissues during surgery with minimally invasive practices.\n\nLaparoscopic surgery was broadly introduced in the 1990s. Natural orifice surgery has followed. Remote surgery is another recent development, with the Lindbergh operation in 2001 as a groundbreaking example.\n\nIn the roaring 1970, all aspects of social, economic and cultural establishment were challenged, and particularly the technocratic approach to medicine was scrutinized, including its polemic terminology such as ‘War on Cancer’, ‘fighting disease’ and ‘Magic Bullets’.\n\nOne of the most influential and disputed critics of medicine was the Austrian philosopher Ivan Illich. In his book \"Medical Nemesis – Limits to Medicine\", first published in 1975, Illich particularly warned against medicalization of society, i.e. that technocratic medicine frequently caused more harm than good and rendered many people in effect lifelong patients. Although Illich frequently choose the confrontation with medical establishment, his criticism was not fully unjust. Since the late 1970s, medical schools started refocusing the training programs of medical doctors and nurses; it was not only important to learn medical facts and technical skills, but increasingly more attention was being paid to social skills, communication with patients and caregivers, and, phrasing it in a way as it was popular in the seventies, to \"an holistic approach of the subject by treating both spirit and body\".\n\nNowadays we would say that caring for the patient became at least as important as curing the patient. Illustrative for how doctors have changed to think is the recent best-seller book \"Being Mortal\" by Atul Gawande. Unlike Illich 40 years earlier, the book received many positive reviews in the established medical journals. Gawande advices caregivers to be frank to the dying patient in discussing limited options, and that refocusing from curing to caring may frequently be the best choice for the old and the dying. Mind you, the author is not a soft-hearted shakra-master from a vague center of mind healing, but he is a hard core surgeon from a famous cancer center where fifty years ago the word Magic Bullet was coined. \n\nIn 1976, the British public health scientist, Thomas McKeown published an essay with the polemic title \"The Role of Modern Medicine: Dream, Mirage or Nemesis?,\" wherein he had collected facts and arguments for what became known as McKeown’s thesis. The McKeown thesis is a chain of reasoning to explain the growth of population. First McKeown made clear that the rise of population since the 18th century was caused by a decline in mortality, and not by a rise in fertility. The decline of mortality could be explained by the decrease in mortality from infectious diseases since the growth of wealth during industrialization. Since particularly in the early time of industrialization, the social circumstances of most people deteriorated, except for the distribution of more and better food, McKeown attributed the steepest decline in mortality to better food. Only at the end of the 19th other public health measures such as clean water, sewage, and other hygienic improvements started to contribute to the improvement of public health and the decline of mortality. Effective personal medical care came only very late in that development, and contributed only very little to the decline of mortality and the rise of population. For instance, the mortality from tuberculosis in the UK since the mid-19th century had already declined by 90 to 95% by the time that an effective antibiotic, streptomycin, entered the market in 1947 and before a nationwide vaccination program was started in 1954. He did not claim that antibiotics and vaccines did not work, but that they were discovered 100 years too late. McKeown has often been criticized for being a medical nihilist, by others, including the economist and Noble prize winner Angus Deaton, McKeown is considered as \"'the founder of social medicine\"'.\n\nThe Scottish physician Archie Cochrane was very critical about the poor quality of medical research, particularly about the low standards of statistical methodology in medical research and public health. He was an adamant promotor of evidence based medicine, and was particularly promoting the randomized controlled trial (RCT) to evaluate the efficacy and efficiency of public health interventions and new medical treatments.\n\n\n\n\n\n\n\n", "id": "14194", "title": "History of medicine"}
{"url": "https://en.wikipedia.org/wiki?curid=14196", "text": "Hamoaze\n\nThe Hamoaze (; ) is an estuarine stretch of the tidal River Tamar, between its confluence with the River Lynher and Plymouth Sound, England.\n\nThe name first appears as \"ryver of Hamose\" in 1588 and it originally most likely applied just to a creek of the estuary that led up to the manor of Ham, north of the present-day Devonport Dockyard. The name evidently later came to be used for the estuary's main channel. The \"ose\" element possibly derives from Old English \"wāse\" meaning 'mud' (as in 'ooze') – the creek consisting of mud-banks at low tide – although this is not confirmed.\n\nThe Hamoaze flows past Devonport Dockyard, which is one of three major bases of the Royal Navy today. The presence of large numbers of small watercraft are a challenge and hazard to the warships using the naval base and dockyard. Navigation on the waterway is controlled by the Queen's Harbour Master for Plymouth.\n\nSettlements on the banks of the Hamoaze are Saltash, Wilcove, Torpoint and Cremyll in Cornwall, as well as Devonport and Plymouth in Devon.\n\nTwo regular ferry services crossing the Hamoaze exist: the Torpoint Ferry (a chain ferry that takes vehicles) and the Cremyll Ferry (passengers and cyclists only).\n", "id": "14196", "title": "Hamoaze"}
{"url": "https://en.wikipedia.org/wiki?curid=14197", "text": "Hanover\n\nHanover or Hannover (; , ), on the River Leine, is the capital and largest city of the German state of Lower Saxony (\"Niedersachsen\"), and was once by personal union the family seat of the Hanoverian Kings of the United Kingdom of Great Britain and Ireland, under their title as the dukes of Brunswick-Lüneburg (later described as the Elector of Hanover). At the end of the Napoleonic Wars, the Electorate was enlarged to become a Kingdom with Hanover as its capital.\n\nFrom 1868 to 1946 Hanover was the capital of the Prussian Province of Hanover and afterwards of the Hanover administrative region until that was abolished in 2005. It is now the capital of the \"Land\" of Lower Saxony. Since 2001 it has been part of the Hanover district (\"Region Hannover\"), which is a municipal body made up from the former district (\"Landkreis Hannover\") and city of Hanover (note: although both \"Region\" and \"Landkreis\" are translated as \"district\" they are not the same).\n\nWith a population of 518,000, Hanover is a major centre of Northern Germany and the country's thirteenth largest city. Hanover also hosts annual commercial trade fairs such as the Hanover Fair and the CeBIT. Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest such festival in Germany. In 2000, Hanover hosted the world fair Expo 2000. The Hanover fairground, due to numerous extensions, especially for the Expo 2000, is the largest in the world. Hanover is of national importance because of its universities and medical school, its international airport and its large zoo. The city is also a major crossing point of railway lines and highways (Autobahnen), connecting European main lines in both the east-west (Berlin–Ruhr area) and north-south (Hamburg–Munich, etc.) directions.\n\n\"Hanover\" is the traditional English spelling. The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopaedias prefer the German spelling, and the local government uses the German spelling on English websites. The English pronunciation , with stress on the first syllable and a reduced second syllable, is applied to both the German and English spellings, which is different from German pronunciation , with stress on the second syllable and a long second vowel. The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover.\n\nHanover was founded in medieval times on the east bank of the River Leine. Its original name \"Honovere\" may mean \"high (river)bank\", though this is debated (cf. \"das Hohe Ufer\"). Hanover was a small village of ferrymen and fishermen that became a comparatively large town in the 13th century due to its position at a natural crossroads. As overland travel was relatively difficult, its position on the upper navigable reaches of the river helped it to grow by increasing trade. It was connected to the Hanseatic League city of Bremen by the Leine, and was situated near the southern edge of the wide North German Plain and north-west of the Harz mountains, so that east-west traffic such as mule trains passed through it. Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia.\n\nIn the 14th century the main churches of Hanover were built, as well as a city wall with three city gates. The beginning of industrialization in Germany led to trade in iron and silver from the northern Harz Mountains, which increased the city's importance.\n\nIn 1636 George, Duke of Brunswick-Lüneburg, ruler of the Brunswick-Lüneburg principality of Calenberg, moved his residence to Hanover. The Dukes of Brunswick-Lüneburg were elevated by the Holy Roman Emperor to the rank of Prince-Elector in 1692, and this elevation was confirmed by the Imperial Diet in 1708. Thus the principality was upgraded to the Electorate of Brunswick-Lüneburg, colloquially known as the Electorate of Hanover after Calenberg's capital (see also: House of Hanover). Its electors would later become monarchs of Great Britain (and from 1801, of the United Kingdom of Great Britain and Ireland). The first of these was George I Louis, who acceded to the British throne in 1714. The last British monarch who ruled in Hanover was William IV. Semi-Salic law, which required succession by the male line if possible, forbade the accession of Queen Victoria in Hanover. As a male-line descendant of George I, Queen Victoria was herself a member of the House of Hanover. Her descendants, however, bore her husband's titular name of Saxe-Coburg-Gotha. Three kings of Great Britain, or the United Kingdom, were concurrently also Electoral Princes of Hanover.\n\nDuring the time of the personal union of the crowns of the United Kingdom and Hanover (1714–1837), the monarchs rarely visited the city. In fact, during the reigns of the final three joint rulers (1760–1837), there was only one short visit, by George IV in 1821. From 1816 to 1837 Viceroy Adolphus represented the monarch in Hanover.\n\nDuring the Seven Years' War, the Battle of Hastenbeck was fought near the city on 26 July 1757. The French army defeated the Hanoverian Army of Observation, leading to the city's occupation as part of the Invasion of Hanover. It was recaptured by Anglo-German forces led by Ferdinand of Brunswick the following year.\n\nAfter Napoleon imposed the Convention of Artlenburg (Convention of the Elbe) on July 5, 1803, about 35,000 French soldiers occupied Hanover. The Convention also required disbanding the army of Hanover. However, George III did not recognize the Convention of the Elbe. This resulted in a great number of soldiers from Hanover eventually emigrating to Great Britain, where the King's German Legion was formed. It was the only German arm to fly against France throughout the entire Napoleonic wars. The Legion later played an important role in the Battle of Waterloo in 1815. The Congress of Vienna in 1815 elevated the electorate to the Kingdom of Hanover. The capital town Hanover expanded to the western bank of the Leine and since then has grown considerably.\n\nIn 1827, the personal union of the United Kingdom and Hanover ended because William IV's heir in the United Kingdom was female (Queen Victoria). Hanover could be inherited only by male heirs. Thus, Hanover passed to William IV's brother, Ernest Augustus, and remained a kingdom until 1866, when it was annexed by Prussia during the Austro-Prussian war. Despite Hanover being expected to defeat Prussia at the Battle of Langensalza, Prussia employed Moltke the Elder's Kesselschlacht order of battle to instead destroy the Hanoverian army. The city of Hanover became the capital of the Prussian Province of Hanover. After the annexation, the people of Hanover generally opposed the Prussian government.\n\nHanover's industry, however, the new connection with Prussia meant an improvement in business. The introduction of free trade promoted economic growth, and led to the recovery of the Gründerzeit (the founders' era). Between 1879 and 1902 Hanover's population grew from 87,600 to 313,940. \n\nIn 1842 the first horse railway was inaugurated, and from 1893 an electric tram was installed. In 1887 Hanover's Emile Berliner invented the record and the gramophone.\n\nAfter 1937 the Lord Mayor and the state commissioners of Hanover were members of the NSDAP (Nazi party). A large Jewish population then existed in Hanover. In October 1938, 484 Hanoverian Jews of Polish origin were expelled to Poland, including the Grynszpan family. However, Poland refused to accept them, leaving them stranded at the border with thousands of other Polish-Jewish deportees, fed only intermittently by the Polish Red Cross and Jewish welfare organisations. The Gryszpan's son Herschel Grynszpan was in Paris at the time. When he learned of what was happening, he drove to the German embassy in Paris and shot the German diplomat Eduard Ernst vom Rath, who died shortly afterwards.\n\nThe Nazis took this act as a pretext to stage a nationwide pogrom known as Kristallnacht. It was in Hanover on 9 November 1938 that the synagogue, designed in 1870 by Edwin Oppler in neo-romantic style, was burnt by the Nazis.\n\nIn September 1941, through the \"Action Lauterbacher\" plan, a ghettoisation of the remaining Hanoverian Jewish families began. Even before the Wannsee Conference, on 15 December 1941, the first Jews from Hanover were deported to Riga. A total of 2,400 people were deported, and very few survived. During the war seven concentration camps were constructed in Hanover, in which many Jews were confined. Of the approximately 4,800 Jews who had lived in Hannover in 1938, fewer than 100 were still in the city when troops of the United States Army arrived on 10 April 1945 to occupy Hanover at the end of the war. Today, a memorial at the Opera Square is a reminder of the persecution of the Jews in Hanover. \nAfter the war a large group of Orthodox Jewish survivors of the nearby Bergen-Belsen concentration camp settled in Hanover.\nAs an important railroad and road junction and production center, Hanover was a major target for strategic bombing during World War II, including the Oil Campaign. Targets included the AFA (Stöcken), the Deurag-Nerag refinery (Misburg), the Continental plants (Vahrenwald and Limmer), the United light metal works (VLW) in Ricklingen and Laatzen (today Hanover fairground), the Hanover/Limmer rubber reclamation plant, the Hanomag factory (Linden) and the tank factory \"M.N.H. Maschinenfabrik Niedersachsen\" (Badenstedt). Forced labourers were sometimes used from the Hannover-Misburg subcamp of the Neuengamme concentration camp. Residential areas were also targeted, and more than 6,000 civilians were killed by the Allied bombing raids. More than 90% of the city center was destroyed in a total of 88 bombing raids. After the war, the was not rebuilt and its ruins were left as a war memorial.\n\nThe Allied ground advance into Germany reached Hanover in April 1945. The US 84th Infantry Division captured the city on 10 April 1945.\n\nHanover was in the British zone of occupation of Germany, and became part of the new state (Land) of Lower Saxony in 1946.\n\nToday Hanover is a Vice-President City of Mayors for Peace, an international mayoral organisation mobilising cities and citizens worldwide to abolish and eliminate nuclear weapons by the year 2020.\n\nHanover experiences an oceanic climate (Köppen climate classification \"Cfb\").\n\n\n\nOne of the most famous sights is the \"Royal Gardens of Herrenhausen\":\n\nThe \"Great Garden\" is an important European baroque garden. The palace itself, however, was largely destroyed by Allied bombing but is currently under reconstruction. Some points of interest are the \"Grotto\" (the interior was designed by the French artist Niki de Saint-Phalle), the \"Gallery Building\", the \"Orangerie\" and the two pavilions by Remy de la Fosse. The Great Garden consists of several parts. The most popular ones are the \"Great Ground\" and the \"Nouveau Jardin\". At the centre of the Nouveau Jardin is Europe's highest garden fountain. The historic \"Garden Theatre\" \"inter alia\" hosted the musicals of the German rock musician Heinz Rudolf Kunze.\n\nThe \"Berggarten\" is an important European botanical garden. Some points of interest are the \"Tropical House\", the \"Cactus House\", the \"Canary House\" and the \"Orchid House\", which hosts one of the world's biggest collection of orchids, and free-flying birds and butterflies. Near the entrance to the Berggarten is the historic \"Library Pavillon\". The \"Mausoleum\" of the Guelphs is also located in the Berggarten. Like the Great Garden, the Berggarten also consists of several parts, for example the \"Paradies\" and the \"Prairie Garden\". There is also the \"Sea Life Centre Hanover\", which is the first tropical aquarium in Germany.\n\nThe \"Georgengarten\" is an English landscape garden. The \"Leibniz Temple\" and the \"Georgen Palace\" are two points of interest there.\n\nThe landmark of Hanover is the New Town Hall (\"Neues Rathaus\"). Inside the building are four scale models of the city. A worldwide unique diagonal/arch elevator goes up the large dome to an observation deck.\n\nThe \"Hanover Zoo\" is one of the most spectacular and best zoos in Europe. The zoo received the Park Scout Award for the fourth year running in 2009/10, placing it among the best zoos in Germany.\nThe zoo consists of several theme areas: Sambesi, Meyers Farm, Gorilla-Mountain, Jungle-Palace, and Mullewapp. Some smaller areas are Australia, the wooded area for wolves, and the so-called swimming area with many seabirds. There is also a tropical house, a jungle house, and a show arena. The new Canadian-themed area, Yukon Bay, opened in 2010. In 2010 the Hanover Zoo had over 1.6 million visitors.\n\nAnother point of interest is the \"Old Town\". In the centre are the large Marktkirche (Church St. Georgii et Jacobi, preaching venue of the bishop of the Lutheran Landeskirche Hannovers) and the \"Old Town Hall\". Nearby are the \"Leibniz House\", the \"Nolte House\", and the \"Beguine Tower\". A very nice quarter of the Old Town is the \"Kreuz-Church-Quarter\" around the \"Kreuz Church\" with many nice little lanes. Nearby is the old royal sports hall, now called the \"Ballhof\" theatre. On the edge of the Old Town are the \"Market Hall\", the \"Leine Palace\", and the ruin of the \"Aegidien Church\" which is now a monument to the victims of war and violence. Through the \"Marstall Gate\" you arrive at the bank of the river \"Leine\", where the world-famous \"Nanas\" of Niki de Saint-Phalle are located. They are part of the \"Mile of Sculptures\", which starts from Trammplatz, leads along the river bank, crosses Königsworther Square, and ends at the entrance of the Georgengarten. Near the Old Town is the district of Calenberger Neustadt where the Catholic Basilica Minor of \"St. Clemens\", the \"Reformed Church\" and the Lutheran Neustädter Hof- und Stadtkirche St. Johannis are located.\n\nSome other popular sights are the \"Waterloo Column\", the \"Laves House\", the \"Wangenheim Palace\", the \"Lower Saxony State Archives\", the \"Hanover Playhouse\", the \"Kröpcke Clock\", the \"Anzeiger Tower Block\", the \"Administration Building of the NORD/LB\", the \"Cupola Hall\" of the Congress Centre, the \"Lower Saxony Stock\", the \"Ministry of Finance\", the \"Garten Church\", the \"Luther Church\", the \"Gehry Tower\" (designed by the American architect Frank O. Gehry), the specially designed \"Bus Stops\", the \"Opera House\", \"the Central Station\", the \"Maschsee\" lake and the city forest \"Eilenriede\", which is one of the largest of its kind in Europe. With around 40 parks, forests and gardens, a couple of lakes, two rivers and one canal, Hanover offers a large variety of leisure activities.\n\nSince 2007 the historic \"Leibniz Letters\", which can be viewed in the \"Gottfried Wilhelm Leibniz Library\", are on UNESCO's Memory of the World Register.\n\nOutside the city centre is the \"EXPO-Park\", the former site of EXPO 2000. Some points of interest are the \"Planet M.\", the former \"German Pavillon\", some nations' vacant pavilions, the \"Expowale\", the \"EXPO-Plaza\" and the \"EXPO-Gardens\" (Parc Agricole, EXPO-Park South and the Gardens of change). The fairground can be reached by the \"Exponale\", one of the largest pedestrian bridges in Europe.\n\nThe \"Hanover fairground\" is the largest Exhibition Centre in the world.\nIt provides 496,000 square metres of covered indoor space, 58,000 square metres of open-air space, 27 halls and pavilions. Many of the Exhibition Centre's halls are architectural highlights. Furthermore, it offers the Convention Center with its 35 function rooms, glassed-in areas between halls, grassy park-like recreation zones and its own heliport.\n\nTwo important sights on the fairground are the \"Hermes Tower\" (88.8 metres high) and the \"EXPO Roof\", the largest wooden roof in the world.\n\nIn the district of Anderten is the \"European Cheese Centre\", the only Cheese Experience Centre in Europe. Another tourist sight in Anderten is the \"Hindenburg Lock\", which was the biggest lock in Europe at the time of its construction in 1928. The \"Tiergarten\" (literally the \"animals' garden\") in the district of Kirchrode is a large forest originally used for deer and other game for the king's table.\n\nIn the district of Groß-Buchholz the 282-metre-high \"Telemax\" is located, which is the tallest building in Lower Saxony and the highest television tower in Northern Germany. Some other notable towers are the \"VW-Tower\" in the city centre and the old towers of the former middle-age defence belt: \"Döhrener Tower\", \"Lister Tower\" and the \"Horse Tower\".\n\nThe 36 most important sights of the city centre are connected with a -long red line, which is painted on the pavement. This so-called \"Red Thread\" marks out a walk that starts at the Tourist Information Office and ends on the Ernst-August-Square in front of the central station. There is also a guided sightseeing-bus tour through the city.\n\nThe \"Historic Museum\" describes the history of Hanover, from the medieval settlement \"Honovere\" to the world-famous Exhibition City of today. The museum focuses on the period from 1714 to 1834 when Hanover had a strong relationship with the British royal house.\n\nWith more than 4,000 members, the \"Kestnergesellschaft\" is the largest art society in Germany. The museum hosts exhibitions from classical modernist art to contemporary art. One big focus is put on film, video, contemporary music and architecture, room installments and big presentations of contemporary paintings, sculptures and video art.\n\nThe \"Kestner-Museum\" is located in the \"House of 5.000 windows\". The museum is named after August Kestner and exhibits 6,000 years of applied art in four areas: Ancient cultures, ancient Egypt, applied art and a valuable collection of historic coins.\n\nThe \"KUBUS\" is a forum for contemporary art. It features mostly exhibitions and projects of famous and important artists from Hanover.\n\nThe \"Kunstverein Hannover\" (Art Society Hanover) shows contemporary art and was established in 1832 as one of the first art societies in Germany. It is located in the \"Künstlerhaus\" (House of artists). There are around 7 international monografic and thematic Exhibitions in one year.\n\nThe \"Lower Saxony State Museum\" is the largest museum in Hanover. The \"State Gallery\" shows the European Art from the 11th to the 20th century, the \"Nature Department\" shows the zoology, geology, botanic, geology and a \"Vivarium\" with fishes, insects, reptiles and amphibians. The \"Primeval Department\" shows the primeval history of Lower Saxony and the \"Folklore Department\" shows the cultures from all over the world.\n\nThe \"Sprengel Museum\" shows the art of the 20th century. It is one of the most notable art museums in Germany. The focus is put on the classical modernist art with the collection of \"Kurt Schwitters\", works of German expressionism, and French cubism, the cabinet of abstracts, the graphics and the department of photography and media. Furthermore, the museum shows the famous works of the French artist Niki de Saint-Phalle.\n\nThe \"Theatre Museum\" shows an exhibition of the history of the theatre in Hanover from the 17th century up to now: opera, concert, drama and ballet. The museum also hosts several touring exhibitions during the year.\n\nThe \"Wilhelm Busch Museum\" is the \"German Museum of Caricature and Critical Graphic Arts\". The collection of the works of Wilhelm Busch and the extensive collection of cartoons and critical graphics is this museum unique in Germany. Furthermore, the museum hosts several exhibitions of national and international artists during the year.\n\nA cabinet of coins is the \"Münzkabinett der TUI-AG\". The \"Polizeigeschichtliche Sammlung Niedersachsen\" is the largest police museum in Germany. Textiles from all over the world can be visited in the \"Museum for textile art\". The \"EXPOseeum\" is the museum of the world-exhibition \"EXPO 2000 Hannover\". Carpets and objects from the orient can be visited in the \"Oriental Carpet Museum\". The \"Blind Man Museum\" is a rarity in Germany, another one is only in Berlin. The \"Museum of veterinary medicine\" is unique in Germany. The \"Museum for Energy History\" describes the 150 years old history of the application of energy. The \"Home Museum Ahlem\" shows the history of the district of Ahlem. The \"Mahn- und Gedenkstätte Ahlem\" describes the history of the Jewish people in Hanover and the \"Stiftung Ahlers Pro Arte / Kestner Pro Arte\" shows modern art. Modern art is also the main topic of the \"Kunsthalle Faust\", the \"Nord/LB Art Gellery\" and of the \"Foro Artistico / Eisfabrik\".\n\nSome leading art events in Hanover are the \"Long Night of the museums\" and the \"Zinnober Kunstvolkslauf\" which features all the galleries in Hanover.\n\nPeople who are interested in astronomy should visit the \"Observatory Geschwister Herrschel\" on the Lindener Mountain or the small planetarium inside of the Bismarck School.\n\nAround 40 theatres are located in Hanover. The \"Opera House\", the \"Schauspielhaus\" (Play House), the \"Ballhofeins\", the \"Ballhofzwei\" and the \"Cumberlandsche Galerie\" belong to the \"Lower Saxony State Theatre\". The \"Theater am Aegi\" is Hanover's big theatre for musicals, shows and guest performances. The \"Neues Theater\" (New Theatre) is the Boulevard Theatre of Hanover. The \"Theater für Niedersachsen\" is another big theatre in Hanover, which also has an own Musical-Company. Some of the most important Musical-Productions are the rock musicals of the German rock musician Heinz Rudolph Kunze, which take place at the \"Garden-Theatre\" in the Great Garden.\n\nSome important theatre-events are the \"Tanztheater International\", the \"Long Night of the Theatres\", the \"Festival Theaterformen\" and the \"International Competition for Choreographs\".\n\nHanover's leading cabaret-stage is the \"GOP Variety theatre\" which is located in the \"Georgs Palace\". Some other famous cabaret-stages are the \"Variety Marlene\", the \"Uhu-Theatre\". the theatre \"Die Hinterbühne\", the \"Rampenlich Variety\" and the revue-stage \"TAK\". The most important Cabaret-Event is the \"Kleines Fest im Großen Garten\" (Little Festival in the Great Garden) which is the most successful Cabaret Festival in Germany. It features artists from around the world. Some other important events are the \"Calenberger Cabaret Weeks\", the \"Hanover Cabaret Festival\" and the \"Wintervariety\".\n\nHanover has two symphony orchestras: The Lower Saxon State Orchestra Hanover and the North German Radio Philharmonic Orchestra (NDR Radiophilharmonie). Two notable choirs have their homes in Hanover: the Girls Choir Hanover (Mädchenchor Hannover) and the Boys Choir Hanover (Knabenchor Hannover).\n\nThere are/were two big international competitions for classical music in Hanover:\n\nThe rock bands Scorpions and Fury in the Slaughterhouse are originally from Hanover. Acclaimed DJ Mousse T also has his main recording studio in the area. Rick J. Jordan, member of the band Scooter was born here in 1968. Eurovision Song Contest winner of 2010, Lena (Lena Meyer-Landrut), is also from Hanover.\n\nHannover 96 (nickname \"Die Roten\" or 'The Reds') is the top local football team that played in the Bundesliga top division until being relegated to the 2. Bundesliga after the 2015-2016 season. Home games are played at the HDI-Arena, which hosted matches in the 1974 and 2006 World Cups and the Euro 1988. Their reserve team \"Hannover 96 II\" plays in the fourth league. Their home games were played in the traditional \"Eilenriedestadium\" till they moved to the HDI Arena due to DFL directives. \"Arminia Hannover\" is another very traditional soccer team in Hanover that has played in the first league for years and plays now in the \"Niedersachsen-West Liga\" (Lower Saxony League West). Home matches are played in the \"Rudolf-Kalweit-Stadium\".\n\nThe Hannover Indians are the local ice hockey team. They play in the third tier. Their home games are played at the traditional Eisstadion am Pferdeturm. The Hannover Scorpions played in Hanover in Germany's top league until 2013 when they sold their license and moved to Langenhagen.\n\nHanover is one of the Rugby union capitals in Germany. The first German Rugby team was founded in Hanover in 1878. Hanover-based teams dominated the German Rugby scene for a long time. \"DRC Hannover\" plays in the first division, and \"SV Odin von 1905\" as well as \"SG 78/08 Hannover\" play in the second division.\n\nThe first German Fencing Club was founded in Hanover in 1862. Today there are three more Fencing Clubs in Hanover.\n\nThe Hannover Korbjäger are the city's top basketball team. They play their home games at the \"IGS Linden\".\n\nHanover is a centre for Water sports. Thanks to the lake \"Maschsee\", the rivers \"Ihme\" and \"Leine\" and to the channel \"Mittellandkanal\" Hanover hosts sailing schools, yacht schools, waterski clubs, rowing clubs, canoe clubs and paddle clubs. The water polo team \"WASPO W98\" plays in the first division.\n\nThe \"Hannover Regents\" play in the third Bundesliga (baseball) division.\n\nThe \"Hannover Grizzlies\" are the local American Football Team.\n\nThe \"Hannover Marathon\" is the biggest running event in Hanover with more than 11.000 participants and usually around 200.000 spectators. Some other important running events are the \"Gilde Stadtstaffel\" (relay), the \"Sport-Check Nachtlauf\" (night-running), the \"Herrenhäuser Team-Challenge\", the \"Hannoversche Firmenlauf\" (company running) and the \"Silvesterlauf\" (sylvester running).\n\nHanover hosts also an important international cycle race: The \"Nacht von Hannover\" (night of Hanover). The race takes place around the Market Hall.\n\nThe lake \"Maschsee\" hosts the \"International Dragon Boat Races\" and the \"Canoe　Polo-Tournament\". Many regattas take place during the year. \"Head of the river Leine\" on the river \"Leine\" is one of the biggest rowing regattas in Hanover.\n\nOne of Germanys most successful dragon boat teams, the All Sports Team Hannover, which has won since its foundation in year 2000 more than 100 medals on national and international competitions, is doing practising on the Maschsee in the heart of Hannover. The All Sports Team has received the award \"Team of the Year 2013\" in Lower Saxony \n\nSome other important sport events are the \"Lower Saxony Beach Volleyball Tournament\", the international horse show \"German Classics\" and the international ice hockey tournament \"Nations Cup\".\n\nHanover is one of the leading Exhibition Cities in the world. Each year Hanover hosts more than 60 international and national exhibitions. The most popular ones are the \"CeBIT\", the \"Hanover Fair\", the \"Domotex\", the \"Ligna\", the \"IAA Nutzfahrzeuge\" and the \"Agritechnica\". Hanover also hosts a huge number of congresses and symposiums like \"International Symposium on Society and Resource Management\"\n\nBut Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen. The \"Schützenfest Hannover\" is the largest Marksmen's Fun Fair in the world and takes place once a year (late June to early July) (2014 - July 4th to the 13th). It consists of more than 260 rides and inns, five large beer tents and a big entertainment programme. The highlight of this fun fair is the long \"Parade of the Marksmen\" with more than 12.000 participants from all over the world, among them around 5.000 marksmen, 128 bands and more than 70 wagons, carriages and big festival vehicles. It is the longest procession in Europe. Around 2 million people visit this fun fair every year. The landmark of this Fun Fair is the biggest transportable Ferris Wheel in the world ( high). The origins of this fun fair is located in the year 1529.\n\nHanover also hosts one of the two largest Spring Festivals in Europe with around 180 rides and inns, 2 large beer tents and around 1.5 million visitors each year. The Oktoberfest Hannover is the second largest Oktoberfest in the world with around 160 rides and inns, two large beer tents and around 1 million visitors each year.\n\nThe \"Maschsee Festival\" takes place around the Maschsee Lake. Each year around 2 million visitors come to enjoy live music, comedy, cabaret and much more. It is the largest Volksfest of its kind in Northern Germany.\n\nThe Great Garden hosts every year the \"International Fireworks Competition\", and the \"International Festival Weeks Herrenhausen\" with lots of music and cabaret.\n\nThe \"Carnival Procession\" is around long and consists of 3.000 participants, around 30 festival vehicles and around 20 bands and takes place every year.\n\nSome more festivals are for example the Festival \"Feuer und Flamme\" (Fire and Flames), the \"Gartenfestival\" (Garden Festival), the \"Herbstfestival\" (Autumn Festival), the \"Harley Days\", the \"Steintor Festival\" (Steintor is a party area in the city centre) and the \"Lister-Meile-Festival\" (Lister Meile is a large pedestrian area).\n\nHanover also hosts Food Festivals, for example the \"Wine Festival\" and the \"Gourmet Festival\".\n\nFurthermore, Hanover hosts some special markets. The \"Old Town Flea Market\" is the oldest flea market in Germany and the \"Market for Art and Trade\" has a high reputation. Some other big markets are of course the \"Christmas Markets of the City of Hanover\" in the Old Town and city centre and the Lister Meile.\n\nThe city's central station, Hannover Hauptbahnhof, is a hub of the German high-speed ICE network. It is the starting point of the Hanover-Würzburg high-speed rail line and also the central hub for the Hanover S-Bahn. It offers many international and national connections.\n\nHanover and its area is served by Hanover/Langenhagen International Airport (IATA code: HAJ; ICAO code: EDDV)\n\nHanover is also an important hub of Germany's Autobahn network; the junction of two major autobahns, the A2 and A7 is at \"Kreuz Hannover-Ost\", at the northeastern edge of the city.\n\nLocal autobahns are A 352 (a short cut between A7 (north) and A2 (west), also known as the \"airport autobahn\" because it passes \"Hanover Airport\") and the A 37.\n\nThe Schnellweg \"(en: expressway)\" system, a number of Bundesstraße roads, forms a structure loosely resembling a large ring road together with A2 and A7. The roads are B 3, B 6 and B 65, called Westschnellweg (B6 on the northern part, B3 on the southern part), Messeschnellweg (B3, becomes A37 near Burgdorf, crosses A2, becomes B3 again, changes to B6 at \"Seelhorster Kreuz\", then passes the Hanover fairground as B6 and becomes A37 again before merging into A7) and Südschnellweg (starts out as B65, becomes B3/B6/B65 upon crossing \"Westschnellweg\", then becomes B65 again at \"Seelhorster Kreuz\").\n\nHanover has an extensive Stadtbahn and bus system, operated by üstra. The city is famous for its designer buses and tramways, the TW 6000 and TW 2000 trams being the most well-known examples.\n\nCycle paths are very common in the city centre. At off-peak hours you are allowed to take your bike on a tram or bus.\n\nVarious industrial businesses are located in Hannover. The Volkswagen Commercial Vehicles Transporter (VWN) factory at Hannover-Stöcken is the biggest employer in the region and operates a huge plant at the northern edge of town adjoining the Mittellandkanal and Motorway A2. Jointly with a factory of German tire and automobile parts manufacturer Continental AG, they have a coal-burning power plant. Continental AG, founded in Hanover in 1871, is one of the city's major companies, as is Sennheiser. Since 2008 a take-over is in progress: the Schaeffler Group from Herzogenaurach (Bavaria) holds the majority of the stock but were required due to the financial crisis to deposit the options as securities at banks.\nTUI AG has its HQ in Hanover.\nHanover is home to many insurance companies, many of which operate only in Germany. One major global reinsurance company is Hannover Re, whose headquarters are east of the city centre.\n\nIn 2012, the city generated a GDP of €29,5 billion which is equivalent to €74,822 per employee. The Gross value of production in 2012 was €26,4 billion which is equivalent to €66,822 per employee.\nAround 300,000 employees were counted in 2014. 189,000 of these had their primary residence in Hanover while 164,892 commute into the city every day.\n\nIn 2014 the city was home to 34,198 businesses, of which 9,342 were registered in the German Trade Register and 24,856 counted as small businesses. Hence, more than half of the metropolitan area's businesses in the German Trade Register are located in Hanover (17,485 total).\nHannoverimpuls is a joint business development company from the city and region of Hannover. The company was founded in 2003 and supports the start-up, growth and relocation of businesses in the Hannover Region. The focus is on seven sectors, which stand for sustainable economic growth: Automotive, Energy Solutions, Information and Communications Technology, Life Sciences, Optical Technologies, Creative Industries and Production Engineering.\n\nA range of programmes supports companies from the key industries in their expansion plans in Hannover or abroad. Three regional centres specifically promote international economic relations with Russia, India and Turkey.\n\nThe Leibniz University Hannover is the largest funded institution in Hanover for providing higher education to the students from around the world. Below are the names of the universities and some of the important schools including newly opened Hannover Medical Research School in 2003 for attracting the students from biology background from around the world.\n\nThere are several universities in Hanover:\nThere is one University of Applied Science and Arts in Hanover:\n\nThe \"Schulbiologiezentrum Hannover\" maintains practical biology schools in four locations (Botanischer Schulgarten Burg, Freiluftschule Burg, Zooschule Hannover, and Botanischer Schulgarten Linden). The University of Veterinary Medicine Hanover also maintains its own botanical garden specializing in medicinal and poisonous plants, the Heil- und Giftpflanzengarten der Tierärztlichen Hochschule Hannover.\n\nThe following is a selection of famous Hanover-natives, personalities connected with the city and honorary citizens:\n\nHanover is twinned with:\n\n\n", "id": "14197", "title": "Hanover"}
{"url": "https://en.wikipedia.org/wiki?curid=14199", "text": "Handheld game console\n\nA handheld game console is a small, portable self-contained video game console with a built-in screen, game controls, and speakers. Handheld game consoles are smaller than home video game consoles and contain the console, screen, speakers, and controls in one unit, allowing people to carry them and play them at any time or place.\n\nIn 1976, Mattel introduced the first handheld electronic game with the release of \"Auto Race\". Later, several companies—including Coleco and Milton Bradley—made their own single-game, lightweight table-top or handheld electronic game devices. The oldest true handheld game console with interchangeable cartridges is the Milton Bradley Microvision in 1979.\n\nNintendo is credited with popularizing the handheld console concept with the release of the Game Boy in 1989 and as of 2014 continues to dominate the handheld console market with their 3DS systems.\n\nThe origins of handheld game consoles are found in handheld and tabletop electronic game devices of the 1970s and early 1980s. These electronic devices are capable of playing only a single game, they fit in the palm of the hand or on a tabletop, and they may make use of a variety of video displays such as LED, VFD, or LCD. In 1978, handheld electronic games were described by \"Popular Electronics\" magazine as \"nonvideo electronic games\" and \"non-TV games\" as distinct from devices that required use of a television screen. Handheld electronic games, in turn, find their origins in the synthesis of previous handheld and tabletop electro-mechanical devices such as Waco's \"Electronic Tic-Tac-Toe\" (1972) Cragstan's \"Periscope-Firing Range\" (1951), and the emerging optoelectronic-display-driven calculator market of the early 1970s. This synthesis happened in 1976, when \"Mattel began work on a line of calculator-sized sports games that became the world's first handheld electronic games. The project began when Michael Katz, Mattel's new product category marketing director, told the engineers in the electronics group to design a game the size of a calculator, using LED (light-emitting diode) technology.\"\n\nThe result was the 1976 release of \"Auto Race\". Followed by \"Football\" later in 1977, the two games were so successful that according to Katz, \"these simple electronic handheld games turned into a '$400 million category.'\" Mattel would later win the honor of being recognized by the industry for innovation in handheld game device displays. Soon, other manufacturers including Coleco, Parker Brothers, Milton Bradley, Entex, and Bandai began following up with their own tabletop and handheld electronic games.\n\nIn 1979 the LCD-based Microvision, designed by Smith Engineering and distributed by Milton-Bradley, became the first handheld game console and the first to use interchangeable game cartridges. The Microvision game \"Cosmic Hunter\" (1981) also introduced the concept of a directional pad on handheld gaming devices, and is operated by using the thumb to manipulate the on-screen character in any of four directions.\n\nIn 1979, Gunpei Yokoi, traveling on a bullet train, saw a bored businessman playing with an LCD calculator by pressing the buttons. Yokoi then thought of an idea for a watch that doubled as a miniature game machine for killing time. Starting in 1980, Nintendo began to release a series of electronic games designed by Yokoi called the Game & Watch games. Taking advantage of the technology used in the credit-card-sized calculators that had appeared on the market, Yokoi designed the series of LCD-based games to include a digital time display in the corner of the screen. For later, more complicated Game & Watch games, Yokoi invented a cross shaped directional pad or \"D-pad\" for control of on-screen characters. Yokoi also included his directional pad on the NES controllers, and the cross-shaped thumb controller soon became standard on game console controllers and ubiquitous across the video game industry since. When Yokoi began designing Nintendo's first handheld game console, he came up with a device that married the elements of his Game & Watch devices and the Famicom console, including both items' D-pad controller. The result was the Nintendo Game Boy.\n\nIn 1982, the Bandai LCD Solarpower was the first solar-powered gaming device. Some of its games, such as the horror-themed game \"Terror House\", featured two LCD panels, one stacked on the other, for an early 3D effect. In 1983, Takara Tomy's Tomytronic 3D simulated 3D by having two LED panels that were lit by external light through a window on top of the device, making it the first dedicated home video 3D hardware.\n\nThe late 1980s and early 1990s saw the beginnings of the handheld game console industry as we know it, after the demise of the Microvision. As backlit LCD game consoles with color graphics consume a lot of power, they were not battery-friendly like the non-backlit original Game Boy whose monochrome graphics allowed longer battery life. By this point, rechargeable battery technology had not yet matured and so the more advanced game consoles of the time such as the Sega Game Gear and Atari Lynx did not have nearly as much success as the Game Boy.\n\nEven though third-party rechargeable batteries were available for the battery-hungry alternatives to the Game Boy, these batteries employed a nickel-cadmium process and had to be completely discharged before being recharged to ensure maximum efficiency; lead-acid batteries could be used with automobile circuit limiters (cigarette lighter plug devices); but the batteries had mediocre portability. The later NiMH batteries, which do not share this requirement for maximum efficiency, were not released until the late 1990s, years after the Game Gear, Atari Lynx, and original Game Boy had been discontinued. During the time when technologically superior handhelds had strict technical limitations, batteries had a very low mAh rating since batteries with heavy power density were not yet available.\n\nModern game systems such as the Nintendo DS and PlayStation Portable have rechargeable Lithium-Ion batteries with proprietary shapes. Other seventh-generation consoles such as the GP2X use standard alkaline batteries. Because the mAh rating of alkaline batteries has increased since the 1990s, the power needed for handhelds like the GP2X may be supplied by relatively few batteries.\n\nNintendo released the Game Boy on April 21, 1989 (or in September 1990 for UK). The design team headed by Gunpei Yokoi had also been responsible for the Game & Watch system, as well as the Nintendo Entertainment System games \"Metroid\" and \"Kid Icarus\". The Game Boy came under scrutiny by some industry critics, saying that the monochrome screen was too small, and the processing power was inadequate. The design team had felt that low initial cost and battery economy were more important concerns, and when compared to the Microvision, the Game Boy was a huge leap forward.\n\nYokoi recognized that the Game Boy needed a killer app—at least one game that would define the console, and persuade customers to buy it. In June 1988, Minoru Arakawa, then-CEO of Nintendo of America saw a demonstration of the game \"Tetris\" at a trade show. Nintendo purchased the rights for the game, and packaged it with the Game Boy system. It was almost an immediate hit. By the end of the year more than a million units were sold in the US. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell over 118 million units worldwide.\n\nIn 1987, Epyx created the Handy Game; a device that would turn into the Atari Lynx in 1989. It was the first color handheld console ever made, as well as the first with a backlit screen. It also featured networking support with up to 17 other players, and advanced hardware that allowed the zooming and scaling of sprites. The Lynx could also be turned upside down to accommodate left-handed players. However, all these features came at a very high price point, which drove consumers to seek cheaper alternatives. The Lynx was also very unwieldy, consumed batteries very quickly, and lacked the third-party support enjoyed by its competitors. Due to its high price, short battery life, production shortages, a dearth of compelling games, and Nintendo's aggressive marketing campaign, and despite a redesign in 1991, the Lynx became a commercial failure. Despite this, companies like Telegames helped to keep the system alive long past its commercial relevance, and when new owner Hasbro released the rights to develop for the public domain, independent developers like Songbird have managed to release new commercial games for the system every year until 2004's \"Winter Games\".\n\nThe TurboExpress was a portable version of the TurboGrafx, released in 1990 for $249.99 (the price was briefly raised to $299.99, soon dropped back to $249.99, and by 1992 it was $199.99). Its Japanese equivalent was the PC Engine GT.\n\nIt was the most advanced handheld of its time and could play all the TurboGrafx-16's games (which were on a small, credit-card sized media called HuCards). It had a 66 mm (2.6 in.) screen, the same as the original Game Boy, but in a much higher resolution. And could display 64 sprites at once, 16 per scanline, in 512 colors. Although the hardware could only handle 481 simultaneous colors. It had 8 kilobytes of RAM. The Turbo ran it HuC6820 CPU at 1.79 or 7.16 MHz.\n\nThe optional \"TurboVision\" TV tuner included RCA audio/video input, allowing users to use TurboExpress as a video monitor. The \"TurboLink\" allowed two-player play. \"Falcon\", a flight simulator, included a \"head-to-head\" dogfight mode that could only be accessed via TurboLink. However, very few TG-16 games offered co-op play modes especially designed with the TurboExpress in mind.\n\nThe Bitcorp Gamate was the one of the first handheld game systems created in response to the Nintendo Game Boy. It was released in Asia in 1990 and distributed worldwide by 1991.\n\nLike the Sega Game Gear, it was horizontal in orientation and like the Game Boy, required 4 AA batteries. Unlike many later Game Boy clones, its internal components were professionally assembled (no \"glop-top\" chips). Unfortunately the system's fatal flaw was its screen. Even by the standards of the day, its screen was rather difficult to use, suffering from similar motion blur problems that were common complaints with the first generation Game Boys. Likely because of this fact sales were quite poor, and Bitcorp closed by 1992. However, new games continued to be published for the Asian market, possibly as late as 1994. The total number of games released for the system remains unknown.\n\nInterestingly, Gamate games were designed for stereo sound, but the console was only equipped with a mono speaker. To appreciate the full sound palette, a user must plug into the head phone jack. Doing so reveals very sophisticated music.\n\nThe Game Gear was the third color handheld console, after the Lynx and the TurboExpress; produced by Sega. Released in Japan in 1990 and in North America and Europe in 1991, it was based on the Master System, which gave Sega the ability to quickly create Game Gear games from its large library of games for the Master System. While never reaching the level of success enjoyed by Nintendo, the Game Gear proved to be a fairly durable competitor, lasting longer than any other Game Boy rivals.\n\nWhile the Game Gear is most frequently seen in black or navy blue, it was also released in a variety of additional colors: red, light blue, yellow, clear, and violet. All of these variations were released in small quantities and frequently only in the Asian market.\n\nFollowing Sega's success with the Game Gear, they began development on a successor during the early 1990s, which was intended to feature a touchscreen interface, many years before the Nintendo DS. However, such a technology was very expensive at the time, and the handheld itself was estimated to have cost around $289 were it to be released. Sega eventually chose to shelve the idea and instead release the Genesis Nomad, a handheld version of the Genesis, as the successor.\n\nThe Watara Supervision was released in 1992 in an attempt to compete with the Nintendo Game Boy. The first model was designed very much like a Game Boy, but it was grey in color and had a slightly larger screen. The second model was made with a hinge across the center and could be bent slightly to provide greater comfort for the user. While the system did enjoy a modest degree of success, it never impacted the sales of Nintendo or Sega. The Supervision was redesigned a final time as \"The Magnum\". Released in limited quantities it was roughly equivalent to the Game Boy Pocket. It was available in three colors: yellow, green and grey. Watara designed many of the games themselves, but did receive some third party support, most notably from Sachen.\n\nA TV adapter was available in both PAL and NTSC formats that could transfer the Supervision's black-and-white palette to 4 colors, similar in some regards to the Super Game Boy from Nintendo.\n\nThe Hartung Game Master was an obscure handheld released at an unknown point in the early 1990s. Its graphics were much lower than most of its contemporaries, similar in complexity to the Atari 2600. It was available in black, white, and purple, and was frequently rebranded by its distributors, such as Delplay, Videojet and Virella.\nThe exact number of games released is not known, but is likely around 20. The system most frequently turns up in Europe and Australia.\n\nBy this time, the lack of significant development in Nintendo's product line began allowing more advanced systems such as the Neo Geo Pocket Color and the WonderSwan Color to achieve moderate success.\n\nThe Game.com (pronounced in TV commercials as \"game com\", not \"game dot com\", and not capitalized in marketing material) was a handheld game console released by Tiger Electronics in September 1997. It featured many new ideas for handheld consoles and was aimed at an older target audience, sporting PDA-style features and functions such as a touch screen and stylus. However, Tiger hoped it would also challenge Nintendo's Game Boy and gain a following among younger gamers too. Unlike other handheld game consoles, the first game.com consoles included two slots for game cartridges, which would not happen again until the Tapwave Zodiac, the DS and DS Lite, and could be connected to a 14.4 kbit/s modem. Later models had only a single cartridge slot.\n\nThe Game Boy Color (also referred to as GBC or CGB) is Nintendo's successor to the Game Boy and was released on October 21, 1998, in Japan and in November of the same year in the United States. It features a color screen, and is slightly bigger than the Game Boy Pocket. The processor is twice as fast as a Game Boy's and has twice as much memory. It also had an infrared communications port for wireless linking which did not appear in later versions of the Game Boy, such as the Game Boy Advance.\n\nThe Game Boy Color was a response to pressure from game developers for a new system, as they felt that the Game Boy, even in its latest incarnation, the Game Boy Pocket, was insufficient. The resulting product was backward compatible, a first for a handheld console system, and leveraged the large library of games and great installed base of the predecessor system. This became a major feature of the Game Boy line, since it allowed each new launch to begin with a significantly larger library than any of its competitors. As of March 31, 2005, the Game Boy and Game Boy Color combined to sell 118.69 million units worldwide.\n\nThe console was capable of displaying up to 56 different colors simultaneously on screen from its palette of 32,768, and could add basic four-color shading to games that had been developed for the original Game Boy. It could also give the sprites and backgrounds separate colors, for a total of more than four colors.\n\nThe Neo Geo Pocket Color (or NGPC) was released in 1999 in Japan, and later that year in the United States and Europe. It was a 16-bit color handheld game console designed by SNK, the maker of the Neo Geo home console and arcade machine. It came after SNK's original Neo Geo Pocket monochrome handheld, which debuted in 1998 in Japan.\n\nIn 2000 following SNK's purchase by Japanese Pachinko manufacturer Aruze, the Neo Geo Pocket Color was dropped from both the US and European markets, purportedly due to commercial failure.\n\nThe system seemed well on its way to being a success in the U.S. It was more successful than any Game Boy competitor since Sega's Game Gear, but was hurt by several factors, such as SNK's infamous lack of communication with third-party developers, and anticipation of the Game Boy Advance. The decision to ship U.S. games in cardboard boxes in a cost-cutting move rather than hard plastic cases that Japanese and European releases were shipped in may have also hurt US sales.\n\nThe WonderSwan Color is a handheld game console designed by Bandai. It was released on December 9, 2000, in Japan, and was a moderate success.\n\nThe original WonderSwan had only a black and white screen. Although the WonderSwan Color was slightly larger and heavier (7 mm and 2 g) compared to the original WonderSwan, the color version featured 512 kB of RAM and a larger color LCD screen. In addition, the WonderSwan Color is compatible with the original WonderSwan library of games.\n\nPrior to WonderSwan's release, Nintendo had virtually a monopoly in the Japanese video game handheld market. After the release of the WonderSwan Color, Bandai took approximately 8% of the market share in Japan partly due to its low price of 6800 yen (approximately US$65).\n\nAnother reason for the WonderSwan's success in Japan was the fact that Bandai managed to get a deal with Square to port over the original Famicom \"Final Fantasy\" games with improved graphics and controls. However, with the popularity of the Game Boy Advance and the reconciliation between Square and Nintendo, the WonderSwan Color and its successor, the SwanCrystal quickly lost its competitive advantage.\n\nThe 2000s saw a major leap in innovation, particularly in the second half with the release of the DS and PSP.\n\nIn 2001, Nintendo released the Game Boy Advance (GBA or AGB), which added two shoulder buttons, a larger screen, and more computing power than the Game Boy Color.\n\nThe design was revised two years later when the Game Boy Advance SP (GBA SP), a more compact version, was released. The SP featured a \"clamshell\" design (folding open and closed, like a laptop computer), as well as a frontlit color display and rechargeable battery. Despite the smaller form factor, the screen remained the same size as that of the original. In 2005, the Game Boy Micro was released. This revision sacrificed screen size and backwards compatibility with previous Game Boys for a dramatic reduction in total size and a brighter backlit screen. A new SP model with a backlit screen was released in some regions around the same time.\n\nAlong with the Nintendo GameCube, the GBA also introduced the concept of \"connectivity\": using a handheld system as a console controller. A handful of games use this feature, most notably \"Animal Crossing\", \"Pac-Man Vs.\", \"Final Fantasy Crystal Chronicles\", \"\", \"\", \"Metroid Prime\", and \"\".\n\nAs of December 31, 2007, the GBA, GBA SP, and the Game Boy Micro combined have sold 80.72 million units worldwide.\n\nThe original GP32 was released in 2001 by the South Korean company Game Park a few months after the launch of the Game Boy Advance. It featured a 32-bit CPU, 133 MHz processor, MP3 and Divx player, and e-book reader. SmartMedia cards were used for storage, and could hold up to 128mb of anything downloaded through a USB cable from a PC. The GP32 was redesigned in 2003. A front-lit screen was added and the new version was called GP32 FLU (Front Light Unit). In summer 2004, another redesign, the GP32 BLU, was made, and added a backlit screen. This version of the handheld was planned for release outside South Korea; in Europe, and it was released for example in Spain (VirginPlay was the distributor). While not a commercial success on a level with mainstream handhelds (only 30,000 units were sold), it ended up being used mainly as a platform for user-made applications and emulators of other systems, being popular with developers and more technically adept users.\n\nNokia released the N-Gage in 2003. It was designed as a combination MP3 player, cellphone, PDA, radio, and gaming device. The system received much criticism alleging defects in its physical design and layout, including its vertically oriented screen and requirement of removing the battery to change game cartridges. The most well known of these was \"sidetalking\", or the act of placing the phone speaker and receiver on an edge of the device instead of one of the flat sides, causing the user to appear as if they are speaking into a taco.\n\nThe N-Gage QD was later released to address the design flaws of the original. However, certain features available in the original N-Gage, including MP3 playback, FM radio reception, and USB connectivity were removed.\n\nSecond generation of N-Gage launched on April 3, 2008 in the form of a service for selected Nokia Smartphones.\n\nThe Cybiko was a Russian hand-held computer introduced in May 2000 by David Yang's company and designed for teenage audiences, featuring its own two-way radio text messaging system. It had over 430 \"official\" freeware games and applications. Because of the text messaging system, it features a QWERTY keyboard that was used with a stylus. An MP3 player add-on was made for the unit as well as a SmartMedia card reader. The company stopped manufacturing the units after two product versions and only a few years on the market. Cybikos can communicate with each other up to a maximum range of 300 metres (0.19 miles). Several Cybikos can chat with each other in a wireless chatroom.\n\nCybiko Classic:\n\nThere were two models of the Classic Cybiko. Visually, the only difference was that the original version had a power switch on the side, whilst the updated version used the \"escape\" key for power management. Internally, the differences between the two models were in the internal memory, and the location of the firmware.\n\nCybiko Xtreme:\n\nThe Cybiko Xtreme was the second-generation Cybiko handheld. It featured various improvements over the original Cybiko, such as a faster processor, more RAM, more ROM, a new operating system, a new keyboard layout and case design, greater wireless range, a microphone, improved audio output, and smaller size.\n\nIn 2003, Tapwave released the Zodiac. It was designed to be a PDA-handheld game console hybrid. It supported photos, movies, music, Internet, and documents. The Zodiac used a special version Palm OS 5, 5.2T, that supported the special gaming buttons and graphics chip. Two versions were available, Zodiac 1 and 2, differing in memory and looks. The Zodiac line ended in July 2005 when Tapwave declared bankruptcy.\n\nThe Nintendo DS was released in November 2004. Among its new features were the incorporation of two screens, a touchscreen, wireless connectivity, and a microphone port. As with the Game Boy Advance SP, the DS features a clamshell design, with the two screens aligned vertically on either side of the hinge.\n\nThe DS's lower screen is touch sensitive, designed to be pressed with a stylus, a user's finger or a special \"thumb pad\" (a small plastic pad attached to the console's wrist strap, which can be affixed to the thumb to simulate an analog stick). More traditional controls include four face buttons, two shoulder buttons, a D-pad, and \"Start\" and \"Select\" buttons. The console also features online capabilities via the Nintendo Wi-Fi Connection and ad-hoc wireless networking for multiplayer games with up to sixteen players. It is backwards-compatible with all Game Boy Advance games, but not games designed for the Game Boy or Game Boy Color.\n\nIn January 2006, Nintendo revealed an updated version of the DS: the Nintendo DS Lite (released on March 2, 2006, in Japan) with an updated, smaller form factor (42% smaller and 21% lighter than the original Nintendo DS), a cleaner design, longer battery life, and brighter, higher-quality displays, with adjustable brightness. It is also able to connect wirelessly with Nintendo's Wii console.\n\nIn October 2008, Nintendo announced the Nintendo DSi, with larger, 3.25-inch screens and two integrated cameras. It has an SD card storage slot in place of the Game Boy Advance slot, plus internal flash memory for storing downloaded games. It was released on November 1, 2008, in Japan, and was released in North America April 5, 2009, and April 3, 2009, in Europe.\n\nAs of December 31, 2009, the Nintendo DS, Nintendo DS Lite and Nintendo DSi combined have sold 125.13 million units worldwide. In 2010 Nintendo released a larger version of the DSi, called the DSi XL.\n\nThe GameKing was a handheld game console released by the Chinese company TimeTop in 2004. The first model while original in design owes a large debt to Nintendo's Game Boy Advance. The second model, the GameKing 2, is believed to be inspired by Sony's PSP. This model also was upgraded with a backlit screen, with a distracting background transparency (which can be removed by opening up the console). A color model, the GameKing 3 apparently exists, but was only made for a brief time and was difficult to purchase outside of Asia. Whether intentionally or not, the GameKing has the most primitive graphics of any handheld released since the Game Boy of 1989. \n\nAs many of the games have an \"old school\" simplicity, the device has developed a small cult following. The Gameking's speaker is quite loud and the cartridges' sophisticated looping soundtracks (sampled from other sources) are seemingly at odds with its primitive graphics.\n\nTimeTop made at least one additional device sometimes labeled as \"GameKing\", but while it seems to possess more advanced graphics, is essentially an emulator that plays a handful of multi-carts (like the GB Station Light II). Outside of Asia (especially China) however the Gameking remains relatively unheard of due to the enduring popularity of Japanese handhelds such as those manufactured by Nintendo and Sony.\n\nThe PlayStation Portable (officially abbreviated PSP) is a handheld game console manufactured and marketed by Sony Computer Entertainment. Development of the console was first announced during E3 2003, and it was unveiled on May 11, 2004, at a Sony press conference before E3 2004. The system was released in Japan on December 12, 2004, in North America on March 24, 2005, and in the PAL region on September 1, 2005.\n\nThe PlayStation Portable is the first handheld video game console to use an optical disc format, Universal Media Disc (UMD), for distribution of its games. UMD Video discs with movies and television shows were also released. The PSP utilized the Sony/SanDisk Memory Stick Pro Duo format as its primary storage medium. Other distinguishing features of the console include its large viewing screen, multi-media capabilities, and connectivity with the PlayStation 3, other PSPs, and the Internet.\n\nTiger's Gizmondo came out in the UK during March 2005 and it was released in the U.S. during October 2005. It is designed to play music, movies, and games, have a camera for taking and storing photos, and have GPS functions. It also has Internet capabilities. It has a phone for sending text and multimedia messages. Email was promised at launch, but was never released before Gizmondo, and ultimately Tiger Telematics', downfall in early 2006. Users obtained a second service pack, unreleased, hoping to find such functionality. However, Service Pack B did not activate the e-mail functionality.\n\nThe GP2X is an open-source, Linux-based handheld video game console and media player created by GamePark Holdings of South Korea, designed for homebrew developers as well as commercial developers. It is commonly used to run emulators for game consoles such as Neo-Geo, Genesis, Master System, Game Gear, Amstrad CPC, Commodore 64, Nintendo Entertainment System, TurboGrafx-16, MAME and others.\n\nA new version called the \"F200\" was released October 30, 2007, and features a touchscreen, among other changes. Followed by GP2X Wiz (2009) and GP2X Caanoo (2010).\n\nThe Dingoo A-320 is a micro-sized gaming handheld that resembles the Game Boy Micro and is open to game development. It also supports music, radio, emulators (8 bit and 16 bit) and video playing capabilities with its own interface much like the PSP. There is also an onboard radio and recording program. It is currently available in two colors — white and black. Other similar products from the same manufacturer are the Dingoo A-330 (also known as Geimi), Dingoo A-360, Dingoo A-380 (available in pink, white and black) and the recently released Dingoo A-320E.\n\nThe PSP Go is a version of the PlayStation Portable handheld game console manufactured by Sony. It was released on October 1, 2009, in American and European territories, and on November 1 in Japan. It was revealed prior to E3 2009 through Sony's Qore VOD service. Although its design is significantly different from other PSPs, it is not intended to replace the PSP 3000, which Sony continued to manufacture, sell, and support. On April 20, 2011, the manufacturer announced that the PSP Go would be discontinued so that they may concentrate on the PlayStation Vita. Sony later said that only the European and Japanese versions were being cut, and that the console would still be available in the US.\nUnlike previous PSP models, the PSP Go does not feature a UMD drive, but instead has 16 GB of internal flash memory to store games, video, pictures, and other media. This can be extended by up to 32 GB with the use of a Memory Stick Micro (M2) flash card. Also unlike previous PSP models, the PSP Go's rechargeable battery is not removable or replaceable by the user. The unit is 43% lighter and 56% smaller than the original PSP-1000, and 16% lighter and 35% smaller than the PSP-3000. It has a 3.8\" 480 × 272 LCD (compared to the larger 4.3\" 480 × 272 pixel LCD on previous PSP models). The screen slides up to reveal the main controls. The overall shape and sliding mechanism are similar to that of Sony's mylo COM-2 internet device.\n\nThe Pandora is a handheld game console/UMPC/PDA hybrid designed to take advantage of existing open source software and to be a target for home-brew development. It runs a full distribution of Linux, and in functionality is like a small PC with gaming controls. It is developed by OpenPandora, which is made up of former distributors and community members of the GP32 and GP2X handhelds.\n\nOpenPandora began taking pre-orders for one batch of 4000 devices in November 2008 and after manufacturing delays, began shipping to customers on May 21, 2010.\n\nThe FC-16 Go is a portable Super NES hardware clone manufactured by Yobo Gameware in 2009. It features a 3.5-inch display, two wireless controllers, and CRT cables that allow cartridges to be played on a television screen. Unlike other Super NES clone consoles, it has region tabs that only allow NTSC North American cartridges to be played. Later revisions feature stereo sound output, larger shoulder buttons, and a slightly re-arranged button, power, and A/V output layout.\n\nThe Nintendo 3DS is the successor to Nintendo's DS handheld. The autostereoscopic device is able to project stereoscopic three-dimensional effects without requirement of active shutter or passive polarized glasses, which are required by most current 3D televisions to display the 3D effect. The 3DS was released in Japan on February 26, 2011; in Europe on March 25, 2011; in North America on March 27, 2011, and in Australia on March 31, 2011. The system features backward compatibility with Nintendo DS series software, including Nintendo DSi software. It also features an online service called the Nintendo eShop, launched on June 6, 2011, in North America and June 7, 2011, in Europe and Japan, which allows owners to download games, demos, applications and information on upcoming film and game releases. On November 24, 2011, a limited edition Legend of Zelda 25th Anniversary 3DS was released that contained a unique Cosmo Black unit decorated with gold Legend of Zelda related imagery, along with a copy of The Legend of Zelda: Ocarina of Time 3D.\n\nThere are also other models including the Nintendo 2DS and the New Nintendo 3DS, the latter with a larger (XL/LL) variant, like the original Nintendo 3DS.\nThe Sony Ericsson Xperia PLAY is a handheld game console smartphone produced by Sony Ericsson under the Xperia smartphone brand. The device runs Android 2.3 Gingerbread, and is the first to be part of the PlayStation Certified program which means that it can play PlayStation Suite games. The device is a horizontally sliding phone with its original form resembling the Xperia X10 while the slider below resembles the slider of the PSP Go. The slider features a D-pad on the left side, a set of standard PlayStation buttons (, , and ) on the right, a long rectangular touchpad in the middle, start and select buttons on the bottom right corner, a menu button on the bottom left corner, and two shoulder buttons (L and R) on the back of the device. It is powered by a 1 GHz Qualcomm Snapdragon processor, a Qualcomm Adreno 205 GPU, and features a display measuring 4.0 inches (100 mm) (854 × 480), an 8-megapixel camera, 512 MB RAM, 8 GB internal storage, and a micro-USB connector. It supports microSD cards, versus the Memory Stick variants used in PSP consoles. The device was revealed officially for the first time in a Super Bowl ad on Sunday, February 6, 2011. On February 13, 2011, at Mobile World Congress (MWC) 2011, it was announced that the device would be shipping globally in March 2011, with a launch lineup of around 50 software titles.\nThe PlayStation Vita is the successor to Sony's PlayStation Portable (PSP) Handheld series. It was released in Japan on December 17, 2011 and in Europe, Australia, North and South America on February 22, 2012.\n\nThe handheld includes two analog sticks, a 5-inch (130 mm) OLED/LCD multi-touch capacitive touchscreen, and supports Bluetooth, Wi-Fi and optional 3G. Internally, the PS Vita features a 4 core ARM Cortex-A9 MPCore processor and a 4 core SGX543MP4+ graphics processing unit, as well as LiveArea software as its main user interface, which succeeds the XrossMediaBar.\n\nThe device is fully backwards-compatible with PlayStation Portable games digitally released on the PlayStation Network via the PlayStation Store. However, PSone Classics and PS2 titles were not compatible at the time of the primary public release in Japan. The Vita's dual analog sticks will be supported on selected PSP games. The graphics for PSP releases will be up-scaled, with a smoothing filter to reduce pixelation.\nThe Razer Switchblade was a prototype pocket-sized like a Nintendo DSi XL designed to run Windows 7, featured a multi-touch LCD screen and an adaptive keyboard that changed keys depending on the game you play. It also was to feature a full mouse.\n\nIt was first unveiled on January 5, 2011, on the Consumer Electronics Show (CES). The Switchblade won The Best of CES 2011 People's Voice award. It has since been in development and the release date is still unknown. The device has likely been suspended indefinitely.\nProject Shield is a handheld system developed by Nvidia announced at CES 2013. It runs on Android 4.2 and uses Nvidia Tegra 4 SoC. The hardware includes a 5-inches multitouch screen with support for HD graphics (720p). The console allows for the streaming of games running on a compatible desktop PC, or laptop.\n\nThe Nintendo Switch is a hybrid device that can be used as either a tablet-like portable console, or inserted into a docking station that is attached to a television. The Switch features two detachable wireless controllers, which can be used individually or attached to a grip to provide a traditional gamepad form.\n\n\n\n\n\n\n\n\n\n", "id": "14199", "title": "Handheld game console"}
{"url": "https://en.wikipedia.org/wiki?curid=14200", "text": "Heinrich Abeken\n\nHeinrich Abeken (August 19, 1809August 8, 1872) was a German theologian and Prussian Privy Legation Councillor in the Ministry of Foreign Affairs in Berlin.\n\nAbeken was born and raised in the city of Osnabrück as a son of a merchant, he was incited to a higher education by the example of his uncle Bernhard Rudolf Abeken. After finishing the college in Osnabrück, he moved in 1827 to visit the University of Berlin to study theology. He soon combined philosophical and philological studies and was interested in art and modern literature.\n\nIn 1831, Abeken acquired a licenciate of theology. At the end of the year he visited Rome, and was welcomed in the house of Christian Karl Josias, Freiherr von Bunsen. Abeken participated in Bunsen's works, namely an evangelic prayer and hymn-book. In 1834 became chaplain to the Prussian embassy in Rome. He married his first wife, who died soon thereafter.\nBunsen left Rome in 1838 and Abeken followed soon thereafter to Germany. In 1841, he was sent to England to help founding a German-English evangelic episcopacy in Jerusalem. In the same year, he was sent by Frederick William IV of Prussia to Egypt and Ethiopia, where he joined an expedition led by professor Karl Richard Lepsius. In 1845 and 1846 he returned via Jerusalem and Rome to Germany. He became Legation Councillor in Berlin, later Council Referee at the Ministry of Foreign Affairs.\n\nIn 1848 he received an appointment in the Prussian ministry for foreign affairs, and in 1853 was promoted to be privy councillor of legation (\"Geheimer Legationsrath\"). Abeken remained in charge for more than twenty years of Prussian politics, assisting Otto Theodor Freiherr von Manteuffel and Chancellor Otto von Bismarck. The latter was so much pleased with Abeken's work that officials started to call Abeken \"the quill [i.e., the scribe] of Bismarck.\" Abeken married in 1866 Hedwig von Olfers, daughter of the general director of the royal museums, Privy Council von Olfers.\n\nHe was much employed by Bismarck in the writing of official despatches, and stood high in the favour of King William, whom he often accompanied on his journeys as representative of the foreign office. He was present with the king during the campaigns of 1866 and 1870-71. In 1851 he published anonymously \"Babylon und Jerusalem,\" a slashing criticism of the views of the Countess von Hahn-Hahn.\n\nDuring the war against Austria in 1866 as well as in the wars against France in 1870 and 1871, Abeken stayed in the Prussian headquarters. A major part of the dispatches of the time have been written by him. Unfortunately his health was damaged by the endeavours of these travels, and he died after an illness of several months. Emperor Wilhelm I described Abeken in a condolence letter to his widow: \"One of my most reliable advisors, standing on my side in the most decisive moments; His loss is irreplaceable to me; In him his fatherland has lost one of the most noble and most loyal men and officials.\"\n\nDespite his engagement in politics, Abeken never lost his interest in theology and continued to publish and speak in this sector during all of his life. He was interested in art and archeology, and was sponsor of the Archeological Institute of Rome and member of the Archeological Society of Rome. He founded a Circle of Friends of the Greek Literature in Berlin and was member of the prize commission for the royal Schiller-Prize.\n\nSee \"Heinrich Abeken, ein schlichtes Leben in bewegter Zeit\" (Berlin, 1898), by his widow. This is valuable by reason of the letters written from the Prussian headquarters.\n\n\n", "id": "14200", "title": "Heinrich Abeken"}
{"url": "https://en.wikipedia.org/wiki?curid=14201", "text": "Henry Bruce, 1st Baron Aberdare\n\nHenry Austin Bruce, 1st Baron Aberdare (16 April 181525 February 1895) was a British Liberal Party politician, who served in government most notably as Home Secretary (1868–1873) and as Lord President of the Council.\n\nHenry Bruce was born at Duffryn, Aberdare, Glamorganshire, the son of John Bruce, a Glamorganshire landowner, and his first wife Sarah, daughter of Reverend Hugh Williams Austin. John Bruce's original family name was Knight, but on coming of age in 1805 he assumed the name of Bruce: his mother, through whom he inherited the Duffryn estate, was the daughter of William Bruce, high sheriff of Glamorganshire.\n\nHenry was educated from the age of twelve at the Bishop Gore School, Swansea (Swansea Grammar School). In 1837 he was called to the bar from Lincoln's Inn. Shortly after he had begun to practice, the discovery of coal beneath the Duffryn and other Aberdare Valley estates brought his family great wealth. From 1847 to 1854 Bruce was stipendiary magistrate for Merthyr Tydfil and Aberdare, resigning the position in the latter year, after entering parliament as Liberal member for Merthyr Tydfil.\n\nBruce was returned unopposed as MP for Merthyr Tydfil in December 1852, with the enthusiastic support of the late member's political allies, notably the iron masters of Dowlais, and he was thereafter regarded by his political opponents, most notably in the Aberdare Valley, as their nominee. Even so, Bruce's parliamentary record demonstrated support for liberal policies, with the exception of the ballot. The electorate in the constituency at this time remained relatively small, excluding the vast majority of the working classes.\n\nSignificantly, however, Bruce's relationship with the miners of the Aberdare Valley, in particular, deteriorated as a result of the Aberdare Strike of 1857-8. In a speech to a large audience of miners at the Aberdare Market Hall, Bruce sought to strike a conciliatory tone in persuading the miners to return to work. In a second speech, however, he delivered a broadside against the trade union movement generally, referring to the violence engendered elsewhere as a result of strikes and to alleged examples of intimidation and violence in the immediate locality. The strike damaged his reputation and may well have contributed to his eventual election defeat ten years later. In 1855, Bruce was appointed a trustee of the Dowlais Iron Company and played a role in the further development of the iron industry.\n\nIn November 1862, after nearly ten years in Parliament, he became Under-Secretary of State for the Home Department, and held that office until April 1864. He became a Privy Councillor and a Charity Commissioner for England and Wales in 1864, when he was moved to be Vice-President of the Council of Education.\n\nAt the 1868 General Election, Merthyr Tydfil became a two-member constituency with a much-increased electorate as a result of the Second Reform Act of 1867. Since the formation of the constituency, Merthyr Tydfil had dominated representation as the vast majority of the electorate lived in the town and its vicinity, whereas there was a much lower number of electors in the neighbouring Aberdare Valley. During the 1850s and 1860s, however, the population of Aberdare grew rapidly, and the franchise changes in 1867 gave the vote to large numbers of miners in that valley. Amongst these new electors, Bruce, as noted above, remained unpopular as a result of his actions during the 1857 -8 dispute. Initially, it appeared that the Aberdare iron master, Richard Fothergill, would be elected to the second seat alongside Bruce. However, the appearance of a third Liberal candidate, Henry Richard, a nonconformist radical popular in both Merthyr and Aberdare, left Bruce on the defensive and he was ultimately defeated, finishing in third place behind both Richard and Fothergill.\n\nAfter losing his seat, Bruce was elected for Renfrewshire on 25 January 1869, he was made Home Secretary by William Ewart Gladstone. His tenure of this office was conspicuous for a reform of the licensing laws, and he was responsible for the Licensing Act 1872, which made the magistrates the licensing authority, increased the penalties for misconduct in public-houses and shortened the number of hours for the sale of drink. In 1873 Bruce relinquished the home secretaryship, at Gladstone's request, to become Lord President of the Council, and was elevated to the peerage as Baron Aberdare, of Duffryn in the County of Glamorgan, on 23 August that year. Being a Gladstonian Liberal, Aberdare had hoped for a much more radical proposal to keep existing licensee holders for a further ten years, and to prevent any new applicants. Its unpopularity pricked his nonconformist's conscience, when like Gladstone himself he had a strong leaning towards Temperance. He had already pursued 'moral improvement' on miners in the regulations attempting to further ban boys from the pits. The Trades Union Act 1871 was another more liberal regime giving further rights to unions, and protection from malicious prosecutions.\n\nThe defeat of the Liberal government in the following year terminated Lord Aberdare's official political life, and he subsequently devoted himself to social, educational and economic questions. Education became one of Lord Aberdare's main interests in later life. His interest had been shown by the speech on Welsh education which he had made on 5 May 1862. In 1880, he was appointed to chair the Departmental Committee on Intermediate and Higher Education in Wales and Monmouthshire, whose report ultimately led to the Welsh Intermediate Education Act of 1889. The report also stimulated the campaign for the provision of university education in Wales. In 1883, Lord Aberdare was elected the first president of the University College of South Wales and Monmouthshire. In his inaugural address he declared that the framework of Welsh education would not be complete until there was a University of Wales. The University was eventually founded in 1893 and Aberdare became its first chancellor.\n\nIn 1876 he was elected a Fellow of the Royal Society; from 1878 to 1891 he was president of the Royal Historical Society. and in 1881 he became president of both the Royal Geographical Society and the Girls' Day School Trust. In 1888 he headed the commission that established the Official Table of Drops, listing how far a person of a particular weight should be dropped when hanged for a capital offence (the only method of 'judicial execution' in the United Kingdom at that time), to ensure an instant and painless death, by cleanly breaking the neck between the 2nd and 3rd vertebrae, an 'exacting science', eventually brought to perfection by Chief Executioner Albert Pierrepoint. Prisoners health, clothing and discipline was a particular concern even at the end of his career. In the Lords he spoke at some length to the Home Affairs Committee chaired by Arthur Balfour about the prison rules system. Aberdare had always maintained a healthy skepticism about intemperate working-classes; in 1878 urging greater vigilance against the vice of excessive drinking, he took evidence on miners and railway colliers habitual imbibing. The committee tried racinate special legislation based on a link between Sunday Opening and absenteeism established in 1868. Aberdare had been interested in the plight of working class drinkers since Gladstone had appointed him Home Secretary. The defeat of the Licensing Bill by the Tory 'beerage' and publicans was drafted to limit hours and protect the public, but it persuaded a convinced Anglican forever more of the iniquities.\n\nIn 1882 he began a connection with West Africa which lasted the rest of his life, by accepting the chairmanship of the National African Company, formed by Sir George Goldie, which in 1886 received a charter under the title of the Royal Niger Company and in 1899 was taken over by the British government, its territories being constituted the protectorate of Nigeria. West African affairs, however, by no means exhausted Lord Aberdare's energies, and it was principally through his efforts that a charter was in 1894 obtained for the University College of South Wales and Monmouthshire,a constituent institution of the University of Wales. This is now Cardiff University. Lord Aberdare, who in 1885 was made a Knight Grand Cross of the Order of the Bath, presided over several Royal Commissions at different times.\n\nHenry Bruce married firstly Annabella, daughter of Richard Beadon, of Clifton by Annabella A'Court, sister of 1st Baron Heytesbury, on 6 January 1846. They had one son and three daughters. \n\nAfter her death on 28 July 1852 he married secondly on 17 August 1854 Norah Creina Blanche, youngest daughter of Lt-Gen Sir William Napier, KCB the historian of the Peninsular War, whose biography he edited, by Caroline Amelia, second daughter of Gen. Hon Henry Edwrad Fox, son of the Earl of Ilchester. They had seven daughters and two sons, of whom :\n\nLord Aberdare died at his London home, 39 Princes Gardens, W, on 25 February 1895, aged 79, and was succeeded in the barony by his only son by his first marriage, Henry. He was survived by his wife, Lady Aberdare, born 1827, who died on 27 April 1897. She was a proponent of women's education and active in the establishment of Aberdare Hall in Cardiff.\n\nHenry Austin Bruce is buried at Aberffrwd Cemetery in Mountain Ash, Wales. His large family plot is surrounded by a chain, and his grave is a simple Celtic cross with double plinth and kerb. In place is written \"To God the Judge of all and to the spirits of just men more perfect.\"\n\n\n", "id": "14201", "title": "Henry Bruce, 1st Baron Aberdare"}
{"url": "https://en.wikipedia.org/wiki?curid=14203", "text": "Harpers Ferry (disambiguation)\n\nHarpers Ferry is the name of several places in the United States of America:\n\nHarpers Ferry may also refer to:\n", "id": "14203", "title": "Harpers Ferry (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=14204", "text": "Halophile\n\nHalophiles are organisms that thrive in high salt concentrations. They are a type of extremophile organisms. The name comes from the Greek word for \"salt-loving\". While most halophiles are classified into the Archaea domain, there are also bacterial halophiles and some eukaryota, such as the alga \"Dunaliella salina\" or fungus \"Wallemia ichthyophaga\". Some well-known species give off a red color from carotenoid compounds, notably bacteriorhodopsin. Halophiles can be found anywhere with a concentration of salt five times greater than the salt concentration of the ocean, such as the Great Salt Lake in Utah, Owens Lake in California, the Dead Sea, and in evaporation ponds.\n\nHalophiles are categorized as slight, moderate, or extreme, by the extent of their halotolerance. Slight halophiles prefer 0.3 to 0.8 M (1.7 to 4.8% — seawater is 0.6 M or 3.5%), moderate halophiles 0.8 to 3.4 M (4.7 to 20%), and extreme halophiles 3.4 to 5.1 M (20 to 30%) salt content. Halophiles require sodium chloride (salt) for growth, in contrast to halotolerant organisms, which do not require salt but can grow under saline conditions.\n\nHigh salinity represents an extreme environment to which relatively few organisms have been able to adapt and occupy. Most halophilic and all halotolerant organisms expend energy to exclude salt from their cytoplasm to avoid protein aggregation ('salting out'). To survive the high salinities, halophiles employ two differing strategies to prevent desiccation through osmotic movement of water out of their cytoplasm. Both strategies work by increasing the internal osmolarity of the cell. In the first (which is employed by the majority of halophilic bacteria, some archaea, yeasts, algae and fungi), organic compounds are accumulated in the cytoplasm — osmoprotectants which are known as compatible solutes. These can be either synthesised or accumulated from the environment. The most common compatible solutes are neutral or zwitterionic, and include amino acids, sugars, polyols, betaines, and ectoines, as well as derivatives of some of these compounds.\n\nThe second, more radical, adaptation involves the selective influx of potassium (K) ions into the cytoplasm. This adaptation is restricted to the moderately halophilic bacterial order Halanaerobiales, the extremely halophilic archaeal family Halobacteriaceae, and the extremely halophilic bacterium \"Salinibacter ruber\". The presence of this adaptation in three distinct evolutionary lineages suggests convergent evolution of this strategy, it being unlikely to be an ancient characteristic retained in only scattered groups or passed on through massive lateral gene transfer. The primary reason for this is the entire intracellular machinery (enzymes, structural proteins, etc.) must be adapted to high salt levels, whereas in the compatible solute adaptation, little or no adjustment is required to intracellular macromolecules; in fact, the compatible solutes often act as more general stress protectants, as well as just osmoprotectants.\n\nOf particular note are the extreme halophiles or haloarchaea (often known as halobacteria), a group of archaea, which require at least a 2 M salt concentration and are usually found in saturated solutions (about 36% w/v salts). These are the primary inhabitants of salt lakes, inland seas, and evaporating ponds of seawater, such as the deep salterns, where they tint the water column and sediments bright colors. These species most likely perish if they are exposed to anything other than a very high-concentration, salt-conditioned environment. These prokaryotes require salt for growth. The high concentration of sodium chloride in their environment limits the availability of oxygen for respiration. Their cellular machinery is adapted to high salt concentrations by having charged amino acids on their surfaces, allowing the retention of water molecules around these components. They are heterotrophs that normally respire by aerobic means. Most halophiles are unable to survive outside their high-salt native environments. Indeed, many cells are so fragile that when placed in distilled water, they immediately lyse from the change in osmotic conditions.\n\nHalophiles may use a variety of energy sources. They can be aerobic or anaerobic. Anaerobic halophiles include phototrophic, fermentative, sulfate-reducing, homoacetogenic, and methanogenic species.\n\nThe Haloarchaea, and particularly the family Halobacteriaceae, are members of the domain Archaea, and comprise the majority of the prokaryotic population in hypersaline environments. Currently, 15 recognised genera are in the family. The domain Bacteria (mainly \"Salinibacter ruber\") can comprise up to 25% of the prokaryotic community, but is more commonly a much lower percentage of the overall population. At times, the alga \"Dunaliella salina\" can also proliferate in this environment.\n\nA comparatively wide range of taxa has been isolated from saltern crystalliser ponds, including members of these genera: \"Haloferax, Halogeometricum, Halococcus, Haloterrigena, Halorubrum, Haloarcula\", and \"Halobacterium\". However, the viable counts in these cultivation studies have been small when compared to total counts, and the numerical significance of these isolates has been unclear. Only recently has it become possible to determine the identities and relative abundances of organisms in natural populations, typically using PCR-based strategies that target 16S small subunit ribosomal ribonucleic acid (16S rRNA) genes. While comparatively few studies of this type have been performed, results from these suggest that some of the most readily isolated and studied genera may not in fact be significant in the \"in situ\" community. This is seen in cases such as the genus \"Haloarcula\", which is estimated to make up less than 0.1% of the\" in situ\" community, but commonly appears in isolation studies.\n\nThe comparative genomic and proteomic analysis showed distinct molecular signatures exist for environmental adaptation of halophiles. At the protein level, the halophilic species are characterized by low hydrophobicity, overrepresentation of acidic residues, underrepresentation of Cys, lower propensities for helix formation, and higher propensities for coil structure. The core of these proteins is less hydrophobic, such as DHFR, that was found to have narrower β-strands \nAt the DNA level, the halophiles exhibit distinct dinucleotide and codon usage.\n\n\"Halobacterium\" is a genus of the Archaea that has a high tolerance for elevated levels of salinity. Some species of halobacteria have acidic proteins that resist the denaturing effects of salts. \"Halococcus\" is a specific genus of the family Halobacteriaceae.\n\nSome hypersaline lakes are a habitat to numerous families of halophiles. For example, the Makgadikgadi Pans in Botswana form a vast, seasonal, high-salinity water body that manifests halophilic species within the diatom genus \"Nitzschia\" in the family Bacillariaceae, as well as species within the genus \"Lovenula\" in the family Diaptomidae. Owens Lake in California also contains a large population of the halophilic bacterium \"Halobacterium halobium\".\n\n\"Wallemia ichthyophaga\" is a basidiomycetous fungus, which requires at least 1.5 M sodium chloride for \"in vitro\" growth, and it thrives even in media saturated with salt. Obligate requirement for salt is an exception in fungi. Even species that can tolerate salt concentrations close to saturation (for example \"Hortaea werneckii\") in almost all cases grow well in standard microbiological media without the addition of salt.\n\nThe fermentation of salty foods (such as soy sauce, Chinese fermented beans, salted cod, salted anchovies, sauerkraut, etc.) often involves halobacteria, as either essential ingredients or accidental contaminants. One example is \"Chromohalobacter beijerinckii\", found in salted beans preserved in brine and in salted herring. \"Tetragenococcus halophilus\" is found in salted anchovies and soy sauce.\n\nNorth Ronaldsay Sheep are a breed of sheep originating from Orkney, Scotland. They have limited access to fresh water sources on the island and to their only food source is seaweed. They have adapted to handle salt concentrations that would kill other breeds of sheep.\n\n\n\n", "id": "14204", "title": "Halophile"}
{"url": "https://en.wikipedia.org/wiki?curid=14205", "text": "Herbert A. Simon\n\nHerbert Alexander Simon (June 15, 1916 – February 9, 2001) was an American political scientist, economist, sociologist, psychologist, and computer scientist whose research ranged across the fields of cognitive psychology, cognitive science, computer science, public administration, economics, management, philosophy of science, sociology, and political science, unified by studies of decision-making. With almost a thousand highly cited publications, he was one of the most influential social scientists of the twentieth century. For many years he held the post of Richard King Mellon Professor at Carnegie Mellon University\n\nSimon was among the pioneers of several of today's important scientific domains, including artificial intelligence, information processing, decision-making, problem-solving, organization theory, complex systems, and computer simulation of scientific discovery.\n\nHe coined the terms \"bounded rationality\" and \"satisficing\", and was among the earliest to analyze the architecture of complexity and to propose a preferential attachment mechanism to explain power law distributions.\n\nHe also received many top-level honors later in life. These include: becoming a fellow of the American Academy of Arts and Sciences in 1959; election to the National Academy of Sciences in 1967; APA Award for Distinguished Scientific Contributions to Psychology (1969);the ACM's Turing Award for making \"basic contributions to artificial intelligence, the psychology of human cognition, and list processing\" (1975); the Nobel Memorial Prize in Economics \"for his pioneering research into the decision-making process within economic organizations\" (1978); the National Medal of Science (1986); the APA's Award for Outstanding Lifetime Contributions to Psychology (1993); ACM fellow (1994); and IJCAI Award for Research Excellence (1995). Simon is currently, as of 2016 the most cited person in Artificial Intelligence and Cognitive Psychology on Google Scholar.\n\nAs a testament to his interdisciplinary approach, Simon was affiliated with such varied Carnegie Mellon departments as the School of Computer Science, Tepper School of Business, departments of Philosophy, Social and Decision Sciences, and Psychology. Simon received an honorary Doctor of Political science degree from University of Pavia in 1988 and an honorary Doctor of Laws (LL.D.) degree from Harvard University in 1990.\n\nHerbert Alexander Simon was born in Milwaukee, Wisconsin on June 15, 1916. His father, Arthur Simon (1881–1948), was an electrical engineer who had come to the United States from Germany in 1903 after earning his engineering degree from the Technische Hochschule of Darmstadt. An inventor who was granted \"several dozen patents\", his father also was an independent patent attorney. His mother, Edna Marguerite Merkel, was an accomplished pianist whose ancestors had come from Prague and Cologne. His European ancestors had been piano makers, goldsmiths, and vintners. Simon's father was Jewish and his mother came from a family with Jewish, Lutheran, and Catholic backgrounds. Simon called himself an atheist.\n\nSimon was educated as a child in the public school system in Milwaukee where he developed an interest in science. He found schoolwork to be interesting and easy. Unlike many children, Simon was exposed to the idea that human behavior could be studied scientifically at a relatively young age due to the influence of his mother’s younger brother, Harold Merkel, who had studied economics at the University of Wisconsin–Madison under John R. Commons. Through his uncle’s books on economics and psychology, Simon discovered the social sciences. Among his earliest influences, Simon has cited Richard Ely’s economics textbook, Norman Angell’s \"The Great Illusion\", and Henry George’s \"Progress and Poverty\". At that time, Simon argued \"from conviction, rather than cussedness\" in favor of George's controversial 'single tax' on land rents.\n\nIn 1933, Simon entered the University of Chicago, and following those early influences, he studied the social sciences and mathematics. He was interested in biology, but chose not to study it because of his \"color-blindness and awkwardness in the laboratory\". He chose instead to focus on political science and economics. His most important mentor at the University was Henry Schultz who was an econometrician and mathematical economist. Simon received both his B.A. (1936) and his Ph.D. (1943) in political science, from the University of Chicago, where he studied under Harold Lasswell, Nicholas Rashevsky, Rudolf Carnap, Henry Schultz, and Charles Edward Merriam.\n\nAfter enrolling in a course on \"Measuring Municipal Governments,\" Simon was invited to be a research assistant for Clarence Ridley, with whom he coauthored the book, \"Measuring Municipal Activities\", in 1938, the same year that he and Dorothea married. Eventually his studies led him to the field of organizational decision-making, which would become the subject of his doctoral dissertation.\n\nAfter graduating with his undergraduate degree, Simon obtained a research assistantship in municipal administration which turned into a directorship at the University of California, Berkeley.\n\nFrom 1942 to 1949, Simon was a professor of political science and also served as department chairman at Illinois Institute of Technology in Chicago. There, he began participating in the seminars held by the staff of the Cowles Commission who at that time included Trygve Haavelmo, Jacob Marschak, and Tjalling Koopmans. He thus began a more in-depth study of economics in the area of institutionalism. Marschak brought Simon in to assist in the study he was currently undertaking with Sam Schurr of the “prospective economic effects of atomic energy”.\n\nFrom 1949 to 2001, Simon was a faculty at Carnegie Mellon. In 1949, Simon became a professor of administration and chairman of the Department of Industrial Management at Carnegie Tech (later to become Carnegie Mellon University). Simon later also taught psychology and computer science in the same university, (occasionally visiting other universities.).\n\nSimon married Dorothea Pye in 1938. Their marriage lasted 63 years until his death from a cancerous tumor. In January 2001, Simon underwent surgery at UPMC Presbyterian to remove a cancerous tumor in his abdomen. Although the surgery was successful, Simon later succumbed to the complications that followed.\n\nThey had three children, Katherine, Peter, and Barbara. His wife died in 2002, during the year following his death in 2001.\n\nFrom 1950 to 1955, Simon studied mathematical economics and during this time, together with David Hawkins, discovered and proved the Hawkins–Simon theorem on the “conditions for the existence of positive solution vectors for input-output matrices.\" He also developed theorems on near-decomposability and aggregation. Having begun to apply these theorems to organizations, by 1954 Simon determined that the best way to study problem-solving was to simulate it with computer programs, which led to his interest in computer simulation of human cognition. Founded during the 1950s, he was among the first members of the Society for General Systems Research.\n\nSimon had a keen interest in the arts, as he was a pianist. He was a friend of Robert Lepper and Richard Rappaport. Rappaport also painted Simon's commissioned portrait at Carnegie Mellon University.\n\nSeeking to replace the highly simplified classical approach to economic modeling, Simon became best known for his theory of corporate decision in his book \"Administrative Behavior\". In this book he based his concepts with an approach that recognized multiple factors that contribute to decision making. His organization and administration interest allowed him to not only serve three times as a university department chairman, but he also played a big part in the creation of the Economic Cooperation Administration in 1948; administrative team that administered aid to the Marshall Plan for the U. S Government, serving on President Johnson's Science Advisory Committee, and also the National Academy of Science. Herbert Simon has made a great number of profound and in depth contributions to both economic analysis and applications. Because of this, his work can be found in a number of economic literary works, making contributions to areas such as mathematical economics including theorem, human rationality, behavioral study of firms, theory of casual ordering, and the analysis of the identification problem in econometrics.\n\n\"Administrative Behavior\",\nfirst appearing in 1947, and updated across the years was based on Simon’s doctoral dissertation. It served as the foundation for his life's work. The centerpiece of this book is the behavioral and cognitive processes of humans making rational choices, that is, decisions. By his definition, an operational administrative decision should be correct and efficient, and it must be practical to implement with a set of coordinated means.\nSimon recognised that a theory of administration is largely a theory of human decision making, and as such must be based on both economics and on psychology. He states:\nContrary to the \"homo economicus\" stereotype, Simon argued that alternatives and consequences may be partly known, and means and ends imperfectly differentiated, incompletely related, or poorly detailed.\n\nSimons defined the task of rational decision making is to select the alternative that results in the more preferred set of all the possible consequences. Correctness of administrative decisions was thus measured by:\n\nThe task of choice was divided into three required steps:\n\n\nAny given individual or organization attempting to implement this model in a real situation would be unable to comply with the three requirements. Simon argued that knowledge of all alternatives, or all consequences that follow from each alternative is impossible in many realistic cases.\n\nSimon attempted to determine the techniques and/or behavioral processes that a person or organization could bring to bear to achieve approximately the best result given limits on rational decision making. Simon writes:\n\nSimon therefore, describes work in terms of an economic framework, conditioned on human cognitive limitations: Economic man and Administrative man.\n\n\"Administrative Behavior\", as a text, addresses a wide range of human behaviors, cognitive abilities, management techniques, personnel policies, training goals and procedures, specialized roles, criteria for evaluation of accuracy and efficiency, and all of the ramifications of communication processes. Simon is particularly interested in how these factors influence the making of decisions, both directly and indirectly.\n\nSimons argued that the two outcomes of a choice require monitoring and that many members of the organization would be expected to focus on adequacy, but that administrative management must pay particular attention to the efficiency with which the desired result was obtained.\n\nSimon followed Chester Barnard who pointed out that “the decisions that an individual makes as a member of an organization are quite distinct from his personal decisions”. Personal choices may be determined whether an individual joins a particular organization, and continue to be made in his or her extra–organizational private life. As a member of an organization, however, that individual makes decisions not in relationship to personal needs and results, but in an impersonal sense as part of the organizational intent, purpose, and effect. Organizational inducements, rewards, and sanctions are all designed to form, strengthen, and maintain this identification.\n\nSimon saw two universal elements of human social behavior as key to creating the possibility of organizational behavior in human individuals: Authority (addressed in Chapter VII—The Role of Authority) and in Loyalties and Identification (Addressed in Chapter X: Loyalties, and Organizational Identification).\n\nAuthority is a well studied, primary mark of organizational behavior, straightforwardly defined in the organizational context as the ability and right of an individual of higher rank to guide the decisions of an individual of lower rank. The actions, attitudes, and relationships of the dominant and subordinate individuals constitute components of role behavior that may vary widely in form, style, and content, but do not vary in the expectation of obedience by the one of superior status, and willingness to obey from the subordinate.\n\nLoyalty was defined by Simon as the \"process whereby the individual substitutes organizational objectives (service objectives or conservation objectives) for his own aims as the value-indices which determine his organizational decisions\". This entailed evaluating alternative choices in terms of their consequences for the group rather than only for onself or ones family.\n\nDecisions can be complex admixtures of facts and values. Information about facts, especially empirically-proven facts or facts derived from specialized experience, are more easily transmitted in the exercise of authority than are the expressions of values. Simon is primarily interested in seeking identification of the individual employee with the organizational goals and values. Following Lasswell, he states that “a person identifies himself with a group when, in making a decision, he evaluates the several alternatives of choice in terms of their consequences for the specified group”. A person may identify himself with any number of social, geographic, economic, racial, religious, familial, educational, gender, political, and sports groups. Indeed, the number and variety are unlimited. The fundamental problem for organizations is to recognize that personal and group identifications may either facilitate or obstruct correct decision making for the organization. A specific organization has to determine deliberately, and specify in appropriate detail and clear language, its own goals, objectives, means, ends, and values.\n\nSimon has been critical of traditional economics’ elementary understanding of decision-making, and argues it \"is too quick to build an idealistic, unrealistic picture of the decision-making process and then prescribe on the basis of such unrealistic picture.\" His contributions to research in the area of administrative decision-making have become increasingly mainstream in the business community.\n\nSimon was a pioneer in the field of artificial intelligence, creating with Allen Newell the Logic Theory Machine (1956) and the General Problem Solver (GPS) (1957) programs. GPS may possibly be the first method developed for separating problem solving strategy from information about particular problems. Both programs were developed using the Information Processing Language (IPL) (1956) developed by Newell, Cliff Shaw, and Simon. Donald Knuth mentions the development of list processing in IPL, with the linked list originally called \"NSS memory\" for its inventors. In 1957, Simon predicted that computer chess would surpass human chess abilities within \"ten years\" when, in reality, that transition took about forty years.\n\nIn the early 1960s psychologist Ulric Neisser asserted that while machines are capable of replicating 'cold cognition' behaviors such as reasoning, planning, perceiving, and deciding, they would never be able to replicate 'hot cognition' behaviors such as pain, pleasure, desire, and other emotions. Simon responded to Neisser's views in 1963 by writing a paper on emotional cognition, which he updated in 1967 and published in \"Psychological Review\". Simon's work on emotional cognition was largely ignored by the artificial intelligence research community for several years, but subsequent work on emotions by Sloman and Picard helped refocus attention on Simon's paper and eventually, made it highly influential on the topic.\n\nSimon also collaborated with James G. March on several works in organization theory.\n\nWith Allen Newell, Simon developed a theory for the simulation of human problem solving behavior using production rules. The study of human problem solving required new kinds of human measurements and, with Anders Ericsson, Simon developed the experimental technique of verbal protocol analysis. Simon was interested in the role of knowledge in expertise. He said that to become an expert on a topic required about ten years of experience and he and colleagues estimated that expertise was the result of learning roughly 50,000 chunks of information. A chess expert was said to have learned about 50,000 chunks or chess position patterns.\n\nHe was awarded the ACM A.M. Turing Award along with Allen Newell in 1975. \"In joint scientific efforts extending over twenty years, initially in collaboration with J. C. (Cliff) Shaw at the RAND Corporation, and with numerous faculty and student colleagues at Carnegie Mellon University, they have made basic contributions to artificial intelligence, the psychology of human cognition, and list processing.\"\n\nSimon was interested in how humans learn and, with Edward Feigenbaum, he developed the EPAM (Elementary Perceiver and Memorizer) theory, one of the first theories of learning to be implemented as a computer program. EPAM was able to explain a large number of phenomena in the field of verbal learning. Later versions of the model were applied to concept formation and the acquisition of expertise. With Fernand Gobet, he has expanded the EPAM theory into the CHREST computational model. The theory explains how simple chunks of information form the building blocks of schemata, which are more complex structures. CHREST has been used predominantly, to simulate aspects of chess expertise.\n\nSimon has been credited for revolutionary changes in microeconomics. He is responsible for the concept of organizational decision-making as it is known today. He also was the first to discuss this concept in terms of uncertainty; i.e. it is impossible to have perfect and complete information at any given time to make a decision. While this notion was not entirely new, Simon is best known for its origination. It was in this area that he was awarded the Nobel Prize in 1978.\n\nAt the Cowles Commission, Simon’s main goal was to link economic theory to mathematics and statistics. His main contributions were to the fields of general equilibrium and econometrics. He was greatly influenced by the marginalist debate that began in the 1930s. The popular work of the time argued that it was not apparent empirically that entrepreneurs needed to follow the marginalist principles of profit-maximization/cost-minimization in running organizations. The argument went on to note that profit-maximization was not accomplished, in part, because of the lack of complete information. In decision-making, Simon believed that agents face uncertainty about the future and costs in acquiring information in the present. These factors limit the extent to which agents may make a fully rational decision, thus they possess only “bounded rationality” and must make decisions by “satisficing,” or choosing that which might not be optimal, but which will make them happy enough. Bounded rationality is a central theme in behavioral economics. It is concerned with the ways in which the actual decision making process influences decision. Theories of bounded rationality relax one or more assumptions of standard expected utility theory.\n\nFurther, Simon emphasized that psychologists invoke a \"procedural\" definition of rationality, whereas economists employ a \"substantive\" definition. Gustavos Barros argued that the procedural rationality concept does not have a significant presence in the economics field and has never had nearly as much weight as the concept of bounded rationality.\nHowever, in an earlier article, Bhargava (1997) noted the importance of Simon's arguments and emphasized that there are several applications of the \"procedural\" definition of rationality in econometric analyses of data on health. In particular, economists should employ \"auxiliary assumptions\" that reflect the knowledge in the relevant biomedical fields, and guide the specification of econometric models for health outcomes.\n\nSimon was also known for his research on industrial organization. He determined that the internal organization of firms and the external business decisions thereof, did not conform to the Neoclassical theories of “rational” decision-making. Simon wrote many articles on the topic over the course of his life mainly focusing on the issue of decision-making within the behavior of what he termed “bounded rationality”. “Rational behavior, in economics, means that individuals maximize their utility function under the constraints they face (e.g., their budget constraint, limited choices, ...) in pursuit of their self-interest. This is reflected in the theory of subjective expected utility. The term, bounded rationality, is used to designate rational choice that takes into account the cognitive limitations of both knowledge and cognitive capacity. Bounded rationality is a central theme in behavioral economics. It is concerned with the ways in which the actual decision-making process influences decisions. Theories of bounded rationality relax one or more assumptions of standard expected utility theory”.\n\nSimon determined that the best way to study these areas was through computer simulation modeling. As such, he developed an interest in computer science. Simon's main interests in computer science were in artificial intelligence, human-computer interaction, principles of the organization of humans and machines as information processing systems, the use of computers to study (by modeling) philosophical problems of the nature of intelligence and of epistemology, and the social implications of computer technology.\n\nIn his youth, Simon took an interest in land economics and Georgism, an idea known at the time as 'single tax'. The system is meant to redistribute unearned economic rent to the public and improve land use. In 1979, Simon still maintained these ideas and argued that land value tax should replace taxes on wages.\n\nSome of Simon's economic research was directed toward understanding technological change in general and the information processing revolution in particular.\n\nSimon's work has strongly influenced John Mighton, developer of a program that has achieved significant success in improving mathematics performance among elementary and high school students. Mighton cites a 2000 paper by Simon and two co-authors that counters arguments by French mathematics educator, Guy Brousseau, and others suggesting that excessive practice hampers children's understanding:\n\n\nSimon was prolific, and authored 27 books and almost a thousand papers.\n\n\n\n\n\n\n\n \n", "id": "14205", "title": "Herbert A. Simon"}
{"url": "https://en.wikipedia.org/wiki?curid=14207", "text": "Hematite\n\nHematite, also spelled as haematite, is the mineral form of iron(III) oxide (FeO), one of several iron oxides. Hematite crystallizes in the rhombohedral lattice system, and it has the same crystal structure as ilmenite and corundum. Hematite and ilmenite form a complete solid solution at temperatures above .\n\nHematite is colored black to steel or silver-gray, brown to reddish brown, or red. It is mined as the main ore of iron. Varieties include \"kidney ore\", \"martite\" (pseudomorphs after magnetite), \"iron rose\" and \"specularite\" (specular hematite). While the forms of hematite vary, they all have a rust-red streak. Hematite is harder than pure iron, but much more brittle. Maghemite is a hematite- and magnetite-related oxide mineral.\n\nHuge deposits of hematite are found in banded iron formations. Gray hematite is typically found in places that can have still standing water or mineral hot springs, such as those in Yellowstone National Park in North America. The mineral can precipitate out of water and collect in layers at the bottom of a lake, spring, or other standing water. Hematite can also occur without water, however, usually as the result of volcanic activity.\n\nClay-sized hematite crystals can also occur as a secondary mineral formed by weathering processes in soil, and along with other iron oxides or oxyhydroxides such as goethite, is responsible for the red color of many tropical, ancient, or otherwise highly weathered soils.\n\nThe name hematite is derived from the Greek word for blood αἷμα \"haima\" because hematite can be red, as in rouge, a powdered form of hematite. The color of hematite lends itself to use as a pigment. The English name of the stone is derived from Middle French: Hématite Pierre, which was imported from Latin: Lapis Hæmatites around the 15th century, which originated from Ancient Greek: αἱματίτης λίθος (\"haimatitēs\" lithos, \"blood-red stone\").\n\nOchre is a clay that is colored by varying amounts of hematite, varying between 20% and 70%. Red ochre contains unhydrated hematite, whereas yellow ochre contains hydrated hematite (FeO • HO). The principal use of ochre is for tinting with a permanent color.\n\nThe red chalk writing of this mineral was one of the earliest in the history of humans. The powdery mineral was first used 164,000 years ago by the Pinnacle-Point man possibly for social purposes. Hematite residues are also found in graves from 80,000 years ago. Near Rydno in Poland and Lovas in Hungary red chalk mines have been found that are from 5000 BC, belonging to the Linear Pottery culture at the Upper Rhine.\n\nRich deposits of hematite have been found on the island of Elba that have been mined since the time of the Etruscans.\n\nHematite is an antiferromagnetic material below the Morin transition at 250 kelvin (K) or -9.7 degrees Fahrenheit (°F), and a canted antiferromagnet or weakly ferromagnetic above the Morin transition and below its Néel temperature at 948 K, above which it is paramagnetic.\n\nThe magnetic structure of a-hematite was the subject of considerable discussion and debate in the 1950s because it appeared to be ferromagnetic with a Curie temperature of around 1000 K, but with an extremely tiny magnetic moment (0.002 µ). Adding to the surprise was a transition with a decrease in temperature at around 260 K to a phase with no net magnetic moment. It was shown that the system is essentially antiferromagnetic, but that the low symmetry of the cation sites allows spin–orbit coupling to cause canting of the moments when they are in the plane perpendicular to the c axis. The disappearance of the moment with a decrease in temperature at 260 K is caused by a change in the anisotropy which causes the moments to align along the c axis. In this configuration, spin canting does not reduce the energy. The magnetic properties of bulk hematite differ from their nanoscale counterparts. For example, the Morin transition temperature of hematite decreases with a decrease in the particle size. The suppression of this transition has also been observed in some of the hematite nanoparticles, and the presence of impurities, water molecules and defects in the crystals were attributed to the absence of a Morin transition. Hematite is part of a complex solid solution oxyhydroxide system having various contents of water, hydroxyl groups and vacancy substitutions that affect the mineral's magnetic and crystal chemical properties. Two other end-members are referred to as protohematite and hydrohematite.\n\nEnhanced magnetic coercivities for hematite have been achieved by dry-heating a 2-line ferrihydrite precursor prepared from solution. Hematite exhibited temperature-dependent magnetic coercivity values ranging from 289 to 5,027 Oe. The origin of these high coercivity values has been interpreted as a consequence of the subparticle structure induced by the different particle and crystallite size growth rates at increasing annealing temperature. These differences in the growth rates are translated into a progressive development of a subparticle structure at the nanoscale. At lower temperatures (350–600 °C), single particles crystallize however; at higher temperatures (600-1000 °C), the growth of crystalline aggregates with a subparticle structure is favored.\n\nHematite is present in the waste tailings of iron mines. A recently developed process, magnetation, uses magnets to glean waste hematite from old mine tailings in Minnesota's vast Mesabi Range iron district. Falu red is a pigment used in traditional Swedish house paints. Originally, it was made from tailings of the Falu mine.\n\nThe spectral signature of hematite was seen on the planet Mars by the infrared spectrometer on the NASA Mars Global Surveyor (\"MGS\") and 2001 Mars Odyssey spacecraft in orbit around Mars. The mineral was seen in abundance at two sites on the planet, the Terra Meridiani site, near the Martian equator at 0° longitude, and the Aram Chaos site near the Valles Marineris. Several other sites also showed hematite, e.g., Aureum Chaos. Because terrestrial hematite is typically a mineral formed in aqueous environments or by aqueous alteration, this detection was scientifically interesting enough that the second of the two Mars Exploration Rovers was sent to a site in the Terra Meridiani region designated Meridiani Planum. In-situ investigations by the Opportunity rover showed a significant amount of hematite, much of it in the form of small spherules that were informally named \"blueberries\" by the science team. Analysis indicates that these spherules are apparently concretions formed from a water solution.\n\"Knowing just how the hematite on Mars was formed will help us characterize the past environment and determine whether that environment was favorable for life\".\n\nHematite's popularity in jewelry was at its highest in Europe during the Victorian era. Certain types of hematite or iron oxide-rich clay, especially Armenian bole, have been used in gilding. Hematite is also used in art such as in the creation of intaglio engraved gems. Hematine is a synthetic material sold as \"magnetic hematite\".\n\n\n", "id": "14207", "title": "Hematite"}
{"url": "https://en.wikipedia.org/wiki?curid=14208", "text": "Holocene extinction\n\nThe Holocene extinction, otherwise referred to as the Sixth extinction or Anthropocene extinction, is the ongoing extinction event of species during the present Holocene epoch mainly due to human activity. The large number of extinctions span numerous families of plants and animals including mammals, birds, amphibians, reptiles and arthropods. with widespread degradation of highly biodiverse habitats such as coral reefs and rainforest, as well as other areas, the vast majority are thought to be undocumented. According to the species-area theory and based on upper-bound estimating, the present rate of extinction may be up to 140,000 species per year, making it the greatest loss of biodiversity since the Cretaceous–Paleogene extinction event.\n\nThe Holocene extinction includes the disappearance of large land animals known as megafauna, starting at the end of the last Ice Age. Megafauna outside of the African continent, which did not evolve alongside humans, proved highly sensitive to the introduction of new predation, and many died out shortly after early humans began spreading and hunting across the Earth (additionally, many African species have also gone extinct in the Holocene). These extinctions, occurring near the Pleistocene–Holocene boundary, are sometimes referred to as the Quaternary extinction event.\n\nThe arrival of humans on different continents coincide with megafaunal extinction. The most popular theory is that human overhunting species added to existing stress conditions. Although there is debate on how much human predation affected their decline, certain population declines have been directly correlated with human activity, such as the extinction events of New Zealand and Hawaii. Aside from humans, climate change may have been a driving factor in the megafaunal extinctions, especially at the end of the Quaternary.\n\nThe ecology of humanity has been noted as being that of an unprecedented \"global superpredator\" that regularly preys on the adults of other apex predators and has worldwide effects on food webs. Extinctions of species have occurred on every land mass and ocean, with many famous examples within Africa, Asia, Europe, Australia, North and South America, and on smaller islands. Overall, the Holocene extinction can be characterized by the human impact on the environment. The Holocene extinction continues into the 21st century, with meat consumption, overfishing, ocean acidification and the amphibian crisis being a few broader examples of an almost universal, cosmopolitan decline in biodiversity. A ballooning human population along with profligate consumption are considered to be the primary drivers of this rapid decline.\n\nThe Holocene extinction is also known as the \"Sixth extinction\", due to it being the sixth mass extinct event, proceeding the Ordovician–Silurian extinction events, the Late Devonian extinction, the Permian–Triassic extinction event, the Triassic–Jurassic extinction event, and the Cretaceous–Paleogene extinction event. There is no general agreement on where the Holocene, or anthropogenic, extinction begins, and the Quaternary extinction event, which includes climate change resulting in the end of the last ice age, ends, or if they should be considered separate events at all. Some have suggested that anthropogenic extinctions may have begun as early as when the first modern humans spread out of Africa between 100,000 and 200,000 years ago, which is supported by rapid megafaunal extinction following recent human colonisation in Australia, New Zealand and Madagascar, in a similar way that any large, adaptable predator moving into a new ecosystem would. In many cases, it is suggested even minimal hunting pressure was enough to wipe out large fauna, particularly on geographically isolated islands. Only during the most recent parts of the extinction have plants also suffered large losses.\n\nIn \"The Future of Life\" (2002), E.O. Wilson of Harvard calculated that, if the current rate of human disruption of the biosphere continues, one-half of Earth's higher lifeforms will be extinct by 2100. A 1998 poll conducted by the American Museum of Natural History found that seventy percent of biologists acknowledge the existence of the anthropogenic extinction. Numerous scientific studies — such as a 2004 report published in \"Nature\", and papers authored by the IUCN's annual Red List of threatened species — have since reinforced this conviction. At present, the rate of extinction of species is estimated at 100 to 1,000 times higher than the \"base\" or historically typical rate of extinction (in terms of the natural evolution of the planet) and also the current rate of extinction is, therefore, 10 to 100 times higher than any of the previous mass extinctions in the history of Earth. It is also the only known mass extinction of plants. One scientist estimates the current extinction rate may be 10,000 times the background extinction rate. Nevertheless, most scientists predict a much lower extinction rate than this outlying estimate. Stuart Pimm stated \"the current rate of species extinction is about 100 times the natural rate\" for plants. Mass extinctions are characterized by the loss of at least 75% of species within a geologically short period of time.\n\nIn a pair of studies published in 2015, extrapolation from observed extinction of Hawaiian snails led to the conclusion that 7% of all species on Earth may have been lost already.\n\nThe abundance of species extinctions considered anthropogenic, or due to human activity, have sometimes (especially when referring to hypothesized future events) been collectively called the \"Anthropocene extinction\". \"Anthropocene\" is a term introduced in 2000. It is now posited by some that a new geological epoch has begun, characterised by the most abrupt and widespread extinction of species since the Cretaceous–Paleogene extinction event 66 million years ago.\n\nThe term \"anthropocene\" is used by few scientists, and some commentators may refer to the current and projected future extinctions as part of a longer Holocene extinction. The Holocene–Anthropocene boundary is contested, with some commentators asserting significant human influence on climate for much of what is normally regarded as the Holocene Epoch. Other commentators place the Holocene–Anthropocene boundary at the industrial revolution while also saying that, \"[f]ormal adoption of this term in the near future will largely depend on its utility, particularly to earth scientists working on late Holocene successions.\" \n\nIt has been suggested that human activity has made the period following the mid-20th century different enough from the rest of the Holocene to consider it a new geological epoch, known as the Anthropocene, which was considered for implementation into the timeline of Earth's history by the International Commission on Stratigraphy in 2016. In order to constitute the Holocene as an extinction event, scientists must determine exactly when anthropogenic greenhouse gas emissions began to measurably alter natural atmospheric levels at a global scale and when these alterations caused changes to global climate. Employing chemical proxies from Antarctic ice cores, researchers have estimated the fluctuations of carbon dioxide (CO) and methane gases (CH) in the earth’s atmosphere for the late Pleistocene and Holocene epochs. Based on studies that estimated fluctuations of carbon dioxide and methane in the atmosphere using chemical proxies from Antarctic ice cores, general argumentation of when the peak of the Anthropocene occurred pertains to the timeframe within the previous two centuries; typically beginning with the Industrial Revolution, when greenhouse gas levels were recorded by contemporary methods at its highest.\n\nThe Holocene extinction is mainly caused by human activity. Extinction of animals, plants, and other organisms caused by human actions may go as far back as the late Pleistocene, over 12,000 years ago. There is a correlation between megafaunal extinction and the arrival of humans, and human population growth, most prominently in the past two centuries, is regarded as one of the underlying causes of extinction.\n\nMegafauna was once found on every continent of the world and large islands such as New Zealand and Madagascar, but is now almost exclusively found on the continent of Africa, with notable comparisons on Australia and the islands previously mentioned experiences population crashes and trophic cascades shortly after the earliest human settlers. It has been suggested that the African megafauna survived as they evolved alongside humans. The timing of South American megafaunal extinction does not appear to correspond to human arrival, although the possibility of whether human activity at the time may have impacted the global climate enough to cause such an extinction has been suggested. It has been noted, in the face of such evidence, humans are unique in ecology as an unprecedented 'global superpredator', regularly preying on large numbers of fully grown terrestrial and marine apex predators, and with a great deal of influence over food webs and climatic systems worldwide. Although significant debate exists as to how much human predation and indirect effects contributed to prehistoric extinctions, certain population crashes have been directly correlated with human arrival.\n\nHuman civilization flourished in accordance to the efficiency and intensification of prevailing subsistence systems. Local communities that acquire more subsistence strategies increased in number to combat competitive pressures of land utilization. Therefore, the Holocene developed competition on the basis of agriculture. The growth of agriculture has then introduced newer means of climate change, pollution, and ecological development.\nHabitat destruction by humans, including oceanic devastation, such as through overfishing and contamination; and the modification and destruction of vast tracts of land and river systems around the world to meet solely human-centered ends (with 13 percent of Earth's ice-free land surface now used as row-crop agricultural sites, 26 percent used as pastures, and 4 percent urban-industrial areas), thus replacing the original local ecosystems. Other, related human causes of the extinction event include deforestation, hunting, pollution, the introduction in various regions of non-native species, and the widespread transmission of infectious diseases spread through livestock and crops. \n\nRecent investigations about hunter-gatherer landscape burning has a major implication for the current debate about the timing of the Anthropocene and the role that humans may have played in the production of greenhouse gases prior to the Industrial Revolution. Studies on early hunter-gatherers raises questions about the current use of population size or density as a proxy for the amount of land clearance and anthropogenic burning that took place in pre-industrial times. Scientists have questioned the correlation between population size and early territorial alterations. Ruddiman and Ellis' research paper in 2009 makes the case that early farmers involved in systems of agriculture used more land per capita than growers later in the Holocene, who intensified their labor to produce more food per unit of area (thus, per laborer); arguing that agricultural involvement in rice production implemented thousands of years ago by relatively small populations have created significant environmental impacts through large-scale means of deforestation.\n\nWhile a number of human-derived factors are recognized as potentially contributing to rising atmospheric concentrations of CH and CO, deforestation and territorial clearance practices associated with agricultural development may be contributing most to these concentrations globally. Scientists that are employing a variance of archaeological and paleoecological data argue that the processes contributing to substantial human modification of the environment spanned many thousands of years ago on a global scale and thus, not originating as early as the Industrial Revolution. Gaining popularity on his uncommon hypothesis, Palaeoclimatologist William Ruddiman in 2003, stipulated that in the early Holocene 11,000 years ago, atmospheric carbon dioxide and methane levels has fluctuated in a pattern which was different from the Pleistocene epoch before it. He argued that the patterns of the significant decline of CO levels during the last ice age of the Pleistocene inversely correlates to the Holocene where there has been dramatic increases of CO around 8000 years ago and CH levels 3000 years after that. The correlation between the decrease of CO in the Pleistocene and the increase of it during the Holocene implies that the causation of this spark of greenhouse gases into the atmosphere was the growth of human agriculture during the Holocene such as the anthropogenic expansion of (human) land use and irrigation.\n\nHuman arrival in the Caribbean around 6,000 years ago is correlated with the extinction of many species. Examples include many different genera of ground and arboreal sloths across all islands. These sloths were generally smaller than those found on the South American continent. \"Megalocnus\" were the largest genus at up to , \"Acratocnus\" were medium-sized relatives of modern two-toed sloths endemic to Cuba, \"Imagocnus\" also of Cuba, \"Neocnus\" and many others. \n\nRecent research, based on archaeological and paleontological digs on 70 different Pacific islands has shown that numerous species became extinct as people moved across the Pacific, starting 30,000 years ago in the Bismarck Archipelago and Solomon Islands. It is currently estimated that among the bird species of the Pacific, some 2000 species have gone extinct since the arrival of humans, representing a 20% drop in the biodiversity of birds worldwide. \n\nThe first settlers are thought to have arrived in the islands between 300 and 800 CE, with European arrival in the 16th century. Hawaii is notable for its endemism of plants, birds, insects, mollusks and fish; 30% of its organisms are endemic. Many of its species are endangered or have gone extinct, primarily due to accidentally introduced species and livestock grazing. Over 40% of its bird species have gone extinct, and it is the location of 75% of extinctions in the United States. Extinction has increased in Hawaii over the last 200 years and is relatively well documented, with extinctions among native snails used as estimates for global extinction rates.\n\nAustralia was once home to a large assemblage of megafauna, with many parallels to those found on the African continent today. Australia's fauna is characterised by primarily marsupial mammals, and many reptiles and birds, all existing as giant forms until recently. Humans arrived on the continent very early, about 50,000 years ago. The extent human arrival contributed is controversial; climatic drying of Australia 40,000–60,000 years ago was an unlikely cause, as it was less severe in speed or magnitude than previous regional climate change which failed to kill off megafauna. \nExtinctions in Australia continued from original settlement until today in both plants and animals, whilst many more animals and plants have declined or are endangered. \n\nDue to the older timeframe and the soil chemistry on the continent, very little subfossil preservation evidence exists relative to elsewhere. However, continent-wide extinction of all genera weighing over 100 kilograms, and six of seven genera weighing between 45 and 100 kilograms occurred around 46,400 years ago (4,000 years after human arrival) and the fact that megafauna survived until a later date on the island of Tasmania following the establishment of a land bridge suggest direct hunting or anthropogenic ecosystem disruption such as fire-stick farming as likely causes. The first evidence of direct human predation leading to extinction in Australia was published in 2016.\n\nWithin 500 years of the arrival of humans between 2,500–2,000 years ago, nearly all of Madagascar's distinct, endemic and geographically isolated megafauna became extinct. The largest animals, of more than , were extinct very shortly after the first human arrival, with large and medium-sized species dying out after prolonged hunting pressure from an expanding human population moving into more remote regions of the island around 1000 years ago. Smaller fauna experienced initial increases due to decreased competition, and then subsequent declines over the last 500 years. All fauna weighing over died out. The primary reasons for this are human hunting and habitat loss from early aridification, both of which persist and threaten Madagascar's remaining taxa today.\n\nThe eight or more species of elephant birds, giant flightless ratites in the genera \"Aepyornis\" and \"Mullerornis\", are extinct from over-hunting, as well as 17 species of lemur, known as giant, subfossil lemurs. Some of these lemurs typically weighed over , and fossils have provided evidence of human butchery on many species.\n\nNew Zealand is characterised by its geographic isolation and island biogeography, and had been isolated from mainland Australia for 80 million years. It was the last large land mass to be colonised by humans. The arrival of Polynesian settlers circa 12th century resulted in the extinction of all of the islands' megafaunal birds within several hundred years. The last moa, large flightless ratites, became extinct within 200 years of the arrival of human settlers. The Polynesians also introduced the Polynesian rat. This may have put some pressure on other birds but at the time of early European contact (18th Century) and colonisation (19th Century) the bird life was prolific. With them, the Europeans brought ship rats, possums, cats and mustelids which decimated native bird life, some of which had adapted flightlessness and ground nesting habits and others had no defensive behavior as a result of having no extant endemic mammalian predators. The kakapo, the world's biggest parrot, which is flightless, now only exists in managed breeding sanctuaries and NZ's national emblem, the kiwi, is on the endangered bird list.\n\nThere has been a debate as to the extent to which the disappearance of megafauna at the end of the last glacial period can be attributed to human activities by hunting, or even by slaughter of prey populations. Discoveries at Monte Verde in South America and at Meadowcroft Rock Shelter in Pennsylvania have caused a controversy regarding the Clovis culture. There likely would have been human settlements prior to the Clovis Culture, and the history of humans in the Americas may extend back many thousands of years before the Clovis culture. The amount of correlation between human arrival and megafauna extinction is still being debated: for example, in Wrangel Island in Siberia the extinction of dwarf woolly mammoths (approximately 2000 BCE) did not coincide with the arrival of humans, nor did megafaunal mass extinction on the South American continent, although it has been suggested climate changes induced by anthropogenic effects elsewhere in the world may have contributed.\n\nComparisons are sometimes made between recent extinctions (approximately since the industrial revolution) and the Pleistocene extinction near the end of the last glacial period. The latter is exemplified by the extinction of large herbivores such as the woolly mammoth and the carnivores that preyed on them. Humans of this era actively hunted the mammoth and the mastodon but it is not known if this hunting was the cause of the subsequent massive ecological changes, widespread extinctions and climate changes.\n\nThe ecosystems encountered by the first Americans had not been exposed to human interaction, and may have been far less resilient to human made changes than the ecosystems encountered by industrial era humans. Therefore, the actions of the Clovis people, despite seeming insignificant by today's standards could indeed have had a profound effect on the ecosystems and wild life which was entirely unused to human influence.\n\nAfrica experienced the smallest decline in megafauna compared to the other continents. This is presumably due to the idea that Afroeurasian megafauna evolved alongside humans, and thus developed a healthy fear of them, unlike the comparatively tame animals of other continents. Unlike other continents, the megafauna of Eurasia went extinct over a relatively long period of time, possibly due to climate fluctuations fragmenting and decreasing populations, leaving them vulnerable to over-exploitation, as with the steppe bison (\"Bison priscus\"). The warming of the arctic region caused the rapid decline of grasslands, which had a negative effect on the grazing megafauna of Eurasia. Most of what once was mammoth steppe has been converted to mire, rendering the environment incapable of supporting them, notably the woolly mammoth.\n\nOne of the main theories to the extinction is climate change. The climate change theory has suggested that a change in climate near the end of the late Pleistocene stressed the megafauna to the point of extinction. Some scientists favor abrupt climate change as the catalyst for the extinction of the mega-fauna at the end of the Pleistocene, but there are many who believe increased hunting from early modern humans also played a part, with others even suggesting that the two interacted. However, the annual mean temperature of the current interglacial period for the last 10,000 years is no higher than that of previous interglacial periods, yet some of the same megafauna survived similar temperature increases. In the Americas, a controversial explanation for the shift in climate is presented under the Younger Dryas impact hypothesis, which states that the impact of comets cooled global temperatures.\n\nMegafauna play a significant role in the lateral transport of mineral nutrients in an ecosystem, tending to translocate them from areas of high to those of lower abundance. They do so by their movement between the time they consume the nutrient and the time they release it through elimination (or, to a much lesser extent, through decomposition after death). In South America's Amazon Basin, it is estimated that such lateral diffusion was reduced over 98% following the megafaunal extinctions that occurred roughly 12,500 years ago. Given that phosphorus availability is thought to limit productivity in much of the region, the decrease in its transport from the western part of the basin and from floodplains (both of which derive their supply from the uplift of the Andes) to other areas is thought to have significantly impacted the region's ecology, and the effects may not yet have reached their limits. The extinction of the mammoths allowed grasslands they had maintained through grazing habits to become birch forests. The new forest and the resulting forest fires may have induced climate change. Such disappearances might be the result of the proliferation of modern humans.\n\nLarge populations of megaherbivores have the potential to contribute greatly to the atmospheric concentration of methane, which is an important greenhouse gas. Modern ruminant herbivores produce methane as a byproduct of foregut fermentation in digestion, and release it through belching or flatulence. Today, around 20% of annual methane emissions come from livestock methane release. In the Mesozoic, it has been estimated that sauropods could have emitted 520 million tons of methane to the atmosphere annually, contributing to the warmer climate of the time (up to 10 °C warmer than at present). This large emission follows from the enormous estimated biomass of sauropods, and because methane production of individual herbivores is believed to be almost proportional to their mass.\n\nRecent studies have indicated that the extinction of megafaunal herbivores may have caused a reduction in atmospheric methane. This hypothesis is relatively new. One study examined the methane emissions from the bison that occupied the Great Plains of North America before contact with European settlers. The study estimated that the removal of the bison caused a decrease of as much as 2.2 million tons per year. Another study examined the change in the methane concentration in the atmosphere at the end of the Pleistocene epoch after the extinction of megafauna in the Americas. After early humans migrated to the Americas about 13,000 BP, their hunting and other associated ecological impacts led to the extinction of many megafaunal species there. Calculations suggest that this extinction decreased methane production by about 9.6 million tons per year. This suggests that the absence of megafaunal methane emissions may have contributed to the abrupt climatic cooling at the onset of the Younger Dryas. The decrease in atmospheric methane that occurred at that time, as recorded in ice cores, was 2-4 times more rapid than any other decrease in the last half million years, suggesting that an unusual mechanism was at work.\n\nThe hyperdisease hypothesis, proposed by Ross MacPhee in 1997, states that the megafaunal die-off was due to an indirect transmission of diseases by newly arriving aboriginal humans. According to MacPhee, aboriginals or animals travelling with them, such as domestic dogs or livestock, introduced one or more highly virulent diseases into new environments whose native population had no immunity to, eventually leading to their extinction. K-selection animals, such as the now-extinct megafauna, are especially vulnerable to diseases, as opposed to r-selection animals who have a shorter gestation period and a higher population size. Humans are thought to be the sole cause as other earlier migrations of animals into North America from Eurasia did not cause extinctions.\n\nThere are many problems with this theory in the scientific community, as this disease would have to meet several criteria: it has to be able to sustain itself in an environment with no hosts; it has to have a high infection rate; and be extremely lethal, with a mortality rate of 50–75%. Disease has to be very virulent to kill off all the individuals in a genus or species, and even such a virulent disease as West Nile Virus is unlikely to have caused extinction.\n\nHowever, diseases have been the cause for some extinctions. The introduction of avian malaria and avipoxvirus, for example, have had a negative impact on the endemic birds of Hawaii.\n\nThe loss of species from ecological communities, defaunation, is primarily driven by human activity. This has resulted in empty forests, ecological communities depleted of large vertebrates. This is not to be confused with extinction, as it includes both the disappearance of species and declines in abundance. Defaunation effects were first implied at the Symposium of Plant-Animal Interactions at the University of Campinas, Brazil in 1988 in the context of neotropical forests. Since then, the term has gained broader usage in conservation biology as a global phenomenon.\n\nBig cat populations have been decimated over the last half-century and could face extinction in the following decades. According to IUCN estimates: lions are down to 25,000, from 450,000; leopards are down to 50,000, from 750,000; cheetahs are down to 12,000, from 45,000; tigers are down to 3,000 in the wild, from 50,000. A December 2016 study by the Zoological Society of London, Panthera Corporation and Wildlife Conservation Society showed that cheetahs are far closer to extinction than previously thought, with only 7,100 remaining in the wild, and crammed within only 9% of their historic range. Human pressures are to blame for the cheetah population crash, including prey loss due to overhunting by people, retaliatory killing from farmers, habitat loss and the illegal wildlife trade.\n\nThe term pollinator decline refers to the reduction in abundance of insect and other animal pollinators in many ecosystems worldwide beginning at the end of the twentieth century, and continuing into the present day. Pollinators, which are necessary for 75% of food crops, are declining globally in both abundance and diversity.\n\nVarious species are predicted to become extinct in the near future, among them the rhinoceros, primates, pangolins, and giraffes. Hunting alone threatens hundreds of mammalian species around the world. Scientists claim that the growing demand for meat is contributing to biodiversity loss as this is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon region, are being converted to agriculture for meat production. According to the World Wildlife Fund's 2016 Living Planet Index, 67% of the world’s wildlife could disappear by 2020 due to habitat destruction, over-hunting and pollution. 189 countries, which are signatory to the Convention on Biological Diversity (Rio Accord), have committed to preparing a Biodiversity Action Plan, a first step at identifying specific endangered species and habitats, country by country.\n\nRecent extinctions are more directly attributable to human influences, whereas prehistoric extinctions can be attributed to other factors, such as global climate change. The International Union for Conservation of Nature (IUCN) characterises 'recent' extinction as those that have occurred past the cut-off point of 1500, and at least 875 species have gone extinct since that time and 2012. Some species, such as the Père David's deer and the Hawaiian crow, are extinct in the wild, and survive solely in captive populations. Other species, such as the Florida panther, are ecologically extinct, surviving in such low numbers that that they essentially have no impact on the ecosystem. Other populations are only locally extinct (extirpated), still existence elsewhere, but reduced in distribution, as with the extinction of gray whales in the Atlantic, and of the leatherback sea turtle in Malaysia.\n\nGlobal warming is widely accepted as being a contributor to extinction worldwide, in a similar way that previous extinction events have generally included a rapid change in global climate and meteorology. It is also expected to disrupt sex ratios in many reptiles which have temperature-dependent sex determination.\nThe removal of land to clear way for palm oil plantations releases carbon emissions held in the peatlands of Indonesia. Palm oil mainly serves as a cheap cooking oil, and also as a (controversial) biofuel. However, damage to peatland contributes to 4% of global greenhouse gas emissions, and 8% of those caused by burning fossil fuels. Palm oil cultivation has also been criticized for other impacts to the environment, including deforestation, which has threatened critically endangered species such as the orangutan. The IUCN stated in 2016 that the species could go extinct within a decade if measures are not taken to preserve the rainforests in which they live.\n\nRising levels of carbon dioxide are resulting in influx of this gas into the ocean, increasing its acidity. Marine organisms which possess Calcium Carbonate shells or exoskeletons experience physiological pressure as the carbonate reacts with acid. This is already resulting in coral bleaching on various coral reefs worldwide, which provide valuable habitat for very high biodiversity. Marine gastropods, bivalves and other invertebrates are also affected, as are any organisms that feed on them.\n\nSome researchers suggest that by 2050 there could be more plastic than fish in the oceans by weight.\n\nOverhunting can reduce the local population of game animals by more than half, as well as reducing population density, and may lead to extinction for some species. Populations located nearer to villages are significantly more at risk of depletion.\n\nThe surge in the mass killings by poachers involved in the illegal ivory trade along with habitat loss is threatening African elephant populations. In 1979, their populations stood at 1.7 million; at present there are fewer than 400,000 remaining. Prior to European colonization, scientists believe Africa was home to roughly 20 million elephants. According to the Great Elephant Census, 30% of African elephants (or 144,000 individuals) disappeared over a seven-year period, 2007 to 2014. African elephants could become extinct within 20 years if poaching rates continue.\nFishing has had a devastating effect on marine organism populations for several centuries even before the explosion of destructive and highly effective fishing practices like trawling. Humans are unique among predators in that they regularly predate on other adult apex predators, particularly in marine environments; bluefin tuna, blue whales, and various sharks in particular are particularly vulnerable to predation pressure from human fishing. A 2016 study published in \"Science\" concludes that humans tend to hunt larger species, and this could disrupt ocean ecosystems for millions of years.\n\nThe decline of amphibian populations has also been identified as an indicator of environmental degradation. As well as habitat loss, introduced predators and pollution, Chytridiomycosis, a fungal infection thought to have been accidentally spread by human travel, has caused severe population drops of several species of frogs, including (among many others) the extinction of the golden toad in Costa Rica and the Gastric-brooding frog in Australia. Many other amphibian species now face extinction, including the reduction of Rabb's fringe-limbed treefrog to an endling, and the extinction of the Panamanian golden frog in the wild. Chytrid fungus has spread across Australia, New Zealand, Central America and Africa, including countries with high amphibian diversity such as cloud forests in Honduras and Madagascar. \"Batrachochytrium salamandrivorans\" is a similar infection currently threatening salamanders. Amphibians are now the most endangered vertebrate group, having existed for more than 300 million years through three other mass extinctions.\n\nMillions of bats in the US have been dying off since 2012 due to a fungal infection spread from European bats, which appear to be immune. Population drops have been as great as 90% within five years, and extinction of at least one bat species is predicted. There is currently no form of treatment, and such declines have been described as \"unprecedented\" in bat evolutionary history by Alan Hicks of the New York State Department of Environmental Conservation.\n\n\n", "id": "14208", "title": "Holocene extinction"}
{"url": "https://en.wikipedia.org/wiki?curid=14209", "text": "Hollywood-style Lindy Hop\n\nHollywood-style Lindy Hop is a variety of Lindy Hop, an American vernacular dance. It is also sometimes referred to as Dean Collins or Smooth-style, but these terms also sometimes refer to different styles of Lindy Hop.\n\nHollywood is the style reconstructed by Erik Robison and Sylvia Skylar based on movies from 1930s and 1940s featuring dancers like Dean Collins, Jewel McGowan, Jean Veloz and others.. They were the first to call it \"Hollywood Style\".\n\nThe swingout (the basic step of Lindy) is danced in a position often described as someone about to sit on a stool, thereby bringing their center point of balance closer to the ground. This piked position is the classic look of Hollywood with the back straight and a slight forward tilt. The Hollywood style is also a slotted dance, meaning the follower travels in a straight line instead of the more elliptical or circular Savoy-style Lindy Hop.\n\nA popular variation of Hollywood-Style Lindy Hop called LA-style Lindy Hop has a few technical changes in the footwork and fewer steps. The steps are shortened or \"cheated\" to create this look. The style is geared towards performance and is heavily based on short choreographies. Originating in Los Angeles, California, LA-style is a favorite on the West Coast of the United States.\n", "id": "14209", "title": "Hollywood-style Lindy Hop"}
{"url": "https://en.wikipedia.org/wiki?curid=14210", "text": "Harrison Narcotics Tax Act\n\nThe Harrison Narcotics Tax Act (Ch. 1, ) was a United States federal law that regulated and taxed the production, importation, and distribution of opiates and coca products. The act was proposed by Representative Francis Burton Harrison of New York and was approved on December 17, 1914.\n\n\"An Act To provide for the registration of, with collectors of internal revenue, and to impose a special tax on all persons who produce, import, manufacture, compound, deal in, dispense, sell, distribute, or give away opium or coca leaves, their salts, derivatives, or preparations, and for other purposes.\" The courts interpreted this to mean that physicians could prescribe narcotics to patients in the course of normal treatment, but not for the treatment of addiction.\n\nThe Harrison Anti-Narcotic legislation consisted of three U.S. House bills imposing restrictions on the availability and consumption of the psychoactive drug opium. U.S. House bills and passed conjointly with House bill or the Opium and Coca Leaves Trade Restrictions Act.\n\nAlthough technically illegal for purposes of distribution and use, the distribution, sale and use of cocaine was still legal for registered companies and individuals.\n\nFollowing the Spanish–American War the U.S. acquired the Philippines from Spain. At that time, opium addiction constituted a significant problem in the civilian population of the Philippines.\n\nCharles Henry Brent was an American Episcopal bishop who served as Missionary Bishop of the Philippines beginning in 1901. He convened a Commission of Inquiry, known as the Brent Commission, for the purpose of examining alternatives to a licensing system for opium addicts. The Commission recommended that narcotics should be subject to international control. The recommendations of the Brent Commission were endorsed by the United States Department of State and in 1906 President Theodore Roosevelt called for an international conference, the International Opium Commission, which was held in Shanghai in February 1909. A second conference was held at The Hague in May 1911, and out of it came the first international drug control treaty, the International Opium Convention of 1912.\n\nIn the 1800s opiates and cocaine were mostly unregulated drugs. In the 1890s the Sears & Roebuck catalogue, which was distributed to millions of Americans homes, offered a syringe and a small amount of cocaine for $1.50. On the other hand, as early as 1880 some states and localities had already passed laws against smoking opium, at least in public.\n\nAt the beginning of the 20th century, cocaine began to be linked to crime. In 1900, the \"Journal of the American Medical Association\" published an editorial stating, \"Negroes in the South are reported as being addicted to a new form of vice – that of 'cocaine sniffing' or the 'coke habit.'\" Some newspapers later claimed cocaine use caused blacks to rape white women and was improving their pistol marksmanship. Chinese immigrants were blamed for importing the opium-smoking habit to the U.S. The 1903 blue-ribbon citizens' panel, the Committee on the Acquirement of the Drug Habit, concluded, \"If the Chinaman cannot get along without his dope we can get along without him.\"\n\nTheodore Roosevelt appointed Dr. Hamilton Wright as the first Opium Commissioner of the United States in 1908. In 1909, Wright attended the International Opium Commission in Shanghai as the American delegates. He was accompanied by Charles Henry Brent, the Episcopal Bishop. On March 12, 1911, Dr. Wright was quoted in as follows in an article in the New York Times: \"Of all the nations of the world, the United States consumes most habit-forming drugs per capita. Opium, the most pernicious drug known to humanity, is surrounded, in this country, with far fewer safeguards than any other nation in Europe fences it with.\" Wright further claimed that \"it has been authoritatively stated that cocaine is often the direct incentive to the crime of rape by the negroes of the South and other sections of the country,\" though he failed to mention specifically \"which\" authorities had stated that, and did not provide any evidence for his claim. Wright also stated that \"one of the most unfortunate phases of smoking opium in this country is the large number of women who have become involved and were living as common-law wives or cohabitating with Chinese in the Chinatowns of our various cities\".\n\nOpium usage had begun to decline by 1914 after rising dramatically in the post Civil War Era, peaking at around one-half million pounds per year in 1896. Demand gradually declined thereafter in response to mounting public concern, local and state regulations, and the Pure Food and Drugs Act of 1906, which required labeling of patent medicines that contained opiates, cocaine, alcohol, cannabis and other intoxicants. As of 1911, an estimated one U.S. citizen in 400 (0.25%) was addicted to some form of opium. The opium addicts were mostly women who were prescribed and dispensed legal opiates by physicians and pharmacist for “female problems” (probably pain at menstruation) or white men and Chinese at the Opium dens. Between two-thirds and three-quarters of these addicts were women. By 1914, forty-six states had regulations on cocaine and twenty-nine states had laws against opium, morphine, and heroin.\n\nSeveral authors have argued that the debate was merely to regulate trade and collect a tax. However, the committee report prior to the debate on the house floor and the debate itself, discussed the rise of opiate use in the United States. Harrison stated that \"The purpose of this Bill can hardly be said to raise revenue, because it prohibits the importation of something upon which we have hitherto collected revenue.\" Later Harrison stated, \"We are not attempting to collect revenue, but regulate commerce.\" House representative Thomas Sisson stated, \"The purpose of this bill—and we are all in sympathy with it—is to prevent the use of opium in the United States, destructive as it is to human happiness and human life.\"\n\nThe drafters played on fears of “drug-crazed, sex-mad negroes” and made references to Negroes under the influence of drugs murdering whites, degenerate Mexicans smoking marijuana, and “Chinamen” seducing white women with drugs. Dr. Hamilton Wright, testified at a hearing for the Harrison Act. Wright alleged that drugs made blacks uncontrollable, gave them superhuman powers and caused them to rebel against white authority. Dr. Christopher Koch of the State Pharmacy Board of Pennsylvania testified that \"Most of the attacks upon the white women of the South are the direct result of a cocaine-crazed Negro brain\".\n\nBefore the Act was passed, on February 8, 1914, The \"New York Times\" published an article entitled \"Negro Cocaine 'Fiends' Are New Southern Menace: Murder and Insanity Increasing Among Lower-Class Blacks\" by Edward Huntington Williams, which reported that Southern sheriffs had increased the caliber of their weapons from .32 to .38 to bring down Negroes under the effect of cocaine.\n\nDespite the extreme racialization of the issue that took place in the buildup to the Act's passage, the contemporary research on the subject indicated that black Americans were in fact using cocaine and opium at much \"lower\" rates than white Americans.\n\nEnforcement began in 1915.\n\nThe act appears to be concerned about the marketing of opiates. However a clause applying to doctors allowed distribution \"in the course of his professional practice only.\" This clause was interpreted after 1917 to mean that a doctor could not prescribe opiates to an addict, since addiction was not considered a disease. A number of doctors were arrested and some were imprisoned. The medical profession quickly learned not to supply opiates to addicts. In \"United States v. Doremus\", 249 U.S. 86 (1919), the Supreme Court ruled that the Harrison Act was constitutional, and in \"Webb v. United States\", 249 U.S. 96, 99 (1919) that physicians could not prescribe narcotics solely for maintenance.\n\nThe impact of diminished supply was obvious by mid-1915. A 1918 commission called for sterner law enforcement, while newspapers published sensational articles about addiction-related crime waves. Congress responded by tightening up the Harrison Act—the importation of heroin for any purpose was banned in 1924.\n\nAfter other complementary laws (for example implementing the Uniform State Narcotic Act in 1932), and other actions by the government the number of addicts of opium started to decrease fast from 1925 to a level that in 1945 that was about one tenth of the level in 1914.\n\nThe use of the term 'narcotics' in the title of the act to describe not just opiates but also cocaine—which is a central nervous system stimulant, not a narcotic—initiated a precedent of frequent legislative and judicial misclassification of various substances as 'narcotics'. Today, law enforcement agencies, popular media, the United Nations, other nations and even some medical practitioners can be observed applying the term very broadly and often pejoratively in reference to a wide range of illicit substances, regardless of the more precise definition existing in medical contexts. For this reason, however, 'narcotic' has come to mean any illegally used drug, but it is useful as a shorthand for referring to a controlled drug in a context where its legal status is more important than its physiological effects.\n\nThe remaining effect of this act, which has largely been superseded by the Controlled Substances Act of 1970, is the warning \"*Warning: May be habit forming\" on labels, package inserts, and other places where ingredients are listed in the case of many opioids, barbiturates, medicinal formulations of cocaine, and chloral hydrate.\n\nThe act also marks the beginning of the creation of the modern, criminal drug addict and the American black market for drugs. Within five years the Rainey Committee, a Special Committee on Investigation appointed by Secretary of the Treasury William Gibbs McAdoo and led by Congressman T. Rainey, reported in June, 1919 that drugs were being smuggled into the country by sea, and across the Mexican and Canadian borders by nationally established organisations and that the United States consumed 470,000 pounds of opium annually, compared to 17,000 pounds in both France and Germany. The Monthly Summary of Foreign Commerce of the United States recorded that in the 7 months to January 1920, 528,635 pounds of opium was imported, compared to 74,650 pounds in the same period in 1919.\n\nThe Act's applicability in prosecuting doctors who prescribe narcotics to addicts was successfully challenged in \"Linder v. United States\" in 1925, as Justice McReynolds ruled that the federal government has no power to regulate medical practice.\n\n", "id": "14210", "title": "Harrison Narcotics Tax Act"}
{"url": "https://en.wikipedia.org/wiki?curid=14215", "text": "Horse tack\n\nTack is a piece of equipment or accessory equipped on horses in the course of their use as domesticated animals. Saddles, stirrups, bridles, halters, reins, bits, harnesses, martingales, and breastplates are all forms of horse tack. Equipping a horse is often referred to as tacking up. A room to store such equipment, usually near or in a stable, is a tack room.\n\nSaddles are seats for the rider, fastened to the horse's back by means of a \"girth\" (English-style riding), known as a \"cinch\" in the Western US, a wide strap that goes around the horse at a point about four inches behind the forelegs. Some western saddles will also have a second strap known as a \"flank\" or \"back cinch\" that fastens at the rear of the saddle and goes around the widest part of the horse's belly.\n\nIt is important that the saddle be comfortable for both the rider and the horse as an improperly fitting saddle may create pressure points on the horse's back muscle (Latissimus dorsi) and cause the horse pain and can lead to the horse, rider, or both getting injured.\n\nThere are many types of saddle, each specially designed for its given task.\nSaddles are usually divided into two major categories: \"English saddles\" and \"Western saddles\" according to the riding discipline they are used in. Other types of saddles, such as racing saddles, Australian saddles, sidesaddles and endurance saddles do not necessarily fit neatly in either category.\n\n\nStirrups are supports for the rider's feet that hang down on either side of the saddle. They provide greater stability for the rider but can have safety concerns due to the potential for a rider's feet to get stuck in them. If a rider is thrown from a horse but has a foot caught in the stirrup, they could be dragged if the horse runs away. To minimize this risk, a number of safety precautions are taken. First, most riders wear riding boots with a heel and a smooth sole. Next, some saddles, particularly English saddles, have safety bars that allow a stirrup leather to fall off the saddle if pulled backwards by a falling rider. Other precautions are done with stirrup design itself. Western saddles have wide stirrup treads that make it more difficult for the foot to become trapped. A number of saddle styles incorporate a tapedero, which is covering over the front of the stirrup that keeps the foot from sliding all the way through the stirrup. The English stirrup (or \"iron\") has several design variations which are either shaped to allow the rider's foot to slip out easily or are closed with a very heavy rubber band. The invention of stirrups was of great historic significance in mounted combat, giving the rider secure foot support while on horseback.\n\n\"Bridles\", hackamores, \"halters\" or \"headcollars\", and similar equipment consist of various arrangements of straps around the horse's head, and are used for control and communication with the animal.\n\nA \"halter\" (US) or \"headcollar\" (UK) (occasionally \"headstall\") consists of a noseband and headstall that buckles around the horse's head and allows the horse to be led or tied. The lead rope is separate, and it may be short (from six to ten feet, two to three meters) for everyday leading and tying, or much longer (up to , eight meters) for tasks such as for leading packhorses or for picketing a horse out to graze.\n\nSome horses, particularly stallions, may have a chain attached to the lead rope and placed over the nose or under the jaw to increase the control provided by a halter while being led. Most of the time, horses are not ridden with a halter, as it offers insufficient precision and control. Halters have no bit.\n\nIn Australian and British English, a \"halter\" is a rope with a spliced running loop around the nose and another over the poll, used mainly for unbroken horses or for cattle. The lead rope cannot be removed from the halter. A show halter is made from rolled leather and the lead attaches to form the chinpiece of the noseband. These halters are not suitable for paddock usage or in loose stalls. An \"underhalter\" is a lightweight halter or headcollar which is made with only one small buckle, and can be worn under a bridle for tethering a horse without untacking.\n\nBridles usually have a \"bit\" attached to \"reins\" and are used for riding and driving horses.\n\n\"English Bridles\" have a \"cavesson\" style noseband and are seen in English riding. Their reins are buckled to one another, and they have little adornment or flashy hardware.\n\n\"Western Bridles\" used in Western riding usually have no noseband, are made of thin bridle leather. They may have long, separated \"Split\" reins or shorter closed reins, which sometimes include an attached \"Romal\". Western bridles are often adorned with silver or other decorative features.\n\n\"Double bridles\" are a type of English bridle that use two bits in the mouth at once, a snaffle and a curb. The two bits allow the rider to have very precise control of the horse. As a rule, only very advanced horses and riders use double bridles. Double bridles are usually seen in the top levels of dressage, but also are seen in certain types of show hack and Saddle seat competition.\n\nA \"hackamore\" is a headgear that utilizes a heavy noseband of some sort, rather than a bit, most often used to train young horses or to go easy on an older horse's mouth. Hackamores are more often seen in western riding. Some related styles of headgear that control a horse with a noseband rather than a bit are known as bitless bridles.\n\nThe word \"hackamore\" is derived from the Spanish word \"jáquima.\" Hackamores are seen in western riding disciplines, as well as in endurance riding and English riding disciplines such as show jumping and the stadium phase of eventing. While the classic bosal-style hackamore is usually used to start young horses, other designs, such as various bitless bridles and the mechanical hackamore are often seen on mature horses with dental issues that make bit use painful, horses with certain training problems, and on horses with mouth or tongue injuries. Some riders also like to use them in the winter to avoid putting a frozen metal bit into a horse's mouth.\n\nLike bitted bridles, noseband-based designs can be gentle or harsh, depending on the hands of the rider. It is a myth that a bit is cruel and a hackamore is gentler. The horse's face is very soft and sensitive with many nerve endings. Misuse of a hackamore can cause swelling on the nose, scraping on the nose and jawbone, and extreme misuse may cause damage to the bones and cartilage of the horse's head.\n\nA \"longeing cavesson\" (UK: \"lungeing\") is a special type of halter or noseband used for longeing a horse. Longeing is the activity of having a horse walk, trot and/or canter in a large circle around the handler at the end of a rope that is 25 to long. It is used for training and exercise.\n\nReins consist of leather straps or rope attached to the outer ends of a \"bit\" and extend to the rider's or driver's hands. Reins are the means by which a horse rider or driver communicates directional commands to the horse's head. Pulling on the reins can be used to steer or stop the horse. The sides of a horse's mouth are sensitive, so pulling on the reins pulls the bit, which then pulls the horse's head from side to side, which is how the horse is controlled.\n\nOn some types of harnesses there might be supporting rings to carry the reins over the horse's back. When pairs of horses are used in drawing a wagon or coach it is usual for the outer side of each pair to be connected to reins and the inside of the bits connected by a short bridging strap or rope. The driver carries \"four-in-hand\" or \"six-in-hand\" being the number of reins connecting to the pairs of horses.\n\nA rein may be attached to a halter to lead or guide the horse in a circle for training purposes or to lead a packhorse, but a simple lead rope is more often used for these purposes. A longe line is sometimes called a \"longe rein,\" but it is actually a flat line about long, usually made of nylon or cotton web, about one inch wide, thus longer and wider than even a driving rein.\n\nHorses should never be tied by the reins. Not only do they break easily, but, being attached to a bit in the horse's sensitive mouth, a great deal of pain can be inflicted if a bridled horse sets back against being tied.\n\nA bit is a device placed in a horse's mouth, kept on a horse's head by means of a headstall. There are many types, each useful for specific types of riding and training.\n\nThe mouthpiece of the bit does not rest on the teeth of the horse, but rather rests on the gums or \"bars\" of the horse's mouth in an interdental space behind the front incisors and in front of the back molars. It is important that the style of bit is appropriate to the horse's needs and is fitted properly for it to function properly and be as comfortable as possible for the horse.\n\nThe basic \"classic\" styles of bits are:\n\nWhile there are literally hundreds of types of bit mouthpieces, bit rings and bit shanks, essentially there are really only two broad categories: direct pressure bits, broadly termed snaffle bits; and leverage bits, usually termed curbs.\n\nBits that act with direct pressure on the tongue and lips of the bit are in the general category of \"snaffle\" bits. Snaffle bits commonly have a single jointed mouthpiece and act with a nutcracker effect on the bars, tongue and occasionally roof of the mouth. However, regardless of mouthpiece, any bit that operates only on direct pressure is a \"snaffle\" bit.\n\nLeverage bits have shanks coming off the mouthpiece to create leverage that applies pressure to the poll, chin groove and mouth of the horse are in the category of \"curb\" bits. Any bit with shanks that works off of leverage is a \"curb\" bit, regardless of whether the mouthpiece is solid or jointed.\n\nSome combination or hybrid bits combine direct pressure and leverage, such as the Kimblewick or Kimberwicke, which adds slight leverage to a two-rein design that resembles a snaffle; and the four rein designs such as the single mouthpiece Pelham bit and the double bridle, which places a curb and a snaffle bit simultaneously in the horse's mouth.\n\nIn the wrong hands even the mildest bit can hurt the horse. Conversely, a very severe bit, in the right hands, can transmit subtle commands that cause no pain to the horse. Bit commands should be given with only the quietest movements of the hands, and much steering and stopping should be done with the legs and seat.\n\nA horse harness is a set of devices and straps that attaches a horse to a cart, carriage, sledge or any other load. There are two main styles of harnesses - breaststrap and collar and hames style. These differ in how the weight of the load is attached. Most Harnesses are made from leather, which is the traditional material for harnesses, though some designs are now made of nylon webbing or synthetic biothane.\n\nA breaststrap harness has a wide leather strap going horizontally across the horses' breast, attached to the traces and then to the load. This is used only for lighter loads. A collar and hames harness has a collar around the horses' neck with wood or metal hames in the collar. The traces attach from the hames to the load. This type of harness is needed for heavy draft work.\n\nBoth types will also have a bridle and reins. A harness that is used to support shafts, such as on a cart pulled by a single horse, will also have a \"saddle\" attached to the harness to help the horse support the shafts and \"breeching\" to brake the forward motion of the vehicle, especially when stopping or moving downhill. Horses guiding vehicles by means of a pole, such as two-horse teams pulling a wagon, a hay-mower, or a dray, will have \"pole-straps\" attached to the lower part of the horse collar.\n\nBreastplates, breastcollars or breastgirths attach to the front of the saddle, cross the horse's chest, and usually have a strap that runs between the horse's front legs and attaches to the girth. They keep the saddle from sliding back or sideways. They are usually seen in demanding, fast-paced sports. They are crucial pieces of safety equipment for English riding activities requiring jumping, such as eventing, show jumping, polo, and fox hunting. They are also seen in Western riding events, particularly in rodeo, reining and cutting, where it is particularly important to prevent a saddle from shifting. They may also be worn in other horse show classes for decorative purposes.\n\nA martingale is a piece of equipment that keeps a horse from raising its head too high. Various styles can be used as a control measure, to prevent the horse from avoiding rider commands by raising its head out of position; or as a safety measure to keep the horse from tossing its head high or hard enough to smack its rider in the face.\n\nThey are allowed in many types of competition, especially those where speed or jumping may be required, but are not allowed in most \"flat\" classes at horse shows, though an exception is made in a few classes limited exclusively to young or \"green\" horses who may not yet be fully trained.\n\nMartingales are usually attached to the horse one of two ways. They are either attached to the center chest ring of a breastplate or, if no breastplate is worn, they are attached by two straps, one that goes around the horse's neck, and the other that attaches to the girth, with the martingale itself beginning at the point in the center of the chest where the neck and girth straps intersect.\n\nMartingale types include:\n\n\n\nThere are other training devices that fall loosely in the martingale category, in that they use straps attached to the reins or bit which limit the movement of the horse's head or add leverage to the rider's hands in order to control the horse's head. Common devices of this nature include the overcheck, the chambon, de Gogue, grazing reins, draw reins and the \"bitting harness\" or \"bitting rig\". However, most of this equipment is used for training purposes and is not legal in any competition. In some disciplines, use of leverage devices, even in training, is controversial.\n\n", "id": "14215", "title": "Horse tack"}
{"url": "https://en.wikipedia.org/wiki?curid=14216", "text": "Hausa language\n\nHausa () (\"Yaren Hausa\" or \"Harshen Hausa\") is the Chadic language (a branch of the Afroasiatic language family) with the largest number of speakers, spoken as a first language by about 35 million people, and as a second language by millions more in Nigeria, and millions more in other countries, for a total of at least 41 million speakers. Originally the language of the Hausa people stretching across southern Niger and northern Nigeria, it has developed into a lingua franca across much of western Africa for purposes of trade. In the 20th and 21st centuries, it has become more commonly published in print and online.\n\nThere are a few traditional dialects, differing mostly due to tonality. The language was commonly written with a variant of the Arabic script known as ajami but is more often written with the Latin alphabet known as boko.\n\nHausa belongs to the West Chadic languages subgroup of the Chadic languages group, which in turn is part of the Afroasiatic language family.\n\nNative speakers of Hausa, the Hausa people, are mostly found in Niger, in the north of Nigeria, and in Chad. Furthermore, the language is used as a trade language across a much larger swathe of West Africa (Benin, Ghana, Cameroon, Togo, Ivory Coast etc.), Central Africa (Chad, Central African Republic, Gabon) and in northwestern Sudan, particularly amongst Muslims.\n\nIt is taught at universities in Africa and around the world. The language is the most commonly spoken language in Nigeria, but unlike Yoruba and Igbo, it is also widely spoken outside Nigeria, especially in Niger, Ghana, Cameroon and Sudan. Radio stations like BBC, Radio France Internationale, China Radio International, Voice of Russia, Voice of America, Arewa 24 Deutsche Welle, and IRIB broadcast in Hausa.\n\nEastern Hausa dialects include \"Dauranchi\" in Daura, \"Kananci\" in Kano, \"Bausanchi\" in Bauchi, \"Gudduranci\" in Katagum Misau and part of Borno, and \"Hadejanci\" in Hadejiya.\n\nWestern Hausa dialects include \"Sakkwatanci\" in Sokoto, \"Katsinanci\" in Katsina, \"Arewanci\" in Gobir, Adar, Kebbi, and Zamfara, and \"Kurhwayanci\" in Kurfey in Niger. Katsina is transitional between Eastern and Western dialects.\n\nNorthern Hausa dialects include \"Arewa\" and \"Arawci\".\n\n\"Zazzaganci\" in Zaria is the major Southern dialect.\n\nThe Daura (\"Dauranchi\") and Kano (\"Kananci\") dialect are the standard. The BBC, Deutsche Welle, Radio France Internationale and Voice of America offer Hausa services on their international news web sites using Dauranci and Kananci.\n\nThe western to eastern Hausa dialects of \"Kurhwayanci\", \"Daragaram\" and \"Aderawa\", represent the traditional northernmost limit of native Hausa communities. These are spoken in the northernmost sahel and mid-Saharan regions in west and central Niger in the Tillaberi, Tahoua, Dosso, Maradi, Agadez and Zinder regions. While mutually comprehensible with other dialects (especially \"Sakkwatanci\", and to a lesser extent \"Gaananci\"), the northernmost dialects have slight grammatical and lexical differences owing to frequent contact with the Zarma and Tuareg groups and cultural changes owing to the geographical differences between the grassland and desert zones. These dialects also have the quality of being non-tonal or pitch accent dialects.\n\nThis link between non-tonality and geographic location is not limited to Hausa alone, but is exhibited in other northern dialects of neighbouring languages; such as the difference within Songhay language (between the non-tonal northernmost dialects of Koyra Chiini in Timbuktu and Koyraboro Senni in Gao; and the tonal southern Zarma dialect, spoken from western Niger to northern Ghana), and within the Soninke language (between the non-tonal northernmost dialects of Imraguen and Nemadi spoken in east-central Mauritania; and the tonal southern dialects of Senegal, Mali and the sahel).\n\nThe Ghanaian Hausa dialect (\"Gaananci\"), spoken in Ghana, Togo, and western Ivory Coast, is a distinct western native Hausa dialect-bloc with adequate linguistic and media resources available. Separate smaller Hausa dialects are spoken by an unknown number of Hausa further west in parts of Burkina Faso, and in the Haoussa Foulane, Badji Haoussa, Guezou Haoussa, and Ansongo districts of northeastern Mali (where it is designated as a minority language by the Malian government), but there are very little linguistic resources and research done on these particular dialects at this time.\n\nGaananci forms a separate group from other Western Hausa dialects, as it now falls outside the contiguous Hausa-dominant area, and is usually identified by the use of \"c\" for \"ky\", and \"j\" for \"gy\". This is attributed to the fact that Ghana's Hausa population descend from Hausa-Fulani traders settled in the zongo districts of major trade-towns up and down the previous Asante, Gonja and Dagomba kingdoms stretching from the sahel to coastal regions, in particular the cities of Tamale, Salaga, Bawku, Bolgatanga, Achimota, Nima and Kumasi.\n\nGaananci exhibits noted inflected influences from Zarma, Gur, Dyula and Soninke, as Ghana is the westernmost area in which the Hausa language is a major lingua-franca; as well as it being the westernmost area both the Hausa and Djerma ethnic groups inhabit in large numbers. Immediately west from Ghana (in Ivory Coast, Togo, and Burkina Faso), Hausa is abruptly replaced with Dioula–Bambara as the main lingua-franca of what become predominantly Mandinka areas, and native Hausa populations plummet to a very small urban minority.\n\nBecause of this, and the presence of surrounding Akan, Gur and Mande languages, Gaananci was historically isolated from the other Hausa dialects. Despite this difference, grammatical similarities between \"Sakkwatanci\" and Ghanaian Hausa determine that the dialect, and the origin of the Ghanaian Hausa people themselves, are derived from the northwestern Hausa area surrounding Sokoto.\n\nHausa is also widely spoken by non-native Gur and Mande Ghanaian Muslims, but differs from Gaananci, and rather has features consistent with non-native Hausa dialects.\n\nHausa is also spoken various parts of Cameroon and Chad, which combined the mixed dialects of northern Nigeria and Niger. In addition, Arabic has had a great influence in the way Hausa is spoken by the native Hausa speakers in these areas.\n\nIn West Africa, Hausa's use as a lingua franca has given rise to non-native pronunciation that differs vastly from native pronunciation by way of key omissions of implosive and ejective consonants present in native Hausa dialects, such as \"ɗ\", \"ɓ\" and \"kʼ/ƙ\", which are pronounced by non-native speakers as \"d\", \"b\" and \"k\" respectively. This creates confusion among non-native and native Hausa speakers, as non-native pronunciation does not distinguish words like ' (\"correct\") and ' (\"one-by-one\"). Another difference between native and non-native Hausa is the omission of vowel length in words and change in the standard tone of native Hausa dialects (ranging from native Fulani and Tuareg Hausa-speakers omitting tone altogether, to Hausa speakers with Gur or Yoruba mother tongues using additional tonal structures similar to those used in their native languages). Use of masculine and feminine gender nouns and sentence structure are usually omitted or interchanged, and many native Hausa nouns and verbs are substituted with non-native terms from local languages.\n\nNon-native speakers of Hausa numbered more than 25 million and, in some areas, live close to native Hausa. It has replaced many other languages especially in the north-central and north-eastern part of Nigeria and continues to gain popularity in other parts of Africa as a result of Hausa movies and musics which spread out throughout the region.\n\nThere are several pidgin forms of Hausa. Barikanchi was formerly used in the colonial army of Nigeria. Gibanawa is currently in widespread use in Jega in northwestern Nigeria, south of the native Hausa area.\n\nHausa has between 23 and 25 consonant phonemes depending on the speaker.\n\nThe three-way contrast between palatalized velars , plain velars , and labialized velars is found only before long and short , e.g. ('grass'), ('to increase'), ('shea-nuts'). Before front vowels, only palatalized and labialized velars occur, e.g. ('jealousy') vs. ('side of body'). Before rounded vowels, only labialized velars occur, e.g. ('ringworm').\n\nHausa has glottalic consonants (implosives and ejectives) at four or five places of articulation (depending on the dialect). They require movement of the glottis during pronunciation and have a staccato sound.\n\nThey are written with modified versions of Latin letters. They can also be denoted with an apostrophe, either before or after depending on the letter, as shown below.\n\n\nHausa has five phonetic vowel sounds, which can be either short or long, giving a total of 10 monophthongs. In addition, there are four joint vowels (diphthongs), giving a total number of 14 vowel phonemes.\n\n\n\nHausa is a tonal language. Each of its five vowels may have low tone, high tone or falling tone. In standard written Hausa, tone is not marked. In recent linguistic and pedagogical materials, tone is marked by means of diacritics.\n\nAn acute accent () may be used for high tone, but the usual practice is to leave high tone unmarked.\n\nHausa's modern official orthography is a Latin-based alphabet called \"boko\", which was introduced in the 1930s by the British colonial administration.\nThe letter \"ƴ\" (y with a right hook) is used only in Niger; in Nigeria it is written \"ʼy\".\n\nTone, vowel length, and the distinction between and (which does not exist for all speakers) are not marked in writing. So, for example, \"from\" and \"battle\" are both written \"daga\".\n\nHausa has also been written in \"ajami\", an Arabic alphabet, since the early 17th century. There is no standard system of using \"ajami\", and different writers may use letters with different values. Short vowels are written regularly with the help of vowel marks, which are seldom used in Arabic texts other than the Quran. Many medieval Hausa manuscripts in \"ajami\", similar to the Timbuktu Manuscripts, have been discovered recently; some of them even describe constellations and calendars.\n\nIn the following table, vowels are shown with the Arabic letter for \"t\" () as an example.\n\nHausa is one of three indigenous languages of Nigeria which has been rendered in braille.\n\nAt least three other writing systems for Hausa have been proposed or \"discovered.\" None of these are in active use beyond perhaps some individuals.\n\n\n\n\n", "id": "14216", "title": "Hausa language"}
{"url": "https://en.wikipedia.org/wiki?curid=14220", "text": "History of mathematics\n\nThe area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, to a lesser extent, an investigation into the mathematical methods and notation of the past.\n\nBefore the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. The most ancient mathematical texts available are \"Plimpton 322\" (Babylonian c. 1900 BC), the \"Rhind Mathematical Papyrus\" (Egyptian c. 2000–1800 BC) and the \"Moscow Mathematical Papyrus\" (Egyptian c. 1890 BC). All of these texts concern the so-called Pythagorean theorem, which seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.\n\nThe study of mathematics as a demonstrative discipline begins in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek \"μάθημα\" (\"mathema\"), meaning \"subject of instruction\". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Chinese mathematics made early contributions, including a place value system. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, likely evolved over the course of the first millennium AD in India and were transmitted to the west via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Many Greek and Arabic texts on mathematics were then translated into Latin, which led to further development of mathematics in medieval Europe.\n\nFrom ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 16th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day.\n\nThe origins of mathematical thought lie in the concepts of number, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the \"number\" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between \"one\", \"two\", and \"many\", but not of numbers larger than two.\n\nPrehistoric artifacts discovered in Africa, dated 20,000 years old or more suggest early attempts to quantify time. The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of tally marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that \"no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10.\" The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this, however, is disputed.\n\nPredynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design. All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.\n\nBabylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period). It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.\n\nIn contrast to the sparsity of sources in Egyptian mathematics, our knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.\n\nThe earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.\nBabylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different than multiplying integers, similar to our modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computation accuracy and power; for example, the Babylonian tablet YBC 7289 gives an approximation of √2 accurate to five decimal places. The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions. This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.\n\nOther topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time. Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem. However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.\n\nEgyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars.\n\nThe most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.\n\nAnother significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called \"word problems\" or \"story problems\", which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).\n\nFinally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.\n\nGreek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.\n\nGreek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.\n\nGreek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.\nThales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was \"All is number\". It was the Pythagoreans who coined the term \"mathematics\", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers. Although he was preceded by the Babylonians and the Chinese, the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum). The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the \"mensa Pythagorica\".\nPlato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came. Plato also discussed the foundations of mathematics,\n\nEudoxus (408–c.355 BC) developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c.322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.\n\nIn the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the \"Elements\", widely considered the most successful and influential textbook of all time. The \"Elements\" introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the \"Elements\" were already known, Euclid arranged them into a single, coherent logical framework. The \"Elements\" was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the \"Elements\" was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.\nArchimedes (c.287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, 3 < π < 3. He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious method of exponentiation for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.\n\nApollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola (\"place beside\" or \"comparison\"), \"ellipse\" (\"deficiency\"), and \"hyperbola\" (\"a throw beyond\"). His work \"Conics\" is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.\nAround the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the \"Golden Age\" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the \"Almagest\" of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.\n\nFollowing a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the \"Silver Age\" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as \"Diophantine analysis\". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the \"Arithmetica\", a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The \"Arithmetica\" had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the \"Arithmetica\" (that of dividing a square into two squares). Diophantus also made significant advances in notation, the \"Arithmetica\" being the first instance of algebraic symbolism and syncopation.\n\nThe first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed.\n\nAn analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development. The oldest extant mathematical text from China is the \"Zhoubi Suanjing\", variously dated to between 1200 BC and 100 BC, though a date of about 300 BC appears reasonable. However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.\n\nOf particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called \"rod numerals\" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for \"1\", followed by the symbol for \"100\", then the symbol for \"2\" followed by the symbol for \"10\", followed by the symbol for \"3\". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the \"suan pan\", or Chinese abacus. The date of the invention of the \"suan pan\" is not certain, but the earliest written mention dates from AD 190, in Xu Yue's \"Supplementary Notes on the Art of Figures\".\n\nThe oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The \"Mo Jing\" described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well.\n\nIn 212 BC, the Emperor Qin Shi Huang (Shi Huang-ti) commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is \"The Nine Chapters on the Mathematical Art\", the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles and values of π. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. Liu Hui commented on the work in the 3rd century AD, and gave a value of π accurate to 5 decimal places. Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places, which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.\n\nThe high-water mark of Chinese mathematics occurs in the 13th century (latter part of the Song period), with the development of Chinese algebra. The most important text from that period is the \"Precious Mirror of the Four Elements\" by Chu Shih-chieh (fl. 1280–1303), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The \"Precious Mirror\" also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).\n\nEven after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.\n\nThe earliest civilization on the Indian subcontinent is the Indus Valley Civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.\n\nThe oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.\n\nPāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called \"mātrāmeru\").\n\nThe next significant mathematical documents from India after the \"Sulba Sutras\" are the \"Siddhantas\", astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words \"sine\" and \"cosine\" derive from the Sanskrit \"jiya\" and \"kojiya\".\n\nIn the 5th century AD, Aryabhata wrote the \"Aryabhatiya\", a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. Though about half of the entries are wrong, it is in the \"Aryabhatiya\" that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the \"Aryabhatiya\" as a \"mix of common pebbles and costly crystals\".\n\nIn the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in \"Brahma-sphuta-siddhanta\", he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.\n\nIn the 12th century, Bhāskara II lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.\n\nIn the 14th century, Madhava of Sangamagrama, the founder of the so-called Kerala School of Mathematics, found the Madhava–Leibniz series, and, using 21 terms, computed the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the \"Yukti-bhāṣā\". However, the Kerala School did not formulate a systematic theory of differentiation and integration, nor is there any direct evidence of their results being transmitted outside Kerala.\n\nThe Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.\n\nIn the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book \"On the Calculation with Hindu Numerals\", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word \"algorithm\" is derived from the Latinization of his name, Algoritmi, and the word \"algebra\" from the title of one of his works, \"Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala\" (\"The Compendious Book on Calculation by Completion and Balancing\"). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as \"al-jabr\". His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" He also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\nIn Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.\n\nFurther developments in algebra were made by Al-Karaji in his treatise \"al-Fakhri\", where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.\n\nIn the late 11th century, Omar Khayyam wrote \"Discussions of the Difficulties in Euclid\", a book about what he perceived as flaws in Euclid's \"Elements\", especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.\n\nIn the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating \"n\"th roots, which was a special case of the methods given many centuries later by Ruffini and Horner.\n\nOther achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.\n\nDuring the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.\n\nMedieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's \"Timaeus\" and the biblical passage (in the \"Book of Wisdom\") that God had \"ordered all things in measure, and number, and weight\".\n\nBoethius provided a place for mathematics in the curriculum in the 6th century when he coined the term \"quadrivium\" to describe the study of arithmetic, geometry, astronomy, and music. He wrote \"De institutione arithmetica\", a free translation from the Greek of Nicomachus's \"Introduction to Arithmetic\"; \"De institutione musica\", also derived from Greek sources; and a series of excerpts from Euclid's \"Elements\". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.\n\nIn the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's \"The Compendious Book on Calculation by Completion and Balancing\", translated into Latin by Robert of Chester, and the complete text of Euclid's \"Elements\", translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona. These and other new sources sparked a renewal of mathematics.\n\nLeonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote \"Liber Abaci\" in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which was used as an unremarkable example within the text.\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.\n\nThomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:\nV = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.\n\nOne of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed \"by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant\".\n\nHeytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that \"a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]\".\n\nNicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's \"Elements\", Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.\n\nDuring the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as \"abbaco\" in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.\n\nPiero della Francesca (c.1415–1492) wrote books on solid geometry and linear perspective, including \"De Prospectiva Pingendi (On Perspective for Painting)\", \"Trattato d’Abaco (Abacus Treatise)\", and \"De corporibus regularibus (Regular Solids)\".\n\nLuca Pacioli's \"Summa de Arithmetica, Geometria, Proportioni et Proportionalità\" (Italian: \"Review of Arithmetic, Geometry, Ratio and Proportion\") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, \"\"Particularis de Computis et Scripturis\"\" (Italian: \"Details of Calculation and Recording\"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In \"Summa Arithmetica\", Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. \"Summa Arithmetica\" was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.\n\nIn Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.\n\nSimon Stevin's book \"De Thiende\" ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.\n\nDriven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his \"Trigonometria\" in 1595. Regiomontanus's table of sines and cosines was published in 1533.\n\nDuring the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.\n\nThe 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.\nThe analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.\n\nBuilding on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, who is arguably one of the most important mathematicians of the 17th century, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.\n\nIn addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.\n\nThe most influential mathematician of the 18th century was arguably Leonhard Euler. His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol \"i\", and he popularized the use of the Greek letter formula_1 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.\n\nOther important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.\n\nThroughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.\nThis century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.\nThe Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.\n\nThe 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.\n\nAugustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.\n\nAlso, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.\n\nAbel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.\n\nIn the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L. E. J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.\n\nThe 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.\n\nIn 1897, Hensel introduced p-adic numbers.\n\nThe 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.\n\nIn a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.\n\nNotable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel used a computer to prove the four color theorem. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.\n\nMathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the \"enormous theorem\"), whose proof between 1955 and 1983 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym \"Nicolas Bourbaki\", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.\nDifferential geometry came into its own when Einstein used it in general relativity. Entire new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.\nMeasure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include, Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.\n\nNon-standard analysis, introduced by Abraham Robinson, rehabillitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.\n\nThe development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the Fast Fourier Transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.\n\nAt the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus one of addition and multiplication, was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.\nOne of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.\n\nPaul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the \"collaborative distance\" between a person and Paul Erdős, as measured by joint authorship of mathematical papers.\n\nEmmy Noether has been described by many as the most important woman in the history of mathematics. She studied the theories of rings, fields, and algebras.\n\nAs in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the world wide web led to online publishing.\n\nIn 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).\n\nMost mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive towards open access publishing, first popularized by the arXiv.\n\nThere are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, the volume of data to be analyzed being produced by science and industry, facilitated by computers, is explosively expanding.\n\n\n\n\n\n\n\n\n\n\n\n", "id": "14220", "title": "History of mathematics"}
{"url": "https://en.wikipedia.org/wiki?curid=14223", "text": "HSK\n\nHSK may refer to:\n", "id": "14223", "title": "HSK"}
{"url": "https://en.wikipedia.org/wiki?curid=14225", "text": "Hydrogen atom\n\nA hydrogen atom is an atom of the chemical element hydrogen. The electrically neutral atom contains a single positively charged proton and a single negatively charged electron bound to the nucleus by the Coulomb force. Atomic hydrogen constitutes about 75% of the elemental (baryonic) mass of the universe.\n\nIn everyday life on Earth, isolated hydrogen atoms (usually called \"atomic hydrogen\" or, more precisely, \"monatomic hydrogen\") are extremely rare. Instead, hydrogen tends to combine with other atoms in compounds, or with itself to form ordinary (diatomic) hydrogen gas, H. \"Atomic hydrogen\" and \"hydrogen atom\" in ordinary English use have overlapping, yet distinct, meanings. For example, a water molecule contains two hydrogen atoms, but does not contain atomic hydrogen (which would refer to isolated hydrogen atoms).\n\nAttempts to develop a theoretical understanding of the hydrogen atom have been important to the history of quantum mechanics.\n\nThe most abundant isotope, hydrogen-1, protium, or light hydrogen, contains no neutrons and is just a proton and an electron. Protium is stable and makes up 99.9885% of naturally occurring hydrogen by absolute number (not mass).\n\nDeuterium contains one neutron and one proton. Deuterium is stable and makes up 0.0115% of naturally occurring hydrogen and is used in industrial processes like nuclear reactors and Nuclear Magnetic Resonance.\n\nTritium contains two neutrons and one proton and is not stable, decaying with a half-life of 12.32 years. Because of the short half life, Tritium does not exist in nature except in trace amounts.\n\nHigher isotopes of hydrogen are only created in artificial accelerators and reactors and have half lives around the order of 10 seconds.\n\nThe formulas below are valid for all three isotopes of hydrogen, but slightly different values of the Rydberg constant (correction formula given below) must be used for each hydrogen isotope.\n\nHydrogen is not found without its electron in ordinary chemistry (room temperatures and pressures), as ionized hydrogen is highly chemically reactive. When ionized hydrogen is written as \"H\" as in the solvation of classical acids such as hydrochloric acid, the hydronium ion, HO, is meant, not a literal ionized single hydrogen atom. In that case, the acid transfers the proton to HO to form HO.\n\nIonized hydrogen without its electron, or free protons, are common in the interstellar medium, and solar wind.\n\nThe hydrogen atom has special significance in quantum mechanics and quantum field theory as a simple two-body problem physical system which has yielded many simple analytical solutions in closed-form.\n\nExperiments by Ernest Rutherford in 1909 showed the structure of the atom to be a dense, positive nucleus with a light, negative charge orbiting around it. This immediately caused problems on how such a system could be stable. Classical electromagnetism had shown that any accelerating charge radiates energy described through the Larmor formula. If the electron is assumed to orbit in a perfect circle and radiates energy continuously, the electron would rapidly spiral into the nucleus with a fall time of:\n\nWhere formula_2 is the Bohr radius and formula_3 is the classical electron radius. If this were true, all atoms would instantly collapse, however atoms seem to be stable. Furthermore, the spiral inward would release a smear of electromagnetic frequencies as the orbit got smaller. Instead, atoms were observed to only emit discrete frequencies of radiation. The resolution would lie in the development of quantum mechanics.\n\nIn 1913, Niels Bohr obtained the energy levels and spectral frequencies of the hydrogen atom after making a number of simple assumptions in order to correct the failed classical model. The assumptions included:\nBohr supposed that the electron's angular momentum is quantized with possible values:\n\nformula_4 where formula_5\n\nand formula_6 is Planck constant over formula_7. He also supposed that the centripetal force which keeps the electron in its orbit is provided by the Coulomb force, and that energy is conserved. Bohr derived the energy of each orbit of the hydrogen atom to be:\n\nwhere formula_9 is the electron mass, formula_10 is the electron charge, formula_11 is the electric permeability, and formula_12 is the quantum number (now known as the principal quantum number). Bohr's predictions matched experiments measuring the hydrogen spectral series to the first order, giving more confidence to a theory that used quantized values.\n\nFor formula_13, the value\nis called the Rydberg unit of energy. It is related to the Rydberg constant formula_15 of atomic physics by formula_16\n\nThe exact value of the Rydberg constant assumes that the nucleus is infinitely massive with respect to the electron. For hydrogen-1, hydrogen-2 (deuterium), and hydrogen-3 (tritium) the constant must be slightly modified to use the reduced mass of the system, rather than simply the mass of the electron. However, since the nucleus is much heavier than the electron, the values are nearly the same. The Rydberg constant \"R\" for a hydrogen atom (one electron), \"R\" is given by\n\nformula_17\n\nwhere formula_18 is the mass of the atomic nucleus. For hydrogen-1, the quantity formula_19 is about 1/1836 (i.e. the electron-to-proton mass ratio). For deuterium and tritium, the ratios are about 1/3670 and 1/5497 respectively. These figures, when added to 1 in the denominator, represent very small corrections in the value of \"R\", and thus only small corrections to all energy levels in corresponding hydrogen isotopes.\n\nThere were still problems with Bohr's model: \n\nMost of these shortcomings were repaired by Arnold Sommerfeld's modification of the Bohr model. Sommerfeld introduced two additional degrees of freedom allowing an electron to move on an elliptical orbit, characterized by its eccentricity and declination with respect to a chosen axis. This introduces two additional quantum numbers, which correspond to the orbital angular momentum and its projection on the chosen axis. Thus the correct multiplicity of states (except for the factor 2 accounting for the yet unknown electron spin) was found. Further applying special relativity theory to the elliptic orbits, Sommerfeld succeeded in deriving the correct expression for the fine structure of hydrogen spectra (which happens to be exactly the same as in the most elaborate Dirac theory). However some observed phenomena such as the anomalous Zeeman effect remain unexplained. These issues were resolved with the full development of quantum mechanics and the Dirac equation. It is often alleged, that the Schrödinger equation is superior to the Bohr-Sommerfeld theory in describing hydrogen atom. This is however not the case, as the most results of both approaches coincide or are very close (a remarkable exception is the problem of hydrogen atom in crossed electric and magnetic fields, which cannot be solved in the framework of the Bohr-Sommerfeld theory self-consistently), and their main shortcomings result from the absence of the electron spin in both theories. It was the complete failure of the Bohr-Sommerfeld theory to explain many-electron systems (such as helium atom or hydrogen molecule) which demonstrated its inadequacy in describing quantum phenomena.\n\nThe Schrödinger equation allows one to calculate the development of quantum systems with time and can give exact, analytical answers for the non-relativistic hydrogen atom.\n\nThe Hamiltonian of the hydrogen atom is the radial kinetic energy operator and coulomb attraction force between the positive proton and negative electron. Using the time-independent Schrödinger equation, ignoring all spin-coupling interactions and using the reduced mass formula_22, the equation is written as:\n\nformula_23\n\nExpanding the Laplacian in spherical coordinates:\n\nformula_24\n\nThis is a separable, partial differential equation which can be solved in terms of special functions. By setting Z=1 (for one proton), the normalized position wavefunctions, given in spherical coordinates are:\n\nwhere:\n\nThe quantum numbers can take the following values: \n\nAdditionally, these wavefunctions are \"normalized\" (i.e., the integral of their modulus square equals 1) and orthogonal:\nwhere formula_36 is the state represented by the wavefunction formula_37 in Dirac notation, and formula_38 is the Kronecker delta function.\n\nThe wavefunctions in momentum space are related to the wavefunctions in position space through a Fourier transform\n\nwhich, for the bound states, results in \n\nwhere formula_41 denotes a Gegenbauer polynomial and formula_42 is in units of formula_43.\n\nThe solutions to the Schrödinger equation for hydrogen are analytical, giving a simple expression for the hydrogen energy levels and thus the frequencies of the hydrogen spectral lines and fully reproduced the Bohr model and went beyond it. It also yields two other quantum numbers and the shape of the electron's wave function (\"orbital\") for the various possible quantum-mechanical states, thus explaining the anisotropic character of atomic bonds.\n\nThe Schrödinger equation also applies to more complicated atoms and molecules. When there is more than one electron or nucleus the solution is not analytical and either computer calculations are necessary or simplifying assumptions must be made.\n\nSince the Schrödinger equation is only valid for non-relativistic quantum mechanics, the solutions it yields for the hydrogen atom are not entirely correct. The Dirac equation of relativistic quantum theory improves these solutions (see below).\n\nThe solution of the Schrödinger equation (wave equation) for the hydrogen atom uses the fact that the Coulomb potential produced by the nucleus is isotropic (it is radially symmetric in space and only depends on the distance to the nucleus). Although the resulting energy eigenfunctions (the \"orbitals\") are not necessarily isotropic themselves, their dependence on the angular coordinates follows completely generally from this isotropy of the underlying potential: the eigenstates of the Hamiltonian (that is, the energy eigenstates) can be chosen as simultaneous eigenstates of the angular momentum operator. This corresponds to the fact that angular momentum is conserved in the orbital motion of the electron around the nucleus. Therefore, the energy eigenstates may be classified by two angular momentum quantum numbers, \"ℓ\" and \"m\" (both are integers). The angular momentum quantum number determines the magnitude of the angular momentum. The magnetic quantum number determines the projection of the angular momentum on the (arbitrarily chosen) \"z\"-axis.\n\nIn addition to mathematical expressions for total angular momentum and angular momentum projection of wavefunctions, an expression for the radial dependence of the wave functions must be found. It is only here that the details of the 1/\"r\" Coulomb potential enter (leading to Laguerre polynomials in \"r\"). This leads to a third quantum number, the principal quantum number . The principal quantum number in hydrogen is related to the atom's total energy.\n\nNote that the maximum value of the angular momentum quantum number is limited by the principal quantum number: it can run only up to \"n\" − 1, i.e. .\n\nDue to angular momentum conservation, states of the same \"ℓ\" but different \"m\" have the same energy (this holds for all problems with rotational symmetry). In addition, for the hydrogen atom, states of the same \"n\" but different \"ℓ\" are also degenerate (i.e. they have the same energy). However, this is a specific property of hydrogen and is no longer true for more complicated atoms which have an (effective) potential differing from the form 1/\"r\" (due to the presence of the inner electrons shielding the nucleus potential).\n\nTaking into account the spin of the electron adds a last quantum number, the projection of the electron's spin angular momentum along the \"z\"-axis, which can take on two values. Therefore, any eigenstate of the electron in the hydrogen atom is described fully by four quantum numbers. According to the usual rules of quantum mechanics, the actual state of the electron may be any superposition of these states. This explains also why the choice of \"z\"-axis for the directional quantization of the angular momentum vector is immaterial: an orbital of given \"ℓ\" and \"m\"′ obtained for another preferred axis \"z\"′ can always be represented as a suitable superposition of the various states of different \"m\" (but same \"l\") that have been obtained for \"z\".\n\nIn 1928, Paul Dirac found an equation that was fully compatible with Special Relativity, and (as a consequence) made the wave function a 4-component \"Dirac spinor\" including \"up\" and \"down\" spin components, with both positive and \"negative\" energy (or matter and antimatter). The solution to this equation gave the following results, more accurate than the Schrödinger solution.\n\nThe energy levels of hydrogen, including fine structure (excluding Lamb shift and hyperfine structure), are given by the Sommerfeld fine structure expression:\n\nwhere \"α\" is the fine-structure constant and \"j\" is the \"total angular momentum\" quantum number, which is equal to |\"ℓ\" ± | depending on the direction of the electron spin. This formula represents a small correction to the energy obtained by Bohr and Schrödinger as given above. The factor in square brackets in the last expression is nearly one; the extra term arises from relativistic effects (for details, see #Features going beyond the Schrödinger solution). It is worth noting that this expression was first obtained by A. Sommerfeld in 1916 based on the relativistic version of the old Bohr theory. Sommerfeld has however used different notation for the quantum numbers.\n\nThe image to the right shows the first few hydrogen atom orbitals (energy eigenfunctions). These are cross-sections of the probability density that are color-coded (black represents zero density and white represents the highest density). The angular momentum (orbital) quantum number \"ℓ\" is denoted in each column, using the usual spectroscopic letter code (\"s\" means \"ℓ\" = 0, \"p\" means \"ℓ\" = 1, \"d\" means \"ℓ\" = 2). The main (principal) quantum number \"n\" (= 1, 2, 3, ...) is marked to the right of each row. For all pictures the magnetic quantum number \"m\" has been set to 0, and the cross-sectional plane is the \"xz\"-plane (\"z\" is the vertical axis). The probability density in three-dimensional space is obtained by rotating the one shown here around the \"z\"-axis.\n\nThe \"ground state\", i.e. the state of lowest energy, in which the electron is usually found, is the first one, the 1\"s\" state (principal quantum level \"n\" = 1, \"ℓ\" = 0).\n\nBlack lines occur in each but the first orbital: these are the nodes of the wavefunction, i.e. where the probability density is zero. (More precisely, the nodes are spherical harmonics that appear as a result of solving Schrödinger's equation in polar coordinates.)\n\nThe quantum numbers determine the layout of these nodes. There are:\n\nThere are several important effects that are neglected by the Schrödinger equation and which are responsible for certain small but measurable deviations of the real spectral lines from the predicted ones:\n\n\nBoth of these features (and more) are incorporated in the relativistic Dirac equation, with predictions that come still closer to experiment. Again the Dirac equation may be solved analytically in the special case of a two-body system, such as the hydrogen atom. The resulting solution quantum states now must be classified by the total angular momentum number \"j\" (arising through the coupling between electron spin and orbital angular momentum). States of the same \"j\" and the same \"n\" are still degenerate. Thus, direct analytical solution of Dirac equation predicts 2S() and 2P() levels of Hydrogen to have exactly the same energy, which is in a contradiction with observations (Lamb-Retherford experiment).\n\n\nFor these developments, it was essential that the solution of the Dirac equation for the hydrogen atom could be worked out exactly, such that any experimentally observed deviation had to be taken seriously as a signal of failure of the theory.\n\nIn the language of Heisenberg's matrix mechanics, the hydrogen atom was first solved by Wolfgang Pauli using a rotational symmetry in four dimension [O(4)-symmetry] generated by the angular momentum \nand the Laplace–Runge–Lenz vector. By extending the symmetry group O(4) to the dynamical group O(4,2),\nthe entire spectrum and all transitions were embedded in a single irreducible group representation.\n\nIn 1979 the (non relativistic) hydrogen atom was solved for the first time within Feynman's path integral formulation\nof quantum mechanics. This work greatly extended the range of applicability of Feynman's method.\n\n\n\n", "id": "14225", "title": "Hydrogen atom"}
{"url": "https://en.wikipedia.org/wiki?curid=14227", "text": "Elagabalus\n\nElagabalus , also known as Heliogabalus (; ; 203 – March 11, 222), was Roman emperor from 218 to 222. A member of the Severan dynasty, he was Syrian, the second son of Julia Soaemias and Sextus Varius Marcellus. In his early youth he served as a priest of the god Elagabal in the hometown of his mother's family, Emesa. As a private citizen, he was probably named Sextus Varius Avitus Bassianus. Upon becoming emperor he took the name Marcus Aurelius Antoninus Augustus. He was called Elagabalus only after his death.\n\nIn 217, the emperor Caracalla was assassinated and replaced by his Praetorian prefect, Marcus Opellius Macrinus. Caracalla's maternal aunt, Julia Maesa, successfully instigated a revolt among the Legio III Gallica to have her eldest grandson (and Caracalla's cousin), Elagabalus, declared emperor in his place. Macrinus was defeated on 8 June 218 at the Battle of Antioch. Elagabalus, barely 14 years old, became emperor, initiating a reign remembered mainly for sex scandals and religious controversy.\n\nLater historians suggest Elagabalus showed a disregard for Roman religious traditions and sexual taboos. He replaced the traditional head of the Roman pantheon, Jupiter, with the deity of whom he was high priest, Elagabalus. He forced leading members of Rome's government to participate in religious rites celebrating this deity, over which he personally presided. Elagabalus was supposedly \"married\" as many as five times, lavished favours on male courtiers popularly thought to have been his lovers, and was reported to have prostituted himself in the imperial palace. His behavior estranged the Praetorian Guard, the Senate, and the common people alike. Amidst growing opposition, Elagabalus, just 18 years old, was assassinated and replaced by his cousin Severus Alexander on 11 March 222, in a plot formulated by his grandmother, Julia Maesa, and carried out by disaffected members of the Praetorian Guard.\n\nElagabalus developed a reputation among his contemporaries for extreme eccentricity, decadence, and zealotry. This tradition has persisted, and with writers of the early modern age he suffers one of the worst reputations among Roman emperors. Edward Gibbon, for example, wrote that Elagabalus \"abandoned himself to the grossest pleasures and ungoverned fury\". According to Barthold Georg Niebuhr, \"The name Elagabalus is branded in history above all others\" because of his \"unspeakably disgusting life\".\n\nElagabalus was born around the year 203 to Sextus Varius Marcellus and Julia Soaemias Bassiana. His father was initially a member of the Equites class, but was later elevated to the rank of senator. His grandmother, Julia Maesa, was the widow of the consul Julius Avitus, the sister of Julia Domna, and the sister-in-law of the emperor Septimius Severus. He had at least one sibling: an unnamed elder brother. His mother, Julia Soaemias, was a cousin of the Roman emperor Caracalla. Other relatives included his aunt Julia Avita Mamaea and uncle Marcus Julius Gessius Marcianus and among their children, their son Severus Alexander. Elagabalus's family held hereditary rights to the priesthood of the sun god Elagabal, of whom Elagabalus was the high priest at Emesa (modern Homs) in Roman Syria.\n\nThe deity Elagabalus was initially venerated at Emesa. This form of the god's name is a Latinized version of the Syrian \"Ilāh hag-Gabal\", which derives from \"Ilāh\" (a Semitic word for \"god\") and \"gabal\" (an Aramaic word for \"mountain\"), resulting in \"the God of the Mountain,\" the Emesene manifestation of the deity. The cult of the deity spread to other parts of the Roman Empire in the 2nd century; a dedication has been found as far away as Woerden (Netherlands), near the Roman \"limes\". The god was later imported and assimilated with the Roman sun god known as Sol Indiges in republican times and as Sol Invictus during the second and third centuries CE. In Greek the sun god is Helios, hence \"Heliogabalus\", a hybrid conjunction of \"Helios\" and \"Elagabalus\".\n\nWhen the emperor Macrinus came to power, he suppressed the threat against his reign from the family of his assassinated predecessor, Caracalla, by exiling them—Julia Maesa, her two daughters, and her eldest grandson Elagabalus—to their estate at Emesa in Syria. Almost upon arrival in Syria, Maesa began a plot with her advisor and Elagabalus' tutor Gannys, to overthrow Macrinus and elevate the fourteen-year-old Elagabalus to the imperial throne.\n\nHis mother publicly declared that he was the illegitimate son of Caracalla, therefore due the loyalties of Roman soldiers and senators who had sworn allegiance to Caracalla. After Julia Maesa displayed her wealth to the Third Legion at Raphana they swore allegiance to Elagabalus. At sunrise on 16 May 218, Publius Valerius Comazon, commander of the legion, declared him emperor. To strengthen his legitimacy through further propaganda, Elagabalus assumed Caracalla's names, \"Marcus Aurelius Antoninus\".\n\nIn response Macrinus dispatched his Praetorian prefect Ulpius Julianus to the region with a contingent of troops he considered strong enough to crush the rebellion. However, this force soon joined the faction of Elagabalus when, during the battle, they turned on their own commanders. The officers were killed and Julianus' head was sent back to the emperor.\n\nMacrinus now sent letters to the Senate denouncing Elagabalus as the \"False Antoninus\" and claiming he was insane. Both consuls and other high-ranking members of Rome's leadership condemned Elagabalus, and the Senate subsequently declared war on both Elagabalus and Julia Maesa.\n\nMacrinus and his son, weakened by the desertion of the Second Legion due to bribes and promises circulated by Julia Maesa, were defeated on 8 June 218 at the Battle of Antioch by troops commanded by Gannys. Macrinus fled toward Italy, disguised as a courier, but was later intercepted near Chalcedon and executed in Cappadocia. His son Diadumenianus, sent for safety to the Parthian court, was captured at Zeugma and also put to death.\n\nElagabalus declared the date of the victory at Antioch to be the beginning of his reign and assumed the imperial titles without prior senatorial approval, which violated tradition but was a common practice among 3rd-century emperors nonetheless. Letters of reconciliation were dispatched to Rome extending amnesty to the Senate and recognizing the laws, while also condemning the administration of Macrinus and his son.\n\nThe senators responded by acknowledging Elagabalus as emperor and accepting his claim to be the son of Caracalla. Caracalla and Julia Domna were both deified by the Senate, both Julia Maesa and Julia Soaemias were elevated to the rank of Augustae, and the memory of both Macrinus and Diadumenianus was condemned by the Senate. The former commander of the Third Legion, Comazon, was appointed commander of the Praetorian Guard.\n\nElagabalus and his entourage spent the winter of 218 in Bithynia at Nicomedia, where the emperor's religious beliefs first presented themselves as a problem. The contemporary historian Cassius Dio suggests that Gannys was in fact killed by the new emperor because he was forcing Elagabalus to live \"temperately and prudently\". To help Romans adjust to the idea of having an oriental priest as emperor, Julia Maesa had a painting of Elagabalus in priestly robes sent to Rome and hung over a statue of the goddess Victoria in the Senate House. This placed senators in the awkward position of having to make offerings to Elagabalus whenever they made offerings to Victoria.\n\nThe legions were dismayed by his behaviour and quickly came to regret having supported his accession. While Elagabalus was still on his way to Rome, brief revolts broke out by the Fourth Legion at the instigation of Gellius Maximus, and by the Third Legion, which itself had been responsible for the elevation of Elagabalus to the throne, under the command of Senator Verus. The rebellion was quickly put down, and the Third Legion disbanded.\n\nWhen the entourage reached Rome in the autumn of 219, Comazon and other allies of Julia Maesa and Elagabalus were given powerful and lucrative positions, to the consternation of many senators who did not consider them worthy of such privileges. After his tenure as Praetorian prefect, Comazon would serve as the city prefect of Rome three times, and as consul twice. Elagabalus soon devalued the Roman currency. He decreased the silver purity of the \"denarius\" from 58% to 46.5% — the actual silver weight dropping from 1.82 grams to 1.41 grams. He also demonetized the \"antoninianus\" during this period in Rome.\n\nElagabalus tried to have his presumed lover, the charioteer Hierocles, declared Caesar, while another alleged lover, the athlete Aurelius Zoticus, was appointed to the non-administrative but influential position of Master of the Chamber, or \"Cubicularius\". His offer of amnesty for the Roman upper class was largely honoured, though the jurist Ulpian was exiled.\n\nThe relationships between Julia Maesa, Julia Soaemias, and Elagabalus were strong at first. His mother and grandmother became the first women to be allowed into the Senate, and both received senatorial titles: Soaemias the established title of \"Clarissima,\" and Maesa the more unorthodox \"Mater Castrorum et Senatus\" (\"Mother of the army camp and of the Senate\"). They held the title of \"Augusta\" as well, indicating that they may have been the power behind the throne. Indeed, they held much influence over the young emperor throughout his reign, and can be found on many coins and inscriptions - a rare honor for Roman women.\n\nSince the reign of Septimius Severus, sun worship had increased throughout the Empire. Elagabalus saw this as an opportunity to install Elagabal as the chief deity of the Roman pantheon. The god was renamed \"Deus Sol Invictus\", meaning \"God the Undefeated Sun\", and honored above Jupiter.\n\nAs a token of respect for Roman religion, however, Elagabalus joined either Astarte, Minerva, Urania, or some combination of the three to Elagabal as wife. A union between Elagabal and a traditional goddess would have served to strengthen ties between the new religion and the imperial cult. In fact, there may have been an effort to introduce Elagabal, Urania, and Athena as the new triad of Rome - replacing that of Jupiter, Juno, and Minerva.\n\nHe stirred further discontent when he himself married the Vestal Virgin Aquilia Severa, claiming the marriage would produce \"godlike children\". This was a flagrant breach of Roman law and tradition, which held that any Vestal found to have engaged in sexual intercourse was to be buried alive.\n\nA lavish temple called the Elagabalium was built on the east face of the Palatine Hill to house Elagabal, who was represented by a black conical meteorite from Emesa. Herodian wrote \"this stone is worshipped as though it were sent from heaven; on it there are some small projecting pieces and markings that are pointed out, which the people would like to believe are a rough picture of the sun, because this is how they see them\".\n\nIn order to become the high priest of his new religion, Elagabalus had himself circumcised. He forced senators to watch while he danced around the altar of Deus Sol Invictus to the accompaniment of drums and cymbals. Each summer solstice he held a festival dedicated to the god, which became popular with the masses because of the free food distributed on such occasions. During this festival, Elagabalus placed the Emesa stone on a chariot adorned with gold and jewels, which he paraded through the city:\n\nThe most sacred relics from the Roman religion were transferred from their respective shrines to the Elagabalium, including the emblem of the Great Mother, the fire of Vesta, the Shields of the Salii and the Palladium, so that no other god could be worshipped except in company with Elagabal.\n\nThe question of Elagabalus' sexual orientation is confused, owing to salacious and unreliable sources. Elagabalus married and divorced five women, three of whom are known. His first wife was Julia Cornelia Paula; the second was the Vestal Virgin Julia Aquilia Severa.\n\nWithin a year, he abandoned her and married Annia Aurelia Faustina, a descendant of Marcus Aurelius and the widow of a man recently executed by Elagabalus. He had returned to his second wife Severa by the end of the year. According to Cassius Dio, his most stable relationship seems to have been with his chariot driver, a blond slave from Caria named Hierocles, whom he referred to as his husband.\n\nThe \"Augustan History\" claims that he also married a man named Zoticus, an athlete from Smyrna, in a public ceremony at Rome. Cassius Dio reported that Elagabalus would paint his eyes, epilate his hair and wear wigs before prostituting himself in taverns, brothels, and even in the imperial palace:\n\nHerodian commented that Elagabalus enhanced his natural good looks by the regular application of cosmetics. He was described as having been \"delighted to be called the mistress, the wife, the queen of Hierocles\" and was reported to have offered vast sums of money to any physician who could equip him with female genitalia. Elagabalus has been characterized by some modern writers as transgender, perhaps transsexual.\n\nBy 221 Elagabalus' eccentricities, particularly his relationship with Hierocles, increasingly provoked the soldiers of the Praetorian Guard. When Elagabalus' grandmother Julia Maesa perceived that popular support for the emperor was waning, she decided that he and his mother, who had encouraged his religious practices, had to be replaced. As alternatives, she turned to her other daughter, Julia Avita Mamaea, and her daughter's son, the thirteen-year-old Severus Alexander.\n\nPrevailing on Elagabalus, she arranged that he appoint his cousin Alexander as his heir and be given the title of \"Caesar\". Alexander shared the consulship with the emperor that year. However, Elagabalus reconsidered this arrangement when he began to suspect that the Praetorian Guard preferred his cousin over himself.\n\nFollowing the failure of various attempts on Alexander's life, Elagabalus stripped his cousin of his titles, revoked his consulship, and circulated the news that Alexander was near death, in order to see how the Praetorians would react. A riot ensued, and the guard demanded to see Elagabalus and Alexander in the Praetorian camp.\n\nThe emperor complied and on 11 March 222 he publicly presented his cousin along with his own mother, Julia Soaemias. On their arrival the soldiers started cheering Alexander while ignoring Elagabalus, who ordered the summary arrest and execution of anyone who had taken part in this display of insubordination. In response, members of the Praetorian Guard attacked Elagabalus and his mother:\n\nFollowing his assassination, many associates of Elagabalus were killed or deposed, including his lover Hierocles. His religious edicts were reversed and the stone of Elagabal was sent back to Emesa. Women were again barred from attending meetings of the Senate. The practice of \"damnatio memoriae\"—erasing from the public record a disgraced personage formerly of note—was systematically applied in his case.\n\nThe source of many of these stories of Elagabalus's depravity is the \"Augustan History\" (\"Historia Augusta\"), which includes controversial claims. The \"Historia Augusta\" was most likely written toward the end of the 4th century during the reign of emperor Theodosius I. The life of Elagabalus as described in the \"Augustan History\" is of uncertain historical merit. Sections 13 to 17, relating to the fall of Elagabalus, are less controversial among historians.\n\nSources often considered more credible than the \"Augustan History\" include the contemporary historians Cassius Dio and Herodian. Cassius Dio lived from the second half of the 2nd century until sometime after 229. Born into a patrician family, he spent the greater part of his life in public service. He was a senator under emperor Commodus and governor of Smyrna after the death of Septimius Severus. Afterwards he served as suffect consul around 205, and as proconsul in Africa and Pannonia.\n\nSeverus Alexander held him in high esteem and made him his consul again. His \"Roman History\" spans nearly a millennium, from the arrival of Aeneas in Italy until the year 229. As a contemporary of Elagabalus, Cassius Dio's account of his reign is generally considered more reliable than the \"Augustan History\", although by his own admission Dio spent the greater part of the relevant period outside of Rome and had to rely on second-hand accounts.\n\nFurthermore, the political climate in the aftermath of Elagabalus' reign, as well as Dio's own position within the government of Alexander, likely influenced the truth of this part of his history for the worse. Dio regularly refers to Elagabalus as Sardanapalus, partly to distinguish him from his divine namesake, but chiefly to do his part in maintaining the \"damnatio memoriae\" and to associate him with another autocrat notorious for a dissolute life.\n\nAnother contemporary of Elagabalus' was Herodian, a minor Roman civil servant who lived from c. 170 until 240. His work, \"History of the Roman Empire since Marcus Aurelius\", commonly abbreviated as \"Roman History\", is an eyewitness account of the reign of Commodus until the beginning of the reign of Gordian III. His work largely overlaps with Dio's own \"Roman History\", but both texts seem to be independently consistent with each other.\n\nAlthough Herodian is not deemed as reliable as Cassius Dio, his lack of literary and scholarly pretensions make him less biased than senatorial historians. Herodian is considered the most important source for the religious reforms which took place during the reign of Elagabalus, which have been confirmed by numismatic and archaeological evidence.\n\nFor readers of the modern age, \"The History of the Decline and Fall of the Roman Empire\" by Edward Gibbon (1737–94) further cemented the scandalous reputation of Elagabalus. Gibbon not only accepted and expressed outrage at the allegations of the ancient historians, but he might have added some details of his own; he is the first historian known to state that Gannys was a eunuch, for example. Gibbon wrote:\n\nSome recent historians argue for a more favourable picture of the emperor's life and reign. For example, Martijn Icks, in \"Images of Elagabalus\" (2008; republished as \"The Crimes of Elagabalus\" in 2012), doubts the reliability of the ancient sources and argues that it was the emperor's unorthodox religious policies that alienated the power elite of Rome, to the point that his grandmother saw fit to eliminate him and replace him with his cousin. Leonardo de Arrizabalaga y Prado, in \"The Emperor Elagabalus: Fact or Fiction?\" (2008), is also critical of the ancient historians and speculates that neither religion nor sexuality played a role in the fall of the young emperor, who was simply the loser in a power struggle within the imperial family; the loyalty of the Praetorian Guards was up for sale, and Julia Maesa had the resources to outmaneuver and outbribe her grandson. According to this version, once Elagabalus, his mother, and his immediate circle had been murdered, a wholesale propaganda war against his memory resulted in a vicious caricature which has persisted to the present, repeated and often embellished by later historians displaying their own prejudices against effeminacy and other vices which Elagabalus had come to epitomize.\n\nDue to the ancient tradition about him, Elagabalus became something of an (anti-)hero in the Decadent movement of the late 19th century. He often appears in literature and other creative media as the epitome of a young, amoral aesthete. His life and character have informed or at least inspired many famous works of art, by Decadents, even by contemporary artists. The most notable of these works include:\n\n\n\n\n\n\n\n\n\n\n\n\n \n", "id": "14227", "title": "Elagabalus"}
{"url": "https://en.wikipedia.org/wiki?curid=14229", "text": "Homeopathy\n\nHomeopathy () or homoeopathy is a system of alternative medicine created in 1796 by Samuel Hahnemann, based on his doctrine of \"like cures like\" (\"similia similibus curentur\"), a claim that a substance that causes the symptoms of a disease in healthy people would cure similar symptoms in sick people. Homeopathy is a pseudoscience – a belief that is incorrectly presented as scientific. Homeopathic preparations are not effective for treating any condition; large-scale studies have found homeopathy to be no more effective than a placebo, suggesting that any positive effects that follow treatment are only due to the placebo effect and normal recovery from illness.\n\nHahnemann believed the underlying causes of disease were phenomena that he termed \"miasms\", and that homeopathic preparations addressed these. The preparations are manufactured using a process of homeopathic dilution, in which a chosen substance is repeatedly diluted in alcohol or distilled water, each time with the containing vessel being bashed against an elastic material, (commonly a leather-bound book). Dilution typically continues well past the point where no molecules of the original substance remain. Homeopaths select homeopathics by consulting reference books known as \"repertories\", and by considering the totality of the patient's symptoms, personal traits, physical and psychological state, and life history.\n\nHomeopathy is not a plausible system of treatment, as its dogmas about how drugs, illness, the human body, liquids and solutions operate are contradicted by a wide range of discoveries across biology, psychology, physics and chemistry made in the two centuries since its invention. Although some clinical trials produce positive results, multiple systematic reviews have indicated that this is because of chance, flawed research methods, and reporting bias. Continued homeopathic practice, despite the evidence that it does not work, has been criticized as unethical because it discourages the use of effective treatments, with the World Health Organization warning against using homeopathy to try to treat severe diseases such as HIV and malaria. The continued practice of homeopathy, despite a lack of evidence of efficacy, has led to it being characterized within the scientific and medical communities as nonsense, quackery, and a sham.\n\nAssessments by the Australian National Health and Medical Research Council, the United Kingdom's House of Commons Science and Technology Committee and the Swiss Federal Health Office have each concluded that homeopathy is ineffective, and recommended against the practice receiving any further funding.\n\nHomeopaths claim that Hippocrates may have originated homeopathy around 400 BC, when he prescribed a small dose of mandrake root to treat mania, knowing it produces mania in much larger doses. In the 16th century, the pioneer of pharmacology Paracelsus declared that small doses of \"what makes a man ill also cures him\". Samuel Hahnemann (1755–1843) gave homeopathy its name and expanded its principles in the late 18th century.\n\nIn the late 18th and 19th centuries, mainstream medicine used methods like bloodletting and purging, and administered complex mixtures, such as Venice treacle, which was made from 64 substances including opium, myrrh, and viper's flesh. These treatments often worsened symptoms and sometimes proved fatal. Hahnemann rejected these practices – which had been extolled for centuries – as irrational and inadvisable;\ninstead, he advocated the use of single drugs at lower doses and promoted an immaterial, vitalistic view of how living organisms function, believing that diseases have spiritual, as well as physical causes.\n\nThe term \"homeopathy\" was coined by Hahnemann and first appeared in print in 1807.\n\nHahnemann conceived of homeopathy while translating a medical treatise by the Scottish physician and chemist William Cullen into German. Being sceptical of Cullen's theory concerning cinchona's use for curing malaria, Hahnemann ingested some bark specifically to investigate what would happen. He experienced fever, shivering and joint pain: symptoms similar to those of malaria itself. From this, Hahnemann came to believe that all effective drugs produce symptoms in healthy individuals similar to those of the diseases that they treat, in accord with the \"law of similars\" that had been proposed by ancient physicians. An account of the effects of eating cinchona bark noted by Oliver Wendell Holmes, and published in 1861, failed to reproduce the symptoms Hahnemann reported. Hahnemann's law of similars is a postulate rather than a scientific law. This led to the name \"\"homeopathy\"\", which comes from the \"hómoios\", \"-like\" and \"páthos\", \"suffering\")\n\nSubsequent scientific work showed that cinchona cures malaria because it contains quinine, which kills the \"Plasmodium falciparum\" parasite that causes the disease; the mechanism of action is unrelated to Hahnemann's ideas.\n\nHahnemann began to test what effects substances produced in humans, a procedure that would later become known as \"homeopathic proving\". These tests required subjects to test the effects of ingesting substances by clearly recording all of their symptoms as well as the ancillary conditions under which they appeared. He published a collection of provings in 1805, and a second collection of 65 preparations appeared in his book, \"Materia Medica Pura\", in 1810.\n\nBecause Hahnemann believed that large doses of drugs that caused similar symptoms would only aggravate illness, he advocated extreme dilutions of the substances; he devised a technique for making dilutions that he believed would preserve a substance's therapeutic properties while removing its harmful effects. Hahnemann believed that this process aroused and enhanced \"the spirit-like medicinal powers of the crude substances\".\nHe gathered and published a complete overview of his new medical system in his 1810 book, \"The Organon of the Healing Art\", whose 6th edition, published in 1921, is still used by homeopaths today.\n\nIn the \"Organon\", Hahnemann introduced the concept of \"miasms\" as \"infectious principles\" underlying chronic disease. Hahnemann associated each miasm with specific diseases, and thought that initial exposure to miasms causes local symptoms, such as skin or venereal diseases. If, however, these symptoms were suppressed by medication, the cause went deeper and began to manifest itself as diseases of the internal organs. Homeopathy maintains that treating diseases by directly alleviating their symptoms, as is sometimes done in conventional medicine, is ineffective because all \"disease can generally be traced to some latent, deep-seated, underlying chronic, or inherited tendency\". The underlying imputed miasm still remains, and deep-seated ailments can be corrected only by removing the deeper disturbance of the vital force.\n\nHahnemann’s hypotheses for the direct or remote cause of all chronic diseases (miasms) originally presented only three, psora (the itch), syphilis (venereal disease) or sycosis (fig-wart disease). Of these three the most important was \"psora\" (Greek for \"itch\"), described as being related to any itching diseases of the skin, supposed to be derived from suppressed scabies, and claimed to be the foundation of many further disease conditions. Hahnemann believed psora to be the cause of such diseases as epilepsy, cancer, jaundice, deafness, and cataracts.\nSince Hahnemann's time, other miasms have been proposed, some replacing one or more of psora's proposed functions, including tuberculosis and cancer miasms.\n\nThe law of susceptibility implies that a negative state of mind can attract hypothetical disease entities called \"miasms\" to invade the body and produce symptoms of diseases. Hahnemann rejected the notion of a disease as a separate thing or invading entity, and insisted it was always part of the \"living whole\". Hahnemann coined the expression \"allopathic medicine\", which was used to pejoratively refer to traditional Western medicine.\n\nHahnemann's miasm theory remains disputed and controversial within homeopathy even in modern times. The theory of miasms has been criticized as an explanation developed by Hahnemann to preserve the system of homeopathy in the face of treatment failures, and for being inadequate to cover the many hundreds of sorts of diseases, as well as for failing to explain disease predispositions, as well as genetics, environmental factors, and the unique disease history of each patient.\n\nHomeopathy achieved its greatest popularity in the 19th century. It was introduced to the United States in 1825 by Hans Birch Gram, a student of Hahnemann. The first homeopathic school in the US opened in 1835, and in 1844, the first US national medical association, the American Institute of Homeopathy, was established. Throughout the 19th century, dozens of homeopathic institutions appeared in Europe and the United States, and by 1900, there were 22 homeopathic colleges and 15,000 practitioners in the United States. Because medical practice of the time relied on ineffective and often dangerous treatments, patients of homeopaths often had better outcomes than those of the doctors of the time. Homeopathic preparations, even if ineffective, would almost surely cause no harm, making the users of homeopathic preparations less likely to be killed by the treatment that was supposed to be helping them. The relative success of homeopathy in the 19th century may have led to the abandonment of the ineffective and harmful treatments of bloodletting and purging and to have begun the move towards more effective, science-based medicine.\nOne reason for the growing popularity of homeopathy was its apparent success in treating people suffering from infectious disease epidemics.\nDuring 19th-century epidemics of diseases such as cholera, death rates in homeopathic hospitals were often lower than in conventional hospitals, where the treatments used at the time were often harmful and did little or nothing to combat the diseases.\n\nFrom its inception, however, homeopathy was criticized by mainstream science. Sir John Forbes, physician to Queen Victoria, said in 1843 that the extremely small doses of homeopathy were regularly derided as useless, \"an outrage to human reason\". James Young Simpson said in 1853 of the highly diluted drugs: \"No poison, however strong or powerful, the billionth or decillionth of which would in the least degree affect a man or harm a fly.\"\n19th-century American physician and author Oliver Wendell Holmes, Sr. was also a vocal critic of homeopathy and published an essay in 1842 entitled \"Homœopathy and Its Kindred Delusions\". The members of the French Homeopathic Society observed in 1867 that some leading homeopathists of Europe not only were abandoning the practice of administering infinitesimal doses but were also no longer defending it. The last school in the US exclusively teaching homeopathy closed in 1920.\n\nAccording to , the Nazi regime in Germany was fascinated by homeopathy, and spent large sums of money on researching its mechanisms, but without gaining a positive result. Unschuld further argues that homeopathy never subsequently took root in the United States, but remained more deeply established in European thinking.\nIn the United States, the \"Food, Drug, and Cosmetic Act\" of 1938 (sponsored by Royal Copeland, a Senator from New York and homeopathic physician) recognized homeopathic preparations as drugs. In the 1950s, there were only 75 pure homeopaths practising in the U.S. However, by the mid to late 1970s, homeopathy made a significant comeback and sales of some homeopathic companies increased tenfold. Some homeopaths give credit for the revival to Greek homeopath George Vithoulkas, who performed a \"great deal of research to update the scenarios and refine the theories and practice of homeopathy\", beginning in the 1970s, but Ernst and Singh consider it to be linked to the rise of the New Age movement. Whichever is correct, mainstream pharmacy chains recognized the business potential of selling homeopathic preparations. The Food and Drug Administration held a hearing April 20 and 21, 2015, requesting public comment on regulation of homeopathic drugs. The FDA cited the growth of sales of over-the-counter homeopathic medicines which was $2.7 billion for 2007.\n\nBruce Hood has argued that the increased popularity of homeopathy in recent times may be due to the comparatively long consultations practitioners are willing to give their patients, and to an irrational preference for \"natural\" products which people think are the basis of homeopathic preparations.\n\nHomeopathic preparations are referred to as \"homeopathics\" or \"remedies\". Practitioners rely on two types of reference when prescribing: \"materia medica\" and repertories. A homeopathic \"materia medica\" is a collection of \"drug pictures\", organized alphabetically. These entries describe the symptom patterns associated with individual preparations. A homeopathic repertory is an index of disease symptoms that lists preparations associated with specific symptoms. In both cases different compilers may dispute particular inclusions. The first symptomatic homeopathic \"materia medica\" was arranged by Hahnemann. The first homeopathic repertory was Georg Jahr's \"Symptomenkodex\", published in German in 1835, and translated into English as the \"Repertory to the more Characteristic Symptoms of Materia Medica\" by Constantine Hering in 1838.\nThis version was less focused on disease categories and would be the forerunner to later works by James Tyler Kent. Repertories, in particular, may be very large.\n\nHomeopathy uses animal, plant, mineral, and synthetic substances in its preparations, generally referring to them using Latin or faux-Latin names. Examples include \"arsenicum album\" (arsenic oxide), \"natrum muriaticum\" (sodium chloride or table salt), \"Lachesis muta\" (the venom of the bushmaster snake), \"opium\", and \"thyroidinum\" (thyroid hormone).\n\nSome homeopaths use so-called \"nosodes\" (from the Greek \"nosos\", disease) made from diseased or pathological products such as fecal, urinary, and respiratory discharges, blood, and tissue. Conversely, preparations made from \"healthy\" specimens are called \"sarcodes\".\n\nSome modern homeopaths use preparations they call \"imponderables\" because they do not originate from a substance but some other phenomenon presumed to have been \"captured\" by alcohol or lactose. Examples include X-rays\nand sunlight.\n\nOther minority practices include paper preparations, where the substance and dilution are written on pieces of paper and either pinned to the patients' clothing, put in their pockets, or placed under glasses of water that are then given to the patients, and the use of radionics to manufacture preparations. Such practices have been strongly criticized by classical homeopaths as unfounded, speculative, and verging upon magic and superstition.\n\nHahnemann found that undiluted doses caused reactions, sometimes dangerous ones, so specified that preparations be given at the lowest possible dose. He found that this reduced potency as well as side-effects, but formed the view that vigorous shaking and striking on an elastic surface – a process he termed \"Schütteln\", translated as \"succussion\" – nullified this. A common explanation for his settling on this process is said to be that he found preparations subjected to agitation in transit, such as in saddle bags or in a carriage, were more \"potent\". Hahnemann had a saddle-maker construct a special wooden striking board covered in leather on one side and stuffed with horsehair. Insoluble solids, such as granite, diamond, and platinum, are diluted by grinding them with lactose (\"trituration\").\n\nThe process of dilution and succussion is termed \"dynamization\" or \"potentization\" by homeopaths. In industrial manufacture this may be done by machine.\n\nSerial dilution is achieved by taking an amount of the mixture and adding solvent, but the \"Korsakovian\" method may also be used, whereby the vessel in which the preparations are manufactured is emptied, refilled with solvent, and the volume of fluid adhering to the walls of the vessel is deemed sufficient for the new batch. The Korsakovian method is sometimes referred to as K on the label of a homeopathic preparation, e.g. 200CK is a 200C preparation made using the Korsakovian method.\n\nFluxion and radionics methods of preparation do not require succussion. There are differences of opinion on the number and force of strikes, and some practitioners dispute the need for succussion at all while others reject the Korsakovian and other non-classical preparations. There are no laboratory assays and the importance and techniques for succussion cannot be determined with any certainty from the literature.\n\nThree main logarithmic potency scales are in regular use in homeopathy. Hahnemann created the \"centesimal\" or \"C scale\", diluting a substance by a factor of 100 at each stage. The centesimal scale was favoured by Hahnemann for most of his life.\n\nA 2C dilution requires a substance to be diluted to one part in 100, and then some of that diluted solution diluted by a further factor of 100.\n\nThis works out to one part of the original substance in 10,000 parts of the solution. A 6C dilution repeats this process six times, ending up with the original substance diluted by a factor of 100=10 (one part in one trillion or 1/1,000,000,000,000). Higher dilutions follow the same pattern.\n\nIn homeopathy, a solution that is more dilute is described as having a higher \"potency\", and more dilute substances are considered by homeopaths to be stronger and deeper-acting. The end product is often so diluted as to be indistinguishable from the diluent (pure water, sugar or alcohol). There is also a decimal potency scale (notated as \"X\" or \"D\") in which the preparation is diluted by a factor of 10 at each stage.\n\nHahnemann advocated 30C dilutions for most purposes (that is, dilution by a factor of 10). Hahnemann regularly used potencies up to 300C but opined that \"there must be a limit to the matter, it cannot go on indefinitely\".\n\nIn Hahnemann's time, it was reasonable to assume the preparations could be diluted indefinitely, as the concept of the atom or molecule as the smallest possible unit of a chemical substance was just beginning to be recognized.\n\nThe greatest dilution reasonably likely to contain even one molecule of the original substance is 12C.\nCritics and advocates of homeopathy alike commonly attempt to illustrate the dilutions involved in homeopathy with analogies.\nHahnemann is reported to have joked that a suitable procedure to deal with an epidemic would be to empty a bottle of poison into Lake Geneva, if it could be succussed 60 times.\nAnother example given by a critic of homeopathy states that a 12C solution is equivalent to a \"pinch of salt in both the North and South Atlantic Oceans\", which is approximately correct.\nOne-third of a drop of some original substance diluted into all the water on earth would produce a preparation with a concentration of about 13C. A popular homeopathic treatment for the flu is a 200C dilution of duck liver, marketed under the name Oscillococcinum. As there are only about 10 atoms in the entire observable universe, a dilution of one molecule in the observable universe would be about 40C. Oscillococcinum would thus require 10 more universes to simply have one molecule in the final substance.\nThe high dilutions characteristically used are often considered to be the most controversial and implausible aspect of homeopathy.\n\nNot all homeopaths advocate high dilutions. Preparations at concentrations below 4X are considered an important part of homeopathic heritage. Many of the early homeopaths were originally doctors and generally used lower dilutions such as \"3X\" or \"6X\", rarely going beyond \"12X\".\nThe split between lower and higher dilutions followed ideological lines.\nThose favouring low dilutions stressed pathology and a stronger link to conventional medicine, while those favouring high dilutions emphasized vital force, miasms and a spiritual interpretation of disease.\nSome products with such relatively lower dilutions continue to be sold, but like their counterparts, they have not been conclusively demonstrated to have any effect beyond that of a placebo.\n\nA homeopathic \"proving\" is the method by which the profile of a homeopathic preparation is determined.\n\nAt first Hahnemann used undiluted doses for provings, but he later advocated provings with preparations at a 30C dilution, and most modern provings are carried out using ultra-dilute preparations in which it is highly unlikely that any of the original molecules remain. During the proving process, Hahnemann administered preparations to healthy volunteers, and the resulting symptoms were compiled by observers into a \"drug picture\".\n\nThe volunteers were observed for months at a time and made to keep extensive journals detailing all of their symptoms at specific times throughout the day. They were forbidden from consuming coffee, tea, spices, or wine for the duration of the experiment; playing chess was also prohibited because Hahnemann considered it to be \"too exciting\", though they were allowed to drink beer and encouraged to exercise in moderation.\n\nAfter the experiments were over, Hahnemann made the volunteers take an oath swearing that what they reported in their journals was the truth, at which time he would interrogate them extensively concerning their symptoms.\n\nProvings are claimed to have been important in the development of the clinical trial, due to their early use of simple control groups, systematic and quantitative procedures, and some of the first application of statistics in medicine. The lengthy records of self-experimentation by homeopaths have occasionally proven useful in the development of modern drugs: For example, evidence that nitroglycerin might be useful as a treatment for angina was discovered by looking through homeopathic provings, though homeopaths themselves never used it for that purpose at that time.\nThe first recorded provings were published by Hahnemann in his 1796 \"Essay on a New Principle\".\nHis \"Fragmenta de Viribus\" (1805) contained the results of 27 provings, and his 1810 \"Materia Medica Pura\" contained 65.\nFor James Tyler Kent's 1905 \"Lectures on Homoeopathic Materia Medica\", 217 preparations underwent provings and newer substances are continually added to contemporary versions.\n\nThough the proving process has superficial similarities with clinical trials, it is fundamentally different in that the process is subjective, not blinded, and modern provings are unlikely to use pharmacologically active levels of the substance under proving. As early as 1842, Holmes noted the provings were impossibly vague, and the purported effect was not repeatable among different subjects.\n\nHomeopaths generally begin with detailed examinations of their patients' histories, including questions regarding their physical, mental and emotional states, their life circumstances and any physical or emotional illnesses. The homeopath then attempts to translate this information into a complex formula of mental and physical symptoms, including likes, dislikes, innate predispositions and even body type.\n\nFrom these symptoms, the homeopath chooses how to treat the patient using \"materia medica\" and repertories. In classical homeopathy, the practitioner attempts to match a single preparation to the totality of symptoms (the \"simlilum\"), while \"clinical homeopathy\" involves combinations of preparations based on the various symptoms of an illness.\n\nHomeopathic pills are made from an inert substance (often sugars, typically lactose), upon which a drop of liquid homeopathic preparation is placed and allowed to evaporate.\n\nThe process of homeopathic dilution results in no objectively detectable active ingredient in most cases, but some preparations (e.g. calendula and arnica creams) do contain pharmacologically active doses. One product, Zicam Cold Remedy, which was marketed as an \"unapproved homeopathic\" product, contains two ingredients that are only \"slightly\" diluted: zinc acetate (2X = 1/100 dilution) and zinc gluconate (1X = 1/10 dilution), which means both are present in a biologically active concentration strong enough to have caused some people to lose their sense of smell, a condition termed anosmia. Zicam also listed several normal homeopathic potencies as \"inactive ingredients\", including \"galphimia glauca\", histamine dihydrochloride (homeopathic name, \"histaminum hydrochloricum\"), \"luffa operculata\", and sulfur.\n\nIsopathy is a therapy derived from homeopathy, invented by Johann Joseph Wilhelm Lux in the 1830s. Isopathy differs from homeopathy in general in that the preparations, known as \"nosodes\", are made up either from things that cause the disease or from products of the disease, such as pus. Many so-called \"homeopathic vaccines\" are a form of isopathy.\n\nFlower preparations can be produced by placing flowers in water and exposing them to sunlight. The most famous of these are the Bach flower remedies, which were developed by the physician and homeopath Edward Bach. Although the proponents of these preparations share homeopathy's vitalist world-view and the preparations are claimed to act through the same hypothetical \"vital force\" as homeopathy, the method of preparation is different. Bach flower preparations are manufactured in allegedly \"gentler\" ways such as placing flowers in bowls of sunlit water, and the preparations are not succussed. There is no convincing scientific or clinical evidence for flower preparations being effective.\n\nThe idea of using homeopathy as a treatment for other animals termed \"veterinary homeopathy\", dates back to the inception of homeopathy; Hahnemann himself wrote and spoke of the use of homeopathy in animals other than humans. The FDA has not approved homeopathic products as veterinary medicine in the U.S. In the UK, veterinary surgeons who use homeopathy may belong to the Faculty of Homeopathy and/or to the British Association of Homeopathic Veterinary Surgeons. Animals may be treated only by qualified veterinary surgeons in the UK and some other countries. Internationally, the body that supports and represents homeopathic veterinarians is the International Association for Veterinary Homeopathy.\n\nThe use of homeopathy in veterinary medicine is controversial; the little existing research on the subject is not of a high enough scientific standard to provide reliable data on efficacy. Other studies have also found that giving animals placebos can play active roles in influencing pet owners to believe in the effectiveness of the treatment when none exists. The British Veterinary Association's position statement on alternative medicines says that it \"cannot endorse\" homeopathy, and the Australian Veterinary Association includes it on its list of \"ineffective therapies\". A 2016 review of peer-reviewed articles from 1981 to 2014 by scientists from the University of Kassel, Germany, concluded that there was insufficient evidence to support the use of homeopathy in livestock as a way to prevent or treat infectious diseases.\n\nThe UK's Department for Environment, Food and Rural Affairs (Defra) has adopted a robust position against use of \"alternative\" pet preparations including homeopathy.\n\nElectrohomeopathy is a treatment devised by Count Cesare Mattei (1809–1896), who proposed that different \"colours\" of electricity could be used to treat cancer. Popular in the late nineteenth century, electrohomeopathy has been described as \"utter idiocy\".\n\nThe use of homeopathy as a preventive for serious infectious diseases is especially controversial, in the context of ill-founded public alarm over the safety of vaccines stoked by the anti-vaccination movement. Promotion of homeopathic alternatives to vaccines has been characterized as dangerous, inappropriate and irresponsible. In December 2014, Australian homeopathy supplier Homeopathy Plus! were found to have acted deceptively in promoting homeopathic alternatives to vaccines.\n\nThe low concentration of homeopathic preparations, which often lack even a single molecule of the diluted substance, has been the basis of questions about the effects of the preparations since the 19th century. Modern advocates of homeopathy have proposed a concept of \"water memory\", according to which water \"remembers\" the substances mixed in it, and transmits the effect of those substances when consumed. This concept is inconsistent with the current understanding of matter, and water memory has never been demonstrated to have any detectable effect, biological or otherwise. Pharmacological research has found instead that stronger effects of an active ingredient come from higher, not lower doses.\n\nJames Randi and the groups have highlighted the lack of active ingredients in most homeopathic products by taking large 'overdoses'. None of the hundreds of demonstrators in the UK, Australia, New Zealand, Canada and the US were injured and \"no one was cured of anything, either\".\n\nOutside of the alternative medicine community, scientists have long considered homeopathy a sham or a pseudoscience, and the mainstream medical community regards it as quackery. There is an overall absence of sound statistical evidence of therapeutic efficacy, which is consistent with the lack of any biologically plausible pharmacological agent or mechanism.\n\nAbstract concepts within theoretical physics have been invoked to suggest explanations of how or why preparations might work, including quantum entanglement, quantum nonlocality, the theory of relativity and chaos theory. Contrariwise, quantum superposition has been invoked to explain why homeopathy does \"not\" work in double-blind trials. However, the explanations are offered by nonspecialists within the field, and often include speculations that are incorrect in their application of the concepts and not supported by actual experiments. Several of the key concepts of homeopathy conflict with fundamental concepts of physics and chemistry. The use of quantum entanglement to explain homeopathy's purported effects is \"patent nonsense\", as entanglement is a delicate state which rarely lasts longer than a fraction of a second. While entanglement may result in certain aspects of individual subatomic particles acquiring linked quantum states, this does not mean the particles will mirror or duplicate each other, nor cause health-improving transformations.\n\nThe proposed mechanisms for homeopathy are precluded from having any effect by the laws of physics and physical chemistry. The extreme dilutions used in homeopathic preparations usually leave none of the original substance in the final product.\n\nA number of speculative mechanisms have been advanced to counter this, the most widely discussed being water memory, though this is now considered erroneous since short-range order in water only persists for about 1 picosecond. No evidence of stable clusters of water molecules was found when homeopathic preparations were studied using nuclear magnetic resonance, and many other physical experiments in homeopathy have been found to be of low methodological quality, which precludes any meaningful conclusion. Existence of a pharmacological effect in the absence of any true active ingredient is inconsistent with the law of mass action and the observed dose-response relationships characteristic of therapeutic drugs (whereas placebo effects are non-specific and unrelated to pharmacological activity).\n\nHomeopaths contend that their methods produce a therapeutically active preparation, selectively including only the intended substance, though critics note that any water will have been in contact with millions of different substances throughout its history, and homeopaths have not been able to account for a reason why only the selected homeopathic substance would be a special case in their process. For comparison, ISO 3696:1987 defines a standard for water used in laboratory analysis; this allows for a contaminant level of ten parts per billion, 4C in homeopathic notation. This water may not be kept in glass as contaminants will leach out into the water.\n\nPractitioners of homeopathy hold that higher dilutions―described as being of higher \"potency\"―produce stronger medicinal effects. This idea is also inconsistent with observed dose-response relationships, where effects are dependent on the concentration of the active ingredient in the body. This dose-response relationship has been confirmed in myriad experiments on organisms as diverse as nematodes, rats, and humans. Some homeopaths contend that the phenomenon of hormesis may support the idea of dilution increasing potency, but the dose-response relationship outside the zone of hormesis declines with dilution as normal, and nonlinear pharmacological effects do not provide any credible support for homeopathy.\n\nPhysicist Robert L. Park, former executive director of the American Physical Society, is quoted as saying,\n\n\"since the least amount of a substance in a solution is one molecule, a 30C solution would have to have at least one molecule of the original substance dissolved in a minimum of 1,000,000,000,000,000,000,000,000,000,000,<wbr>000,000,000,000,000,000,000,000,000,000 [or 10] molecules of water. This would require a container more than 30,000,000,000 times the size of the Earth.\"\nPark is also quoted as saying that, \"to expect to get even one molecule of the 'medicinal' substance allegedly present in 30X pills, it would be necessary to take some two billion of them, which would total about a thousand tons of lactose plus whatever impurities the lactose contained\".\n\nThe laws of chemistry state that there is a limit to the dilution that can be made without losing the original substance altogether. This limit, which is related to Avogadro's number, is roughly equal to homeopathic dilutions of 12C or 24X (1 part in 10).\n\nScientific tests run by both the BBC's \"Horizon\" and ABC's \"20/20\" programmes were unable to differentiate homeopathic dilutions from water, even when using tests suggested by homeopaths themselves.\n\nNo individual preparation has been unambiguously shown by research to be different from placebo. The methodological quality of the primary research was generally low, with such problems as weaknesses in study design and reporting, small sample size, and selection bias. Since better quality trials have become available, the evidence for efficacy of homeopathy preparations has diminished; the highest-quality trials indicate that the preparations themselves exert no intrinsic effect. A review conducted in 2010 of all the pertinent studies of \"best evidence\" produced by the Cochrane Collaboration concluded that \"the most reliable evidence – that produced by Cochrane reviews – fails to demonstrate that homeopathic medicines have effects beyond placebo.\"\n\nGovernment-level reviews have been conducted in recent years by Switzerland (2005), the United Kingdom (2009) and Australia (2015).\n\nThe Swiss \"programme for the evaluation of complementary medicine\" (PEK) resulted in the peer-reviewed Shang publication (see \"Systematic reviews and meta-analyses of efficacy\") and a controversial competing analysis by homeopaths and advocates led by Gudrun Bornhöft and Peter Matthiessen, which has misleadingly been presented as a Swiss government report by homeopathy proponents, a claim that has been repudiated by the Swiss Federal Office of Public Health. The Swiss Government terminated reimbursement, though it was subsequently reinstated after a political campaign and referendum for a further six-year trial period.\n\nThe United Kingdom's House of Commons Science and Technology Committee sought written evidence and submissions from concerned parties and, following a review of all submissions, concluded that there was no compelling evidence of effect other than placebo and recommended that the Medicines and Healthcare products Regulatory Agency (MHRA) should not allow homeopathic product labels to make medical claims, that homeopathic products should no longer be licensed by the MHRA, as they are not medicines, and that further clinical trials of homeopathy could not be justified. They recommended that funding of homeopathic hospitals should not continue, and NHS doctors should not refer patients to homeopaths. The Secretary of State for Health deferred to local NHS on funding homeopathy, in the name of patient choice. By February 2011 only one-third of primary care trusts still funded homeopathy. By 2012, no British universities offered homeopathy courses.\n\nThe Australian National Health and Medical Research Council completed a comprehensive review of the effectiveness of homeopathic preparations in 2015, in which it concluded that \"there were no health conditions for which there was reliable evidence that homeopathy was effective. No good-quality, well-designed studies with enough participants for a meaningful result reported either that homeopathy caused greater health improvements than placebo, or caused health improvements equal to those of another treatment.\"\n\nThe fact that individual randomized controlled trials have given positive results is not in contradiction with an overall lack of statistical evidence of efficacy. A small proportion of randomized controlled trials inevitably provide false-positive outcomes due to the play of chance: a \"statistically significant\" positive outcome is commonly adjudicated when the probability of it being due to chance rather than a real effect is no more than 5%―a level at which about 1 in 20 tests can be expected to show a positive result in the absence of any therapeutic effect. Furthermore, trials of low methodological quality (i.e. ones which have been inappropriately designed, conducted or reported) are prone to give misleading results. In a systematic review of the methodological quality of randomized trials in three branches of alternative medicine, Linde \"et al.\" highlighted major weaknesses in the homeopathy sector, including poor randomization. A separate 2001 systematic review that assessed the quality of clinical trials of homeopathy found that such trials were generally of lower quality than trials of conventional medicine.\n\nA related issue is publication bias: researchers are more likely to submit trials that report a positive finding for publication, and journals prefer to publish positive results. Publication bias has been particularly marked in alternative medicine journals, where few of the published articles (just 5% during the year 2000) tend to report null results. Regarding the way in which homeopathy is represented in the medical literature, a systematic review found signs of bias in the publications of clinical trials (towards negative representation in mainstream medical journals, and \"vice versa\" in alternative medicine journals), but not in reviews.\n\nPositive results are much more likely to be false if the prior probability of the claim under test is low.\n\nBoth meta-analyses, which statistically combine the results of several randomized controlled trials, and other systematic reviews of the literature are essential tools to summarize evidence of therapeutic efficacy. Early systematic reviews and meta-analyses of trials evaluating the efficacy of homeopathic preparations in comparison with placebo more often tended to generate positive results, but appeared unconvincing overall. In particular, reports of three large meta-analyses warned readers that firm conclusions could not be reached, largely due to methodological flaws in the primary studies and the difficulty in controlling for publication bias. The positive finding of one of the most prominent of the early meta-analyses, published in \"The Lancet\" in 1997 by Linde et al., was later reframed by the same research team, who wrote:\n\nThe evidence of bias [in the primary studies] weakens the findings of our original meta-analysis. Since we completed our literature search in 1995, a considerable number of new homeopathy trials have been published. The fact that a number of the new high-quality trials ... have negative results, and a recent update of our review for the most \"original\" subtype of homeopathy (classical or individualized homeopathy), seem to confirm the finding that more rigorous trials have less-promising results. It seems, therefore, likely that our meta-analysis at least overestimated the effects of homeopathic treatments.\n\nSubsequent work by John Ioannidis and others has shown that for treatments with no prior plausibility, the chances of a positive result being a false positive are much higher, and that any result not consistent with the null hypothesis should be assumed to be a false positive.\n\nIn 2002, a systematic review of the available systematic reviews confirmed that higher-quality trials tended to have less positive results, and found no convincing evidence that any homeopathic preparation exerts clinical effects different from placebo.\n\nIn 2005, \"The Lancet\" medical journal published a meta-analysis of 110 placebo-controlled homeopathy trials and 110 matched medical trials based upon the Swiss government's Programme for Evaluating Complementary Medicine, or PEK. The study concluded that its findings were \"compatible with the notion that the clinical effects of homeopathy are placebo effects\". This was accompanied by an editorial pronouncing \"The end of homoeopathy\", which was denounced by the homeopath Peter Fisher.\n\nOther meta-analyses include homeopathic treatments to reduce cancer therapy side-effects following radiotherapy and chemotherapy, allergic rhinitis, attention-deficit hyperactivity disorder and childhood diarrhoea, adenoid vegetation, asthma, upper respiratory tract infection in children, insomnia, fibromyalgia, psychiatric conditions and Cochrane Library reviews of homeopathic treatments for asthma, dementia, attention-deficit hyperactivity disorder, induction of labour, and irritable bowel syndrome. Other reviews covered osteoarthritis, migraines, postoperative ecchymosis and edema, delayed-onset muscle soreness, or eczema and other dermatological conditions.\n\nThe results of these reviews are generally negative or only weakly positive, and reviewers consistently report the poor quality of trials. The finding of Linde \"et. al.\" that more rigorous studies produce less positive results is supported in several and contradicted by none.\n\nSome clinical trials have tested individualized homeopathy, and there have been reviews of this, specifically. A 1998 review found 32 trials that met their inclusion criteria, 19 of which were placebo-controlled and provided enough data for meta-analysis. These 19 studies showed a pooled odds ratio of 1.17 to 2.23 in favour of individualized homeopathy over the placebo, but no difference was seen when the analysis was restricted to the methodologically best trials. The authors concluded that \"the results of the available randomized trials suggest that individualized homeopathy has an effect over placebo. The evidence, however, is not convincing because of methodological shortcomings and inconsistencies.\" Jay Shelton, author of a book on homeopathy, has stated that the claim assumes without evidence that classical, individualized homeopathy works better than nonclassical variations. A systematic review and meta-analysis of trials of individualized homeopathy published in December 2014 concluded that individualized homeopathy may have small effects, but that caution was needed in interpreting the results because of study quality issues – no study included was assessed as being at low risk of bias.\n\nHealth organizations such as the UK's National Health Service, the American Medical Association, the FASEB, and the National Health and Medical Research Council of Australia, have issued statements of their conclusion that there is \"no good-quality evidence that homeopathy is effective as a treatment for any health condition\". In 2009, World Health Organization official Mario Raviglione cricitized the use of homeopathy to treat tuberculosis; similarly, another WHO spokesperson argued there was no evidence homeopathy would be an effective treatment for diarrhoea.\n\nThe American College of Medical Toxicology and the American Academy of Clinical Toxicology recommend that no one use homeopathic treatment for disease or as a preventive health measure. These organizations report that no evidence exists that homeopathic treatment is effective, but that there is evidence that using these treatments produces harm and can bring indirect health risks by delaying conventional treatment.\n\nScience offers a variety of explanations for how homeopathy may appear to cure diseases or alleviate symptoms even though the preparations themselves are inert:\n\nWhile some articles have suggested that homeopathic solutions of high dilution can have statistically significant effects on organic processes including the growth of grain, histamine release by leukocytes, and enzyme reactions, such evidence is disputed since attempts to replicate them have failed. A 2007 systematic review of high-dilution experiments found that none of the experiments with positive results could be reproduced by all investigators.\n\nIn 1987, French immunologist Jacques Benveniste submitted a paper to the journal \"Nature\" while working at INSERM. The paper purported to have discovered that basophils, a type of white blood cell, released histamine when exposed to a homeopathic dilution of anti-immunoglobulin E antibody. The journal editors, sceptical of the results, requested that the study be replicated in a separate laboratory. Upon replication in four separate laboratories the study was published. Still sceptical of the findings, \"Nature\" assembled an independent investigative team to determine the accuracy of the research, consisting of \"Nature\" editor and physicist Sir John Maddox, American scientific fraud investigator and chemist Walter Stewart, and sceptic James Randi. After investigating the findings and methodology of the experiment, the team found that the experiments were \"statistically ill-controlled\", \"interpretation has been clouded by the exclusion of measurements in conflict with the claim\", and concluded, \"We believe that experimental data have been uncritically assessed and their imperfections inadequately reported.\" James Randi stated that he doubted that there had been any conscious fraud, but that the researchers had allowed \"wishful thinking\" to influence their interpretation of the data.\n\nIn 2001 and 2004, Madeleine Ennis published a number of studies which reported that homeopathic dilutions of histamine exerted an effect on the activity of basophils. In response to the first of these studies, \"Horizon\" aired a programme in which British scientists attempted to replicate Ennis' results; they were unable to do so.\n\nThe provision of homeopathic preparations has been described as unethical. Michael Baum, Professor Emeritus of Surgery and visiting Professor of Medical Humanities at University College London (UCL), has described homoeopathy as a \"cruel deception\".\n\nEdzard Ernst, the first \"Professor of Complementary Medicine\" in the United Kingdom and a former homeopathic practitioner, has expressed his concerns about pharmacists who violate their ethical code by failing to provide customers with \"necessary and relevant information\" about the true nature of the homeopathic products they advertise and sell:\n\nPatients who choose to use homeopathy rather than evidence-based medicine risk missing timely diagnosis and effective treatment of serious conditions such as cancer.\n\nIn 2013 the UK Advertising Standards Authority concluded that the Society of Homeopaths were targeting vulnerable ill people and discouraging the use of essential medical treatment while making misleading claims of efficacy for homeopathic products.\n\nIn 2015 the Federal Court of Australia imposed penalties on a homeopathic company, Homeopathy Plus! Pty Ltd and its director, for making false or misleading statements about the efficacy of the whooping cough vaccine and homeopathic remedies as an alternative to the whooping cough vaccine, in breach of the Australian Consumer Law.\n\nSome homeopathic preparations involve poisons such as Belladonna, arsenic, and poison ivy, which are highly diluted in the homeopathic preparation. In rare cases, the original ingredients are present at detectable levels. This may be due to improper preparation or intentional low dilution. Serious adverse effects such as seizures and death have been reported or associated with some homeopathic preparations.\n\nOn 30 September 2016 the FDA issued a safety alert to consumers warning against the use of homeopathic teething gels and tablets following reports of adverse events after their use. The agency recommended that parents discard these products and \"seek advice from their health care professional for safe alternatives\" to homeopathy for teething. The pharmacy CVS announced, also on 30 September, that it was voluntarily withdrawing the products from sale and on 11 October Hyland's (the manufacturer) announced that it was discontinuing their teething medicine in the United States though the products remain on sale in Canada. On 12 October Buzzfeed reported that the regulator had \"examined more than 400 reports of seizures, fever and vomiting, as well as 10 deaths\" over a six year period. The investigation (including analyses of the products) is still ongoing and the FDA does not know yet if the deaths and illnesses were caused by the products. However a previous FDA investigation in 2010, following adverse effects reported then, found that these same products were improperly diluted and contained \"unsafe levels of belladonna, also known as deadly nightshade\" and that the reports of serious adverse events in children using this product were \"consistent with belladonna toxicity\".\n\nInstances of arsenic poisoning have occurred after use of arsenic-containing homeopathic preparations. Zicam Cold remedy Nasal Gel, which contains 2X (1:100) zinc gluconate, reportedly caused a small percentage of users to lose their sense of smell; 340 cases were settled out of court in 2006 for . In 2009, the FDA advised consumers to stop using three discontinued cold remedy Zicam products because it could cause permanent damage to users' sense of smell. Zicam was launched without a New Drug Application (NDA) under a provision in the FDA's Compliance Policy Guide called \"Conditions under which homeopathic drugs may be marketed\" (CPG 7132.15), but the FDA warned Matrixx Initiatives, its manufacturer, via a Warning Letter that this policy does not apply when there is a health risk to consumers.\n\nA 2000 review by homeopaths reported that homeopathic preparations are \"unlikely to provoke severe adverse reactions\". In 2012, a systematic review evaluating evidence of homeopathy's possible adverse effects concluded that \"homeopathy has the potential to harm patients and consumers in both direct and indirect ways\". One of the reviewers, Edzard Ernst, supplemented the article on his blog, writing: \"I have said it often and I say it again: if used as an alternative to an effective cure, even the most 'harmless' treatment can become life-threatening.\" A 2016 systematic review and meta-analysis found that, in homeopathic clinical trials, adverse effects were reported among the patients who received homeopathy about as often as they were reported among patients who received placebo or conventional medicine.\n\nThe lack of convincing scientific evidence supporting its efficacy and its use of preparations without active ingredients have led to characterizations as pseudoscience and quackery, or, in the words of a 1998 medical review, \"placebo therapy at best and quackery at worst\". The Chief Medical Officer for England, Dame Sally Davies, has stated that homeopathic preparations are \"rubbish\" and do not serve as anything more than placebos. Jack Killen, acting deputy director of the National Center for Complementary and Alternative Medicine, says homeopathy \"goes beyond current understanding of chemistry and physics\". He adds: \"There is, to my knowledge, no condition for which homeopathy has been proven to be an effective treatment.\" Ben Goldacre says that homeopaths who misrepresent scientific evidence to a scientifically illiterate public, have \"... walled themselves off from academic medicine, and critique has been all too often met with avoidance rather than argument\". Homeopaths often prefer to ignore meta-analyses in favour of cherry picked positive results, such as by promoting a particular observational study (one which Goldacre describes as \"little more than a customer-satisfaction survey\") as if it were more informative than a series of randomized controlled trials.\n\nReferring specifically to homeopathy, the British House of Commons Science and Technology Committee has stated:\n\nThe National Center for Complementary and Alternative Medicine of the United States' National Institutes of Health states:\n\nBen Goldacre noted that in the early days of homeopathy, when medicine was dogmatic and frequently worse than doing nothing, homeopathy at least failed to make matters worse:\n\nOn clinical grounds, patients who choose to use homeopathy in preference to normal medicine risk missing timely diagnosis and effective treatment, thereby worsening the outcomes of serious conditions. Critics of homeopathy have cited individual cases of patients of homeopathy failing to receive proper treatment for diseases that could have been easily diagnosed and managed with conventional medicine and who have died as a result, and the \"marketing practice\" of criticizing and downplaying the effectiveness of mainstream medicine. Homeopaths claim that use of conventional medicines will \"push the disease deeper\" and cause more serious conditions, a process referred to as \"suppression\". Some homeopaths (particularly those who are non-physicians) advise their patients against immunization. Some homeopaths suggest that vaccines be replaced with homeopathic \"nosodes\", created from biological materials such as pus, diseased tissue, bacilli from sputum or (in the case of \"bowel nosodes\") faeces. While Hahnemann was opposed to such preparations, modern homeopaths often use them although there is no evidence to indicate they have any beneficial effects. Cases of homeopaths advising against the use of anti-malarial drugs have been identified. This puts visitors to the tropics who take this advice in severe danger, since homeopathic preparations are completely ineffective against the malaria parasite. Also, in one case in 2004, a homeopath instructed one of her patients to stop taking conventional medication for a heart condition, advising her on 22 June 2004 to \"Stop ALL medications including homeopathic\", advising her on or around 20 August that she no longer needed to take her heart medication, and adding on 23 August, \"She just cannot take ANY drugs – I have suggested some homeopathic remedies ... I feel confident that if she follows the advice she will regain her health.\" The patient was admitted to hospital the next day, and died eight days later, the final diagnosis being \"acute heart failure due to treatment discontinuation\".\n\nIn 1978, Anthony Campbell, then a consultant physician at the Royal London Homeopathic Hospital, criticized statements by George Vithoulkas claiming that syphilis, when treated with antibiotics, would develop into secondary and tertiary syphilis with involvement of the central nervous system, saying that \"The unfortunate layman might well be misled by Vithoulkas' rhetoric into refusing orthodox treatment\".\nVithoulkas' claims echo the idea that treating a disease with external medication used to treat the symptoms would only drive it deeper into the body and conflict with scientific studies, which indicate that penicillin treatment produces a complete cure of syphilis in more than 90% of cases.\n\nA 2006 review by W. Steven Pray of the College of Pharmacy at Southwestern Oklahoma State University recommends that pharmacy colleges include a required course in unproven medications and therapies, that ethical dilemmas inherent in recommending products lacking proven safety and efficacy data be discussed, and that students should be taught where unproven systems such as homeopathy depart from evidence-based medicine.\n\nIn an article entitled \"Should We Maintain an Open Mind about Homeopathy?\" published in the \"American Journal of Medicine\", Michael Baum and Edzard Ernstwriting to other physicianswrote that \"Homeopathy is among the worst examples of faith-based medicine... These axioms [of homeopathy] are not only out of line with scientific facts but also directly opposed to them. If homeopathy is correct, much of physics, chemistry, and pharmacology must be incorrect...\".\n\nIn 2013, Mark Walport, the UK Government Chief Scientific Adviser and head of the Government Office for Science, had this to say: \"My view scientifically is absolutely clear: homoeopathy is nonsense, it is non-science. My advice to ministers is clear: that there is no science in homoeopathy. The most it can have is a placebo effect – it is then a political decision whether they spend money on it or not.\" His predecessor, John Beddington, referring to his views on homeopathy being \"fundamentally ignored\" by the Government, said: \"The only one [view being ignored] I could think of was homoeopathy, which is mad. It has no underpinning of scientific basis. In fact, all the science points to the fact that it is not at all sensible. The clear evidence is saying this is wrong, but homoeopathy is still used on the NHS.\"\n\nHomeopathy is fairly common in some countries while being uncommon in others; is highly regulated in some countries and mostly unregulated in others. It is practised worldwide and professional qualifications and licences are needed in most countries. In some countries, there are no specific legal regulations concerning the use of homeopathy, while in others, licences or degrees in conventional medicine from accredited universities are required. In Germany, to become a homeopathic physician, one must attend a three-year training programme, while France, Austria and Denmark mandate licences to diagnose any illness or dispense of any product whose purpose is to treat any illness.\n\nSome homeopathic treatment is covered by the public health service of several European countries, including France, the United Kingdom and Luxembourg. In other countries, such as Belgium, homeopathy is not covered. In Austria, the public health service requires scientific proof of effectiveness in order to reimburse medical treatments and homeopathy is listed as not reimbursable, but exceptions can be made; private health insurance policies sometimes include homeopathic treatment. The Swiss government, after a 5-year trial, withdrew coverage of homeopathy and four other complementary treatments in 2005, stating that they did not meet efficacy and cost-effectiveness criteria, but following a referendum in 2009 the five therapies have been reinstated for a further 6-year trial period from 2012.\nThe Indian government recognizes homeopathy as one of its national systems of medicine; it has established AYUSH or the Department of Ayurveda, Yoga and Naturopathy, Unani, Siddha and Homoeopathy under the Ministry of Health & Family Welfare. The south Indian state of Kerala also gives the final nod for AYUSH department where homeopathy and Ayurveda are the main streams along with Sidha, Unani and Yoga. The Central Council of Homoeopathy was established in 1973 to monitor higher education in homeopathy, and National Institute of Homoeopathy in 1975. A minimum of a recognized diploma in homeopathy and registration on a state register or the Central Register of Homoeopathy is required to practise homeopathy in India.\n\nOn 28 September 2016 the UK's Committee of Advertising Practice (CAP) Compliance team wrote to homeopaths in the UK to \"remind them of the rules that govern what they can and can’t say in their marketing materials.\" The letter highlights that \"homeopaths may not currently make either direct or implied claims to treat medical conditions\" and asks them to review their marketing communications \"including websites and social media pages\" to ensure compliance by 3 November 2016. The letter also includes information on sanctions in the event of non-compliance including, ultimately, \"referral by the ASA to Trading Standards under the Consumer Protection from Unfair Trading Regulations 2008.\"\n\nIn the April 1997 edition of FDA Consumer, William T. Jarvis, the President of the National Council Against Health Fraud, said \"Homeopathy is a fraud perpetrated on the public with the government's blessing, thanks to the abuse of political power of Sen. Royal S. Copeland [chief sponsor of the 1938 Food, Drug, and Cosmetic Act].\"\n\nMock \"overdosing\" on homeopathic preparations by individuals or groups in \"mass suicides\" have become more popular since James Randi began taking entire bottles of homeopathic sleeping pills before giving lectures. In 2010 The Merseyside Skeptics Society from the United Kingdom launched the , encouraging groups to publicly overdose as groups. In 2011 the 10:23 campaign expanded and saw sixty-nine groups participate; fifty-four submitted videos. In April 2012, at the Berkeley SkeptiCal conference, over 100 people participated in a mass overdose, taking \"coffea cruda\", which is supposed to treat sleeplessness.\n\nIn 2011, the non-profit, educational organizations Center for Inquiry (CFI) and the associated Committee for Skeptical Inquiry (CSI) have petitioned the U.S. Food and Drug Administration (FDA) to initiate 'rulemaking that would require all over-the-counter homeopathic drugs to meet the same standards of effectiveness as non-homeopathic drugs' and 'to place warning labels on homeopathic drugs until such time as they are shown to be effective'. In a separate petition, CFI and CSI request FDA to issue warning letters to Boiron, maker of Oscillococcinum, regarding their marketing tactic and criticize Boiron for misleading labelling and advertising of Oscillococcinum. In 2015, CFI filed comments urging the Federal Trade Commission to end the false advertising practice of homeopathy. On 15 November 2016, FTC declared that homeopathic products cannot include claims of effectiveness without 'competent and reliable scientific evidence.' If no such evidence exists, they must state this fact clearly on their labeling, and state that the product's claims are based only on 18th-century theories that have been discarded by modern science. Failure to do so will be considered a violation of the FTC Act.\nCFI in Canada is calling for persons that feel they were harmed by homeopathic products to contact them.\n\nIn August 2011, a class action lawsuit was filed against Boiron on behalf of \"all California residents who purchased Oscillo at any time within the past four years\". The lawsuit charged that it \"is nothing more than a sugar pill\", \"despite falsely advertising that it contains an active ingredient known to treat flu symptoms\". In March 2012, Boiron agreed to spend up to $12 million to settle the claims of falsely advertising the benefits of its homeopathic preparations.\n\nIn July 2012, CBC News reporter Erica Johnson for \"Marketplace\" conducted an investigation on the homeopathy industry in Canada; her findings were that it is \"based on flawed science and some loopy thinking\". Center for Inquiry (CFI) Vancouver skeptics participated in a mass overdose outside an emergency room in Vancouver, B.C., taking entire bottles of \"medications\" that should have made them sleepy, nauseous or dead; after 45 minutes of observation no ill effects were felt. Johnson asked homeopaths and company representatives about cures for cancer and vaccine claims. All reported positive results but none could offer any science backing up their statements, only that \"it works\". Johnson was unable to find any evidence that homeopathic preparations contain any active ingredient. Analysis performed at the University of Toronto's chemistry department found that the active ingredient is so small \"it is equivalent to 5 billion times less than the amount of aspirin ... in a single pellet\". Belladonna and ipecac \"would be indistinguishable from each other in a blind test\".\n\nHomeopathic services offered at Bristol Homeopathic Hospital in the UK ceased in October 2015, partly in response to increased public awareness as a result of the and a campaign lead by the Good Thinking Society, University Hospitals Bristol confirmed that it would cease to offer homeopathic therapies from October 2015, at which point homeopathic therapies would no longer be included in the contract. Homeopathic services in the Bristol area were relocated to \"a new independent social enterprise\" at which Bristol Clinical Commissioning Group revealed \"there are currently no (NHS) contracts for homeopathy in place.\" Following a threat of legal action by the Good Thinking Society campaign group, the British government has stated that the Department of Health will hold a consultation in 2016 regarding whether homeopathic treatments should be added to the NHS treatments blacklist (officially, Schedule 1 of the National Health Service (General Medical Services Contracts) (Prescription of Drugs etc.) Regulations 2004), that specifies a blacklist of medicines not to be prescribed under the NHS.\n\nIn March 2016, the University of Barcelona cancelled its Master's degree in Homeopathy citing \"lack of scientific basis\", after advice from the Spanish Ministry of Health stated that \"Homeopathy has not definitely proved its efficacy under any indication or concrete clinical situation\". Shortly afterwards, in April 2016, the University of Valencia announced the elimination of its Masters in Homeopathy for 2017.\n\nIn June 2016, blogger and sceptic Jithin Mohandas launched a petition through Change.org asking the government of Kerala, India, to stop admitting students to homeopathy medical colleges. Mohandas said that government approval of these colleges makes them appear legitimate, leading thousands of talented students to join them and end up with invalid degrees. The petition asks that homeopathy colleges be converted to regular medical colleges and that people with homeopathy degrees be provided with training in scientific medicine.\n\nOn April 20–21, 2015, the FDA held a hearing on homeopathic product regulation. Invitees representing the scientific and medical community, and various pro-homeopathy stakeholders, gave testimonials on homeopathic products and the regulatory role played by the FDA.\nMichael de Dora, a representative from the Center for Inquiry (CFI), on behalf of the organization and dozens of doctors and scientists associated with CFI and the Committee for Skeptical Inquiry (CSI) gave a testimonial which summarized the basis of the organization's objection to homeopathic products, the harm that is done to the general public and proposed regulatory actions:\n\nThe CFI testimonial stated that the principle of homeopathy is at complete odds with the basic principles of modern biology, chemistry and physics and that decades of scientific examination of homeopathic products shows that there is no evidence that it is effective in treating illnesses other than acting as a placebo. Further, it noted a 2012 report by the American Association of Poison Control Centers which listed 10,311 reported cases of poison exposure related to homeopathic agents, among which 8,788 cases were attributed to young children five years of age or younger, as well as examples of harm – including deaths – caused to patients who relied on homeopathics instead of proven medical treatment.\n\nThe CFI urged the FDA to announce and implement strict guidelines that \"require all homeopathic products meet the same standards as non-homeopathic drugs\", arguing that the consumers can only have true freedom of choice (an often used argument from the homeopathy proponents) if they are fully informed of the choices. CFI proposed that the FDA take these three steps:\n\nIn March 2015, the National Health and Medical Research Council of Australia issued the following conclusions and recommendations:\n\n\nIn November 2016, The United States FTC issued an \"Enforcement Policy Statement Regarding Marketing Claims for Over-the-Counter Homeopathic Drugs\" which specified that the FTC will hold efficacy and safety claims for OTC homeopathic drugs to the same standard as other products making similar claims. A November 15, 2016 FTC press release summarized the policy as follows:\n\nIn conjunction with the 2016 FTC Enforcement Policy Statement, the FTC also released its \"Homeopathic Medicine & Advertising Workshop Report\", which summarizes the panel presentations and related public comments in addition to describing consumer research commissioned by the FTC. The report concluded:\n\n\n", "id": "14229", "title": "Homeopathy"}
{"url": "https://en.wikipedia.org/wiki?curid=14231", "text": "Hairpin\n\nA hair pin or hairpin is a long device used to hold a person's hair in place. It may be used simply to secure long hair out of the way for convenience or as part of an elaborate hairstyle or coiffure. The earliest evidence for dressing the hair may be seen in carved \"venus figurines\" such as the Venus of Brassempouy and the Venus of Willendorf. The creation of different hairstyles, especially among women, seems to be common to all cultures and all periods and many past, and current, societies use hairpins.\n\nHairpins made of metal, ivory, bronze, carved wood, etc. were used in ancient Assyria and Egypt for securing decorated hairstyles. Such hairpins suggest, as graves show, that many were luxury objects among the Egyptians and later the Greeks, Etruscans, and Romans. Major success came in 1901 with the invention of the spiral hairpin by New Zealand inventor Ernest Godward. This was a predecessor of the hair clip.\n\nThe hairpin may be decorative and encrusted with jewels and ornaments, or it may be utiliarian, and designed to be almost invisible while holding a hairstyle in place.\n\nSome hairpins are a single straight pin, but modern versions are more likely to be constructed from different lengths of wire that are bent in half with a u-shaped end and a few kinks along the two opposite portions. The finished pin may vary from two to six inches in final length. The length of the wires enables placement in several styles of hairdos to hold the style in place. The kinks enable retaining the pin during normal movements.\n\nA hairpin patent was issued to Kelly Chamandy in 1925.\n\nHairpins (generally known as ; ) are an important symbol in Chinese culture. In ancient China, hairpins were worn by all genders, and they were essential items for everyday hairstyling, mainly for securing and decorating a hair bun. Furthermore, hairpins worn by women could also represent their social status.\n\nIn Han Chinese culture, when young girls reached the age of fifteen, they were allowed to take part in a rite of passage known as \"\" (), or “hairpin initiation” . This ceremony marks the coming of age of young women. Particularly, before the age of fifteen, girls did not use hairpins as they wore their hair in braids, and they were considered as children. When they turned fifteen, they could be considered as young women after the ceremony, and they started to style their hair as buns secured and embellished by hairpins. This practice indicated these young women may now enter into marriage. However, if a young woman hadn't been consented to marriage before age twenty, or she hadn't yet participated in a coming of age ceremony, she must attend a ceremony when she turned twenty.\n\nIn comparison with “”, the male equivalent known as “guan li” () or “hat initiation”, usually took place five years later, at the age of twenty. In the 21st century Hanfu Movement, an attempt to revive the traditional Han Chinese coming-of-age ceremonies has been made, and the ideal age to attend the ceremony is twenty years old for all genders.\n\nWhile hairpins can symbolize the transition from childhood to adulthood, they were closely connected to the concept of marriage as well. At the time of an engagement, the fiancée may take a hairpin from her hair and give it to her fiancé as a pledge: this can be seen as a reversal of the Western tradition, such as the future groom presents an engagement ring to his betrothed. After the wedding ceremony, the husband should put the hairpin back into his spouse’s hair.\n\nHair has always carried many psychological, philosophical, romantic, and cultural meanings in Chinese culture. In Han ethnicity, people call the union between two people “” (), literally means “tying hair”. During the wedding ceremony, some Chinese couples exchange a lock of hair as a pledge, while others break a hairpin into two parts, and then, each of the betrothed take one part with them for keeping. If this couple ever get separated in the future, when they reunite, they can piece the two halves together, and this completed hairpin will serve as a proof of their identities as well as a symbol of their reunion. In addition, a married heterosexual couple is sometimes referred to as “” (), an idiom which implies the relationship between the pair is very intimate and happy, just like how their hair has been tied together.\n\n", "id": "14231", "title": "Hairpin"}
{"url": "https://en.wikipedia.org/wiki?curid=14233", "text": "Hate speech\n\nHate speech is speech which attacks a person or group on the basis of attributes such as gender, ethnic origin, religion, race, disability, or sexual orientation. In the law of some countries, hate speech is described as speech, gesture or conduct, writing, or display which is forbidden because it incites violence or prejudicial action against or by a protected group, or individual on the basis of their membership to the group, or because it disparages or intimidates a protected group, or individual on the basis of their membership to the group. The law may identify a protected group by certain characteristics. In the law of other countries, hate speech is not a legal term.\nIn some countries, a victim of hate speech may seek redress under civil law, criminal law, or both. A website which uses hate speech may be called a \"hate site\". Most of these sites contain Internet forums and news briefs that emphasize a particular viewpoint.\n\nThere has been debate over freedom of speech, hate speech and hate speech legislation. Critics have argued that the term \"hate speech\" is used to silence critics of social policies that have been poorly implemented.\n\nCommunication theory provides some insight into the harms caused by hate speech. According to the ritual model of communication, racist expressions allow minorities to be categorized with negative attributes tied to them, and are directly harmful to them. Matsuda \"et al\". (1993) found that racist speech could cause in the recipient of the message direct physical and emotional changes. The repeated use of such expressions cause and reinforce the subordination of these minorities. The idea that hate speech is a mechanism of subordination is supported by scholarly evidence.\n\nOn May 31, 2016, Facebook, Google, Microsoft, and Twitter, jointly agreed to a European Union code of conduct obligating them to review \"[the] majority of valid notifications for removal of illegal hate speech\" posted on their services within 24 hours.\n\nFollowing a campaign which involved the participation of Women, Action and the Media, the Everyday Sexism Project and the activist Soraya Chemaly, who were among 100 advocacy groups, Facebook agreed to update its policy on hate speech. The campaign highlighted content that promoted domestic and sexual violence against women, and used over 57,000 tweets and more than 4,900 emails to create outcomes such as the withdrawal of advertising from Facebook by 15 companies, including Nissan UK, House of Burlesque, and Nationwide UK. The social media website initially responded by stating that \"While it may be vulgar and offensive, distasteful content on its own does not violate our policies\", but then agreed to take action on May 29, 2013, after it had \"become clear that our systems to identify and remove hate speech have failed to work as effectively as we would like, particularly around issues of gender-based hate.\"\n\nThe International Covenant on Civil and Political Rights (ICCPR) states that \"any advocacy of national, racial or religious hatred that constitutes incitement to discrimination, hostility or violence shall be prohibited by law\". The Convention on the Elimination of All Forms of Racial Discrimination (ICERD) prohibits all incitement of racism. On 3 May 2011, Michael O'Flaherty with the United Nations Human Rights Committee published Draft General Comment No. 34 on the ICCPR, which among other comments expressed concern that many forms of \"hate speech\" do not meet the level of seriousness set out in Article 20. This paragraph does not appear in the final document. Concerning the debate over how freedom of speech applies to the Internet, conferences concerning such sites have been sponsored by the United Nations High Commissioner for Refugees.\n\nHate law regulations can be divided into two types: those which are designed for public order and those which are designed to protect human dignity. Those designed to protect public order seem to be somewhat ineffective because they are rarely enforced. For example, in Northern Ireland, as of 1992 only one person was prosecuted for violating the regulation in twenty one years. Those meant to protect human dignity, however, like those in Canada, Denmark, France, Germany and the Netherlands seem to be frequently enforced.\n\nAustralia's hate speech laws vary by jurisdiction, and seek especially to prevent victimisation on account of race.\n\nThe Belgian Anti-Racism Law, in full, the \"Law of 30 July 1981 on the Punishment of Certain Acts inspired by Racism or Xenophobia\", is a law against hate speech and discrimination passed by the Federal Parliament of Belgium in 1981 which made certain acts motivated by racism or xenophobia illegal. It is also known as the \"Moureaux Law\".\n\nThe Belgian Holocaust denial law, passed on 23 March 1995, bans public Holocaust denial. Specifically, the law makes it illegal to publicly \"deny, play down, justify or approve of the genocide committed by the Nazi Germany regime during the Second World War\". Prosecution is led by the Belgian Centre for Equal Opportunities. The offense is punishable by imprisonment of up to one year and fines of up to 2500 EUR.\n\nIn Brazil, according to the 1988 Brazilian Constitution, racism and other forms of race-related hate speech are \"Offense(s) with no statute of limitations and no right to bail for the defendant\".\n\nIn Canada, advocating genocide against any \"identifiable group\" is an indictable offence under the \"Criminal Code\" and carries a maximum sentence of five years imprisonment. There is no minimum sentence.\n\nPublicly inciting hatred against any identifiable group is also an offence. It can be prosecuted either as an indictable offence with a maximum sentence of two years imprisonment, or as a summary conviction offence with a maximum sentence of six months imprisonment. There are no minimum sentences in either case. The offence of publicly inciting hatred makes exceptions for cases of statements of truth, and subjects of public debate and religious doctrine. The landmark judicial decision on the constitutionality of this law was \"R. v. Keegstra\" (1990).\n\nAn \"identifiable group\" is defined for both offences as \"any section of the public distinguished by colour, race, religion, national or ethnic origin, age, sex, sexual orientation, or mental or physical disability\".\n\nArticle 31 of the \"Ley sobre Libertades de Opinión e Información y Ejercicio del Periodismo\" (statute on freedom of opinion and information and the performance of journalism), punishes with a large fine those who “through any means of social communication makes publications or transmissions intended to promote hatred or hostility towards persons or a group of persons due to their race, sex, religion or nationality\". This law has been applied to expressions transmitted via the internet. There is also a rule increasing the penalties for crimes motivated by discriminatory hatred.\n\nThe Council of Europe has worked intensively on this issue. While Article 10 of the European Convention on Human Rights does not prohibit criminal laws against revisionism such as denial or minimization of genocides or crimes against humanity, as interpreted by the European Court of Human Rights (ECtHR), the Committee of Ministers of the Council of Europe went further and recommended to member governments to combat hate speech under its Recommendation R (97) 20. The ECtHR does not offer an accepted definition for \"hate speech\" but instead offers only parameters by which prosecutors can decide if the \"hate speech\" is entitled to the protection of freedom of speech.\n\nThe Council of Europe also created the European Commission against Racism and Intolerance, which has produced country reports and several general policy recommendations, for instance against anti-Semitism and intolerance against Muslims.\n\nThe Croatian Constitution guarantees freedom of speech, but Croatian penal code prohibits and punishes anyone \"who based on differences of race, religion, language, political or any other belief, wealth, birth, education, social status or other properties, gender, skin color, nationality or ethnicity violates basic human rights and freedoms recognized from international community\".\n\nDenmark prohibits hate speech, and defines it as publicly making statements by which a group is threatened ('), insulted (') or degraded (\"\") due to race, skin colour, national or ethnic origin, faith or sexual orientation.\n\nThere has been considerable debate over the definition of \"hate speech\" (\"vihapuhe\") in the Finnish language.\n\nIf \"hate speech\" is taken to mean ethnic agitation, it is prohibited in Finland and defined in the section 11 of the penal code, \"War crimes and crimes against humanity\", as published information or as an opinion or other statement that threatens or insults a group because of race, nationality, ethnicity, religion or conviction, sexual orientation, disability, or any comparable trait. Ethnic agitation is punishable with a fine or up to 2 years in prison, or 4 months to 4 years if aggravated (such as incitement to genocide).\n\nCritics claim that, in political contexts, labeling certain opinions and statements \"hate speech\" can be used to silence unfavorable or critical opinions and suppress debate. Certain politicians, including Member of Parliament Jussi Halla-aho, consider the term \"hate speech\" problematic because of the lack of an easy definition.\n\nFrance prohibits by its penal code and by its press laws public and private communication which is defamatory or insulting, or which incites discrimination, hatred, or violence against a person or a group of persons on account of place of origin, ethnicity or lack thereof, nationality, race, specific religion, sex, sexual orientation, or handicap. The law prohibits declarations that justify or deny crimes against humanity, for example, the Holocaust (Gayssot Act).\n\nIn Germany, Volksverhetzung (\"incitement of popular hatred\") is a punishable offense under Section 130 of the Strafgesetzbuch (Germany's criminal code) and can lead to up to five years imprisonment. Section 130 makes it a crime to publicly incite hatred against parts of the population or to call for violent or arbitrary measures against them or to insult, maliciously slur or defame them in a manner violating their (constitutionally protected) human dignity. Thus for instance it is illegal to publicly call certain ethnic groups \"maggots\" or \"freeloaders\". Volksverhetzung is punishable in Germany even if committed abroad and even if committed by non-German citizens, if only the incitement of hatred takes effect within German territory, e.g., the seditious sentiment was expressed in German writing or speech and made accessible in Germany (German criminal code's Principle of Ubiquity, Section 9 §1 Alt. 3 and 4 of the Strafgesetzbuch).\n\nIn Iceland, the hate speech law is not confined to inciting hatred, as one can see from Article 233 a. in the Icelandic Penal Code, but includes simply expressing such hatred publicly:\n\nFreedom of speech and expression is protected by article 19 (1) of the constitution of India, but under article 19(2) \"reasonable restrictions\" can be imposed on freedom of speech and expression in the interest of \"the sovereignty and integrity of India, the security of the State, friendly relations with foreign States, public order, decency or morality, or in relation to contempt of court, defamation or incitement to an offence\".\n\nIndonesia has been a signatory to the International Covenant on Civil and Political Rights since 2006, but has not promulgated comprehensive legislation against hate-speech crimes. Calls for a comprehensive anti-hate speech law and associated educational program have followed statements by a leader of a hard-line Islamic organization that Balinese Hindus were mustering forces to protect the \"lascivious Miss World pageant\" in “a war against Islam\" and that \"those who fight on the path of Allah are promised heaven\". The statements are said to be an example of similar messages intolerance being preached throughout the country by radical clerics.\n\nThe National Police ordered all of their personnel to anticipate any potential conflicts in society caused by hate speech. The order is stipulated in the circular signed by the National Police chief General Badrodin Haiti on Oct. 8, 2015.\n\nIn Ireland, the right to free speech is guaranteed under the Constitution (Article 40.6.1.i), however, this is only an implied right provided that liberty of expression \"shall not be used to undermine public order or morality or the authority of the State\". The Prohibition of Incitement to Hatred Act 1989, proscribes words or behaviours which are \"threatening, abusive or insulting and are intended or, having regard to all the circumstances, are likely to stir up hatred\" against \"a group of persons in the State or elsewhere on account of their race, colour, nationality, religion, ethnic or national origins, membership of the travelling community or sexual orientation\".\n\nJapanese law covers threats and slander, but it \"does not apply to hate speech against general groups of people\". Japan became a member of the United Nations International Convention on the Elimination of All Forms of Racial Discrimination in 1995. Article 4 of the convention sets forth provisions calling for the criminalization of hate speech. But the Japanese government has suspended the provisions, saying actions to spread or promote the idea of racial discrimination have not been taken in Japan to such an extent that legal action is necessary. The Foreign Ministry says that this assessment remains unchanged.\n\nIn May 2013, the United Nations Committee on Economic, Social and Cultural Rights (CESCR) warned the Japanese government that it needs to take measures to curb hate speech against so-called \"comfort women\", or Asian women forced into sexual slavery by the Japanese military during World War II. The committee's recommendation called for the Japanese government to better educate Japanese society on the plight of women who were forced into sexual slavery to prevent stigmatization, and to take necessary measures to repair the lasting effects of exploitation, including addressing their right to compensation.\n\nIn 2013, following demonstrations, parades, and comments posted on the Internet threatening violence against foreign residents of Japan, especially Koreans, there are concerns that hate speech is a growing problem in Japan. Prime Minister Shinzo Abe and Justice Minister Sadakazu Tanigaki have expressed concerns about the increase in hate speech, saying that it \"goes completely against the nation's dignity\", but so far have stopped short of proposing any legal action against protesters.\n\nOn 22 September 2013 around 2,000 people participated in the \"March on Tokyo for Freedom\" campaigning against recent hate speech marches. Participants called on the Japanese government to \"sincerely adhere\" to the International Convention on the Elimination of All Forms of Racial Discrimination. Sexual minorities and the disabled also participated in the march.\n\nOn 25 September 2013 a new organization, \"An international network overcoming hate speech and racism\" (Norikoenet), that is opposed to hate speech against ethnic Koreans and other minorities in Japan was launched.\n\nOn 7 October 2013, in a rare ruling on racial discrimination against ethnic Koreans, a Japanese court ordered an anti-Korean group, Zaitokukai, to stop \"hate speech\" protests against a Korean school in Kyoto and pay the school 12.26 million yen ($126,400 U.S.) in compensation for protests that took place in 2009 and 2010.\n\nA United Nations panel urged Japan to ban hate speech.\n\nIn May 2016 Japan passed a law dealing with hate speech. However, it does not ban hate speech and sets no penalty for committing it.\n\nSeveral Jordanian laws seek to prevent the publication or dissemination of material that would provoke strife or hatred: \n\nThe Dutch penal code prohibits both insulting a group (article 137c) and inciting hatred, discrimination or violence (article 137d). The definition of the offences as outlined in the penal code is as follows:\n\nIn January 2009, a court in Amsterdam ordered the prosecution of Geert Wilders, a Dutch Member of Parliament, for breaching articles 137c and 137d. On 23 June 2011, Wilders was acquitted of all charges. In 2016, in a separate case, Wilders was found guilty of both insulting a group and inciting discrimination for promising an audience that he would deliver on their demand for there to be \"fewer Moroccans\".\n\nNew Zealand prohibits hate speech under the Human Rights Act 1993. Section 61 (Racial Disharmony) makes it unlawful to publish or distribute \"threatening, abusive, or insulting...matter or words likely to excite hostility against or bring into contempt any group of persons...on the ground of the colour, race, or ethnic or national or ethnic origins of that group of persons\". Section 131 (Inciting Racial Disharmony) lists offences for which \"racial disharmony\" creates liability.\n\nNorway prohibits hate speech, and defines it as publicly making statements that threaten or ridicule someone or that incite hatred, persecution or contempt for someone due to their skin colour, ethnic origin, homosexual orientation, religion or philosophy of life. At the same time, the Norwegian Constitution guarantees the right to free speech, and there has been an ongoing public and judicial debate over where the right balance between the ban against hate speech and the right to free speech lies. Norwegian courts have been restrictive in the use of the hate speech law and only few persons have been sentenced for violating the law since its implementation in 1970. A public Free Speech committee (1996-1999) recommended to abolish the hate speech law but the Norwegian Parliament instead voted to slightly strengthen it.\n\nThe hate speech laws in Poland punish those who offend the feelings of the religious by e.g. disturbing a religious ceremony or creating public calumny. They also prohibit public expression that insults a person or a group on account of national, ethnic, racial, or religious affiliation or the lack of a religious affiliation.\n\nAccording to Article 282 of the Criminal Code, 'Raising hates or hostility, or equally humiliation of human dignity':\n\nActions aimed at the incitement of hatred or enmity, as well as the humiliation of a person or group of persons on grounds of sex, race, nationality, language, origin, attitude to religion, as well as affiliation to any social group, committed publicly or with the use of media or information and telecommunication networks, including the network \"Internet\" shall be punished by a fine of 300 000 to 500 000 rubles or the salary or other income for a period of 2 to 3 years, or community service for a period of 1 year to four years, with disqualification to hold certain positions or engage in certain activities up to 3 years, or imprisonment for a term of 2 to 5 years.\n\nThe Serbian constitution guarantees freedom of speech, but restricts it in certain cases to protect the rights of others. The criminal charge of \"Provoking ethnic, racial and religion based animosity and intolerance\" carries a minimum six months prison term and a maximum of ten years.\n\nSingapore has passed numerous laws that prohibit speech that causes disharmony among various religious groups. The Maintenance of Religious Harmony Act is an example of such legislation. The Penal Code criminalizes the deliberate promotion by someone of enmity, hatred or ill-will between different racial and religious groups on grounds of race or religion. It also makes it an offence for anyone to deliberately wound the religious or racial feelings of any person.\n\nIn South Africa, hate speech (along with incitement to violence and propaganda for war) is specifically excluded from protection of free speech in the Constitution. The Promotion of Equality and Prevention of Unfair Discrimination Act, 2000 contains the following clause:\n\nThe \"prohibited grounds\" include race, gender, sex, pregnancy, marital status, ethnic or social origin, colour, sexual orientation, age, disability, religion, conscience, belief, culture, language and birth.\n\nThe crime of \"crimen injuria\" (\"unlawfully, intentionally and seriously impairing the dignity of another\") may also be used to prosecute hate speech.\n\nIn 2011, a South African court banned \"Dubula iBhunu (Shoot the Boer)\", a derogatory song degrading Afrikaners, on the basis that it violated a South African law prohibiting speech that demonstrates a clear intention to be hurtful, to incite harm, or to promote hatred.\n\nSweden prohibits hate speech, and defines it as publicly making statements that threaten or express disrespect for an ethnic group or similar group regarding their race, skin colour, national or ethnic origin, faith, or sexual orientation. The crime does not prohibit a pertinent and responsible debate (\"en saklig och vederhäftig diskussion\"), nor statements made in a completely private sphere. There are constitutional restrictions pertaining to which acts are criminalized, as well limits set by the European Convention on Human Rights. The crime is called \"Hets mot folkgrupp\" in Swedish which directly translated can be translated to \"Incitement (of hatred/violence) towards population groups.\"\n\nThe sexual orientation provision, added in 2002, was used to convict Pentecostalist pastor Åke Green of hate speech based on a 2003 sermon. His conviction was later overturned.\n\nIn Switzerland public discrimination or invoking to rancor against persons or a group of people because of their race, ethnicity, is getting penalized with a term of imprisonment until 3 years or a mulct. In 1934, the authorities of the Basel-Stadt canton criminalized anti-Jewish hate speech, e.g., the accusation of ritual murders, mostly in reaction against a pro-Nazi antisemitic group and newspaper, the .\n\nIn the United Kingdom, several statutes criminalize hate speech against several categories of persons. The statutes forbid communication which is hateful, threatening, or abusive, and which targets a person on account of disability, ethnic or national origin, nationality (including citizenship), race, religion, sexual orientation, or skin colour. The penalties for hate speech include fines, imprisonment, or both. Legislation against Sectarian hate in Scotland, which is aimed principally at football matches, does not criminalise jokes about people's beliefs, nor outlaw “harsh” comment about their religious faith.\n\nThe 1789 Constitution of the United States of America dealt only with the three heads of power—legislative, executive, and judicial—and sketched the basic outlines of federalism in the last four articles. The protection of civil rights was not written into the original Constitution but was added two years later with the Bill of Rights, implemented as several amendments to the Constitution. The First Amendment, ratified December 15, 1791, states:\nAlthough this section was considered only to apply to the federal congress (i.e. the legislative branch), the 14th Amendment, ratified on July 9, 1868, clarifies that this prohibition applies to laws of the states as well.\n\nSome state constitutions also have a \"free speech\" provision, most notably, that of California.\n\nSome limits on expression were contemplated by the framers and have been read into the Constitution by the Supreme Court (SCOTUS). In 1942, Justice Frank Murphy summarized the case law: \"There are certain well-defined and limited classes of speech, the prevention and punishment of which have never been thought to raise a Constitutional problem. These include the lewd and obscene, the profane, the libelous and the insulting or 'fighting' words – those which by their very utterances inflict injury or tend to incite an immediate breach of the peace.\"\n\nTraditionally, however, if the speech did not fall within one of the above categorical exceptions, it was protected speech. In 1969, the Supreme Court protected a Ku Klux Klan member’s speech and created the \"imminent danger\" test to determine on what grounds speech can be limited. The court ruled in \"Brandenburg v. Ohio\" that; \"The constitutional guarantees of free speech and free press do not permit a state to forbid or proscribe advocacy of the use of force, or of law violation except where such advocacy is directed to inciting imminent lawless action and is likely to incite or produce such action.\"\n\nThis test has been modified very little from its inception in 1969 and the formulation is still good law in the United States. Only speech that poses an imminent danger of unlawful action, where the speaker has the intention to incite such action and there is the likelihood that this will be the consequence of his or her speech, may be restricted and punished by that law.\n\nIn \"R.A.V. v. City of St. Paul\", (1992), the issue of banning hate speech arose again when a gang of white people burned a cross in the front yard of a black family. The local ordinance in St. Paul, Minnesota, criminalized such expressions considered racist and the teenager was charged thereunder. Associate Justice Antonin Scalia, writing for the Supreme Court, held that the prohibition against hate speech was unconstitutional as it contravened the First Amendment. The Supreme Court struck down the ordinance. Scalia explicated the fighting words exception as follows: “The reason why fighting words are categorically excluded from the protection of the First Amendment is not that their content communicates any particular idea, but that their content embodies a particularly intolerable (and socially unnecessary) mode of expressing whatever idea the speaker wishes to convey”. Because the hate speech ordinance was not concerned with the mode of expression, but with the content of expression, it was a violation of the freedom of speech. Thus, the Supreme Court embraced the idea that speech in general is permissible unless it will lead to imminent violence. The opinion noted \"This conduct, if proved, might well have violated various Minnesota laws against arson, criminal damage to property\", among a number of others, none of which was charged, including threats to any person, not to only protected classes.\n\nIn 2011, the Supreme Court issued their ruling on \"Snyder v. Phelps,\" which concerned the right of the Westboro Baptist Church to protest with signs found offensive by many Americans. The issue presented was whether the 1st Amendment protected the expressions written on the signs. In an 8–1 decision the court sided with Phelps, the head of Westboro Baptist Church, thereby confirming their historically strong protection of freedom of speech, so long as it doesn't promote imminent violence. The Court explained, \"speech deals with matters of public concern when it can 'be fairly considered as relating to any matter of political, social, or other concern to the community' or when it 'is a subject of general interest and of value and concern to the public.\"\n\nIn the 1980s and 1990s, more than 350 public universities adopted \"speech codes\" regulating discriminatory speech by faculty and students. These codes have not fared well in the courts, where they are frequently overturned as violations of the First Amendment. Debate over restriction of \"hate speech\" in public universities has resurfaced with the adoption of anti-harassment codes covering discriminatory speech.\n\nIn 1992, Congress directed the National Telecommunications and Information Administration (NTIA) to examine the role of telecommunications, including broadcast radio and television, cable television, public access television, and computer bulletin boards, in advocating or encouraging violent acts and the commission of hate crimes against designated persons and groups. The NTIA study investigated speech that fostered a climate of hatred and prejudice in which hate crimes may occur. The study failed to link telecommunication to hate crimes, but did find that \"individuals have used telecommunications to disseminate messages of hate and bigotry to a wide audience.\" Its recommendation was that the best way to fight hate speech was through additional speech promoting tolerance, as opposed to government regulation.\n\n", "id": "14233", "title": "Hate speech"}
{"url": "https://en.wikipedia.org/wiki?curid=14236", "text": "Henrik Ibsen\n\nHenrik Johan Ibsen (; ; 20 March 1828 – 23 May 1906) was a major 19th-century Norwegian playwright, theatre director, and poet. He is often referred to as \"the father of realism\" and is one of the founders of Modernism in theatre. His major works include \"Brand\", \"Peer Gynt\", \"An Enemy of the People\", \"Emperor and Galilean\", \"A Doll's House\", \"Hedda Gabler\", \"Ghosts\", \"The Wild Duck\", \"When We Dead Awaken\", \"Pillars of Society\", \"The Lady from the Sea\", \"Rosmersholm\", \"The Master Builder\" and \"John Gabriel Borkman\". He is the most frequently performed dramatist in the world after Shakespeare, and \"A Doll's House\" became the world's most performed play by the early 20th century.\n\nSeveral of his later dramas were considered scandalous to many of his era, when European theatre was expected to model strict morals of family life and propriety. Ibsen's later work examined the realities that lay behind many façades, revealing much that was disquieting to many contemporaries. It utilized a critical eye and free inquiry into the conditions of life and issues of morality. The poetic and cinematic early play \"Peer Gynt\", however, has strong surreal elements.\n\nIbsen is often ranked as one of the most distinguished playwrights in the European tradition. Richard Hornby describes him as \"a profound poetic dramatist—the best since Shakespeare\". He is widely regarded as the most important playwright since Shakespeare. He influenced other playwrights and novelists such as George Bernard Shaw, Oscar Wilde, Arthur Miller, James Joyce, Eugene O'Neill and Miroslav Krleža. Ibsen was nominated for the Nobel Prize in Literature in 1902, 1903 and 1904.\n\nIbsen wrote his plays in Danish (the common written language of Denmark and Norway) and they were published by the Danish publisher Gyldendal. Although most of his plays are set in Norway—often in places reminiscent of Skien, the port town where he grew up—Ibsen lived for 27 years in Italy and Germany, and rarely visited Norway during his most productive years. Born into a merchant family connected to the patriciate of Skien, Ibsen shaped his dramas according to his family background. He was the father of Prime Minister Sigurd Ibsen. Ibsen's dramas continue in their influence upon contemporary culture and film with notable film productions including \"A Doll's House\" featuring Jane Fonda and \"A Master Builder\" featuring Wallace Shawn.\n\nIbsen was born to Knud Ibsen (1797–1877) and Marichen Altenburg (1799–1869), a well-to-do merchant family, in the small port town of Skien in Telemark county, a city which was noted for shipping timber. As he wrote in an 1882 letter to critic and scholar Georg Brandes, \"my parents were members on both sides of the most respected families in Skien\", explaining that he was closely related with \"just about all the patrician families who then dominated the place and its surroundings\", mentioning the families Paus, Plesner, von der Lippe, Cappelen and Blom. Ibsen's grandfather, ship captain Henrich Ibsen (1765–1797), had died at sea in 1797, and Knud Ibsen was raised on the estate of ship-owner Ole Paus (1766–1855), after his mother Johanne, Plesner (1770–1847), remarried. Knud Ibsen's half-brothers included lawyer and politician Christian Cornelius Paus, banker and ship-owner Christopher Blom Paus, and lawyer Henrik Johan Paus, who grew up with Ibsen's mother in the Altenburg home and after whom Henrik (Johan) Ibsen was named.\nKnud Ibsen's paternal ancestors were ship captains of Danish origin, but he decided to become a merchant, having initial success. His marriage to Marichen Altenburg, a daughter of ship-owner Johan Andreas Altenburg (1763–1824) and Hedevig Christine Paus (1763–1848), was a successful match. Theodore Jorgenson points out that \"Henrik's ancestry [thus] reached back into the important Telemark family of Paus both on the father's and on the mother's side. Hedvig Paus must have been well known to the young dramatist, for she lived until 1848.\" Henrik Ibsen was fascinated by his parents' \"strange, almost incestuous marriage,\" and would treat the subject of incestuous relationships in several plays, notably his masterpiece \"Rosmersholm\".\n\nWhen Henrik Ibsen was around seven years old, however, his father's fortunes took a significant turn for the worse, and the family was eventually forced to sell the major Altenburg building in central Skien and move permanently to their small summer house, \"Venstøp\", outside of the city. Henrik's sister Hedvig would write about their mother: \"She was a quiet, lovable woman, the soul of the house, everything to her husband and children. She sacrificed herself time and time again. There was no bitterness or reproach in her.\" The Ibsen family eventually moved to a city house, Snipetorp, owned by Knud Ibsen's half-brother, wealthy banker and ship-owner Christopher Blom Paus.\n\nHis father's financial ruin would have a strong influence on Ibsen's later work; the characters in his plays often mirror his parents, and his themes often deal with issues of financial difficulty as well as moral conflicts stemming from dark secrets hidden from society. Ibsen would both model and name characters in his plays after his own family. A central theme in Ibsen's plays is the portrayal of suffering women, echoing his mother Marichen Altenburg; Ibsen's sympathy with women would eventually find significant expression with their portrayal in dramas such as \"A Doll's House\" and \"Rosmersholm\".\n\nAt fifteen, Ibsen was forced to leave school. He moved to the small town of Grimstad to become an apprentice pharmacist and began writing plays. In 1846, when Ibsen was age 18, a liaison with a servant produced an illegitimate child, whose upbringing Ibsen had to pay for until the boy was in his teens, though Ibsen never saw the boy. Ibsen went to Christiania (later renamed Kristiania and then Oslo) intending to matriculate at the university. He soon rejected the idea (his earlier attempts at entering university were blocked as he did not pass all his entrance exams), preferring to commit himself to writing. His first play, the tragedy \"Catilina\" (1850), was published under the pseudonym \"Brynjolf Bjarme\", when he was only 22, but it was not performed. His first play to be staged, \"The Burial Mound\" (1850), received little attention. Still, Ibsen was determined to be a playwright, although the numerous plays he wrote in the following years remained unsuccessful. Ibsen's main inspiration in the early period, right up to \"Peer Gynt\", was apparently Norwegian author Henrik Wergeland and the Norwegian folk tales as collected by Peter Christen Asbjørnsen and Jørgen Moe. In Ibsen's youth, Wergeland was the most acclaimed, and by far the most read, Norwegian poet and playwright.\n\nHe spent the next several years employed at Det norske Theater (Bergen), where he was involved in the production of more than 145 plays as a writer, director, and producer. During this period, he published five new, though largely unremarkable, plays. Despite Ibsen's failure to achieve success as a playwright, he gained a great deal of practical experience at the Norwegian Theater, experience that was to prove valuable when he continued writing.\n\nIbsen returned to Christiania in 1858 to become the creative director of the Christiania Theatre. He married Suzannah Thoresen on 18 June 1858 and she gave birth to their only child Sigurd on 23 December 1859. The couple lived in very poor financial circumstances and Ibsen became very disenchanted with life in Norway. In 1864, he left Christiania and went to Sorrento in Italy in self-imposed exile. He didn't return to his native land for the next 27 years, and when he returned to it he was a noted, but controversial, playwright.\n\nHis next play, \"Brand\" (1865), brought him the critical acclaim he sought, along with a measure of financial success, as did the following play, \"Peer Gynt\" (1867), to which Edvard Grieg famously composed incidental music and songs. Although Ibsen read excerpts of the Danish philosopher Søren Kierkegaard and traces of the latter's influence are evident in \"Brand\", it was not until after \"Brand\" that Ibsen came to take Kierkegaard seriously. Initially annoyed with his friend Georg Brandes for comparing Brand to Kierkegaard, Ibsen nevertheless read \"Either/Or\" and \"Fear and Trembling\". Ibsen's next play \"Peer Gynt\" was consciously informed by Kierkegaard.\n\nWith success, Ibsen became more confident and began to introduce more and more of his own beliefs and judgements into the drama, exploring what he termed the \"drama of ideas\". His next series of plays are often considered his Golden Age, when he entered the height of his power and influence, becoming the center of dramatic controversy across Europe.\n\nIbsen moved from Italy to Dresden, Germany, in 1868, where he spent years writing the play he regarded as his main work, \"Emperor and Galilean\" (1873), dramatizing the life and times of the Roman emperor Julian the Apostate. Although Ibsen himself always looked back on this play as the cornerstone of his entire works, very few shared his opinion, and his next works would be much more acclaimed. Ibsen moved to Munich in 1875 and began work on his first contemporary realist drama \"The Pillars of Society\", first published and performed in 1877. \"A Doll's House\" followed in 1879. This play is a scathing criticism of the marital roles accepted by men and women which characterized Ibsen's society.\n\n\"Ghosts\" followed in 1881, another scathing commentary on the morality of Ibsen's society, in which a widow reveals to her pastor that she had hidden the evils of her marriage for its duration. The pastor had advised her to marry her fiancé despite his philandering, and she did so in the belief that her love would reform him. But his philandering continued right up until his death, and his vices are passed on to their son in the form of syphilis. The mention of venereal disease alone was scandalous, but to show how it could poison a respectable family was considered intolerable.\n\nIn \"An Enemy of the People\" (1882), Ibsen went even further. In earlier plays, controversial elements were important and even pivotal components of the action, but they were on the small scale of individual households. In \"An Enemy\", controversy became the primary focus, and the antagonist was the entire community. One primary message of the play is that the individual, who stands alone, is more often \"right\" than the mass of people, who are portrayed as ignorant and sheeplike. Contemporary society's belief was that the community was a noble institution that could be trusted, a notion Ibsen challenged. In \"An Enemy of the People\", Ibsen chastised not only the conservatism of society, but also the liberalism of the time. He illustrated how people on both sides of the social spectrum could be equally self-serving. \"An Enemy of the People\" was written as a response to the people who had rejected his previous work, \"Ghosts\". The plot of the play is a veiled look at the way people reacted to the plot of \"Ghosts\". The protagonist is a physician in a vacation spot whose primary draw is a public bath. The doctor discovers that the water is contaminated by the local tannery. He expects to be acclaimed for saving the town from the nightmare of infecting visitors with disease, but instead he is declared an 'enemy of the people' by the locals, who band against him and even throw stones through his windows. The play ends with his complete ostracism. It is obvious to the reader that disaster is in store for the town as well as for the doctor.\n\nAs audiences by now expected, Ibsen's next play again attacked entrenched beliefs and assumptions; but this time, his attack was not against society's mores, but against overeager reformers and their idealism. Always an iconoclast, Ibsen was equally willing to tear down the ideologies of any part of the political spectrum, including his own.\n\n\"The Wild Duck\" (1884) is by many considered Ibsen's finest work, and it is certainly the most complex. It tells the story of Gregers Werle, a young man who returns to his hometown after an extended exile and is reunited with his boyhood friend Hjalmar Ekdal. Over the course of the play, the many secrets that lie behind the Ekdals' apparently happy home are revealed to Gregers, who insists on pursuing the absolute truth, or the \"Summons of the Ideal\". Among these truths: Gregers' father impregnated his servant Gina, then married her off to Hjalmar to legitimize the child. Another man has been disgraced and imprisoned for a crime the elder Werle committed. Furthermore, while Hjalmar spends his days working on a wholly imaginary \"invention\", his wife is earning the household income.\n\nIbsen displays masterful use of irony: despite his dogmatic insistence on truth, Gregers never says what he thinks but only insinuates, and is never understood until the play reaches its climax. Gregers hammers away at Hjalmar through innuendo and coded phrases until he realizes the truth; Gina's daughter, Hedvig, is not his child. Blinded by Gregers' insistence on absolute truth, he disavows the child. Seeing the damage he has wrought, Gregers determines to repair things, and suggests to Hedvig that she sacrifice the wild duck, her wounded pet, to prove her love for Hjalmar. Hedvig, alone among the characters, recognizes that Gregers always speaks in code, and looking for the deeper meaning in the first important statement Gregers makes which does not contain one, kills herself rather than the duck in order to prove her love for him in the ultimate act of self-sacrifice. Only too late do Hjalmar and Gregers realize that the absolute truth of the \"ideal\" is sometimes too much for the human heart to bear.\n\nLate in his career, Ibsen turned to a more introspective drama that had much less to do with denunciations of society's moral values. In such later plays as \"Hedda Gabler\" (1890) and \"The Master Builder\" (1892), Ibsen explored psychological conflicts that transcended a simple rejection of current conventions. Many modern readers, who might regard anti-Victorian didacticism as dated, simplistic or hackneyed, have found these later works to be of absorbing interest for their hard-edged, objective consideration of interpersonal confrontation. \"Hedda Gabler\" is probably Ibsen's most performed play, with the title role regarded as one of the most challenging and rewarding for an actress even in the present day. \"Hedda Gabler\" and \"A Doll's House\" center on female protagonists whose almost demonic energy proves both attractive and destructive for those around them, and while Hedda has a few similarities with the character of Nora in \"A Doll's House\", many of today's audiences and theatre critics feel that Hedda's intensity and drive are much more complex and much less comfortably explained than what they view as rather routine feminism on the part of Nora.\n\nIbsen had completely rewritten the rules of drama with a realism which was to be adopted by Chekhov and others and which we see in the theatre to this day. From Ibsen forward, challenging assumptions and directly speaking about issues has been considered one of the factors that makes a play art rather than entertainment. His works were brought to an English-speaking audience, largely thanks to the efforts of William Archer and Edmund Gosse. These in turn had a profound influence on the young James Joyce who venerates him in his early autobiographical novel \"Stephen Hero\". Ibsen returned to Norway in 1891, but it was in many ways not the Norway he had left. Indeed, he had played a major role in the changes that had happened across society. Modernism was on the rise, not only in the theatre, but across public life.\n\nOn 23 May 1906, Ibsen died in his home at Arbins gade 1 in Kristiania (now Oslo) after a series of strokes in March 1900. When, on 22 May, his nurse assured a visitor that he was a little better, Ibsen spluttered his last words \"On the contrary\" (\"Tvertimod!\"). He died the following day at 2:30 p.m.\n\nIbsen was buried in Vår Frelsers gravlund (\"The Graveyard of Our Savior\") in central Oslo.\n\nThe 100th anniversary of Ibsen's death in 2006 was commemorated with an \"Ibsen year\" in Norway and other countries. This year the homebuilding company Selvaag also opened \"Peer Gynt\" Sculpture Park in Oslo, Norway, in Henrik Ibsen's honour, making it possible to follow the dramatic play \"Peer Gynt\" scene by scene. Will Eno's adaptation of Ibsen's \"Peer Gynt\", titled \"Gnit\", had its world premiere at the 37th Humana Festival of New American Plays in March 2013.\n\nOn 23 May 2006, The Ibsen Museum in Oslo reopened to the public the house where Ibsen had spent his last eleven years, completely restored with the original interior, colors, and decor.\n\nOn the occasion of the 100th anniversary of Ibsen's death in 2006, the Norwegian government organised the Ibsen Year, which included celebrations around the world. The NRK produced a miniseries on Ibsen's childhood and youth in 2006, \"An Immortal Man\". Several prizes are awarded in the name of Henrik Ibsen, among them the International Ibsen Award, the Norwegian Ibsen Award and the Ibsen Centennial Commemoration Award.\n\nIbsen's ancestry has been a much studied subject, due to his perceived foreignness and due to the influence of his biography and family on his plays. Ibsen often made references to his family in his plays, sometimes by name, or by modelling characters after them.\n\nThe oldest documented member of the Ibsen family was ship's captain Rasmus Ibsen (1632–1703) from Stege, Denmark. His son, ship's captain Peder Ibsen became a burgher of Bergen in Norway in 1726. Henrik Ibsen had Danish, German, Norwegian and some distant Scottish ancestry. Most of his ancestors belonged to the merchant class of original Danish and German extraction, and many of his ancestors were ship's captains.\n\nIbsen's biographer Henrik Jæger famously wrote in 1888 that Ibsen did not have a drop of Norwegian blood in his veins, stating that \"the ancestral Ibsen was a Dane\". This, however, is not completely accurate; notably through his grandmother Hedevig Paus, Ibsen was descended from one of the very few families of the patrician class of original Norwegian extraction, known since the 15th century. Ibsen's ancestors had mostly lived in Norway for several generations, even though many had foreign ancestry.\n\nThe name Ibsen is originally a patronymic, meaning \"son of Ib\" (Ib is a Danish variant of Jacob). The patronymic became \"frozen\", i.e. it became a permanent family name, already in the 17th century. The phenomenon of patronymics becoming frozen started in the 17th century in bourgeois families in Denmark, and the practice was only widely adopted in Norway from around 1900.\n\nFrom his marriage with Suzannah Thoresen, Ibsen had one son, lawyer and government minister Sigurd Ibsen. Sigurd Ibsen married Bergljot Bjørnson, the daughter of Bjørnstjerne Bjørnson. Their only son was Tancred Ibsen, who became a film director and who was married to Lillebil Ibsen. Their only child was diplomat Tancred Ibsen, Jr. Sigurd Ibsen's daughter, Irene Ibsen, married Josias Bille, a member of the Danish ancient noble Bille family. Their son was Danish actor Joen Bille.\n\nEvery year, since 2008, the annual \"Delhi Ibsen Festival\", is held in Delhi, India, organized by the Dramatic Art and Design Academy (DADA) in collaboration with The Royal Norwegian Embassy in India. It features plays by Ibsen, performed by artists from various parts of the world in varied languages and styles.\n\nIbsen was decorated Knight in 1873, Commander in 1892, and with the Grand Cross of the Order of St. Olav in 1893. He received the Grand Cross of the Danish Order of the Dannebrog, and the Grand Cross of the Swedish Order of the Polar Star, and was Knight, First Class of the Order of Vasa.\n\nIn 1995, the asteroid (5696) Ibsen was named in his memory.\n\nThe authoritative translation in the English language for Ibsen remains the 1928 ten-volume version of the \"Complete Works of Henrik Ibsen\" from Oxford University Press. Many other translations of individual plays by Ibsen have appeared since 1928 though none have purported to be a new version of the complete works of Ibsen.\n\n\nThere have been numerous adaptations of Ibsen's work, particularly in film, theatre and music. Notable are Torstein Blixfjord's \"Terje\" and \"Identity of the Soul\" - two multimedia, film and dance pieces first presented in Yokohama in 2006, based on the poem \"Terje Vigen\".\n\n\n\n", "id": "14236", "title": "Henrik Ibsen"}
{"url": "https://en.wikipedia.org/wiki?curid=14240", "text": "Hawaiian language\n\nThe Hawaiian language (Hawaiian: \"\", ) is a Polynesian language that takes its name from Hawaii, the largest island in the tropical North Pacific archipelago where it developed. Hawaiian, along with English, is an official language of the state of Hawaii. King Kamehameha III established the first Hawaiian-language constitution in 1839 and 1840.\n\nFor various reasons, including territorial legislation establishing English as the official language in schools, the number of native speakers of Hawaiian gradually decreased during the period from the 1830s to the 1950s. Hawaiian was essentially displaced by English on six of seven inhabited islands. In 2001, native speakers of Hawaiian amounted to under 0.1% of the statewide population. Linguists were unsure that Hawaiian and other endangered languages would survive.\n\nNevertheless, from around 1949 to the present day, there has been a gradual increase in attention to and promotion of the language. Public Hawaiian-language immersion preschools called Pūnana Leo were started in 1984; other immersion schools followed soon after that. The first students to start in immersion preschool have now graduated from college and many are fluent Hawaiian speakers. The federal government has acknowledged this development. For example, the Hawaiian National Park Language Correction Act of 2000 changed the names of several national parks in Hawaii, observing the Hawaiian spelling.\n\nA pidgin or creole language spoken in Hawaii is Hawaiian Pidgin (or Hawaii Creole English, HCE). It should not be mistaken for the Hawaiian language nor for a dialect of English.\n\nThe Hawaiian alphabet has 13 letters: five vowels (long and short) and eight consonants, one of them being the okina, a letter which represents the glottal stop.\n\nThe Hawaiian language takes its name from the largest island, Hawaii (\"Hawaii\" in the Hawaiian language), in the tropical North Pacific archipelago where it developed, originally from a Polynesian language of the South Pacific, most likely Marquesan or Tahitian. The island name was first written in English in 1778 by British explorer James Cook and his crew members. They wrote it as \"Owhyhee\" or \"Owhyee\". Explorers Mortimer (1791) and Otto von Kotzebue (1821) used that spelling.\n\nThe initial \"O\" in the name is a reflection of the fact that unique identity is predicated in Hawaiian by using a copula form, \"o\", immediately before a proper noun. Thus, in Hawaiian, the name of the island is expressed by saying \"O Hawaii\", which means \"[This] is Hawaii.\" The Cook expedition also wrote \"Otaheite\" rather than \"Tahiti.\"\n\nThe spelling \"why\" in the name reflects the pronunciation of \"wh\" in 18th-century English (still in active use in parts of the English-speaking world). \"Why\" was pronounced . The spelling \"hee\" or \"ee\" in the name represents the sounds , or .\n\nPutting the parts together, \"O-why-(h)ee\" reflects , a reasonable approximation of the native pronunciation, .\n\nAmerican missionaries bound for Hawaii used the phrases \"Owhihe Language\" and \"Owhyhee language\" in Boston prior to their departure in October 1819 and during their five-month voyage to Hawai'i. They still used such phrases as late as March 1822. However, by July 1823, they had begun using the phrase \"Hawaiian Language.\"\n\nIn Hawaiian, \"Ōlelo Hawaii\" means \"Hawaiian language\", as adjectives follow nouns.\n\nHawaiian is a Polynesian member of the Austronesian language family. It is closely related to other Polynesian languages, such as Marquesan, Tahitian, Māori, Rapa Nui (the language of Easter Island), and less closely to Samoan and Tongan.\n\nAccording to Schütz (1994), the Marquesans colonized the archipelago in roughly 300 CE followed by later waves of immigration from the Society Islands and Samoa-Tonga. Their languages, over time, became the Hawaiian language within the Hawaiian Islands. Kimura and Wilson (1983) also state: \"Linguists agree that Hawaiian is closely related to Eastern Polynesian, with a particularly strong link in the Southern Marquesas, and a secondary link in Tahiti, which may be explained by voyaging between the Hawaiian and Society Islands.\"\n\nThe genetic history of the Hawaiian language is demonstrated primarily through the application of lexicostatistics, which involves quantitative comparison of lexical cognates, and the comparative method.\n\nLexicostatistics is a way of quantifying the degree to which any given languages are genetically related to one another. It is mainly based on determining the number of cognates (genetically shared words) that the languages have in a fixed set of vocabulary items which are nearly universal among all languages. The so-called \"basic vocabulary\" (or Swadesh list) amounts to about 200 words, having meanings such as \"eye\", \"hair\", \"blood\", \"water\", and \"and\". The measurement of a genetic relationship is expressed as a percentage. For example, Hawaiian and English have 0 cognates in the 200-word list, so they are 0% genetically related. By contrast, Hawaiian and Tahitian have about 152 cognates in the list, so they are estimated as being 76% genetically related.\n\nThe comparative method is a technique developed by linguists to determine if two or more languages are genetically related, and if they are, the historical nature of the relationships. For a given meaning, the words of the languages are compared.\nLinguists observe:\n\nIn this method, the definition of \"identical\" is reasonably clear, but those of \"similar\" and \"dissimilar\" are based on phonological criteria which may require professional training to fully understand and which can vary in the contexts of different languages. Basically, a sound's manner and place of articulation, and its phonological features, are the main factors considered in investigating its status as \"similar\" or \"dissimilar\" to other sounds in a particular context. For example, /b/ and /m/ are both voiced labial sounds, but one is a stop and the other a nasal. When linguists find in compared languages that compared words of the same or similar meaning contain sounds which correspond to one another, and find that these same sound correspondences recur regularly in most, or in many, of the comparable words of the languages, then the usual conclusion is that the languages are genetically related.\n\nThe following table provides a limited data set for ten numbers. The asterisk (*) is used to show that these are hypothetical, reconstructed forms. In the table, the year date of the modern forms is rounded off to 2000 CE to emphasize the 6000-year time lapse since the PAN era.\n\nNote: For the number \"10\", the Tongan form in the table is part of the word ('ten'). The Hawaiian cognate is part of the word ('ten days'); however, the more common word for \"10\" used in counting and quantifying is , a different root.\n\nApplication of the lexicostatistical method to the data in the table will show the four languages to be related to one another, with Tagalog having 100% cognacy with PAN, while Hawaiian and Tongan have 100% cognacy with each other, but 90% with Tagalog and PAN. This is because the forms for each number are cognates, except the Hawaiian and Tongan words for the number \"1\", which are cognate with each other, but not with Tagalog and PAN. When the full set of 200 meanings is used, the percentages will be much lower. For example, Elbert found Hawaiian and Tongan to have 49% (98 ÷ 200) shared cognacy. This points out the importance of data-set size for this method, where less data leads to cruder results, while more data leads to better results.\n\nApplication of the comparative method will show partly different genetic relationships. It will point out sound changes, such as:\nThis method will recognize sound change #1 as a shared innovation of Hawaiian and Tongan. It will also take the Hawaiian and Tongan cognates for \"1\" as another shared innovation. Due to these exclusively shared features, Hawaiian and Tongan are found to be more closely related to one another than either is to Tagalog or PAN.\n\nThe forms in the table show that the Austronesian vowels tend to be relatively stable, while the consonants are relatively volatile. It is also apparent that the Hawaiian words for \"3\", \"5\", and \"8\" have remained essentially unchanged for 6000 years.\n\nIn 1778, British explorer James Cook made the first reported European contact with Hawaii, beginning a new phase in the development of Hawaiian. During the next forty years, the sounds of Spanish (1789), Russian (1804), French (1816), and German (1816) arrived in Hawaii via other explorers and businessmen. Hawaiian began to be written for the first time, largely restricted to isolated names and words, and word lists collected by explorers and travelers.\n\nThe early explorers and merchants who first brought European languages to the Hawaiian islands also took on a few native crew members who brought the Hawaiian language into new territory. Hawaiians took these nautical jobs because their traditional way of life changed due to plantations, and although there were not enough of these Hawaiian-speaking explorers to establish any viable speech communities abroad, they still had a noticeable presence. One of them, a boy in his teens known as Obookiah (\"Ōpūkahaia\"), had a major impact on the future of the language. He sailed to New England, where he eventually became a student at the Foreign Mission School in Cornwall, Connecticut. He inspired New Englanders to support a Christian mission to Hawaii, and provided information on the Hawaiian language to the American missionaries there prior to their departure for Hawaii in 1819.\n\nFolk Tales\n\nLike all natural spoken languages, the Hawaiian language was originally just an oral language. The native people of the Hawaiian language relayed religion, traditions, history, and views of their world through stories that were handed down from generation to generation. One form of storytelling most commonly associated with the Hawaiian islands is hula. Nathaniel B. Emerson notes that \"It kept the communal imagination in living touch with the nation's legendary past\".\n\nThe islanders' connection with their stories is argued to be one reason why Captain James Cook received a pleasant welcome. Marshall Sahlins has observed that Hawaiian folktales began bearing similar content to those of the Western world in the eighteenth century. He argues this was caused by the timing of Captain Cook's arrival, which was coincidentally when the indigenous Hawaiians were celebrating the Makahiki festival. The islanders' story foretold of the god Lono's return at the time of the Makahiki festival.\n\nIn 1820, Protestant missionaries from New England arrived in Hawaii, inspired by the presence of several young Hawaiian men, especially Obookiah (\"Ōpūkahaia\"), at the Foreign Mission School in Cornwall, Connecticut. The missionaries began to learn the Hawaiian language so that they could form relationships with the locals and publish a Hawaiian Bible. To that end, they developed a successful alphabet for Hawaiian by 1826, taught Hawaiians to read and write the language, published various educational materials in Hawaiian, and eventually finished translating the Bible. Missionaries also influenced King Kamehameha III to establish the first Hawaiian-language constitutions in 1839 and 1840.\n\nAdelbert von Chamisso might have consulted with a native speaker of Hawaiian in Berlin, Germany, before publishing his grammar of Hawaiian (\"\") in 1837. When Hawaiian King David Kalākaua took a trip around the world, he brought his native language with him. When his wife, Queen Kapiolani, and his sister, Princess (later Queen) Liliuokalani, took a trip across North America and on to the British Islands, in 1887, Liliuokalani's composition \"Aloha Oe\" was already a famous song in the U.S.\nIn 1834, the first Hawaiian-language newspapers were published by missionaries working with locals. The missionaries also played a significant role in publishing a vocabulary (1836) grammar (1854) and dictionary (1865) of Hawaiian. Literacy in Hawaiian was widespread among the local population, especially ethnic Hawaiians. Use of the language among the general population might have peaked around 1881. Even so, some people worried, as early as 1854, that the language was \"soon destined to extinction.\"\n\nThe increase in travel to and from Hawaii during the 19th century introduced a number of fatal illnesses such as smallpox, influenza, and leprosy, which killed large numbers of native speakers of Hawaiian. Meanwhile, native speakers of other languages, especially English, Chinese, Japanese, Portuguese, and Ilokano, continued to immigrate to Hawaii. As a result, the actual number, as well as the percentage, of native speakers of Hawaiian in the local population decreased sharply, and continued to fall throughout the nineteenth century.\n\nAs the status of Hawaiian fell, the status of English in Hawaii rose. In 1885, the Prospectus of the Kamehameha Schools announced that \"instruction will be given only in English language\" (see published opinion of the United States Court of Appeals for the Ninth Circuit, Doe v. Kamehameha Schools, case no. 04-15044, page 8928, filed August 2, 2005). Around 1900, students began to be punished for speaking Hawaiian in schools, and the number of native speakers of Hawaiian diminished from 37,000 at the turn of the twentieth century to 1,000 in 1997; half of these remaining are now in their seventies or eighties (see Ethnologue report below for citations). Due to immersion programs the number of speakers has risen to 24,000 according to the 2011 US census.\n\nThere has been some controversy over the reasons for this decline. One school of thought claims that the most important cause for the decline of the Hawaiian language was its voluntary abandonment by the majority of its native speakers. According to Mary Kawena Pukui, they wanted their own children to speak English, as a way to promote their success in a rapidly changing modern environment, so they refrained from using Hawaiian with their own children. The Hawaiian language schools disappeared as their enrollments dropped: parents preferred English language schools. Another school of thought emphasizes the importance of other factors that discouraged the use of the language, such as the fact that the English language was made the only medium of instruction in all schools in 1896 and the fact that schools punished the use of Hawaiian (see \"\"Banning\" of Hawaiian\" below.) General prejudice against ethnic Hawaiians (\"kanaka\") has also been blamed for the decline of the language.\n\nA new dictionary was published in 1957, a new grammar in 1979, and new second-language textbooks in 1951, 1965, 1977, and 1989. Master's theses and doctoral dissertations on specific facets of Hawaiian appeared in 1951, 1975, 1976, and 1996.\n\nAccording to Mary Kawena Pukui and Samuel Elbert, ' (') is a \"Hidden meaning, as in Hawaiian poetry; concealed reference, as to a person, thing, or place; words with double meanings that might bring good or bad fortune.\" Pukui lamented, “in spite of years of dedicated work, it is impossible to record any language completely. How true this seems for Hawaiian, with its rich and varied background, its many idioms heretofore undescribed, and its ingenious and sophisticated use of figurative language.” On page xiii of the 1986 dictionary she warned: \"Hawaiian has more words with multiple meanings than almost any other language. One wishing to name a child, a house, a T-shirt, or a painting, should be careful that the chosen name does not have a naughty or vulgar meaning. The name of a justly respectable children's school, Hana Hauoli, means happy activity and suggests a missionary author, but among older Hawaiians it has another, less 'innocent' meaning that should not concern little children. A Honolulu street (and formerly the name of a hotel) is Hale Lea 'joyous house', but lea also means orgasm.\"\n\nUnderstanding the kaona of the language requires a comprehensive knowledge of Hawaiian legends, history and cosmology.\n\nThe law cited as banning the Hawaiian language is identified as Act 57, sec. 30 of the 1896 Laws of the Republic of Hawaii:\n\nThe English Language shall be the medium and basis of instruction in all public and private schools, provided that where it is desired that another language shall be taught in addition to the English language, such instruction may be authorized by the Department, either by its rules, the curriculum of the school, or by direct order in any particular instance. Any schools that shall not conform to the provisions of this section shall not be recognized by the Department. [signed] June 8, 1896 Sanford B. Dole, President of the Republic of Hawaii \n\nThis law established English as the medium of instruction for the government-recognized schools both \"public and private\". While it did not ban or make illegal the Hawaiian language in other contexts, its implementation in the schools had far reaching effects. Those who had been pushing for English-only schools took this law as licence to extinguish the native language at the early education level. While the law stopped short of making Hawaiian illegal (it was still the dominant language spoken at the time), many children who spoke Hawaiian at school, including on the playground, were disciplined. This included corporal punishment and going to the home of the offending child to strongly advise them to stop speaking it in their home. Moreover, the law specifically provided for teaching languages \"in addition to the English language,\" reducing Hawaiian to the status of a foreign language, subject to approval by the Department. Hawaiian was not taught initially in any school, including the all-Hawaiian Kamehameha Schools. This is largely because when these schools were founded, like Kamehameha Schools founded in 1887 (nine years before this law), Hawaiian was being spoken in the home. Once this law was enacted, individuals at these institutions took it upon themselves to enforce a ban on Hawaiian. Beginning in 1900, Mary Kawena Pukui, who was later the co-author of the Hawaiian–English Dictionary, was punished for speaking Hawaiian by being rapped on the forehead, allowed to eat only bread and water for lunch, and denied home visits on holidays. Winona Beamer was expelled from Kamehameha Schools in 1937 for chanting Hawaiian.\n\nHawaiian-language newspapers were published for over a hundred years, through the period of the suppression. Very few pro-Hawaiian papers made it through the period of the overthrow of the kingdom and the subsequent Act 57. Most papers that survived that period had a distinctly pro-U.S.Annexation perspective. list fourteen Hawaiian newspapers. According to them, the newspapers entitled \"Ka Lama Hawaii\" and \"Ke Kumu Hawaii\" began publishing in 1834, and the one called \"Ka Hoku o Hawaii\" ceased publication in 1948. The longest run was that of \"Ka Nupepa Kuokoa\": about 66 years, from 1861 to 1927.\n\nIn 1949, the legislature of the Territory of Hawaii commissioned Mary Pukui and Samuel Elbert to write a new dictionary of Hawaiian, either revising the Andrews-Parker work or starting from scratch. Pukui and Elbert took a middle course, using what they could from the Andrews dictionary, but making certain improvements and additions that were more significant than a minor revision. The dictionary they produced, in 1957, introduced an era of gradual increase in attention to the language and culture.\n\nEfforts to promote the language have increased in recent decades. Hawaiian-language \"immersion\" schools are now open to children whose families want to reintroduce Hawaiian language for future generations. The Aha Pūnana Leo’s Hawaiian language preschools in Hilo, Hawaii, have received international recognition. The local National Public Radio station features a short segment titled \"Hawaiian word of the day\" and a Hawaiian language news broadcast. Honolulu television station KGMB ran a weekly Hawaiian language program, \"Āhai Ōlelo Ola\", as recently as 2010. Additionally, the Sunday editions of the \"Honolulu Star-Advertiser\", the largest newspaper in Hawaii, feature a brief article called \"Kauakukalahale\" written entirely in Hawaiian by teachers, students, and community members.\n\nToday, on six of the seven permanently inhabited islands, Hawaiian has been largely displaced by English. The number of native speakers of Hawaiian, which was under 0.1% of the statewide population in 1997, has risen to 2,000, out of 24,000 total who are fluent in the language, according to the US 2011 census. Native speakers of Hawaiian who live on the island named \"Niihau\" have remained fairly isolated and have continued to use Hawaiian almost exclusively.\n\nThe isolated island of Niʻihau, located off the southwest coast of Kauai, is the one island where Hawaiian is still spoken as the language of daily life. Children are taught Hawaiian as a first language, and learn English at about age eight. Reasons for the language's predominance on this island include:\n\nNative speakers of Niʻihau Hawaiian have three distinct modes of speaking Hawaiian:\n\nThe last mode of speaking may be further restricted to a certain subset of Niʻihauans, and is rarely even overheard by non- Niʻihauans. In addition to being able to speak Hawaiian in different ways, most Niʻihauans can speak English as well.\n\nHawaiians had no written language prior to western contact, except for petroglyph symbols.\nThe modern Hawaiian alphabet, \"ka pīāpā Hawaii\", is based on the Latin script. Hawaiian words end \"only\" in vowels, and every consonant must be followed by a vowel. The Hawaiian alphabetical order has all of the vowels before the consonants, as in the following chart.\nThis writing system was developed by American Protestant missionaries during 1820–1826. It was the first thing they ever printed in Hawaii, on January 7, 1822, and it originally included the consonants \"B, D, R, T,\" and \"V,\" in addition to the current ones (\"H, K, L, M, N, P, W\"), and it had \"F, G, S, Y\" and \"Z\" for \"spelling foreign words\". The initial printing also showed the five vowel letters (\"A, E, I, O, U\") and seven of the short diphthongs (\"AE, AI, AO, AU, EI, EU, OU\").\n\nIn 1826, the developers voted to eliminate some of the letters which represented functionally redundant allophones (called \"interchangeable letters\"), enabling the Hawaiian alphabet to approach the ideal state of one-symbol-one-phoneme, and thereby optimizing the ease with which people could teach and learn the reading and writing of Hawaiian. For example, instead of spelling one and the same word as \"pule, bule, pure,\" and \"bure\" (because of interchangeable \"p/b\" and \"l/r\"), the word is spelled only as \"pule\".\n\nHowever, hundreds of words were very rapidly borrowed into Hawaiian from English, Greek, Hebrew, Latin, and Syriac. Although these loan words were necessarily Hawaiianized, they often retained some of their \"non-Hawaiian letters\" in their published forms. For example, \"Brazil\" fully Hawaiianized is \"Palakila\", but retaining \"foreign letters\" it is \"Barazila\". Another example is \"Gibraltar\", written as \"Kipalaleka\" or \"Gibaraleta\". While and are not regarded as Hawaiian sounds, , , and were represented in the original alphabet, so the letters (\"b\", \"r\", and \"t\") for the latter are not truly \"non-Hawaiian\" or \"foreign\", even though their post-1826 use in published matter generally marked words of foreign origin.\n\n\"ʻOkina\" (\"oki\" 'cut' + \"-na\" '-ing') is the modern Hawaiian name for the symbol (a letter) that represents the glottal stop. It was formerly known as \"uina\" ('snap').\n\nFor examples of the okina, consider the Hawaiian words \"Hawaii\" and \"Oahu\" (often simply \"Hawaii\" and \"Oahu\" in English orthography). In Hawaiian, these words can be pronounced and , and can be written with an okina where the glottal stop is pronounced.\n\nElbert & Pukui's \"Hawaiian Grammar\" says \"The glottal stop, ‘, is made by closing the glottis or space between the vocal cords, the result being something like the hiatus in English \"oh-oh\".\".\n\nAs early as 1823, the missionaries made some limited use of the apostrophe to represent the glottal stop, but they did not make it a letter of the alphabet. In publishing the Hawaiian Bible, they used it to distinguish \"kou\" ('my') from \"kou\" ('your'). In 1864, William DeWitt Alexander published a grammar of Hawaiian in which he made it clear that the glottal stop (calling it \"guttural break\") is definitely a true consonant of the Hawaiian language. He wrote it using an apostrophe. In 1922, the Andrews-Parker dictionary of Hawaiian made limited use of the opening single quote symbol, called \"reversed apostrophe\" or \"inverse comma\", to represent the glottal stop. Subsequent dictionaries have preferred to use that symbol. Today, many native speakers of Hawaiian do not bother, in general, to write any symbol for the glottal stop. Its use is advocated mainly among students and teachers of Hawaiian as a second language, and among linguists.\n\nThe okina is written in various ways for electronic uses:\n\nBecause many people who want to write the ʻokina are not familiar with these specific characters and/or do not have access to the appropriate fonts and input and display systems, it is sometimes written with more familiar and readily available characters:\n\nA modern Hawaiian name for the macron symbol is \"kahakō\" (\"kaha\" 'mark' + \"kō\" 'long'). It was formerly known as \"mekona\" (Hawaiianization of \"macron\"). It can be written as a diacritical mark which looks like a hyphen or dash written above a vowel, i.e., \"ā ē ī ō ū\" and \"Ā Ē Ī Ō Ū\". It is used to show that the marked vowel is a \"double\", or \"geminate\", or \"long\" vowel, in phonemic terms. (See: Vowel length)\n\nAs early as 1821, at least one of the missionaries, Hiram Bingham, was using macrons (and breves) in making handwritten transcriptions of Hawaiian vowels. The missionaries specifically requested their sponsor in Boston to send them some type (fonts) with accented vowel characters, including vowels with macrons, but the sponsor made only one response and sent the wrong font size (pica instead of small pica). Thus, they could not print ā, ē, ī, ō, nor ū (at the right size), even though they wanted to.\n\nDue to extensive allophony, Hawaiian has more than 13 phones. Although vowel length is phonemic, long vowels are not always pronounced as such, even though under the rules for assigning stress in Hawaiian, a long vowel will always receive stress.\n\nHawaiian is known for having very few consonant phonemes – eight: . It is notable that Hawaiian has allophonic variation of with , with , and (in some dialects) with . The – variation is quite unusual among the world's languages (although related to how hard and soft C developed in many European languages), and is likely a product both of the small number of consonants in Hawaiian, and the recent shift of historical *t to modern –, after historical *k had shifted to . In some dialects, remains as in some words. These variations are largely free, though there are conditioning factors. tends to especially in words with both and , such as in the island name \"Lānai\" (–), though this is not always the case: \"eleele\" or \"eneene\" \"black\". The allophone is almost universal at the beginnings of words, whereas is most common before the vowel . is also the norm after and , whereas is usual after and . After and initially, however, and are in free variation.\"A consonant occurs only before a vowel; thus two consonants never occur in succession and a syllable always ends with a vowel\".\n\nHawaiian has five short and five long vowels, plus diphthongs.\n\nHawaiian has five pure vowels. The short vowels are , and the long vowels, if they are considered separate phonemes rather than simply sequences of like vowels, are . When stressed, short and tend to become and , while when unstressed they are and . also tends to become next to , , and another , as in \"Pele\" . Some grammatical particles vary between short and long vowels. These include \"a\" and \"o\" \"of\", \"ma\" \"at\", \"na\" and \"no\" \"for\". Between a back vowel or and a following non-back vowel (), there is an epenthetic , which is generally not written. Between a front vowel or and a following non-front vowel (), there is an epenthetic (a \"y\" sound), which is never written.\n\nThe short-vowel diphthongs are . In all except perhaps , these are falling diphthongs. However, they are not as tightly bound as the diphthongs of English, and may be considered vowel sequences. (The second vowel in such sequences may receive the stress, but in such cases it is not counted as a diphthong.) In fast speech, tends to and tends to , conflating these diphthongs with and .\n\nThere are only a limited number of vowels which may follow long vowels, and some authors treat these as diphthongs as well: .\n\nHawaiian syllable structure is (C)V. All CV syllables occur except for \"wū\"; \"wu\" occurs only in two words borrowed from English. As shown by Schütz, Hawaiian word-stress is predictable in words of one to four syllables, but not in words of five or more syllables. Hawaiian phonological processes include palatalization and deletion of consonants, as well as raising, diphthongization, deletion, and compensatory lengthening of vowels. Phonological reduction (or \"decay\") of consonant phonemes during the historical development of the language has resulted in the phonemic glottal stop. Ultimate loss (deletion) of intervocalic consonant phonemes has resulted in Hawaiian long vowels and diphthongs.\n\nHawaiian is an analytic language with verb–subject–object word order. While there is no use of inflection for verbs, in Hawaiian, like other Austronesian personal pronouns, declension is found in the differentiation between a- and o-class genitive case personal pronouns in order to indicate inalienable possession in a binary possessive class system. Also like many Austronesian languages, Hawaiian pronouns employ separate words for inclusive and exclusive we (clusivity), and distinguish singular, dual, and plural. The grammatical function of verbs is marked by adjacent particles (short words) and by their relative positions, that indicate tense–aspect–mood.\n\nSome examples of verb phrase patterns:\n\nNouns can be marked with articles:\n\n\"ka\" and \"ke\" are singular definite articles. \"ke\" is used before words beginning with a-, e-, o- and k-, and with some words beginning - and p-. \"ka\" is used in all other cases. \"nā\" is the plural definite article.\n\nTo show part of a group, the word \"kekahi\" is used. To show a bigger part, you would insert \"mau\" to pluralize the subject.\n\nExamples:\n\n\n\n", "id": "14240", "title": "Hawaiian language"}
{"url": "https://en.wikipedia.org/wiki?curid=14245", "text": "Second Polish Republic\n\nThe Second Polish Republic, also known as the Second Commonwealth of Poland or interwar Poland, refers to the country of Poland between the First and Second World Wars (1918–1939). Officially known as the Republic of Poland or the Commonwealth of Poland (), the Polish state was recreated in 1918, in the aftermath of World War I. When, after several regional conflicts, the borders of the state were fixed in 1922, Poland's neighbours were Czechoslovakia, Germany, the Free City of Danzig, Lithuania, Latvia, Romania and the Soviet Union. It had access to the Baltic Sea via a short strip of coastline either side of the city of Gdynia. Between March and August 1939, Poland also shared a border with the then-Hungarian governorate of Subcarpathia. Despite internal and external pressures, it continued to exist until 1939, when Poland was invaded by Nazi Germany, the Soviet Union and the Slovak Republic, marking the beginning of World War II in Europe. The Second Republic was significantly different in territory to the current Polish state. It included substantially more territory in the east and less in the west.\n\nThe Second Republic's land area was 388,634 km, making it, in October 1938, the sixth largest country in Europe. After the annexation of Zaolzie, this grew to 389,720 km. According to the 1921 census, the number of inhabitants was 27.2 million. By 1939, just before the outbreak of World War II, this had grown to an estimated 35.1 million. Almost a third of population came from minority groups: 13.9% Ukrainians; 10% Jews; 3.1% Belarusians; 2.3% Germans and 3.4% Czechs, Lithuanians and Russians. At the same time, a significant number of ethnic Poles lived outside the country borders, many in the Soviet Union. The Republic endured and expanded, despite a variety of difficulties in the aftermath of World War I, including conflicts with the Ukrainians, Czechoslovakia, Lithuania, Soviet Russia and Soviet Ukraine; the Weimar Republic over Greater Poland, and Upper Silesia; and in spite of increasing hostility from Nazi Germany.\n\nPoland maintained a slow (see: trade embargo) but steady level of economic development. The cultural hubs of interwar PolandWarsaw, Kraków, Poznań, Wilno and Lwówbecame major European cities and the sites of internationally acclaimed universities and other institutions of higher education. By 1939, the Republic had become \"one of Europe's major powers\".\n\nAfter more than a century of Partitions between the Austrian, the Prussian, and the Russian imperial powers, Poland re-emerged as a sovereign state at the end of the First World War in Europe in 1917-1918. The victorious Allies of World War I confirmed the rebirth of Poland in the Treaty of Versailles of June 1919. It was one of the great stories of the 1919 Paris Peace Conference. Poland solidified its independence in a series of border wars fought by the newly formed Polish Army from 1918 to 1921. The extent of the eastern half of the interwar territory of Poland was settled diplomatically in 1922 and internationally recognized by the League of Nations.\n\nIn the course of World War I (1914-1918), Germany gradually gained overall dominance on the Eastern Front as the Imperial Russian Army fell back. German and Austro-Hungarian armies seized the Russian-ruled part of what became Poland. In a failed attempt to resolve the Polish question as quickly as possible, Berlin set up a German puppet state on 5 November 1916, with a governing Provisional Council of State and (from 15 October 1917) a Regency Council (\"Rada Regencyjna Królestwa Polskiego\"). The Council administered the country under German auspices (see also Mitteleuropa), pending the election of a king. A month before Germany surrendered on 11 November 1918 and the war ended, the Regency Council had dissolved the Council of State, and announced its intention to restore Polish independence (7 October 1918). With the notable exception of the Marxist-oriented Social Democratic Party of the Kingdom of Poland and Lithuania (\"SDKPiL\"), most Polish political parties supported this move. On 23 October the Regency Council appointed a new government under Józef Świeżyński and began conscription into the Polish Army.\n\nIn 1918–1919, over 100 workers' councils sprang up on Polish territories; on 5 November 1918, in Lublin, the first Soviet of Delegates was established. On 6 November socialists proclaimed the Republic of Tarnobrzeg at Tarnobrzeg in Austrian Galicia. The same day the Socialist, Ignacy Daszyński, set up a Provisional People's Government of the Republic of Poland (\"Tymczasowy Rząd Ludowy Republiki Polskiej\") in Lublin. On Sunday, 10 November at 7 a.m., Józef Piłsudski, newly freed from 16 months in a German prison in Magdeburg, returned by train to Warsaw. Piłsudski, together with Colonel Kazimierz Sosnkowski, was greeted at Warsaw's railway station by Regent Zdzisław Lubomirski and by Colonel Adam Koc. Next day, due to his popularity and support from most political parties, the Regency Council appointed Piłsudski as Commander in Chief of the Polish Armed Forces. On 14 November, the Council dissolved itself and transferred all its authority to Piłsudski as Chief of State (\"Naczelnik Państwa\"). After consultation with Piłsudski, Daszyński's government dissolved itself and a new government formed under Jędrzej Moraczewski. In 1918 Italy became the first country in Europe to recognise Poland's renewed sovereignty.\nCenters of government that formed at that time in Galicia (formerly Austrian-ruled southern Poland) included the National Council of the Principality of Cieszyn (established in November 1918), the Republic of Zakopane and the Polish Liquidation Committee (28 October). Soon afterward, the Polish–Ukrainian War broke out in Lwów (1 November 1918) between forces of the Military Committee of Ukrainians and the Polish irregular units made up of students known as the Lwów Eaglets, who were later supported by the Polish Army (see Battle of Lwów (1918), Battle of Przemyśl (1918)). Meanwhile, in western Poland, another war of national liberation began under the banner of the Greater Poland Uprising (1918–19). In January 1919 Czechoslovakian forces attacked Polish units in the area of Zaolzie (see Polish–Czechoslovak War). Soon afterwards the Polish–Lithuanian War (ca 1919-1920) began, and in August 1919 Polish-speaking residents of Upper Silesia initiated a series of three Silesian Uprisings. The most critical military conflict of that period, however, the Polish–Soviet War (1919-1921), ended in a decisive Polish victory. In 1919 the Warsaw government suppressed the Republic of Tarnobrzeg and the workers' councils.\n\nThe Second Polish Republic was a parliamentary democracy from 1919 (see Small Constitution of 1919) to 1926, with the President having limited powers. The Parliament elected him, and he could appoint the Prime Minister as well as the government with the Sejm's (lower house's) approval, but he could only dissolve the Sejm with the Senate's consent. Moreover, his power to pass decrees was limited by the requirement that the Prime Minister and the appropriate other Minister had to verify his decrees with their signatures. Poland was one of the first countries in the world to recognize women's suffrage. Women in Poland were granted the right to vote on 28 November 1918 by a decree of Józef Piłsudski.\n\nThe major political parties at this time were the Polish Socialist Party, National Democrats, various Peasant Parties, Christian Democrats, and political groups of ethnic minorities (German: German Social Democratic Party of Poland, Jewish: General Jewish Labour Bund in Poland, United Jewish Socialist Workers Party, and Ukrainian: Ukrainian National Democratic Alliance). Frequently changing governments (see Polish legislative election, 1919, Polish legislative election, 1922) and other negative publicity the politicians received (such as accusations of corruption or 1919 Polish coup attempt), made them increasingly unpopular. Major politicians at this time, in addition to Piłsudski, included peasant activist Wincenty Witos (Prime Minister three times) and right-wing leader Roman Dmowski. Ethnic minorities were represented in the Sejm; e.g. in 1928 – 1930 there was the Ukrainian-Belarusian Club, with 26 Ukrainian and 4 Belarusian members.\nAfter the Polish – Soviet war, Marshal Piłsudski led an intentionally modest life, writing historical books for a living. After he took power by a military coup in May 1926, he emphasized that he wanted to heal the Polish society and politics of excessive partisan politics. His regime, accordingly, was called Sanacja in Polish. The 1928 parliamentary elections were still considered free and fair, although the pro-Piłsudski Nonpartisan Bloc for Cooperation with the Government won them. The following three parliamentary elections (in 1930, 1935 and 1938) were manipulated, with opposition activists sent to Bereza Kartuska prison (see also Brest trials). As a result, pro-government party Camp of National Unity won huge majorities in them. Piłsudski died just after an authoritarian constitution was approved in the spring of 1935. During the last four years of the Second Polish Republic, the major politicians included President Ignacy Mościcki, Foreign Minister Józef Beck and the Commander-in-Chief of the Polish Army, Edward Rydz-Śmigły. The country was divided into 104 electoral districts, and those politicians who were forced to leave Poland, founded Front Morges in 1936. The government that ruled Second Polish Republic in its final years is frequently referred to as Piłsudski's colonels.\nThe interwar Poland had a considerably large army of 283,000 soldiers on active duty: in 37 infantry divisions, 11 cavalry brigades, and two armored brigades, plus artillery units. Another 700,000 men served in the reserves. At the outbreak of the war, the Polish army was able to put in the field almost one million soldiers, 2,800 guns, 500 tanks and 400 aircraft.\n\nThe training of the Polish army was thorough. The N.C.O.s were a competent body of men with expert knowledge and high ideals. The officers, both senior and junior, constantly refreshed their training in the field and in the lecture-hall, where modern technical achievement and the lessons of contemporary wars were demonstrated and discussed. The equipment of the Polish army was less developed technically than that of the enemy and its rearmament was slowed down by a recrudescence of optimism in western Europe and the usual budget difficulties.\n\nAfter regaining its independence, Poland was faced with major economic difficulties. In addition to the devastation wrought by World War I, the exploitation of the Polish economy by the German and Russian occupying powers, and the sabotage performed by retreating armies, the new republic was faced with the task of economically unifying disparate economic regions, which had previously been part of different countries. Within the borders of the Republic were the remnants of three different economic systems, with five different currencies (the German mark, the Russian ruble, the Austrian crown, the Polish marka and the Ostrubel) and with little or no direct infrastructural links. The situation was so bad that neighboring industrial centers as well as major cities lacked direct railroad links, because they had been parts of different nations. For example, there was no direct railroad connection between Warsaw and Kraków until 1934. This situation was described by Melchior Wańkowicz in his book Sztafeta.\n\nOn top of this was the massive destruction left after both World War I and the Polish–Soviet War. There was also a great economic disparity between the eastern (commonly called \"Poland B\") and western (called \"Poland A\") parts of the country, with the western half, especially areas that had belonged to the German Empire being much more developed and prosperous. Frequent border closures and a customs war with Germany also had negative economic impacts on Poland. In 1924 Prime Minister and Economic Minister Władysław Grabski introduced the złoty as a single common currency for Poland (it replaced the Polish marka), which remained a stable currency. The currency helped Poland to control the massive hyperinflation. It was the only country in Europe able to do this without foreign loans or aid. The average annual growth rate (GDP per capita) was 5.24% in 1920–29 and 0.34% in 1929–38.\n\nHostile relations with neighbours were a major problem for the economy of interbellum Poland. In the year 1937, foreign trade with all neighbours amounted to only 21% of Poland's total. Trade with Germany, Poland's most important neighbour, accounted for 14.3% of Polish exchange. Foreign trade with the Soviet Union (0.8%) was virtually nonexistent. Czechoslovakia accounted for 3.9%, Latvia for 0.3%, and Romania for 0.8%. By mid-1938, after the Anschluss of Austria, Greater Germany was responsible for as much as 23% of Polish foreign trade.\nThe basis of Poland's gradual recovery after the Great Depression was its mass economic development plans (see Four Year Plan), which oversaw the building of three key infrastructural elements. The first was the establishment of the Gdynia seaport, which allowed Poland to completely bypass Gdańsk (which was under heavy German pressure to boycott Polish coal exports). The second was construction of the 500-kilometer rail connection between Upper Silesia and Gdynia, called Polish Coal Trunk-Line, which served freight trains with coal. The third was the creation of a central industrial district, named \"COP – Central Industrial Region\" (Centralny Okręg Przemysłowy). Unfortunately, these developments were interrupted and largely destroyed by the German and Soviet invasion and the start of World War II. Other achievements of interbellum Poland included Stalowa Wola (a brand new city, built in a forest around a steel mill), Mościce (now a district of Tarnów, with a large nitrate factory), and the creation of a central bank. There were several trade fairs, with the most popular being Poznań International Fair, Lwów's Targi Wschodnie, and Wilno's Targi Północne. Polish Radio had ten stations (see Radio stations in interwar Poland), with the eleventh one planned to be opened in the autumn of 1939. Furthermore, in 1935 Polish engineers began working on the TV services. By early 1939, experts of the Polish Radio built four TV sets. The first movie broadcast by experimental Polish TV was Barbara Radziwiłłówna, and by 1940, regular TV service was scheduled to begin operation.\n\nInterbellum Poland was also a country with numerous social problems. Unemployment was high, and poverty was widespread, which resulted in several cases of social unrest, such as the 1923 Kraków riot, and 1937 peasant strike in Poland. There were conflicts with national minorities, such as Pacification of Ukrainians in Eastern Galicia (1930), relations with Polish neighbors were sometimes complicated (see Soviet raid on Stołpce, Polish–Czechoslovak border conflicts, 1938 Polish ultimatum to Lithuania). On top of this, there were natural disasters, such as 1934 flood in Poland.\n\nInterbellum, Poland was unofficially divided into two parts – better developed \"Poland A\" in the west, and underdeveloped \"Poland B\" in the east. Polish industry was concentrated in the west, mostly in Polish Upper Silesia, and the adjacent Lesser Poland's province of Zagłębie Dąbrowskie, where the bulk of coal mines and steel plants was located. Furthermore, heavy industry plants were located in Częstochowa (\"Huta Częstochowa\", founded in 1896), Ostrowiec Świętokrzyski (\"Huta Ostrowiec\", founded in 1837–1839), Stalowa Wola (brand new industrial city, which was built from scratch in 1937 – 1938), Chrzanów (\"Fablok\", founded in 1919), Jaworzno, Trzebinia (oil refinery, opened in 1895), Łódź (the seat of Polish textile industry), Poznań (H. Cegielski – Poznań), Kraków and Warsaw (Ursus Factory). Further east, in Kresy, industrial centers included two major cities of the region – Lwów and Wilno (Elektrit).\n\nBesides coal mining, Poland also had deposits of oil in Borysław, Drohobycz, Jasło and Gorlice (see Polmin), potassium salt (TESP), and basalt (Janowa Dolina). Apart from already-existing industrial areas, in the mid-1930s, an ambitious, state-sponsored project of Central Industrial Region was started under Minister Eugeniusz Kwiatkowski. One of characteristic features of Polish economy in the interbellum was gradual nationalization of major plants. This was the case of Ursus Factory (see Państwowe Zakłady Inżynieryjne), and several steelworks, such as \"Huta Pokój\" in Ruda Śląska – Nowy Bytom, \"Huta Królewska\" in Chorzów – Królewska Huta, \"Huta Laura\" in Siemianowice Śląskie, as well as \"Scheibler and Grohman Works\" in Łódź.\n\nAccording to the 1939 Statistical Yearbook of Poland, total length of railways of Poland (as for 31 December 1937) was . Rail density was per . Railways were very dense in western part of the country, while in the east, especially Polesie, rail was non-existent in some counties. During the interbellum period, the Polish government constructed several new lines, mainly in the central part of the country (see also Polish State Railroads Summer 1939). Construction of extensive Warszawa Główna railway station was never finished due to the war, and Polish railroads were famous for their punctuality (see Luxtorpeda, Strzała Bałtyku, Latający Wilnianin).\n\nIn the interbellum, road network of Poland was dense, but the quality of the roads was very poor – only 7% of all roads was paved and ready for automobile use, and none of the major cities were connected with each other by a good-quality highway. \nIn the mid-1930s, Poland had of roads, but only 58,000 had hard surface (gravel, cobblestone or sett), and 2,500 were modern, with asphalt or concrete surface. In different parts of the country, there were sections of paved roads, which suddenly ended, and were followed by dirt roads. The poor condition of the roads was the result of both long-lasting foreign dominance and inadequate funding. On 29 January 1931, the Polish Parliament created the State Road Fund, the purpose of which was to collect money for the construction and conservation of roads. The government drafted a 10-year plan, with road priorities: a highway from Wilno, through Warsaw and Cracow, to Zakopane (called Marshall Pilsudski Highway), asphalt highways from Warsaw to Poznań and Łódź, as well as a Warsaw ring road. However, the plan turned out to be too ambitious, with insufficient money in the national budget to pay for it. In January 1938, the Polish Road Congress estimated that Poland would need to spend three times as much money on roads to keep up with Western Europe.\n\nIn 1939, before the outbreak of the war, LOT Polish Airlines, which was established in 1929, had its hub at Warsaw Okęcie Airport. At that time, LOT maintained several services, both domestic and international. Warsaw had regular domestic connections with Gdynia-Rumia, Danzig-Langfuhr, Katowice-Muchowiec, Kraków-Rakowice-Czyżyny, Lwów-Skniłów, Poznań-Ławica, and Wilno-Porubanek. Furthermore, in cooperation with Air France, LARES, Lufthansa, and Malert, international connections were maintained with Athens, Beirut, Berlin, Bucharest, Budapest, Helsinki, Kaunas, London, Paris, Prague, Riga, Rome, Tallinn, and Zagreb.\n\nStatistically, the majority of citizens lived in the countryside (75% in 1921). Farmers made up 65% of the population. In 1929, agricultural production made up 65% of Poland's GNP. After 123 years of partitions, regions of the country were very unevenly developed. Lands of former German Empire were most advanced; in Greater Poland and Pomerelia, crops were on Western European level. The situation was much worse in former Congress Poland, Kresy, and former Galicia, where agriculture was most backward and primitive, with a large number of small farms, unable to succeed in either the domestic and international market. Another problem was the overpopulation of the countryside, which resulted in chronic unemployment. Living conditions were so bad that in several regions, such as counties inhabited by the Hutsuls, there was permanent starvation. Farmers rebelled against the government (see: 1937 peasant strike in Poland), and the situation began to change in the late 1930s, due to construction of several factories for the Central Industrial Region, which gave employment to thousands of countryside residents.\n\nBeginning in June 1925 there was a customs' war with the revanchist Weimar Republic imposing trade embargo against Poland for nearly a decade; involving tariffs, and broad economic restrictions. After 1933 the trade war ended. The new agreements regulated and promoted trade. Germany became Poland's largest trading partner, followed by Britain. In October 1938 Germany granted a credit of Rm 60,000,000 to Poland (120,000,000 zloty, or £4,800,000) which was never realized, due to the outbreak of war. Germany would deliver factory equipment and machinery in return for Polish timber and agricultural produce. This new trade was to be \"in addition\" to the existing German-Polish trade agreements.\n\nIn 1919, the Polish government introduced compulsory education for all children aged 7 to 14, in an effort to limit illiteracy, which was widespread especially in the former Russian Partition and the Austrian Partition of eastern Poland. In 1921, one-third of citizens of Poland remained illiterate (38% in the countryside). The process was slow, but by 1931, the illiteracy level had dropped to 23% overall (27% in the countryside) and further down to 18% in 1937. By 1939, over 90% of children attended school. In 1932, Minister of Religion and Education Janusz Jędrzejewicz carried out a major reform which introduced two main levels of education: \"common school\" (\"szkoła powszechna\"), with three levels – 4 grades + 2 grades + 1 grade; and \"middle school\" (\"szkoła średnia\"), with two levels – 4 grades of comprehensive middle school and 2 grades of specified high school (classical, humanistic, natural and mathematical). A graduate of middle school received a \"small matura\", while a graduate of high school received a \"big matura\", which enabled them to seek university-level education.\n\nBefore 1918, Poland had three universities: Jagiellonian University, University of Warsaw and Lwów University. Catholic University of Lublin was established in 1918; Adam Mickiewicz University, Poznań, in 1919; and finally, in 1922, after the annexation of Republic of Central Lithuania, Wilno University became the Republic's sixth university. There were also three technical colleges: the Warsaw University of Technology, Lwów Polytechnic and the AGH University of Science and Technology in Kraków, established in 1919. Warsaw University of Life Sciences was an agricultural institute. By 1939, there were around 50,000 students enrolled in further education. Women made up 28% of university students, the second highest proportion in Europe.\nPolish science in the interbellum was renowned for its mathematicians gathered around the Lwów School of Mathematics, the Kraków School of Mathematics, as well as Warsaw School of Mathematics. There were world-class philosophers in the Lwów–Warsaw school of logic and philosophy. Florian Znaniecki founded Polish sociological studies. Rudolf Weigl invented a vaccine against typhus. Bronisław Malinowski counted among the most important anthropologists of the 20th century. In Polish literature, the 1920s were marked by the domination of poetry. Polish poets were divided into two groups – the Skamanderites (Jan Lechoń, Julian Tuwim, Antoni Słonimski and Jarosław Iwaszkiewicz) and the Futurists (Anatol Stern, Bruno Jasieński, Aleksander Wat, Julian Przyboś). Apart from well-established novelists (Stefan Żeromski, Władysław Reymont), new names appeared in the interbellum – Zofia Nałkowska, Maria Dąbrowska, Jarosław Iwaszkiewicz, Jan Parandowski, Bruno Schultz, Stanisław Ignacy Witkiewicz, Witold Gombrowicz. Among other notable artists there were sculptor Xawery Dunikowski, painters Julian Fałat, Wojciech Kossak and Jacek Malczewski, composers Karol Szymanowski, Feliks Nowowiejski, and Artur Rubinstein, singer Jan Kiepura. Theatre was very popular in the interbellum, with three main centers in the cities of Warsaw, Wilno and Lwów. Altogether, there were 103 theaters in Poland and a number of other theatrical institutions (including 100 folk theaters). In 1936, different shows were seen by 5 million people, and main figures of Polish theatre of the time were Juliusz Osterwa, Stefan Jaracz, and Leon Schiller. Also, before the outbreak of the war, there were about a million radios (see Radio stations in interwar Poland).\n\nThe administrative division of the Republic was based on a three-tier system. On the lowest rung were the \"gminy\", local town and village governments akin to districts or parishes. These were then grouped together into \"powiaty\" (akin to counties), which, in turn, were grouped as \"województwa\" (voivodeships, akin to provinces).\n\nHistorically, Poland was a nation of many nationalities. This was especially true after independence was regained in the wake of World War I and the subsequent Polish–Soviet War ending at Peace of Riga. The census of 1921 shows 30.8% of the population consisted of ethnic minorities, compared with 10% today. The first spontaneous flight of about 500,000 Poles from the Soviet Union occurred during the reconstitution of sovereign Poland. In the second wave, between November 1919 and June 1924 some 1,200,000 people left the territory of the USSR for Poland. It is estimated that some 460,000 of them spoke Polish as the first language. According to the 1931 Polish Census: 68.9% of the population was Polish, 13.9% were Ukrainian, around 10% Jewish, 3.1% Belarusian, 2.3% German and 2.8% other, including Lithuanian, Czech, Armenian, Russian, and Romani. The situation of minorities was a complex subject and changed during the period.\nPoland was also a nation of many religions. In 1921, 16,057,229 Poles (approx. 62.5%) were Roman (Latin) Catholics, 3,031,057 citizens of Poland (approx. 11.8%) were Eastern Rite Catholics (mostly Ukrainian Greek Catholics and Armenian Rite Catholics), 2,815,817 (approx. 10.95%) were Greek Orthodox, 2,771,949 (approx. 10.8%) were Jewish, and 940,232 (approx. 3.7%) were Protestants (mostly Lutheran).\n\nBy 1931, Poland had the second largest Jewish population in the world, with one-fifth of all the world's Jews residing within its borders (approx. 3,136,000). The urban population of interbellum Poland was rising steadily; in 1921, only 24% of Poles lived in the cities, in the late 1930s, that proportion grew to 30%. In more than a decade, the population of Warsaw grew by 200,000, Łódź by 150,000, and Poznań – by 100,000. This was due not only to internal migration, but also to an extremely high birth rate.\n\nThe Second Polish Republic was mainly flat with average elevation of above sea level, except for the southernmost Carpathian Mountains (after World War II and its border changes, the average elevation of Poland decreased to ). Only 13% of territory, along the southern border, was higher than . The highest elevation in the country was Mount Rysy, which rises in the Tatra Range of the Carpathians, approximately south of Kraków. Between October 1938 and September 1939, the highest elevation was Lodowy Szczyt (known in the Slovak language as \"Ľadový štít\"), which rises above sea level. The largest lake was Lake Narach.\nThe country's total area, after the annexation of Zaolzie, was . It extended from north to south and from east to west. On 1 January 1938, total length of boundaries was , including: of coastline (out of which were made by the Hel Peninsula), the with Soviet Union, 948 kilometers with Czechoslovakia (until 1938), with Germany (together with East Prussia), and with other countries (Lithuania, Romania, Latvia, Danzig). The warmest yearly average temperature was in Kraków among major cities of the Second Polish Republic, at in 1938; and the coldest in Wilno ( in 1938). Extreme geographical points of Poland included Przeświata River in Somino to the north (located in the Braslaw county of the Wilno Voivodeship); Manczin River to the south (located in the Kosów county of the Stanisławów Voivodeship); Spasibiorki near railway to Połock to the east (located in the Dzisna county of the Wilno Voivodeship); and Mukocinek near Warta River and Meszyn Lake to the west (located in the Międzychód county of the Poznań Voivodeship).\n\nAlmost 75% of the territory of interbellum Poland was drained northward into the Baltic Sea by the Vistula (total area of drainage basin of the Vistula within boundaries of the Second Polish Republic was , the Niemen (), the Odra () and the Daugava (). The remaining part of the country was drained southward, into the Black Sea, by the rivers that drain into the Dnieper (Pripyat, Horyn and Styr, all together ) as well as Dniester ()\n\nThe Second World War in 1939 put an end to the sovereign Second Polish Republic. The German invasion of Poland began on 1 September 1939, one week after Nazi Germany and the Soviet Union signed the secret Molotov–Ribbentrop Pact. On that day, Germany and Slovakia attacked Poland, and on 17 September the Soviets attacked eastern Poland. Warsaw fell to the Nazis on 28 September after a twenty-day siege. Open organized Polish resistance ended on 6 October 1939 after the Battle of Kock, with Germany and the Soviet Union occupying most of the country. Lithuania annexed the area of Wilno, and Slovakia seized areas along Poland's southern border - including Górna Orawa and Tatranská Javorina - which Poland had annexed from Czechoslovakia in October 1938. Poland did not surrender to the invaders, but continued fighting under the auspices of the Polish government-in-exile and of the Polish Underground State. After the signing of the German–Soviet Treaty of Friendship, Cooperation and Demarcation on 28 September 1939, Polish areas occupied by Nazi Germany either became directly annexed to the Third Reich, or became part of the so-called General Government. The Soviet Union, following rigged Elections to the People's Assemblies of Western Ukraine and Western Belarus (22 October 1939), annexed eastern Poland partly to the Byelorussian Soviet Socialist Republic, and partly to the Ukrainian Soviet Socialist Republic (November 1939).\nPolish war plans (Plan West and Plan East) failed as soon as Germany invaded in 1939. The Polish losses in combat against Germans (killed and missing in action) amounted to ca. 70,000 men. Some 420,000 of them were taken prisoners. Losses against the Red Army (which invaded Poland on 17 September) added up to 6,000 to 7,000 of casualties and MIA, 250,000 were taken prisoners. Although the Polish army – considering the inactivity of the Allies – was in an unfavorable position – it managed to inflict serious losses to the enemies: 14,000 German soldiers were killed or MIA, 674 tanks and 319 armored vehicles destroyed or badly damaged, 230 aircraft shot down; the Red Army lost (killed and MIA) about 2,500 soldiers, 150 combat vehicles and 20 aircraft. The Soviet invasion of Poland, and lack of promised aid from the Western Allies, contributed to the Polish forces defeat by 6 October 1939.\nA popular myth is that Polish cavalry armed with lances charged German tanks during the September 1939 campaign. This often repeated account, first reported by Italian journalists as German propaganda, concerned an action by the Polish 18th Lancer Regiment near Chojnice. This arose from misreporting of a single clash on 1 September 1939 near Krojanty, when two squadrons of the Polish 18th Lancers armed with sabers surprised and wiped out a German infantry formation with a mounted sabre charge. Shortly after midnight the 2nd (Motorized) Division was compelled to withdraw by Polish cavalry, before the Poles were caught in the open by German armored cars. The story arose because some German armored cars appeared and gunned down 20 troopers as the cavalry escaped. Even this failed to persuade everyone to reexamine their beliefs—there were some who thought Polish cavalry had been improperly employed in 1939.\n\nBetween 1939 and 1990, the Polish government-in-exile operated in Paris and later in London, presenting itself as the only legal and legitimate representative of the Polish nation. In 1990 the last president in exile, Ryszard Kaczorowski handed the presidential insignia to the newly elected President, Lech Wałęsa, signifying continuity between the Second and Third republics.\n\n\n\n\n\n\n\n", "id": "14245", "title": "Second Polish Republic"}
{"url": "https://en.wikipedia.org/wiki?curid=14246", "text": "Hedwig\n\nHedwig is a feminine German given name, see Hedwig (name). Hedwig may also refer to:\n\n\n\n\n", "id": "14246", "title": "Hedwig"}
{"url": "https://en.wikipedia.org/wiki?curid=14251", "text": "HMS Resolution\n\nSeveral ships of the Royal Navy have borne the name HMS \"Resolution\". However, the first English warship to bear the name \"Resolution\" was actually the first rate (built in 1610 and rebuilt in 1641), which was renamed \"Resolution\" in 1650 following the inauguration of the Commonwealth, and continued to bear that name until 1660, when the name \"Prince Royal\" was restored. The name \"Resolution\" was bestowed on the first of the vessels listed below:\n\n\n\n\n\n", "id": "14251", "title": "HMS Resolution"}
{"url": "https://en.wikipedia.org/wiki?curid=14254", "text": "Helen Keller\n\nHelen Adams Keller (June 27, 1880 – June 1, 1968) was an American author, political activist, and lecturer. She was the first deaf-blind person to earn a bachelor of arts degree. The story of how Keller's teacher, Anne Sullivan, broke through the isolation imposed by a near complete lack of language, allowing the girl to blossom as she learned to communicate, has become widely known through the dramatic depictions of the play and film \"The Miracle Worker\". Her birthplace in West Tuscumbia, Alabama, is now a museum and sponsors an annual \"Helen Keller Day\". Her birthday on June 27 is commemorated as Helen Keller Day in the U.S. state of Pennsylvania and was authorized at the federal level by presidential proclamation by President Jimmy Carter in 1980, the 100th anniversary of her birth.\n\nA prolific author, Keller was well-traveled and outspoken in her convictions. A member of the Socialist Party of America and the Industrial Workers of the World, she campaigned for women's suffrage, labor rights, socialism, antimilitarism, and other similar causes. She was inducted into the Alabama Women's Hall of Fame in 1971 and was one of twelve inaugural inductees to the Alabama Writers Hall of Fame on June 8, 2015. Helen proved to the world that deaf people could all learn to communicate and that they could survive in the hearing world. She also taught that deaf people are capable of doing things that hearing people can do. She is one of the most famous deaf people in history and she is an idol to many deaf people in the world.\n\nHelen Adams Keller was born on June 27, 1880, in Tuscumbia, Alabama. Her family lived on a homestead, Ivy Green, that Helen's grandfather had built decades earlier. She had two younger siblings, Mildred Campbell and Phillip Brooks Keller, and two older half-brothers from her father's prior marriage, James and William Simpson Keller.\n\nHer father, Arthur H. Keller, spent many years as an editor for the Tuscumbia \"North Alabamian\", and had served as a captain for the Confederate Army. Her paternal grandmother was the second cousin of Robert E. Lee. Her mother, Kate Adams, was the daughter of Charles W. Adams, a Confederate general. Though originally from Massachusetts, Charles Adams also fought for the Confederate Army during the American Civil War, earning the rank of colonel (and acting brigadier-general). Her paternal lineage was traced to Casper Keller, a native of Switzerland. One of Helen's Swiss ancestors was the first teacher for the deaf in Zurich. Keller reflected on this coincidence in her first autobiography, stating \"that there is no king who has not had a slave among his ancestors, and no slave who has not had a king among his.\"\nHelen Keller was born with the ability to see and hear. At 19 months old, she contracted an illness described by doctors as \"an acute congestion of the stomach and the brain\", which might have been scarlet fever or meningitis. The illness left her both deaf and blind. At that time, she was able to communicate somewhat with Martha Washington, the six-year-old daughter of the family cook, who understood her signs; by the age of seven, Keller had more than 60 home signs to communicate with her family. Even though blind and Deaf, Helen Keller had passed through many obstacles and she learned to live with her disabilities. She learned how to tell which person was walking by from the vibrations their footsteps would make. The sex and age of the person could be identified by how strong and continuous the steps were.\n\nIn 1886, Keller's mother, inspired by an account in Charles Dickens' \"American Notes\" of the successful education of another deaf and blind woman, Laura Bridgman, dispatched the young Keller, accompanied by her father, to seek out physician J. Julian Chisolm, an eye, ear, nose, and throat specialist in Baltimore, for advice. Chisholm referred the Kellers to Alexander Graham Bell, who was working with deaf children at the time. Bell advised them to contact the Perkins Institute for the Blind, the school where Bridgman had been educated, which was then located in South Boston. Michael Anagnos, the school's director, asked 20-year-old former student Anne Sullivan, herself visually impaired, to become Keller's instructor. It was the beginning of a 49-year-long relationship during which Sullivan evolved into Keller's governess and eventually her companion.\n\nAnne Sullivan arrived at Keller's house in March 1887, and immediately began to teach Helen to communicate by spelling words into her hand, beginning with \"d-o-l-l\" for the doll that she had brought Keller as a present. Keller was frustrated, at first, because she did not understand that every object had a word uniquely identifying it. In fact, when Sullivan was trying to teach Keller the word for \"mug\", Keller became so frustrated she broke the mug. Keller's big breakthrough in communication came the next month, when she realized that the motions her teacher was making on the palm of her hand, while running cool water over her other hand, symbolized the idea of \"water\"; she then nearly exhausted Sullivan demanding the names of all the other familiar objects in her world. Helen Keller was viewed as isolated but she was in fact, very in touch with the outside world. She was able to enjoy music by feeling the beat and she was able to have a strong connection with animals through touch. She was delayed at picking up language, but that did not stop her from having a voice.\n\nStarting in May 1888, Keller attended the Perkins Institute for the Blind. In 1894, Helen Keller and Anne Sullivan moved to New York to attend the Wright-Humason School for the Deaf, and to learn from Sarah Fuller at the Horace Mann School for the Deaf. In 1896, they returned to Massachusetts, and Keller entered The Cambridge School for Young Ladies before gaining admittance, in 1900, to Radcliffe College, where she lived in Briggs Hall, South House. Her admirer, Mark Twain, had introduced her to Standard Oil magnate Henry Huttleston Rogers, who, with his wife Abbie, paid for her education. In 1904, at the age of 24, Keller graduated from Radcliffe, becoming the first deaf blind person to earn a Bachelor of Arts degree. She maintained a correspondence with the Austrian philosopher and pedagogue Wilhelm Jerusalem, who was one of the first to discover her literary talent.\n\nDetermined to communicate with others as conventionally as possible, Keller learned to speak, and spent much of her life giving speeches and lectures. She learned to \"hear\" people's speech by reading their lips with her hands—her sense of touch had become extremely subtle. She became proficient at using braille and reading sign language with her hands as well. Shortly before World War I, with the assistance of the Zoellner Quartet she determined that by placing her fingertips on a resonant tabletop she could experience music played close by.\n\nOn January 22, 1916, Helen Keller and her companion, Anne Sullivan Macy, traveled to the small town of Menomonie in western Wisconsin to deliver a lecture at the Mabel Tainter Memorial Building. Details of her talk were provided in the weekly \"Dunn County News\" on January 22, 1916:\n“A message of optimism, of hope, of good cheer, and of loving service was brought to Menomonie Saturday — a message that will linger long with those fortunate enough to have received it. This message came with the visit of Helen Keller and her teacher, Mrs. John Macy, and both had a hand in imparting it Saturday evening to a splendid audience that filled The Memorial. The wonderful girl who has so brilliantly triumphed over the triple afflictions of blindness, dumbness and deafness, gave a talk with her own lips on “Happiness,” and it will be remembered always as a piece of inspired teaching by those who heard it.”\n\nWhen part of the account was reprinted in the January 20, 2016, edition of the paper under the heading \"From the Files,\" the column compiler added, \"According to those who attended, Helen Keller spoke of the joy that life gave her. She was thankful for the faculties and abilities that she did possess and stated that the most productive pleasures she had were curiosity and imagination. Keller also spoke of the joy of service and the happiness that came from doing things for others . . . Keller imparted that 'helping your fellow men were one’s only excuse for being in this world and in the doing of things to help one’s fellows lay the secret of lasting happiness.' She also told of the joys of loving work and accomplishment and the happiness of achievement. Although the entire lecture lasted only a little over an hour, the lecture had a profound impact on the audience.\"\n\nAnne Sullivan stayed as a companion to Helen Keller long after she taught her. Anne married John Macy in 1905, and her health started failing around 1914. Polly Thomson was hired to keep house. She was a young woman from Scotland who had no experience with deaf or blind people. She progressed to working as a secretary as well, and eventually became a constant companion to Keller.\n\nKeller moved to Forest Hills, Queens, together with Anne and John, and used the house as a base for her efforts on behalf of the American Foundation for the Blind. \"While in her thirties Helen had a love affair, became secretly engaged, and defied her teacher and family by attempting an elopement with the man she loved.\" He was \"Peter Fagan, a young Boston Herald reporter who was sent to Helen's home to act as her private secretary when lifelong companion, Anne, fell ill.\"\n\nAnne Sullivan died in 1936 after a coma, with Keller holding her hand. Keller and Thomson moved to Connecticut. They traveled worldwide and raised funds for the blind. Thomson had a stroke in 1957 from which she never fully recovered, and died in 1960. Winnie Corbally, a nurse whom they originally hired to care for Thomson in 1957, stayed on after her death and was Keller's companion for the rest of her life.\n\nKeller went on to become a world-famous speaker and author. She is remembered as an advocate for people with disabilities, amid numerous other causes.The Deaf community was widely impacted by her. She traveled to twenty-five different countries giving motivational speeches about Deaf people's conditions. She was a suffragette, a pacifist, an opponent of Woodrow Wilson, a radical socialist and a birth control supporter. In 1915 she and George Kessler founded the Helen Keller International (HKI) organization. This organization is devoted to research in vision, health and nutrition. In 1920, she helped to found the American Civil Liberties Union (ACLU). Keller traveled to over 40 countries with Sullivan, making several trips to Japan and becoming a favorite of the Japanese people. Keller met every U.S. President from Grover Cleveland to Lyndon B. Johnson and was friends with many famous figures, including Alexander Graham Bell, Charlie Chaplin and Mark Twain. Keller and Twain were both considered radicals at the beginning of the 20th century, and as a consequence, their political views have been forgotten or glossed-over in the popular mind.\n\nKeller was a member of the Socialist Party and actively campaigned and wrote in support of the working class from 1909 to 1921. Many of her speeches and writings were about women’s right to vote and the impacts of war. She had speech therapy in order to have her voice heard better by the public. When the Rockefeller-owned press refused to print her articles, she protested until her work was finally published. She supported Socialist Party candidate Eugene V. Debs in each of his campaigns for the presidency. Before reading \"Progress and Poverty\", Helen Keller was already a socialist who believed that Georgism was a good step in the right direction. She later wrote of finding \"in Henry George’s philosophy a rare beauty and power of inspiration, and a splendid faith in the essential nobility of human nature.\"\n\nKeller claimed that newspaper columnists who had praised her courage and intelligence before she expressed her socialist views now called attention to her disabilities. The editor of the \"Brooklyn Eagle\" wrote that her \"mistakes sprung out of the manifest limitations of her development.\" Keller responded to that editor, referring to having met him before he knew of her political views: \n\nKeller joined the Industrial Workers of the World (the IWW, known as the Wobblies) in 1912, saying that parliamentary socialism was \"sinking in the political bog\". She wrote for the IWW between 1916 and 1918. In \"Why I Became an IWW\", Keller explained that her motivation for activism came in part from her concern about blindness and other disabilities:\n\nThe last sentence refers to prostitution and syphilis, the former a frequent cause of the latter, and the latter a leading cause of blindness. In the same interview, Keller also cited the 1912 strike of textile workers in Lawrence, Massachusetts for instigating her support of socialism.\n\nLike Alexander Graham Bell and others, Keller supported eugenics.\n\nKeller expressed concerns about human overpopulation.\n\nKeller wrote a total of 12 published books and several articles.\n\nOne of her earliest pieces of writing, at age 11, was \"The Frost King\" (1891). There were allegations that this story had been plagiarized from \"The Frost Fairies\" by Margaret Canby. An investigation into the matter revealed that Keller may have experienced a case of cryptomnesia, which was that she had Canby's story read to her but forgot about it, while the memory remained in her subconscious.\n\nAt age 22, Keller published her autobiography, \"The Story of My Life\" (1903), with help from Sullivan and Sullivan's husband, John Macy. It recounts the story of her life up to age 21 and was written during her time in college.\n\nKeller wrote \"The World I Live In\" in 1908, giving readers an insight into how she felt about the world. \"Out of the Dark\", a series of essays on socialism, was published in 1913.\n\nWhen Keller was young, Anne Sullivan introduced her to Phillips Brooks, who introduced her to Christianity, Keller famously saying: \"I always knew He was there, but I didn't know His name!\"\n\nHer spiritual autobiography, \"My Religion\", was published in 1927 and then in 1994 extensively revised and re-issued under the title \"Light in My Darkness\". It advocates the teachings of Emanuel Swedenborg, the Christian revelator and theologian who gives a spiritual interpretation of the teachings of the Bible and who claims that the second coming of Jesus Christ has already taken place. Adherents use several names to describe themselves, including Second Advent Christian, Swedenborgian, and New Church.\n\nKeller described the progressive views of her belief in these words:\n\nBut in Swedenborg's teaching it [Divine Providence] is shown to be the government of God's Love and Wisdom and the creation of uses. Since His Life cannot be less in one being than another, or His Love manifested less fully in one thing than another, His Providence must needs be universal . . . He has provided religion of some kind everywhere, and it does not matter to what race or creed anyone belongs if he is faithful to his ideals of right living.\n\nWhen Keller visited Akita Prefecture in Japan in July 1937, she inquired about Hachikō, the famed Akita dog that had died in 1935. She told a Japanese person that she would like to have an Akita dog; one was given to her within a month, with the name of Kamikaze-go. When he died of canine distemper, his older brother, Kenzan-go, was presented to her as an official gift from the Japanese government in July 1938. Keller is credited with having introduced the Akita to the United States through these two dogs.\n\nBy 1939, a breed standard had been established, and dog shows had been held, but such activities stopped after World War II began. Keller wrote in the \"Akita Journal\":\nKeller suffered a series of strokes in 1961 and spent the last years of her life at her home.\n\nOn September 14, 1964, President Lyndon B. Johnson awarded her the Presidential Medal of Freedom, one of the United States' two highest civilian honors. In 1965 she was elected to the National Women's Hall of Fame at the New York World's Fair.\n\nKeller devoted much of her later life to raising funds for the American Foundation for the Blind. She died in her sleep on June 1, 1968, at her home, Arcan Ridge, located in Easton, Connecticut, a few weeks short of her eighty-eighth birthday. A service was held in her honor at the National Cathedral in Washington, D.C., her body was cremated and her ashes were placed there next to her constant companions, Anne Sullivan and Polly Thomson. She was buried at the Washington National Cathedral.\n\nKeller's life has been interpreted many times. She appeared in a silent film, \"Deliverance\" (1919), which told her story in a melodramatic, allegorical style.\n\nShe was also the subject of the documentaries \"Helen Keller in Her Story\", narrated by Katharine Cornell, and \"The Story of Helen Keller\", part of the Famous Americans series produced by Hearst Entertainment.\n\n\"The Miracle Worker\" is a cycle of dramatic works ultimately derived from her autobiography, \"The Story of My Life\". The various dramas each describe the relationship between Keller and Sullivan, depicting how the teacher led her from a state of almost feral wildness into education, activism, and intellectual celebrity. The common title of the cycle echoes Mark Twain's description of Sullivan as a \"miracle worker.\" Its first realization was the 1957 \"Playhouse 90\" teleplay of that title by William Gibson. He adapted it for a Broadway production in 1959 and an Oscar-winning feature film in 1962, starring Anne Bancroft and Patty Duke. It was remade for television in 1979 and 2000.\n\nIn 1984, Keller's life story was made into a TV movie called \"The Miracle Continues\". This film that entailed the semi-sequel to \"The Miracle Worker\" recounts her college years and her early adult life. None of the early movies hint at the social activism that would become the hallmark of Keller's later life, although a Disney version produced in 2000 states in the credits that she became an activist for social equality.\n\nThe Bollywood movie \"Black\" (2005) was largely based on Keller's story, from her childhood to her graduation.\n\nA documentary called \"Shining Soul: Helen Keller's Spiritual Life and Legacy\" was produced by the Swedenborg Foundation in the same year. The film focuses on the role played by Emanuel Swedenborg's spiritual theology in her life and how it inspired Keller's triumph over her triple disabilities of blindness, deafness and a severe speech impediment.\n\nOn March 6, 2008, the New England Historic Genealogical Society announced that a staff member had discovered a rare 1888 photograph showing Helen and Anne, which, although previously published, had escaped widespread attention. Depicting Helen holding one of her many dolls, it is believed to be the earliest surviving photograph of Anne Sullivan Macy.\n\nVideo footage showing Helen Keller learning to mimic speech sounds also exists.\n\nA biography of Helen Keller was written by the German Jewish author H.J.Kaeser.\n\nA 10-by-7-foot painting titled \"The Advocate - Tribute to Helen Keller\" was created by three artists from Kerala as a tribute to Helen Keller. The Painting was created in association with a non-profit organization Art d'Hope Foundation, artists groups Palette People and XakBoX Design & Art Studio. This painting was created for a fundraising event to help blind students in India and was inaugurated by M. G. Rajamanikyam, IAS (District Collector Ernakulam) on Helen Keller day (June 27, 2016). The painting depicts the major events of Helen Keller's life and is one of the biggest paintings done based on Helen Keller life.\n\nA preschool for the deaf and hard of hearing in Mysore, India, was originally named after Helen Keller by its founder, K. K. Srinivasan.\nIn 1999, Keller was listed in Gallup's Most Widely Admired People of the 20th century.\n\nIn 2003, Alabama honored its native daughter on its state quarter. The Alabama state quarter is the only circulating US coin to feature braille.\n\nThe Helen Keller Hospital in Sheffield, Alabama, is dedicated to her.\n\nStreets are named after Helen Keller in Zürich, Switzerland, in the USA, in Getafe, Spain, in Lod, Israel, in Lisbon, Portugal and in Caen, France.\n\nA stamp was issued in 1980 by the United States Postal Service depicting Keller and Sullivan, to mark the centennial of Keller's birth.\n\nOn October 7, 2009, a bronze statue of Helen Keller was added to the National Statuary Hall Collection, as a replacement for the State of Alabama's former 1908 statue of the education reformer Jabez Lamar Monroe Curry. It is displayed in the United States Capitol Visitor Center and depicts Keller as a seven-year-old child standing at a water pump. The statue represents the seminal moment in Keller's life when she understood her first word: W-A-T-E-R, as signed into her hand by teacher Anne Sullivan. The pedestal base bears a quotation in raised Latin and braille letters: \"The best and most beautiful things in the world cannot be seen or even touched, they must be felt with the heart.\" The statue is the first one of a person with a disability and of a child to be permanently displayed at the U.S. Capitol.\n\nArchival material of Helen Keller stored in New York was lost when the Twin Towers were destroyed in the September 11 attacks.\n\nThe Helen Keller Archives are owned by the American Foundation for the Blind.\n\n\n\n\n", "id": "14254", "title": "Helen Keller"}
{"url": "https://en.wikipedia.org/wiki?curid=14257", "text": "Haddocks' Eyes\n\nHaddocks' Eyes is a term for the name of a poem by Lewis Carroll from \"Through the Looking-Glass\". It is sung by The White Knight in to a tune that he claims as his own invention, but which Alice recognises as \"I give thee all, I can no more\".\n\nBy the time Alice heard it, she was already tired of poetry.\n\nIt is a parody of \"Resolution and Independence\" by William Wordsworth.\n\nThe White Knight explains a confusing nomenclature for the song.\n\n\nThe complicated terminology distinguishing between 'the song, what the song is called, the name of the song, and what the name of the song is called' both uses and mentions the use–mention distinction.\n\nLike \"Jabberwocky,\" another poem published in \"Through the Looking Glass,\" \"Haddocks’ Eyes\" appears to have been revised over the course of many years. In 1856, Carroll published the following poem anonymously under the name \"Upon the Lonely Moor\". It bears an obvious resemblance to \"Haddocks' Eyes.\"\n\n", "id": "14257", "title": "Haddocks' Eyes"}
{"url": "https://en.wikipedia.org/wiki?curid=14260", "text": "Hoosier\n\nHoosier is the official demonym for a resident of the U.S. state of Indiana. The origin of the term remains a matter of debate within the state, but \"Hoosier\" was in general use by the 1840s, having been popularized by Richmond resident John Finley's 1833 poem \"The Hoosier's Nest\". Anyone born in Indiana or a resident at the time is considered to be a Hoosier. Indiana adopted the nickname \"The Hoosier State\" more than 150 years ago.\n\n\"Hoosier\" is used in the names of numerous Indiana-based businesses and organizations. \"Hoosiers\" is also the name of the Indiana University athletic teams and seven active and one disbanded athletic conferences in the Indiana High School Athletic Association have the word \"Hoosier\" in their name. As there is no accepted embodiment of a Hoosier, the IU schools are represented through their letters and colors alone. In addition to universal acceptance by residents of Indiana, the term is also the official demonym according to the U.S. Government Publishing Office.\nOn January 12, 2017, the Federal Government officially changed the nickname of people from the state of Indiana from \"Indianans\" to \"Hoosiers\", making Indiana the first state to not have a version of their state name in their nickname (\"Illinoisans\", \"Texans\", etc.).\nIn addition to \"The Hoosier's Nest\", the term also appeared in the \"Indianapolis Journal\"<nowiki>'</nowiki>s \"Carrier's Address\" on January 1, 1833. There are many suggestions for the derivation of the word, but none is universally accepted.\n\nIn 1900, Meredith Nicholson wrote \"The Hoosiers\", an early attempt to study the etymology of the word as applied to Indiana residents. Jacob Piatt Dunn, longtime secretary of the Indiana Historical Society, published \"The Word Hoosier\", a similar attempt, in 1907. Both chronicled some of the popular and satirical etymologies circulating at the time and focused much of their attention on the use of the word in the Upland South to refer to woodsmen, yokels, and rough people. Dunn traced the word back to the Cumbrian \"hoozer\", meaning anything unusually large, derived from the Old English \"hoo\" (as at Sutton Hoo), meaning \"high\" and \"hill\". The importance of immigrants from northern England and southern Scotland was reflected in numerous placenames including the Cumberland Mountains, the Cumberland River, and the Cumberland Gap. Nicholson defended the people of Indiana against such an association, while Dunn concluded that the early settlers had adopted the nickname self-mockingly and that it had lost its negative associations by the time of Finley's poem.\n\nJohnathan Clark Smith subsequently showed that Nicholson and Dunn's earliest sources within Indiana were mistaken. A letter by James Curtis cited by Dunn and others as the earliest known use of the term was actually written in 1846, not 1826. Similarly, the use of the term in an 1859 newspaper item quoting an 1827 diary entry by Sandford Cox was more likely an editorial comment and not from the original diary. Smith's earliest sources led him to argue that the word originated as a term along the Ohio River for flatboatmen from Indiana and did not acquire its pejorative meanings until 1836, \"after\" Finley's poem.\nWilliam Piersen, a history professor at Fisk University, argued for a connection to the black Methodist minister Rev. Harry Hosier (–May 1806), who evangelized the American frontier at the beginning of the 19th century as part of the Second Great Awakening. \"Black Harry\" had been born a slave in North Carolina and sold north to Baltimore, Maryland, before gaining his freedom and beginning his ministry around the end of the American Revolution. He was a close associate and personal friend of Bishop Francis Asbury, the \"Father of the American Methodist Church\". Dr. Benjamin Rush said that, \"making allowances for his illiteracy, he was the greatest orator in America\" and his sermons called on Methodists to reject slavery and champion the common working man. Piersen proposed that Methodist communities inspired by his example took or were given a variant spelling of his name (possibly influenced by the \"yokel\" slang) during the decades after his ministry.\n\nJorge Santander Serrano, a PhD student from Indiana University has also suggested that \"Hoosier\" might come from the French words for redness \"\"rougeur\"\" or red-faced \"\"rougeaud\"\". According to this theory, the early pejorative use of the word \"Hoosier\" may have a link to the color red (\"rouge\" in French) which is associated with indigenous peoples, pejoratively called “red men” or “red-skins”, and also with poor white people by calling them “red-necks.”\n\nHumorous folk etymologies for the term \"hoosier\" have a long history, as recounted by Dunn in \"The Word Hoosier\".\n\nOne account traces the word to the necessary caution of approaching houses on the frontier. In order to avoid being shot, a traveler would call out from afar to let themselves be known. The inhabitants of the cabin would then reply \"Who's here?\" which in the Appalachian English of the early settlers slurred into \"Who'sh 'ere?\" and thence into \"Hoosier?\" A variant of this account had the Indiana pioneers calling out \"Who'sh 'ere?\" as a general greeting and warning when hearing someone in the bushes and tall grass, to avoid shooting a relative or friend in error.\n\nThe poet James Whitcomb Riley facetiously suggested that the fierce brawling that took place in Indiana involved enough biting that the expression \"Whose ear?\" became notable. This arose from or inspired the story of two 19th-century French immigrants brawling in a tavern in the foothills of southern Indiana. One was cut and a third Frenchman walked in to see an ear on the dirt floor of the tavern, prompting him to slur out \"Whosh ear?\"\n\nTwo related stories trace the origin of the term to gangs of workers from Indiana under the direction of a Mr. Hoosier.\n\nThe account related by Dunn is that a Louisville contractor named Samuel Hoosier preferred to hire workers from communities on the Indiana side of the Ohio River like New Albany rather than Kentuckians. During the excavation of the first canal around the Falls of the Ohio from 1826 to 1833, his employees became known as \"Hoosier's men\" and then simply \"Hoosiers\". The usage spread from these hard-working laborers to all of the Indiana boatmen in the area and then spread north with the settlement of the state. The story was told to Dunn in 1901 by a man who had heard it from a Hoosier relative while traveling in southern Tennessee. Dunn could not find any family of the given name in any directory in the region or anyone else in southern Tennessee who had heard the story and accounted himself dubious. This version was subsequently retold by Gov. Evan Bayh and Sen. Vance Hartke, who introduced the story into the \"Congressional Record\" in 1975, and matches the timing and location of Smith's subsequent research. However, the U.S. Army Corps of Engineers has been unable to find any record of a Hoosier or Hosier in surviving canal company records.\n\nThe word \"hoosier\" is still used in Greater St. Louis to denote a \"yokel\" or \"white trash\". The word is also encountered in sea shanties. In the book \"Shanties from the Seven Seas\" by Stan Hugill, in reference to its former use to denote cotton-stowers, who would move bales of cotton to and from the holds of ships and force them in tightly by means of jackscrews. \"To hoosier\" is sometimes still encountered as a verb meaning \"to trick\" or \"to swindle\".\n\nA Hoosier cabinet, often shortened to \"hoosier\", is a type of free-standing kitchen cabinet popular in the early decades of the twentieth century. Almost all of these cabinets were produced by companies located in Indiana and the name derives from the largest of them, the Hoosier Manufacturing Co. of New Castle, Indiana. Other Indiana businesses include Hoosier Racing Tire and the Hoosier Bat Company, manufacturer of wooden baseball bats.\n\nThe RCA Dome, former home of the Indianapolis Colts, was known as the \"Hoosier Dome\" before RCA purchased the naming rights in 1994. The RCA Dome was replaced by Lucas Oil Stadium in 2008.\n\n\n", "id": "14260", "title": "Hoosier"}
{"url": "https://en.wikipedia.org/wiki?curid=14263", "text": "Horner's method\n\nIn mathematics, Horner's method (also known as Horner scheme in the UK or Horner's rule in the U.S.) is either of two things:\nThe latter is also known as Ruffini–Horner's method.\n\nThese methods are named after the British mathematician William George Horner, although they were known before him by Paolo Ruffini and, six hundred years earlier, by the Chinese mathematician Qin Jiushao.\n\nGiven the polynomial\n\nwhere formula_2 are real numbers, we wish to evaluate the polynomial at a specific value of formula_3, say formula_4.\n\nTo accomplish this, we define a new sequence of constants as follows:\n\nThen formula_6 is the value of formula_7.\n\nTo see why this works, note that the polynomial can be written in the form\n\nThus, by iteratively substituting the formula_9 into the expression,\n\nEvaluate formula_11 for formula_12\n\nWe use synthetic division as follows:\n\nThe entries in the third row are the sum of those in the first two. Each entry in the second row is the product of the \"x\"-value (3 in this example) with the third-row entry immediately to the left. The entries in the first row are the coefficients of the polynomial to be evaluated. Then the remainder of formula_13 on division by formula_14 is 5.\n\nBut by the polynomial remainder theorem, we know that the remainder is formula_15. Thus formula_16\n\nIn this example, if formula_17 we can see that formula_18, the entries in the third row. So, synthetic division is based on Horner's method.\n\nAs a consequence of the polynomial remainder theorem, the entries in the third row are the coefficients of the second-degree polynomial, the quotient of formula_13 on division by formula_20. \nThe remainder is 5. This makes Horner's method useful for polynomial long division.\n\nDivide formula_21 by formula_22:\n\nThe quotient is formula_23.\n\nLet formula_24 and formula_25. Divide formula_26 by formula_27 using Horner's method.\n\nThe third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer is\n\nHorner's method is a fast, code-efficient method for multiplication and division of binary numbers on a microcontroller with no hardware multiplier. One of the binary numbers to be multiplied is represented as a trivial polynomial, where, (using the above notation): a = 1, and x = 2. Then, x (or x to some power) is repeatedly factored out. In this binary numeral system (base 2), x = 2, so powers of 2 are repeatedly factored out.\n\nFor example, to find the product of two numbers, (0.15625) and \"m\":\n\nTo find the product of two binary numbers, d and m:\n\nIn general, for a binary number with bit values: (formula_30) the product is:\nAt this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or division by zero is not an issue, despite this implication in the factored equation:\n\nThe denominators all equal one (or the term is absent), so this reduces to:\nor equivalently (as consistent with the \"method\" described above):\n\nIn binary (base 2) math, multiplication by a power of 2 is merely a register shift operation. Thus, multiplying by 2 is calculated in base-2 by an arithmetic shift. The factor (2) is a right arithmetic shift, a (0) results in no operation (since 2 = 1, is the multiplicative identity element), and a (2) results in a left arithmetic shift.\nThe multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.\n\nThe method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate. Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the \"canonical signed digit\" (CSD) form is used), and uses only 20% of the code space.\n\nUsing Horner's method in combination with Newton's method, it is possible to approximate the real roots of a polynomial. The algorithm works as follows. Given a polynomial formula_35 of degree formula_36 with zeros formula_37 make some initial guess formula_38 such that formula_39. Now iterate the following two steps:\n\n1. Using Newton's method, find the largest zero formula_40 of formula_35 using the guess formula_4.\n\n2. Using Horner's method, divide out formula_43 to obtain formula_44. Return to step 1 but use the polynomial formula_44 and the initial guess formula_40.\n\nThese two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials.\n\nConsider the polynomial\n\nwhich can be expanded to\n\nFrom the above we know that the largest root of this polynomial is 7 so we are able to make an initial guess of 8. Using Newton's method the first zero of 7 is found as shown in black in the figure to the right. Next formula_49 is divided by formula_50 to obtain\n\nwhich is drawn in red in the figure to the right. Newton's method is used to find the largest zero of this polynomial with an initial guess of 7. The largest zero of this polynomial which corresponds to the second largest zero of the original polynomial is found at 3 and is circled in red. The degree 5 polynomial is now divided by formula_52 to obtain\n\nwhich is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtain\n\nwhich is shown in green and found to have a zero at −3. This polynomial is further reduced to\n\nwhich is shown in blue and yields a zero of −5. The final root of the original polynomial may be found by either using the final zero as an initial guess for Newton's method, or by reducing formula_56 and solving the linear equation. As can be seen, the expected roots of −8, −5, −3, 2, 3, and 7 were found.\n\nThe following Octave code was used in the example above to implement Horner's method.\n\nThe following Python code implements Horner's method.\n\nThe following C code implements Horner's method.\n\nHere is a slightly optimized version using explicit fused Multiply–accumulate operation, often execute faster than the above when running on a computer built with a processor supporting FMA instruction:\n\nHorner's method can be used to convert between different positional numeral systems – in which case \"x\" is the base of the number system, and the \"a\" coefficients are the digits of the base-\"x\" representation of a given number – and can also be used if \"x\" is a matrix, in which case the gain in computational efficiency is even greater. In fact, when \"x\" is a matrix, further acceleration is possible which exploits the structure of matrix multiplication, and only formula_57 instead of \"n\" multiplies are needed (at the expense of requiring more storage) using the 1973 method of Paterson and Stockmeyer.\n\nEvaluation using the monomial form of a degree-\"n\" polynomial requires at most \"n\" additions and (\"n\" + \"n\")/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually. (This can be reduced to \"n\" additions and 2\"n\" − 1 multiplications by evaluating the powers of \"x\" iteratively.) If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2\"n\" times the number of bits of \"x\" (the evaluated polynomial has approximate magnitude \"x\", and one must also store \"x\" itself). By contrast, Horner's method requires only \"n\" additions and \"n\" multiplications, and its storage requirements are only \"n\" times the number of bits of \"x\". Alternatively, Horner's method can be computed with \"n\" fused multiply–adds. Horner's method can also be extended to evaluate the first \"k\" derivatives of the polynomial with \"kn\" additions and multiplications.\n\nHorner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. Alexander Ostrowski proved in 1954 that the number of additions required is minimal. Victor Pan proved in 1966 that the number of multiplications is minimal. However, when \"x\" is a matrix, Horner's method is not optimal.\n\nThis assumes that the polynomial is evaluated in monomial form and no preconditioning of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-\"n\" polynomial can be evaluated using only formula_58 multiplications and \"n\" additions.\n\nHorner's paper entitled \"A new method of solving numerical equations of all orders, by continuous approximation\" was read before the Royal Society of London, at its meeting on July 1, 1819, with Davies Gilbert, Vice-President and Treasurer, in the chair; this was the final meeting of the session before the Society adjorned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by James Ivory, FRS, were also read. In 1819, it was Horner's paper that got through to publication in the \"Philosophical Transactions\". later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. But Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own survey of Horner-type methods earlier in 1823.\n\nHorner's paper in Part II of \"Philosophical Transactions of the Royal Society of London\" for 1819 was warmly and expansively welcomed by a reviewer in the issue of \"The Monthly Review: or, Literary Journal\" for April, 1820; in comparison, a technical paper by Charles Babbage is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been published by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further review of some of Nicholson's books in the issue of \"The Monthly Review\" for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of \"The Monthly Review\" for September, 1821, with the reviewer concluding that whereas Holdred was the first person to discover a direct and general practical solution of numerical equations, he had not reduced it to its simplest form by the time of Horner's publication, and saying that had Holdred published forty years earlier when he first discovered his method, his contribution could be more easily recognized. The reviewer is exceptionally well-informed, even having sighted Horner's preparatory correspondence with Peter Barlow in 1818, seeking work of Budan. The Bodlean Library, Oxford has the Editor's annotated copy of \"The Monthly Review\" from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow,one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow Geordie, Charles Hutton, another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.\n\nThe feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of Arbogast. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of Paolo Ruffini, except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in \"Philosophical Transactions\" speak volumes: there are none - Ruffini's name only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had Malfatti's Problem in the reformulation of Joseph Diaz Gergonne, or had he written in French, as had , a source quoted by Bonneycastle on series reversion (today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English).\n\nFuller showed that the method in Horner's 1819 paper differs from what afterwards became known as 'Horner's method' and that in consequence the priority for this method should go to Holdred (1920). This view may be compared with the remarks concerning the works of Horner and Holdred in the previous paragraph. Fuller also takes aim at Augustus De Morgan. Precocious though Augustus de Morgan was, he was not the reviewer for \"The Monthly Review\", while several others - Thomas Stephens Davies, J. R. Young, Stephen Fenwick, T. T. Wilkinson - wrote Horner firmly into their records, not least Horner himself, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to \"The Gentleman's Mathematical Companion\", an answer to a problem.\n\nIt is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery that led to \"Horner's method\" being so called in textbooks, but it is true that those suggesting this tend themselves to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method \"qua\" method was known long before Horner. In reverse chronological order, Horner's method was already known to:\n\n\nHowever, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.\n\nQin Jiushao, in his \"Shu Shu Jiu Zhang\" (\"Mathematical Treatise in Nine Sections\"; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician Jia Xian; for example, one method is specifically suited to bi-quintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was Alexander Wylie, writing in \"The North China Herald\" in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method \"Harmoniously Alternating Evolution\" (which does not agree with his Chinese, \"linglong kaifang\", not that at that date he uses pinyin), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. Yoshio Mikami in \"Development of Mathematics in China and Japan\" published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote: \"who can deny the fact of Horner's illustrious process being used in China at least nearly six long centuries earlier than in Europe ... We of course don't intend in any way to ascribe Horner's invention to a Chinese origin, but the lapse of time sufficiently makes it not altogether impossible that the Europeans could have known of the Chinese method in a direct or indirect way.\". However, as Mikami is also aware, it was \"not altogether impossible\" that a related work, \"Si Yuan Yu Jian\" (\"Jade Mirror of the Four Unknowns; 1303)\" by Zhu Shijie might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, \"Suan Xue Qi Meng\", had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. Ulrich Libbrecht (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: \"It is obvious that this procedure is a Chinese invention...the method was not known in India\". He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese. Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by Liu Hui in connection with Problems IV.16 and 22 in \"Jiu Zhang Suan Shu\", while Wang Xiaotong in the 7th century supposes his readers can solve cubics by an approximation method described in his book Jigu Suanjing.\n\n\n\n", "id": "14263", "title": "Horner's method"}
{"url": "https://en.wikipedia.org/wiki?curid=14268", "text": "Hapworth 16, 1924\n\n\"Hapworth 16, 1924\" is the \"youngest\" of J. D. Salinger's Glass family stories, in the sense that the narrated events happen chronologically before those in the rest of the \"Glass series\". It appeared in the June 19, 1965 edition of \"The New Yorker\", infamously taking up almost the entire magazine. Though he lived for nearly 45 years after its publication, this story was the last original work of Salinger's to be published in his lifetime. \n\nBoth contemporary and later literary critics harshly panned \"Hapworth 16, 1924\"; writing in \"The New York Times\", Michiko Kakutani called it \"a sour, implausible and, sad to say, completely charmless story,\" \"filled with digressions, narcissistic asides and ridiculous shaggy-dog circumlocutions.\" Even kind critics have regarded the work as \"a long-winded sob story\" which many have found to be \"simply unreadable,\" and it has been speculated that this negative response was the reason Salinger decided to quit publishing. Conversely, Salinger is said to have considered the story a \"high point of his writing\" and made tentative steps to have it reprinted; nonetheless, these efforts came to nothing.\n\nThe story is presented in the form of a letter from camp written by a seven-year-old Seymour Glass (the main character of \"A Perfect Day for Bananafish\"). In this respect, the plot is identical to Salinger's previous unpublished story \"The Ocean Full of Bowling Balls,\" written eighteen years earlier in 1947. In the course of requesting a veritable library of reading matter from home, Seymour predicts his brother's success as a writer as well as his own death and condemns the ironic \"twist\" endings in the stories of Anatole France, twist endings being an early Salinger device.\n\nAfter the story's appearance in \"The New Yorker\", Salinger—who had already withdrawn to his home in New Hampshire—stopped publishing altogether. Since he never put the story between hard covers, readers had to seek out a copy of that issue or find it on microfilm. Finally, with the release of \"The Complete New Yorker\" on DVD in 2005, the story was once again widely available.\n\nIn the meantime, however, in 1996, Orchises Press, a small publishing house in Virginia, started the process to publish \"Hapworth\" in book form. In an article in \"The Washington Post\", published after Salinger's death, and in a story for \"New York\", Orchises Press owner Roger Lathbury described his efforts to publish the story. According to Lathbury, Salinger was deeply concerned with the proposed book's appearance, even visiting Washington to examine the cloth for the binding. Salinger also sent Lathbury numerous \"infectious and delightful and loving\" letters.\n\nLathbury, following publishing norms, applied for Library of Congress Cataloging in Publication data, unaware of how publicly available the information would be. A writer in Seattle, researching an article on Jeff Bezos, the founder of the then-fledgling Amazon.com, came across the \"Hapworth\" publication date, told his sister, a journalist for the \"Washington Business Journal\", who wrote an article about the upcoming book. This led to substantial coverage in the press. Shortly before the books were to be shipped, Salinger changed his mind, and in accordance with his wishes, Orchises withdrew the work. Although new publication dates were repeatedly announced, the book never appeared. Lathbury said, \"I never reached back out. I thought about writing some letters, but it wouldn't have done any good.\" Three months after Salinger's death, Lathbury published an account of the experience in \"New York Magazine\".\n\n\n", "id": "14268", "title": "Hapworth 16, 1924"}
{"url": "https://en.wikipedia.org/wiki?curid=14269", "text": "Hypnotic\n\nHypnotic (from Greek \"Hypnos\", sleep) or soporific drugs, commonly known as sleeping pills, are a class of psychoactive drugs whose primary function is to induce sleep and to be used in the treatment of insomnia (sleeplessness), or surgical anesthesia.\n\nThis group is related to sedatives. Whereas the term \"sedative\" describes drugs that serve to calm or relieve anxiety, the term \"hypnotic\" generally describes drugs whose main purpose is to initiate, sustain, or lengthen sleep. Because these two functions frequently overlap, and because drugs in this class generally produce dose-dependent effects (ranging from anxiolysis to loss of consciousness) they are often referred to collectively as sedative-hypnotic drugs.\n\nHypnotic drugs are regularly prescribed for insomnia and other sleep disorders, with over 95% of insomnia patients being prescribed hypnotics in some countries. Many hypnotic drugs are habit-forming and, due to a large number of factors known to disturb the human sleep pattern, a physician may instead recommend changes in the environment before and during sleep, better sleep hygiene, and the avoidance of caffeine or other stimulating substances before prescribing medication for sleep. When prescribed, hypnotic medication should be used for the shortest period of time possible.\n\nMost hypnotics prescribed today are either benzodiazepines or nonbenzodiazepines. Among individuals with sleep disorders, 13.7% are taking or prescribed nonbenzodiazepines, while 10.8% are taking benzodiazepines, as of 2010. Early classes of drugs, such as barbiturates, have fallen out of use in most practices but are still prescribed for some patients. In children, prescribing hypnotics is not yet acceptable unless used to treat night terrors or somnambulism. Elderly people are more sensitive to potential side effects of daytime fatigue and cognitive impairments, and a meta-analysis found that the risks generally outweigh any marginal benefits of hypnotics in the elderly. A review of the literature regarding benzodiazepine hypnotics and Z-drugs concluded that these drugs can have adverse effects, such as dependence and accidents, and that optimal treatment uses the lowest effective dose for the shortest therapeutic time period, with gradual discontinuation in order to improve health without worsening of sleep.\n\nFalling outside of the above-mentioned categories, the neuro-hormone melatonin has a hypnotic function.\n\nHypnotica was a class of somniferous drugs and substances tested in medicine of the 1890s and later, including: Urethan, Acetal, Methylal, Sulfonal, Paraldehyd, Amylenhydrate, Hypnon, Chloralurethan and Ohloralamid or Chloralimid.\n\nResearch about using medications to treat insomnia evolved throughout the last half of the 20th century. Treatment for insomnia in psychiatry dates back to 1869 when chloral hydrate was first used as a soporific. Barbiturates emerged as the first class of drugs that emerged in the early 1900s, after which chemical substitution allowed derivative compounds. Although the best drug family at the time (less toxic and with fewer side effects) they were dangerous in overdose.\n\nDuring the 1970s, quinazolinones and benzodiazepines were introduced as safer alternatives to replace barbiturates; by the late 1970s benzodiazepines emerged as the safer drug.\n\nBenzodiazepines are not without their drawbacks; substance dependence is possible, and deaths from overdoses sometimes occur, especially in combination with alcohol and/or other depressants. Questions have been raised as to whether they disturb sleep architecture.\n\nNonbenzodiazepines are the most recent development (1990s–present). Although it's clear that they are less toxic than their predecessors, barbiturates, comparative efficacy over benzodiazepines have not been established. Without longitudinal studies, it is hard to determine; however some psychiatrists recommend these drugs, citing research suggesting they are equally potent with less potential for abuse.\n\nOther sleep remedies that may be considered \"sedative-hypnotics\" exist; psychiatrists will sometimes prescribe medicines off-label if they have sedating effects. Examples of these include mirtazapine (an antidepressant), clonidine (generally prescribed to regulate blood pressure), quetiapine (an antipsychotic), and the over-the-counter sleep aid diphenhydramine (Benadryl – an antihistamine). Off-label sleep remedies are particularly useful when first-line treatment is unsuccessful or deemed unsafe (for example, in patients with a history of substance abuse).\n\nBarbiturates are drugs that act as central nervous system depressants, and can therefore produce a wide spectrum of effects, from mild sedation to total anesthesia. They are also effective as anxiolytics, hypnotics, and anticonvulsants. Barbiturates also have analgesic effects; however, these effects are somewhat weak, preventing barbiturates from being used in surgery in the absence of other analgesics. They have dependence liability, both physical and psychological. Barbiturates have now largely been replaced by benzodiazepines in routine medical practice – for example, in the treatment of anxiety and insomnia – mainly because benzodiazepines are significantly less dangerous in overdose. However, barbiturates are still used in general anesthesia, for epilepsy, and assisted suicide. Barbiturates are derivatives of barbituric acid.\n\nThe principal mechanism of action of barbiturates is believed to be positive allosteric modulation of GABA receptors.\n\nExamples include amobarbital, pentobarbital, phenobarbital, secobarbital, and sodium thiopental.\n\nQuinazolinones are also a class of drugs which function as hypnotic/sedatives that contain a 4-quinazolinone core. Their use has also been proposed in the treatment of cancer.\n\nExamples of quinazolinones include cloroqualone, diproqualone, etaqualone (Aolan, Athinazone, Ethinazone), mebroqualone, mecloqualone (Nubarene, Casfen), and methaqualone (Quaalude).\n\nBenzodiazepines can be useful for short-term treatment of insomnia. Their use beyond 2 to 4 weeks is not recommended due to the risk of dependence. It is preferred that benzodiazepines be taken intermittently and at the lowest effective dose. They improve sleep-related problems by shortening the time spent in bed before falling asleep, prolonging the sleep time, and, in general, reducing wakefulness. Like alcohol, benzodiazepines are commonly used to treat insomnia in the short-term (both prescribed and self-medicated), but worsen sleep in the long-term. While benzodiazepines can put people to sleep (i.e., inhibit NREM stage 1 and 2 sleep), while asleep, the drugs disrupt sleep architecture: decreasing sleep time, delaying time to REM sleep, and decreasing deep slow-wave sleep (the most restorative part of sleep for both energy and mood).\n\nOther drawbacks of hypnotics, including benzodiazepines, are possible tolerance to their effects, rebound insomnia, and reduced slow-wave sleep and a withdrawal period typified by rebound insomnia and a prolonged period of anxiety and agitation. The list of benzodiazepines approved for the treatment of insomnia is fairly similar among most countries, but which benzodiazepines are officially designated as first-line hypnotics prescribed for the treatment of insomnia can vary distinctly between countries. Longer-acting benzodiazepines such as nitrazepam and diazepam have residual effects that may persist into the next day and are, in general, not recommended.\n\nIt is not clear as to whether the new nonbenzodiazepine hypnotics (Z-drugs) are better than the short-acting benzodiazepines. The efficacy of these two groups of medications is similar. According to the US Agency for Healthcare Research and Quality, indirect comparison indicates that side-effects from benzodiazepines may be about twice as frequent as from nonbenzodiazepines. Some experts suggest using nonbenzodiazepines preferentially as a first-line long-term treatment of insomnia. However, the UK National Institute for Health and Clinical Excellence (NICE) did not find any convincing evidence in favor of Z-drugs. A NICE review pointed out that short-acting Z-drugs were inappropriately compared in clinical trials with long-acting benzodiazepines. There have been no trials comparing short-acting Z-drugs with appropriate doses of short-acting benzodiazepines. Based on this, NICE recommended choosing the hypnotic based on cost and the patient's preference.\n\nOlder adults should not use benzodiazepines to treat insomnia unless other treatments have failed to be effective. When benzodiazepines are used, patients, their caretakers, and their physician should discuss the increased risk of harms, including evidence which shows twice the incidence of traffic collisions among driving patients as well as falls and hip fracture for all older patients.\n\nTheir mechanism of action is primarily at GABA receptors.\n\nNonbenzodiazepines are a class of psychoactive drugs that are very \"benzodiazepine-like\" in nature. Nonbenzodiazepines pharmacodynamics are almost entirely the same as benzodiazepine drugs and therefore employ similar benefits, side-effects, and risks. Nonbenzodiazepines, however, have dissimilar or entirely different chemical structures, and therefore are unrelated to benzodiazepines on a molecular level.\n\nExamples include zopiclone (Imovane, Zimovane), eszopiclone (Lunesta), zaleplon (Sonata), and zolpidem (Ambien, Stilnox, Stilnoct).\n\nResearch on nonbenzodiazepines is new and conflicting. A review by a team of researchers suggests the use of these drugs for people that have trouble falling asleep but not staying asleep, as next-day impairments were minimal. The team noted that the safety of these drugs had been established, but called for more research into their long-term effectiveness in treating insomnia. Other evidence suggests that tolerance to nonbenzodiazepines may be slower to develop than with benzodiazepines. A different team was more skeptical, finding little benefit over benzodiazepines.\n\nMelatonin, the hormone produced in the pineal gland in the brain and secreted in dim light and darkness, among its other functions, promotes sleep in diurnal mammals. Due to its hypnotic properties, it is available on prescription in many countries and is over-the-counter in others. A timed-release version, trade name Circadin®, was approved in 2007 in Europe (EU) for use as a treatment for primary insomnia.\n\nAt the beginning of the 21st century, several melatonin receptor agonists that bind to and activate melatonin receptors were developed. These analogues, prescribed for several sleep disorders, include ramelteon, agomelatine, TIK-301 and tasimelteon. Ramelteon (Rozerem®) was approved for treatment of insomnia in the US in 2005. In 2009 agomelatine (Valdoxan®, Melitor®, Thymanax®), primarily used for depression, was approved in Europe. Both TIK-301 (in 2004) and tasimelteon (Hetlioz®) ten years later were approved in the US for the circadian rhythm sleep disorder non-24-hour sleep–wake disorder in totally blind individuals.\n\nIn common use, the term \"antihistamine\" refers only to compounds that inhibit action at the H receptor (and not H, etc.).\n\nClinically, H antagonists are used to treat certain allergies. Sedation is a common side-effect, and some H antagonists, such as diphenhydramine (Benadryl) and doxylamine, are also used to treat insomnia.\n\nSecond-generation antihistamines cross the blood–brain barrier to a much lower degree than the first ones. This results in their primarily affecting peripheral histamine receptors, and therefore having a much lower sedative effect. High doses can still induce the central nervous system effect of drowsiness.\n\nSome antidepressants have sedating effects. Some \"may\" increase actual quality of sleep (biologically) in contrast to Benzodiazepines that decrease quality.\n\nExamples include:\n\nExamples include:\n\nExamples of antipsychotics with sedation as a side effect:\n\n\n\n\n\n\n\n\n", "id": "14269", "title": "Hypnotic"}
{"url": "https://en.wikipedia.org/wiki?curid=14273", "text": "HMS Dunraven\n\nHMS \"Dunraven\" was a Q-Ship of the Royal Navy during World War I.\n\nOn 8 August 1917, 130 miles southwest of Ushant in the Bay of Biscay, disguised as the collier \"Boverton\" and commanded by Gordon Campbell, VC, \"Dunraven\" spotted , commanded by \"Oberleutnant zur See\" Reinhold Saltzwedel. Saltzwedel believed the disguised ship was a merchant vessel. The U-boat submerged and closed with \"Dunraven\" before surfacing astern at 11:43 am and opening fire at long range. \"Dunraven\" made smoke and sent off a panic party (a small number of men who \"abandon ship\" during an attack to continue the impersonation of a merchant).\n\nShells began hitting \"Dunraven\", detonating her depth charges and setting her stern afire. Her crew remained hidden letting the fires burn. Then a 4 inch (102 mm) gun and crew were blown away revealing \"Dunraven\"s identity as a warship, and \"UC-71\" submerged. A second \"panic party\" abandoned ship. \"Dunraven\" was hit by a torpedo. A third \"panic party\" went over the side, leaving only two guns manned. \"UC-71\" surfaced, shelled \"Dunraven\" and again submerged. Campbell replied with two torpedoes that missed, and around 3 pm, the undamaged U-boat left that area. Only one of \"Dunraven\"s crew was killed, but the Q-Ship was sinking.\n\nThe British destroyer picked up \"Dunraven\"s survivors and took her in tow for Plymouth, but \"Dunraven\" sank at 1:30 am early on 10 August 1917 to the north of Ushant.\n\nIn recognition, two Victoria Crosses were awarded, one to the ship's First Lieutenant, Lt. Charles George Bonner RNR, and the other, by ballot, to a gunlayer, Petty Officer Ernest Herbert Pitcher.\n\nCaptain Campbell later wrote:\n\nCaptain Campbell had been previously awarded the Victoria Cross, in February 1917, for the sinking of .\n", "id": "14273", "title": "HMS Dunraven"}
{"url": "https://en.wikipedia.org/wiki?curid=14275", "text": "Hacker ethic\n\nHacker ethic is a term for the moral values and philosophy that are common in the hacker community. Whilst the philosophy originated at the Massachusetts Institute of Technology in the 1950s-1960s, the term \"hacker ethic\" is attributed to journalist Steven Levy as described in his 1984 book titled \".\" The key points within this ethic are access, freedom of information, and improvement to quality of life. While some tenets of hacker ethic were described in other texts like \"Computer Lib/Dream Machines\" (1974) by Ted Nelson, Levy appears to have been the first to document both the philosophy and the founders of the philosophy.\n\nLevy explains that MIT housed an early IBM 704 computer inside the Electronic Accounting Machinery (EAM) room in 1959. This room became the staging grounds for early hackers, as MIT students from the Tech Model Railroad Club sneaked inside the EAM room after hours to attempt programming the 30-ton, computer.\n\nThe MIT group defined a \"hack\" as a project undertaken or a product built to fulfill some constructive goal, but also with some wild pleasure taken in mere involvement. The term \"hack\" arose from MIT lingo, as the word had long been used to describe college pranks that MIT students would regularly devise. However, Levy's hacker ethic also has often been quoted out of context and misunderstood to refer to hacking as in breaking into computers, and so many sources incorrectly imply that it is describing the ideals of white-hat hackers. However, what Levy is talking about does not necessarily have anything particular to do with computer security, but addresses broader issues.\n\nThe hacker ethic was described as a \"new way of life, with a philosophy, an ethic and a dream\". However, the elements of the hacker ethic were not openly debated and discussed; rather they were implicitly accepted and silently agreed upon.\n\nThe free software movement was born in the early 1980s from followers of the hacker ethic. Its founder, Richard Stallman, is referred to by Steven Levy as \"the last true hacker\". Modern hackers who hold true to the hacker ethics—especially the Hands-On Imperative—are usually supporters of free and open source software. This is because free and open source software allows hackers to get access to the source code used to create the software, to allow it to be improved or reused in other projects.\n\nRichard Stallman describes:\n\nThe hacker ethic refers to the feelings of right and wrong, to the ethical ideas this community of people had—that knowledge should be shared with other people who can benefit from it, and that important resources should be utilized rather than wasted.\n\nand states more precisely that hacking (which Stallman defines as playful cleverness) and ethics are two separate issues:\n\nJust because someone enjoys hacking does not mean he has an ethical commitment to treating other people properly. Some hackers care about ethics—I do, for instance—but that is not part of being a hacker, it is a separate trait. [...] Hacking is not primarily about an ethical issue. [...] hacking tends to lead a significant number of hackers to think about ethical questions in a certain way. I would not want to completely deny all connection between hacking and views on ethics.\n\nAs Levy summarized in the preface of \"Hackers\", the general tenets or principles of hacker ethic include:\n\n\nIn addition to those principles, Levy also described more specific hacker ethics and beliefs in chapter 2, \"The Hacker Ethic\": The ethics he described in chapter 2 are:\n\n\nFrom the early days of modern computing through to the 1970s, it was far more common for computer users to have the freedoms that are provided by an ethic of open sharing and collaboration. Software, including source code, was commonly shared by individuals who used computers. Most companies had a business model based on hardware sales, and provided or bundled the associated software free of charge. According to Levy's account, sharing was the norm and expected within the non-corporate hacker culture. The principle of sharing stemmed from the open atmosphere and informal access to resources at MIT. During the early days of computers and programming, the hackers at MIT would develop a program and share it with other computer users.\n\nIf the hack was deemed particularly good, then the program might be posted on a board somewhere near one of the computers. Other programs that could be built upon it and improved it were saved to tapes and added to a drawer of programs, readily accessible to all the other hackers. At any time, a fellow hacker might reach into the drawer, pick out the program, and begin adding to it or \"bumming\" it to make it better. Bumming referred to the process of making the code more concise so that more can be done in fewer instructions, saving precious memory for further enhancements.\n\nIn the second generation of hackers, sharing was about sharing with the general public in addition to sharing with other hackers. A particular organization of hackers that was concerned with sharing computers with the general public was a group called Community Memory. This group of hackers and idealists put computers in public places for anyone to use. The first community computer was placed outside of Leopold's Records in Berkeley, California.\n\nAnother sharing of resources occurred when Bob Albrecht provided considerable resources for a non-profit organization called the People's Computer Company (PCC). PCC opened a computer center where anyone could use the computers there for fifty cents per hour.\n\nThis second generation practice of sharing contributed to the battles of free and open software. In fact, when Bill Gates' version of BASIC for the Altair was shared among the hacker community, Gates claimed to have lost a considerable sum of money because few users paid for the software. As a result, Gates wrote an Open Letter to Hobbyists. This letter was published by several computer magazines and newsletters, most notably that of the Homebrew Computer Club where much of the sharing occurred.\n\nMany of the principles and tenets of hacker ethic contribute to a common goal: the Hands-On Imperative. As Levy described in Chapter 2, \"Hackers believe that essential lessons can be learned about the systems—about the world—from taking things apart, seeing how they work, and using this knowledge to create new and more interesting things.\"\n\nEmploying the Hands-On Imperative requires free access, open information, and the sharing of knowledge. To a true hacker, if the Hands-On Imperative is restricted, then the ends justify the means to make it unrestricted \"so that improvements can be made\". When these principles are not present, hackers tend to work around them. For example, when the computers at MIT were protected either by physical locks or login programs, the hackers there systematically worked around them in order to have access to the machines. Hackers assumed a \"willful blindness\" in the pursuit of perfection.\n\nThis behavior was not malicious in nature: the MIT hackers did not seek to harm the systems or their users. This deeply contrasts with the modern, media-encouraged image of hackers who crack secure systems in order to steal information or complete an act of cyber-vandalism.\n\nThroughout writings about hackers and their work processes, a common value of community and collaboration is present. For example, in Levy's \"Hackers\", each generation of hackers had geographically based communities where collaboration and sharing occurred. For the hackers at MIT, it was the labs where the computers were running. For the hardware hackers (second generation) and the game hackers (third generation) the geographic area was centered in Silicon Valley where the Homebrew Computer Club and the People's Computer Company helped hackers network, collaborate, and share their work.\n\nThe concept of community and collaboration is still relevant today, although hackers are no longer limited to collaboration in geographic regions. Now collaboration takes place via the Internet. Eric S. Raymond identifies and explains this conceptual shift in \"The Cathedral and the Bazaar\":\n\nBefore cheap Internet, there were some geographically compact communities where the culture encouraged Weinberg's egoless programming, and a developer could easily attract a lot of skilled kibitzers and co-developers. Bell Labs, the MIT AI and LCS labs, UC Berkeley: these became the home of innovations that are legendary and still potent.\n\nRaymond also notes that the success of Linux coincided with the wide availability of the World Wide Web. The value of community is still in high practice and use today.\n\nLevy identifies several \"true hackers\" who significantly influenced the hacker ethic. Some well-known \"true hackers\" include:\n\n\nLevy also identified the \"hardware hackers\" (the \"second generation\", mostly centered in Silicon Valley) and the \"game hackers\" (or the \"third generation\"). All three generations of hackers, according to Levy, embodied the principles of the hacker ethic. Some of Levy's \"second-generation\" hackers include:\n\n\nLevy's \"third generation\" practitioners of hacker ethic include:\n\n\nIn 2001, Finnish philosopher Pekka Himanen promoted the hacker ethic in opposition to the Protestant work ethic. In Himanen's opinion, the hacker ethic is more closely related to the virtue ethics found in the writings of Plato and of Aristotle. Himanen explained these ideas in a book, \"The Hacker Ethic and the Spirit of the Information Age\", with a prologue contributed by Linus Torvalds and an epilogue by Manuel Castells.\n\nIn this manifesto, the authors wrote about a hacker ethic centering on passion, hard work, creativity and joy in creating software. Both Himanen and Torvalds were inspired by the Sampo in Finnish mythology. The Sampo, described in the Kalevala saga, was a magical artifact constructed by Ilmarinen, the blacksmith god, that brought good fortune to its holder; nobody knows exactly what it was supposed to be. The Sampo has been interpreted in many ways: a world pillar or world tree, a compass or astrolabe, a chest containing a treasure, a Byzantine coin die, a decorated Vendel period shield, a Christian relic, etc. Kalevala saga compiler Lönnrot interpreted it to be a \"quern\" or mill of some sort that made flour, salt, and gold out of thin air.\n\nThe hacker ethic and its wider context can be associated with liberalism and anarchism.\n\n\n\n", "id": "14275", "title": "Hacker ethic"}
{"url": "https://en.wikipedia.org/wiki?curid=14276", "text": "Hotel\n\nA hotel is an establishment that provides lodging paid on a short-term basis. Facilities provided may range from a modest-quality mattress in a small room to large suites with bigger, higher-quality beds, a dresser, a fridge and other kitchen facilities, upholstered chairs, a flatscreen television and en-suite bathrooms. Small, lower-priced hotels may offer only the most basic guest services and facilities. Larger, higher-priced hotels may provide additional guest facilities such as a swimming pool, business centre (with computers, printers and other office equipment), childcare, conference and event facilities, tennis or basketball courts, gymnasium, restaurants, day spa and social function services. Hotel rooms are usually numbered (or named in some smaller hotels and B&Bs) to allow guests to identify their room. Some boutique, high-end hotels have custom decorated rooms. Some hotels offer meals as part of a room and board arrangement. In the United Kingdom, a hotel is required by law to serve food and drinks to all guests within certain stated hours. In Japan, capsule hotels provide a tiny room suitable only for sleeping and shared bathroom facilities.\n\nThe precursor to the modern hotel was the inn of medieval Europe. For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers. Inns began to cater to richer clients in the mid-18th century. One of the first hotels in a modern sense was opened in Exeter in 1768. Hotels proliferated throughout Western Europe and North America in the early 19th century, and luxury hotels began to spring up in the later part of the 19th century.\n\nHotel operations vary in size, function, and cost. Most hotels and major hospitality companies have set industry standards to classify hotel types. An upscale full-service hotel facility offers luxury amenities, full service accommodations, an on-site restaurant, and the highest level of personalized service, such as a concierge, room service and clothes pressing staff. Full service hotels often contain upscale full-service facilities with a large number of full service accommodations, an on-site full service restaurant, and a variety of on-site amenities. Boutique hotels are smaller independent, non-branded hotels that often contain upscale facilities. Small to medium-sized hotel establishments offer a limited amount of on-site amenities. Economy hotels are small to medium-sized hotel establishments that offer basic accommodations with little to no services. Extended stay hotels are small to medium-sized hotels that offer longer-term full service accommodations compared to a traditional hotel.\n\nTimeshare and destination clubs are a form of property ownership involving ownership of an individual unit of accommodation for seasonal usage. A motel is a small-sized low-rise lodging with direct access to individual rooms from the car park. Boutique hotels are typically hotels with a unique environment or intimate setting. A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London. Some hotels are built specifically as a destination in itself, for example at casinos and holiday resorts.\n\nMost hotel establishments are run by a General Manager who serves as the head executive (often referred to as the \"Hotel Manager\"), department heads who oversee various departments within a hotel (e.g., food service), middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function and class, and is often determined by hotel ownership and managing companies.\n\nThe word \"hotel\" is derived from the French \"hôtel\" (coming from the same origin as \"hospital\"), which referred to a French version of a building seeing frequent visitors, and providing care, rather than a place offering accommodation. In contemporary French usage, \"hôtel\" now has the same meaning as the English term, and \"hôtel particulier\" is used for the old meaning, as well as \"hôtel\" in some place names such as Hôtel-Dieu (in Paris), which has been a hospital since the Middle Ages. The French spelling, with the circumflex, was also used in English, but is now rare. The circumflex replaces the 's' found in the earlier \"hostel\" spelling, which over time took on a new, but closely related meaning. Grammatically, hotels usually take the definite article – hence \"The Astoria Hotel\" or simply \"The Astoria.\"\n\nFacilities offering hospitality to travellers have been a feature of the earliest civilizations. In Greco-Roman culture hospitals for recuperation and rest were built at thermal baths. During the Middle Ages various religious orders at monasteries and abbeys would offer accommodation for travellers on the road.\n\nThe precursor to the modern hotel was the inn of medieval Europe, possibly dating back to the rule of Ancient Rome. These would provide for the needs of travelers, including food and lodging, stabling and fodder for the traveler's horse(s) and fresh horses for the mail coach. Famous London examples of inns include the George and the Tabard. A typical layout of an inn had an inner court with bedrooms on the two sides, with the kitchen and parlour at the front and the stables at the back.\n\nFor a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers (in other words, a roadhouse). Coaching inns stabled teams of horses for stagecoaches and mail coaches and replaced tired teams with fresh teams. Traditionally they were seven miles apart but this depended very much on the terrain.\nSome English towns had as many as ten such inns and rivalry between them was intense, not only for the income from the stagecoach operators but for the revenue for food and drink supplied to the wealthy passengers. By the end of the century, coaching inns were being run more professionally, with a regular timetable being followed and fixed menus for food.\n\nInns began to cater for richer clients in the mid-18th century, and consequently grew in grandeur and the level of service provided. One of the first hotels in a modern sense was opened in Exeter in 1768, although the idea only really caught on in the early 19th century. In 1812 Mivart's Hotel opened its doors in London, later changing its name to Claridge's.\n\nHotels proliferated throughout Western Europe and North America in the 19th century, and luxury hotels, including the Savoy Hotel in the United Kingdom and the Ritz chain of hotels in London and Paris and Tremont House and Astor House in the United States, began to spring up in the later part of the century, catering to an extremely wealthy clientele.\n\nHotels cater to travelers from many countries and languages, since no one country dominates the travel industry.\n\nHotel operations vary in size, function, and cost. Most hotels and major hospitality companies that operate hotels have set widely accepted industry standards to classify hotel types. General categories include the following:\n\nAn upscale full service hotel facility that offers luxury amenities, full service accommodations, on-site full service restaurant(s), and the highest level of personalized and professional service. Luxury hotels are normally classified with at least a Four Diamond or Five Diamond status or a Four or Five Star rating depending on the country and local classification standards. \"Examples may include: InterContinental, Waldorf Astoria, Four Seasons, Conrad, Fairmont, and The Ritz-Carlton.\"\n\nFull service hotels often contain upscale full-service facilities with a large volume of full service accommodations, on-site full service restaurant(s), and a variety of on-site amenities such as swimming pools, a health club, children's activities, ballrooms, on-site conference facilities, and other amenities. Examples include: Holiday Inn, Sheraton, Westin, Hilton, Marriott, and Hyatt hotels.\n\nBoutique hotels are smaller independent non-branded hotels that often contain upscale facilities of varying size in unique or intimate settings with full service accommodations. Boutique hotels are generally 100 rooms or less. Some historic inns and boutique hotels may be classified as luxury hotels. Examples include Hotel Indigo and Kimpton Hotels.\n\nSmall to medium-sized hotel establishments that offer a limited amount of on-site amenities that only cater and market to a specific demographic of travelers, such as the single business traveler. Most focused or select service hotels may still offer full service accommodations but may lack leisure amenities such as an on-site restaurant or a swimming pool. Examples include Crowne Plaza, Courtyard by Marriott and Hilton Garden Inn.\n\nSmall to medium-sized hotel establishments that offer a very limited amount of on-site amenities and often only offer basic accommodations with little to no services, these facilities normally only cater and market to a specific demographic of travelers, such as the budget-minded traveler seeking a \"no frills\" accommodation. Limited service hotels often lack an on-site restaurant but in return may offer a limited complimentary food and beverage amenity such as on-site continental breakfast service. Examples include Ibis Budget, Hampton Inn, Aloft, Holiday Inn Express, Fairfield Inn, Four Points by Sheraton, and Days Inn.\n\nExtended stay hotels are small to medium-sized hotels that offer longer term full service accommodations compared to a traditional hotel. Extended stay hotels may offer non-traditional pricing methods such as a weekly rate that cater towards travelers in need of short-term accommodations for an extended period of time. Similar to limited and select service hotels, on-site amenities are normally limited and most extended stay hotels lack an on-site restaurant. Examples include Staybridge Suites, Candlewood Suites, Homewood Suites by Hilton, Home2 Suites by Hilton, Residence Inn by Marriott, Element, and Extended Stay Hotels.\n\nTimeshare and Destination clubs are a form of property ownership also referred to as a vacation ownership involving the purchase and ownership of an individual unit of accommodation for seasonal usage during a specified period of time. Timeshare resorts often offer amenities similar that of a Full service hotel with on-site restaurant(s), swimming pools, recreation grounds, and other leisure-oriented amenities. Destination clubs on the other hand may offer more exclusive private accommodations such as private houses in a neighborhood-style setting. Examples of timeshare brands include Hilton Grand Vacations, Marriott Vacation Club International, Westgate Resorts, Disney Vacation Club, and Holiday Inn Club Vacations.\n\nA motel, an abbreviation for \"motor hotel\", is a small-sized low-rise lodging establishment similar to a limited service, lower-cost hotel, but typically with direct access to individual rooms from the car park. Motels were built to serve road travellers, including travellers on road trip vacations and workers who drive for their job (travelling salespeople, truck drivers, etc.). Common during the 1950s and 1960s, motels were often located adjacent to a major highway, where they were built on inexpensive land at the edge of towns or along stretches of freeway.\n\nNew motel construction is rare in the 2000s as hotel chains have been building economy-priced, limited service franchised properties at freeway exits which compete for largely the same clientele, largely saturating the market by the 1990s. Motels are still useful in less populated areas for driving travelers, but the more populated an area becomes, the more hotels move in to meet the demand for accommodation. Many of the motels which remain in operation have joined national franchise chains, often rebranding themselves as hotels, inns or lodges.\n\nHotel management is a globally accepted professional career field and academic field of study. Degree programs such as hospitality management studies, a business degree, and/or certification programs formally prepare hotel managers for industry practice.\n\nMost hotel establishments consist of a General Manager who serves as the head executive (often referred to as the \"Hotel Manager\"), department heads who oversee various departments within a hotel, middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function, and is often determined by hotel ownership and managing companies.\n\nBoutique hotels are typically hotels with a unique environment or intimate setting.\nSome hotels have gained their renown through tradition, by hosting significant events or persons, such as Schloss Cecilienhof in Potsdam, Germany, which derives its fame from the Potsdam Conference of the World War II allies Winston Churchill, Harry Truman and Joseph Stalin in 1945. The Taj Mahal Palace & Tower in Mumbai is one of India's most famous and historic hotels because of its association with the Indian independence movement. Some establishments have given name to a particular meal or beverage, as is the case with the Waldorf Astoria in New York City, United States where the Waldorf Salad was first created or the Hotel Sacher in Vienna, Austria, home of the Sachertorte. Others have achieved fame by association with dishes or cocktails created on their premises, such as the Hotel de Paris where the crêpe Suzette was invented or the Raffles Hotel in Singapore, where the Singapore Sling cocktail was devised.\n\nA number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London, through its association with Irving Berlin's song, 'Puttin' on the Ritz'. The Algonquin Hotel in New York City is famed as the meeting place of the literary group, the Algonquin Round Table, and Hotel Chelsea, also in New York City, has been the subject of a number of songs and the scene of the stabbing of Nancy Spungen (allegedly by her boyfriend Sid Vicious).\n\nSome hotels are built specifically as a destination in itself to create a captive trade, example at casinos, amusement parks and holiday resorts. Though of course hotels have always been built in popular destinations, the defining characteristic of a resort hotel is that it exists purely to serve another attraction, the two having the same owners.\n\nOn the Las Vegas Strip there is a tradition of one-upmanship with luxurious and extravagant hotels in a concentrated area. This trend now has extended to other resorts worldwide, but the concentration in Las Vegas is still the world's highest: nineteen of the world's twenty-five largest hotels by room count are on the Strip, with a total of over 67,000 rooms.\n\nIn Europe Center Parcs might be considered a chain of resort hotels, since the sites are largely man-made (though set in natural surroundings such as country parks) with captive trade, whereas holiday camps such as Butlins and Pontin's are probably not considered as resort hotels, since they are set at traditional holiday destinations which existed before the camps.\n\n\nThe Null Stern Hotel in Teufen, Appenzellerland, Switzerland and the Concrete Mushrooms in Albania are former nuclear bunkers transformed into hotels.\n\nThe Cuevas Pedro Antonio de Alarcón (named after the author) in Guadix, Spain, as well as several hotels in Cappadocia, Turkey, are notable for being built into natural cave formations, some with rooms underground. The Desert Cave Hotel in Coober Pedy, South Australia is built into the remains of an opal mine.\n\nLocated on the coast but high above sea level, these hotels offer unobstructed panoramic views and a great sense of privacy without the feeling of total isolation. Some examples from around the globe are the Riosol Hotel in Gran Canaria, Caruso Belvedere Hotel in Amalfi Coast (Italy), Aman Resorts Amankila in Bali, Birkenhead House in Hermanus (South Africa), The Caves in Jamaica and Caesar Augustus in Capri.\n\nCapsule hotels are a type of economical hotel first introduced in Japan, where people sleep in stacks of rectangular containers.\n\nSome hotels fill daytime occupancy with day rooms, for example, Rodeway Inn and Suites near Port Everglades in Fort Lauderdale, Florida. Day rooms are booked in a block of hours typically between 8 am and 5 pm, before the typical night shift. These are similar to transit hotels in that they appeal to travelers, however, unlike transit hotels, they do not eliminate the need to go through Customs.\n\nGarden hotels, famous for their gardens before they became hotels, include Gravetye Manor, the home of garden designer William Robinson, and Cliveden, designed by Charles Barry with a rose garden by Geoffrey Jellicoe.\n\nThe Ice Hotel in Jukkasjärvi, Sweden, was the first ice hotel in the world; first built in 1990, it is built each winter and melts every spring. Other ice hotels include the Igloo Village in Kakslauttanen, Finland, and the Hotel de Glace in Duschenay, Canada. They can also be included within larger ice complexes; for example, the Mammut Snow Hotel in Finland is located within the walls of the Kemi snow castle; and the Lainio Snow Hotel is part of a snow village near Ylläs, Finland.\n\nA referral hotel is a hotel chain that offers branding to independently-operated hotels; the chain itself is founded by or owned by the member hotels as a group. Many former referral chains have been converted to franchises; the largest surviving member-owned chain is Best Western.\n\nFrequently, expanding railway companies built grand hotels at their termini, such as the Midland Hotel, Manchester next to the former Manchester Central Station, and in London the ones above St Pancras railway station and Charing Cross railway station. London also has the Chiltern Court Hotel above Baker Street tube station, there are also Canada's grand railway hotels. They are or were mostly, but not exclusively, used by those traveling by rail.\n\nThe Maya Guesthouse in Nax Mont-Noble in the Swiss Alps, is the first hotel in Europe built entirely with straw bales. Due to the insulation values of the walls it needs no conventional heating or air conditioning system, although the Maya Guesthouse is built at an altitude of in the Alps.\n\nTransit hotels are short stay hotels typically used at international airports where passengers can stay while waiting to change airplanes. The hotels are typically on the airside and do not require a visa for a stay or re-admission through security checkpoints.\n\nSome hotels are built with living trees as structural elements, for example the Treehotel near Piteå, Sweden, the Costa Rica Tree House in the Gandoca-Manzanillo Wildlife Refuge, Costa Rica; the Treetops Hotel in Aberdare National Park, Kenya; the Ariau Towers near Manaus, Brazil, on the Rio Negro in the Amazon; and Bayram's Tree Houses in Olympos, Turkey.\n\nSome hotels have accommodation underwater, such as Utter Inn in Lake Mälaren, Sweden. Hydropolis, project in Dubai, would have had suites on the bottom of the Persian Gulf, and Jules' Undersea Lodge in Key Largo, Florida requires scuba diving to access its rooms.\n\nA resort island is an island or an archipelago that contains resorts, hotels, overwater bungalows, restaurants, tourist attractions and its amenities. Maldives has the most overwater bungalows resorts and topped as having the best island resorts and they have become famous among the top celebrities and sportspersons around the world.\n\nIn 2006, \"Guinness World Records\" listed the First World Hotel in Genting Highlands, Malaysia, as the world's largest hotel with a total of 6,118 rooms. The Izmailovo Hotel in Moscow has the most rooms, with 7,500, followed by The Venetian and The Palazzo complex in Las Vegas (7,117 rooms) and MGM Grand Las Vegas complex (6,852 rooms).\n\nAccording to the Guinness Book of World Records, the oldest hotel in operation is the Nisiyama Onsen Keiunkan in Yamanashi, Japan. The hotel, first opened in 707 A.D. has been operated by the same family for forty-six generations. The title was held until 2011 by the Hoshi Ryokan, in the Awazu Onsen area of Komatsu, Japan, which opened in the year 718, as the history of the Nisiyama Onsen Keiunkan was virtually unknown.\n\nThe Ritz-Carlton, Hong Kong claims to be the world's highest hotel. It is located on the top floors of the International Commerce Centre in Hong Kong, at above ground level.\n\nIn October 2014, the Anbang Insurance Group, based in China, purchased the Waldorf Astoria New York in Manhattan for US$1.95 billion, making it the world's most expensive hotel ever sold.\n\nA number of public figures have notably chosen to take up semi-permanent or permanent residence in hotels.\n\n\n", "id": "14276", "title": "Hotel"}
{"url": "https://en.wikipedia.org/wiki?curid=14277", "text": "Hebrew mythology\n\nHebrew mythology may refer to:\n\n", "id": "14277", "title": "Hebrew mythology"}
{"url": "https://en.wikipedia.org/wiki?curid=14279", "text": "Hugh Hefner\n\nHugh Marston Hefner (born April 9, 1926) is an American men's lifestyle magazine publisher, businessman, and a well-known playboy. Hefner is a native of Chicago, Illinois and a former journalist for \"Esquire\". He is best known for being the founder and chief creative officer of Playboy Enterprises. A self-made multi-millionaire, his net worth is over $43 million due to his success as the founder of Playboy magazine. Hefner is also a political activist and philanthropist active in several causes and public issues. He is a World War II veteran.\n\nHefner was born in Chicago, Illinois on April 9, 1926. He is the first child of Grace Caroline (née Swanson; 1895–1997) and Glenn Lucius Hefner (1896–1976), who both worked as teachers. His parents were from Nebraska. He had a younger brother, Keith (1929–2016). Hefner's mother was of Swedish descent, and his father had German and English ancestry. Through his father's line, Hefner has stated that he is a direct descendant of Plymouth governor William Bradford. He has described his family as \"conservative, Midwestern, [and] Methodist\".\n\nHe attended Sayre Elementary School and Steinmetz High School, then during World War II, served as a writer for a military newspaper in the U.S. Army from 1944 to 1946. Hefner graduated from the University of Illinois at Urbana Champaign with a bachelor of arts in psychology and a double minor in creative writing and art in 1949, earning his degree in two and a half years. After graduation, he took a semester of graduate courses in sociology at Northwestern University but dropped out soon after.\n\nWhile he was working as a copywriter for \"Esquire\", Hefner left in January 1952 after being denied a $5 raise. In 1953, took out a mortgage, generating a bank loan of $600, and raised $8,000 from 45 investors, including $1,000 from his mother (\"Not because she believed in the venture,\" he told \"E!\" in 2006, \"but because she believed in her son.\"), to launch \"Playboy\", which was initially going to be called \"Stag Party\". The first issue, published in December 1953, featured Marilyn Monroe from her 1949 nude calendar shoot and sold over 50,000 copies. (Hefner, who never met Monroe, bought the crypt next to hers at the Westwood Village Memorial Park Cemetery in 1992 for $75,000.)\n\nAfter the Charles Beaumont science fiction short story \"The Crooked Man\" was rejected by \"Esquire\" magazine in 1955, Hefner agreed to publish it in \"Playboy.\" The story highlighted straight men being persecuted in a world where homosexuality was the norm. After the magazine received angry letters, Hefner wrote a response to criticism where he said, \"If it was wrong to persecute heterosexuals in a homosexual society then the reverse was wrong, too.\" In 1961, Hefner watched Dick Gregory perform at the Herman Roberts Show Bar in Chicago. Based on that performance, Hefner hired Gregory to work at the Chicago Playboy Club; Gregory attributes the subsequent launch of his career to that night.\n\nOn June 4, 1963, Hefner was arrested for promoting obscene literature after an issue of \"Playboy\" featuring nude shots of Jayne Mansfield was released. The case went to trial and resulted in a hung jury.\n\nIn the 1993 \"The Simpsons\" episode \"Krusty Gets Cancelled\", Hefner guest-voiced himself.\n\nIn 1999, Hefner financed the Clara Bow documentary, \"Discovering the It Girl.\" \"Nobody has what Clara had. She defined an era and made her mark on the nation,\" he stated.\n\nHefner guest-starred as himself in a 2006 episode of Seth Green's \"Robot Chicken\" on the late-night programming block Adult Swim.\n\nHe has a star on the Hollywood Walk of Fame for television and has made several movie appearances as himself. In 2009, he received a \"worst supporting actor\" nomination for a Razzie award for his performance as himself in \"Miss March\". On his official Twitter account he joked about this nomination: \"Maybe I didn't understand the character.\" \n\nA documentary by Brigitte Berman, \"\", was released on July 30, 2010. He had previously granted full access to documentary filmmaker and television producer Kevin Burns for the A&E \"Biography\" special \"Hugh Hefner: American Playboy\" in 1996. Hefner and Burns later collaborated on numerous other television projects, most notably on \"The Girls Next Door\", a reality series that ran for six seasons (2005–2009) and 90 episodes.\n\nIn 1949, Hefner married Northwestern University student Mildred Williams, who was also born in 1926. They had two children: daughter Christie Hefner (born 1952) and son David (born 1955). Before the wedding, Mildred confessed that she had an affair while he was away in the army. He called the admission \"the most devastating moment of my life.\" A 2006 \"E! True Hollywood Story\" profile of Hefner revealed that Mildred allowed him to have sex with other women, out of guilt for her infidelity and in the fond hope that it would preserve their marriage. The two were divorced in 1959.\n\nHefner remade himself as a bon viveur and man about town, a lifestyle he promoted in his magazine and two TV shows he hosted, \"Playboy's Penthouse\" (1959–1960) and \"Playboy After Dark\" (1969–1970). He admitted to being involved' with maybe eleven out of twelve months' worth of Playmates\" during some of these years. Donna Michelle, Marilyn Cole, Lillian Müller, Shannon Tweed, Barbi Benton, Karen Christy, Sondra Theodore, and Carrie Leigh — who filed a $35 million palimony suit against him — were a few of his many lovers. In 1971, he acknowledged that he experimented in bisexuality. Also in 1971, Hefner established a second residence in Los Angeles with the acquisition of Playboy Mansion West, and in 1975 moved there permanently from Chicago.\n\nHefner had a minor stroke in 1985 at the age of 59. After re-evaluating his lifestyle, he made several changes. The wild, all-night parties were toned down significantly and in 1988, daughter Christie began to run the Playboy empire. The following year, he married Playmate of the Year Kimberley Conrad; they were thirty-six years apart in age. The couple had two sons: Marston Glenn (born 1990) and Cooper Bradford (born 1991). The \"E! True Hollywood Story\" profile noted that the notorious Playboy Mansion had been transformed into a family-friendly homestead. After he and Conrad separated in 1998 she moved into a house next door to the mansion.\n\nIn January 2009, Hefner started dating Crystal Harris, joining the Shannon Twins after his previous \"number one girlfriend\", Holly Madison, had ended their 7-year relationship. On December 24, 2010, he became engaged to Harris, to become his third wife. Harris broke off their engagement on June 14, 2011, five days before their planned wedding. In anticipation of the wedding, the July issue of \"Playboy\", which reached store shelves and customer's homes within days of the wedding date, featured Harris on the cover and in a photo spread as well. The headline on the cover read \"Introducing America's Princess, Mrs. Crystal Hefner\". It was later covered on newsstand issues with a sticker that read \"Runaway Bride\". The two later reconciled, and on December 31, 2012, Harris and Hefner married at the Playboy Mansion in a small private ceremony; he was 86 and she was 26.\n\nHis younger brother Keith succumbed to cancer at the age of 87, merely one day before Hefner's 90th birthday.\n\nHefner became known for moving an ever-changing coterie of young women into the Playboy Mansion, including twins Sandy and Mandy Bentley. He dated as many as seven women concurrently. He also dated Brande Roderick, Izabella St. James, Tina Marie Jordan, Holly Madison, Bridget Marquardt, and Kendra Wilkinson. Madison, Wilkinson and Marquardt appeared on \"The Girls Next Door\" depicting their lives at the Playboy Mansion. In October 2008, all three girls decided to leave the mansion. Hefner soon began dating his new \"Number One\" girlfriend, Crystal Harris, along with identical twin models Kristina and Karissa Shannon. The relationship with the twins ended in January 2010. After an 11-year separation, Hefner filed for divorce from Conrad, citing irreconcilable differences. Hefner has stated that he only remained nominally married to her for the sake of his children, and his youngest child had just turned 18. The divorce was finalized in March 2010. On December 24, 2010, Hefner presented an engagement ring to Crystal Harris, publicly announcing the proposal the following day. Hefner and Harris had planned to marry June 18, 2011. Harris called off the wedding just 5 days before they were due to be wed. Twenty-six-year-old Harris and eighty-six-year-old Hefner reconciled and were married on December 31, 2012.\n\nIn 2012, Hefner announced that his youngest son, Cooper, would likely succeed him as the public face of \"Playboy\".\n\nIn January 2016, the Playboy Mansion was put on the market for $200 million, on condition that Hugh Hefner would continue to work and live in the mansion. It was later sold to Daren Metropoulos, a principal at private equity firm Metropoulos & Co, for $100 million. Metropoulos plans to reconnect the Playboy Mansion property with a neighboring estate that he purchased in 2009, combining the two for a 7.3 acre (3-hectare) compound as his own private residence.\n\nThe Hugh Hefner First Amendment Award was created by Christie Hefner \"to honor individuals who have made significant contributions in the vital effort to protect and enhance First Amendment rights for Americans.\"\n\nHe has donated and raised money for the Democratic Party. However, he has more recently referred to himself as an independent due to disillusionment with both the Democratic and Republican parties.\n\nIn 1978, Hefner helped organize fund-raising efforts that led to the restoration of the Hollywood Sign. He hosted a gala fundraiser at the Playboy Mansion and personally contributed $27,000 (or 1/9 of the total restoration costs) by purchasing the letter Y in a ceremonial auction.\n\nHefner donated $100,000 to the University of Southern California's School of Cinematic Arts to create a course called \"Censorship in Cinema,\" and $2 million to endow a chair for the study of American film.\n\nBoth through his charitable foundation and individually, Hefner also contributes to charities outside the sphere of politics and publishing, throwing fundraiser events for Much Love Animal Rescue as well as Generation Rescue, a controversial anti-vaccinationist campaign organization supported by Jenny McCarthy.\n\nOn November 18, 2010, Children of the Night founder and president Dr. Lois Lee presented Hefner with the organization's first-ever Founder's Hero of the Heart Award in appreciation for his unwavering dedication, commitment and generosity.\n\nOn April 26, 2010, Hefner donated the last $900,000 sought by a conservation group for a land purchase needed to stop the development of the famed vista of the Hollywood Sign.\n\n\"Sylvilagus palustris hefneri\", an endangered subspecies of Marsh rabbit, is named after him in honor of financial support that he provided.\n\nHefner supported legalizing same-sex marriage and he stated that a fight for gay marriage was \"a fight for all our rights. Without it, we will turn back the sexual revolution and return to an earlier, puritanical time.\"\n\n\n", "id": "14279", "title": "Hugh Hefner"}
{"url": "https://en.wikipedia.org/wiki?curid=14280", "text": "Hafizullah Amin\n\nHafizullah Amin ( born 1 August 1929 – 27 December 1979) was an Afghan politician and statesman during the Cold War. Amin was born in Paghman and educated at Kabul University, after which he started his career as a teacher. After a few years in that occupation, he went to the United States to study. He would visit the United States a second time before moving permanently to Afghanistan, and starting his career in radical politics. He ran as a candidate in the 1965 parliamentary election but failed to secure a seat. Amin was the only Khalqist elected to parliament in the 1969 parliamentary election, thus increasing his standing within the party. He was one of the leading organisers of the Saur Revolution which overthrew the government of Mohammad Daoud Khan.\n\nAmin's short-lived presidency was marked by controversies from beginning to end. He came to power by ordering the death of his predecessor Nur Muhammad Taraki. The revolt against communist rule which had begun under Taraki worsened under Amin, and was a problem that his government was unable to solve. The Soviet Union, which alleged that Amin was an agent of the CIA, intervened in Afghanistan while invoking the Twenty-Year Treaty of Friendship between Afghanistan and the Soviet Union. Amin was assassinated by the Soviets in December 1979 as part of Operation Storm-333, having ruled for slightly longer than three months.\n\nHafizullah Amin was born to a Ghilzai Pashtun family in Paghman on 1 August 1929. His father, a civil servant, died when he was still very young. Thanks to his brother Abdullah, a primary school teacher, Amin was able to attend both primary and secondary school, which in turn allowed him to attend Kabul University (KU). After studying mathematics there, he also graduated from the Darul Mualimeen Teachers College in Kabul, and became a teacher. Amin later became vice-principal of the Darul Mualimeen College, and then principal of the prestigious Avesina High School, and in 1957 left Afghanistan for Columbia University in New York City, where he earned M. A. in education. It was at Columbia that Amin became attracted to Marxism, and in 1958 he became a member of the university's Socialist Progressive Club. When he returned to Afghanistan, Amin became a teacher at Kabul University, and later, for the second time, the principal of Avesina High School. During this period Amin became acquainted with Nur Muhammad Taraki, a communist. Around this time, Amin quit his position as principal of Avesina High School in order to become principal of the Darul Mualimeen College.\n\nIt is alleged that Amin became radicalised during his second stay in the United States in 1962, when he enrolled in a work-study group at the University of Wisconsin. Amin studied in the doctoral programme at the Columbia University Teachers College, but started to neglect his studies in favour of politics; in 1963 he became head of the Afghan students' association at the college. When he returned to Afghanistan in the mid-1960s, the route flew to Afghanistan by way of Moscow. There, Amin met the Afghan ambassador to the Soviet Union, his old friend Ali Ahmad Popel, a previous Afghan Minister of Education. During his short stay, Amin became even more radicalised. Some people, Nabi Misdaq for instance, do not believe he travelled through Moscow, but rather West Germany and Lebanon. By the time he had returned to Afghanistan, the Communist People's Democratic Party of Afghanistan (PDPA) had already held its founding congress, which was in 1965. Amin ran as a candidate for the PDPA in the 1965 parliamentary election, and lost by a margin of less than fifty votes.\n\nIn 1966, when the PDPA Central Committee was expanded, Amin was elected as a non-voting member, and in the spring of 1967 he gained full membership. Amin's standing in the Khalq faction of the PDPA increased when he was the only Khalqist elected to parliament in the 1969 parliamentary election. When the PDPA split along factional lines in 1967, between Khalqists led by Nur and Parchamites led by Babrak Karmal, Amin joined the Khalqists. As a member of parliament, Amin tried to win over support from the Pashtun people in the armed forces. According to a biography about Amin, he used his position as member of parliament to fight against imperialism, feudalism, and reactionary tendencies, and fought against the \"rotten\" regime, the monarchy. Amin himself said that he used his membership in parliament to pursue the class struggle against the bourgeoisie. Relations between Khalqists and Parchamites deteriorated during this period. Amin, the only Khalq member of parliament, and Babrak Karmal, the only Parcham member of parliament, did not cooperate with each other. Amin would later, during his short stint in power, mention these events with bitterness. Following the arrest of fellow PDPA members Dastagir Panjsheri and Saleh Mohammad Zeary in 1969, Amin became one of the party's leading members, and was still a pre-eminent party member by the time of their release in 1973.\n\nFrom 1973 until the PDPA unification in 1977, Amin was second only to Taraki in the Khalqist PDPA. When the PDPA ruled Afghanistan, their relationship was referred to as a disciple (Amin) following his mentor (Taraki). This official portrayal of the situation was misleading; their relationship was more work-oriented. Taraki needed Amin's \"tactical and strategic talents\"; Amin's motivations are more uncertain, but it is commonly believed that he associated with Taraki to protect his own position. Amin had attracted many enemies during his career, the most notable being Karmal. According to the official version of events, Taraki protected Amin from party members or others who wanted to hurt the PDPA and the country.\n\nWhen Mohammed Daoud Khan ousted the monarchy, and established the Republic of Afghanistan, the Khalqist PDPA offered its support for the new regime if it established a National Front which presumably included the Khalqist PDPA itself. The Parchamite PDPA had already established an alliance with Daoud at the beginning of his regime, and Karmal called for the dissolution of the Khalqist PDPA. Karmal's call for dissolution only worsened relations between the Khalqist and Parchamite PDPA. However, Taraki and Amin were lucky; Karmal's alliance actually hurt the Parchamites' standing in Afghan politics. Some communists in the armed forces became disillusioned with the government of Daoud, and turned to the Khalqist PDPA because of its apparent independence. Parchamite association with the Daoud government indirectly led to the Khalqist-led PDPA coup of 1978, popularly referred to as the Saur Revolution. From 1973 until the 1978 coup, Amin was responsible for organising party work in the Afghan armed forces. According to the official version, Amin \"met patriotic liaison officers day or night, in the desert or the mountains, in the fields or the forests, enlightening them on the basis of the principles of the working class ideology.\" Amin's success in recruiting military officers lay in the fact that Daoud \"betrayed the left\" soon after taking power. When Amin began recruiting military officers for the PDPA, it was not difficult for him to find disgruntled military officers. In the meantime, relations between the Parchamite and Khalqist PDPA deteriorated; in 1973 it was rumoured that Major Zia Mohammadzai, a Parchamite and head of the Republican Guard, planned to assassinate the entire Khalqist leadership. The plan, if true, failed because the Khalqists found out about it.\n\nThe assassination attempt proved to be a further blow to relations between the Parchamites and Khalqists. The Parchamites deny that they ever planned to assassinate the Khalqist leadership, but historian Beverley Male argues that Karmal's subsequent activities give credence to the Khalqist view of events. Because of the Parchamite assassination attempt, Amin pressed the Khalqist PDPA to seize power in 1976 by ousting Daoud. The majority of the PDPA leadership voted against such a move. The following year, in 1977, the Parchamites and Khalqists officially reconciled, and the PDPA was unified. The Parchamite and Khalqist PDPAs, which had separate general secretaries, politburos, central committees and other organisational structures, were officially unified in the summer of 1977. One reason for unification was that the international communist movement, represented by the Communist Party of India, Iraqi Communist Party and the Communist Party of Australia, called for party unification.\n\nOn 18 April 1978 Mir Akbar Khyber, the chief ideologue of the Parcham faction, was killed; he was commonly believed to have been assassinated by the Daoud government. Khyber's assassination initiated a chain of events which led to the PDPA taking power eleven days later, on 27 April. The assassin was never caught, but Anahita Ratebzad, a Parchamite, believed that Amin had ordered the assassination. Khyber's funeral evolved into a large anti-government demonstration. Daoud, who did not understand the significance of the events, began a mass arrest of PDPA members seven days after Khyber's funeral. Amin, who organised the subsequent revolution against Daoud, was one of the last Central Committee members to be arrested by the authorities. His late arrest can be considered as proof of the regime's lack of information; Amin was the leading revolutionary party organiser. The government's lack of awareness was proven by the arrest of Taraki – Taraki's arrest was the pre-arranged signal for the revolution to commence. When Amin found out that Taraki had been arrested, he ordered the revolution to begin at 9am on 27 April. Amin, in contrast to Taraki, was not imprisoned, but instead put under house arrest. His son, Abdur Rahman, was still allowed freedom of movement. The revolution was successful, thanks to overwhelming support from the Afghan military; for instance, it was supported by Defence Minister Ghulam Haidar Rasuli, Aslam Watanjar the commander of the ground forces, and the Chief of Staff of the Afghan Air Force, Abdul Qadir.\n\nAfter the Saur revolution, Taraki was appointed Chairman of the Presidium of the Revolutionary Council and Chairman of the Council of Ministers, and retained his post as PDPA general secretary. Taraki initially formed a government which consisted of both Khalqists and Parchamites; Karmal became Deputy Chairman of the Revolutionary Council while Amin became Minister of Foreign Affairs and a Deputy Prime Minister, and Mohammad Aslam Watanjar became a Deputy Prime Minister. The two Parchamites Abdul Qadir and Mohammad Rafi became Minister of National Defence and Minister of Public Works respectively. According to Angel Rasanayagam, the appointment of Amin, Karmal and Watanjar as Deputy Prime Ministers led to the establishment of three cabinets; the Khalqists were answerable to Amin, the Parchamites were answerable to Karmal, and the military officers (who were Parchamites) were answerable to Watanjar. The first conflict between the Khalqists and Parchamites arose when the Khalqists wanted to give PDPA Central Committee membership to the military officers who participated in the Saur Revolution. Amin, who had previously opposed the appointment of military officers to the PDPA leadership, switched sides; he now supported their elevation. The PDPA Politburo voted in favour of giving membership to the military officers; the victors (the Khalqists) portrayed the Parchamites as opportunists, implying that the Parchamites had ridden the revolutionary wave, but not actually participated in the revolution. To make matters worse for the Parchamites, the term Parcham was, according to Taraki, a word synonymous with factionalism.\n\nOn 27 June 1978, three months after the revolution, Amin managed to outmaneuver the Parchamites at a Central Committee meeting. The meeting decided that the Khalqists had exclusive rights to formulate and decide policy, a policy which left the Parchamites impotent. Karmal was exiled, but was able to establish a network with the remaining Parchamites in government. A coup to overthrow Amin was planned for September. Its leading members in Afghanistan were Qadir, the defence minister, and Army Chief of Staff General Shahpur Ahmedzai. The coup was planned for 4 September, on the Festival of Eid, because soldiers and officers would be off duty. The conspiracy failed when the Afghan ambassador to India told the Afghan leadership about the plan. A purge was initiated, and Parchamite ambassadors were recalled; few returned, for example Karmal and Mohammad Najibullah both stayed in their assigned countries.\n\nThe Afghan people revolted against the PDPA government when the government introduced several socialist reforms, including land reforms. By early 1979, twenty-five out of Afghanistan's twenty-eight provinces were unsafe because of armed resistance against the government. On 29 March 1979, the Herat uprising began; the uprising turned the revolt into an open war between the Mujahideen and the Afghan government. It was during this period that Amin became Kabul's strongman. Shortly after the Herat uprising had been crushed, the Revolutionary Council convened to ratify the new Five-Year Plan, the Afghan–Soviet Friendship Treaty, and to vote on whether or not to reorganise the cabinet and to enhance the power of the executive (the Chairman of the Revolutionary Council). While the official version of events said that all issues were voted on democratically at the meeting, the Revolutionary Council held another meeting the following day to ratify the new Five-Year Plan and to discuss the reorganisation of the cabinet.\n\nAlexander Puzanov, the Soviet ambassador to Afghanistan, was able to persuade Aslam Watanjar, Sayed Mohammad Gulabzoy and Sherjan Mazdoryar to become part of a conspiracy against Amin. These three men put pressure on Taraki, who by this time believed that \"he really was the 'great leader'\", to sack Amin from office. It is unknown if Amin knew anything about the conspiracy against him, but it was after the cabinet reorganisation that he talked about his dissatisfaction. On 26 March the PDPA Politburo and the Council of Ministers approved the extension of the powers of the executive branch, and the establishment of the Homeland Higher Defence Council (HHDC) to handle security matters. Many analysts of the day regarded Amin's appointment as Prime Minister as an increase in his powers at the expense of Taraki. However, the reorganisation of the cabinet and the strengthening of Taraki's position as Chairman of the Revolutionary Council, had reduced the authority of the Prime Minister. The Prime Minister was, due to the strengthening of the executive, now appointed by the Chairman of the Revolutionary Council. While Amin could appoint and dismiss new ministers, he needed Taraki's consent to actually do so. Another problem for Amin was that while the Council of Ministers was responsible to the Revolutionary Council and its chairman, individual ministers were only responsible to Taraki. When Amin became Prime Minister, he was responsible for planning, finance and budgetary matters, the conduct of foreign policy, and for order and security. The order and security responsibilities had been taken over by the HHDC, which was chaired by Taraki. While Amin was HHDC Deputy Chairman, the majority of HHDC members were members of the anti-Amin faction. For instance, the HHDC membership included Watanjar the Minister of National Defence, Interior Minister Mazdoryar, the President of the Political Affairs of the Armed Forces Mohammad Iqbal, Mohammad Yaqub, the Chief of the General Staff, the Commander of the Afghan Air Force Nazar Mohammad and Assadullah Sarwari the head of ASGA, the Afghan secret police.\n\nThe order of precedence had been institutionalised, whereby Taraki was responsible for defence and Amin responsible for assisting Taraki in defence related matters. Amin's position was given a further blow by the democratisation of the decision-making process, which allowed its members to contribute; most of them were against Amin. Another problem for Amin was that the office of HHDC Deputy Chairman had no specific functions or powers, and the appointment of a new defence minister who opposed him drastically weakened his control over the Ministry of National Defence. The reorganisation of ministers was a further blow to Amin's position; he had lost control of the defence ministry, the interior ministry and the ASGA. Amin still had allies at the top, many of them in strategically important positions, for instance, Yaqub was his brother-in-law and the Security Chief in the Ministry of Interior was Sayed Daoud Taroon, who was also later appointed to the HHDC as an ordinary member in April. Amin succeeded in appointing two more of his allies to important positions; Mohammad Sediq Alemyar as Minister of Planning and Khayal Mohammad Katawazi as Minister of Information and Culture; and Faqir Mohammad Faqir was appointed Deputy Prime Minister in April 1978. Amin's political position was not secure when Alexei Yepishev, the Head of the Main Political Directorate of the Soviet Army and Navy, visited Kabul. Yepishev met personally with Taraki on 7 April, but never met with Amin. The Soviets were becoming increasingly worried about Amin's control over the Afghan military. Even so, during Yepishev's visit Amin's position was actually strengthened; Taroon was appointed Taraki's aide-de-camp.\n\nSoon after, at two cabinet meetings, the strengthening of the executive powers of the Chairman of the Revolutionary Council was proven. Even though Amin was Prime Minister, Taraki chaired the meetings instead of him. Amin's presence at these two meetings was not mentioned at all, and it was made clear that Taraki, through his office as Chairman of the Revolutionary Council, also chaired the Council of Ministers. Another problem facing Amin was Taraki's policy of autocracy; he tried to deprive the PDPA Politburo of its powers as a party and state decision-making organ. The situation deteriorated when Amin personally warned Taraki that \"the prestige and popularity of leaders among the people has no common aspect with a personality cult.\"\n\nFactionalism within the PDPA made it ill prepared to handle the intensified counter-revolutionary activities in the country. Amin tried to win support for the communist government by depicting himself as a devout Muslim. Taraki and Amin blamed different countries for helping the counter-revolutionaries; Amin attacked the United Kingdom and the British Broadcasting Corporation (BBC) and played down American and Chinese involvement, while Taraki blamed American imperialism and Iran and Pakistan for supporting the uprising. Amin's criticism of the United Kingdom and the BBC fed on the traditional anti-British sentiments held by rural Afghans. In contrast to Taraki, \"Amin bent over backwards to avoid making hostile reference to\", China, the United States or other foreign governments. Amin's cautious behavior was in deep contrast to the Soviet Union's official stance on the situation; it seemed, according to Beverley Male, that the Soviet leadership tried to force a confrontation between Afghanistan and its enemies. Amin also tried to appease the Shia communities by meeting with their leaders; despite this, a section of the Shia leadership called for the continuation of the resistance. Subsequently a revolt broke out in a Shia populated district in Kabul; this was the first sign of unrest in Kabul since the Saur Revolution. To add to the government's problems, Taraki's ability to lead the country was questioned – he was a heavy drinker and was not in good health. Amin on the other hand was characterised in this period by portrayals of strong self-discipline. In the summer of 1979 Amin began to disassociate himself from Taraki. On 27 June Amin became a member of the PDPA Politburo, the leading decision-making body in Afghanistan.\n\nIn-mid July the Soviets made their view official when \"Pravda\" wrote an article about the situation in Afghanistan; the Soviets did not wish to see Amin become leader of Afghanistan. This triggered a political crisis in Afghanistan, as Amin initiated a policy of extreme repression, which became one of the main reasons for the Soviet intervention later that year. On 28 July, a vote in the PDPA Politburo approved Amin's proposal of creating a collective leadership with collective decision-making; this was a blow to Taraki, and many of his supporters were replaced by pro-Amin PDPA members. Ivan Pavlovsky, the Commander of the Soviet Ground Forces, visited Kabul in mid-August to study the situation in Afghanistan. Amin, in a speech just a few days after Pavlovsky's arrival, said that he wanted closer relations between Afghanistan and the People's Republic of China; in the same speech he hinted that he had reservations about Soviet meddling in Afghanistan. He likened Soviet assistance to Afghanistan with Vladimir Lenin's assistance to the Hungarian Soviet Republic in 1919. Taraki, a delegate to the conference held by the Non-Aligned Movement in Havana, met personally with Andrei Gromyko, the Soviet Minister of Foreign Affairs, to discuss the Afghanistan situation on 9 September. Shah Wali, the Minister of Foreign Affairs, who was a supporter of Amin, did not participate in the meeting. This, according to Beverley Male, \"suggested that some plot against Amin was in preparation\". Within hours of his return to Kabul on 11 September, Taraki convened the cabinet \"ostensibly to report on the Havana Summit\". Instead of reporting on the summit, Taraki tried to dismiss Amin as Prime Minister. This was a miscalculation, and all but the Gang of Four (consisting of Watanjar, Mazdoryar, Gulabzoi and Sarwari), supported retaining Amin as Prime Minister.\n\nTaraki sought to neutralise Amin's power and influence by requesting that he serve overseas as an ambassador. Amin turned down the proposal, shouting \"You are the one who should quit! Because of drink and old age you have taken leave of your senses.\" The following day Taraki invited Amin to the presidential palace for lunch with him and the Gang of Four. Amin turned down the offer, stating he would prefer their resignation rather than lunching with them. Soviet ambassador Puzanov persuaded Amin to make the visit to the Presidential Palace along with Taroon, the Chief of Police and Nawab Ali (an intelligence officer). Upon arriving at the palace, unknown individuals within the building opened fire on the visitors. Taroon was killed, while Ali sustained an injury and escaped, together with Amin, who was unharmed. Shortly afterward, Amin returned to the palace with a contingent of Army officers, and placed Taraki under arrest. The Gang of Four, however, had \"disappeared\" and their whereabouts would remain unknown for the duration of Amin's 104-day rule. After Taraki's arrest, Amin reportedly discussed the incident with Leonid Brezhnev, and indirectly asked for the permission to kill Taraki. Brezhnev replied that it was his choice. Amin, who now believed he had the full support of the Soviets, ordered the death of Taraki. Taraki was subsequently suffocated with pillows. The Afghan media would report that the ailing Taraki had died, omitting any mention of his murder.\n\nFollowing Taraki's fall from power, Amin was elected Chairman of the Presidum of the Revolutionary Council and General Secretary of the PDPA Central Committee by the PDPA Politburo. The election of Amin as PDPA General Secretary and the removal of Taraki from all party posts was unanimous. The only members of the cabinet replaced when Amin took power were the Gang of Four – Beverley Male saw this as \"a clear indication that he had their [the ministers'] support\". Amin's rise to power was followed by a policy of moderation, and attempts to persuade the Afghan people that the regime was not anti-Islamic. Amin's government began to invest in the reconstruction, or reparation, of mosques. He also promised the Afghan people freedom of religion. Religious groups were given copies of the Quran, and Amin began to refer to Allah in speeches. He even claimed that the Saur Revolution was \"totally based on the principles of Islam\". The campaign proved to be unsuccessful, and many Afghans held Amin responsible for the regime's totalitarian behavior. Amin's rise to power was officially endorsed by the Jamiatul Ulama on 20 September 1979. Their endorsement led to the official announcement that Amin was a pious Muslim – Amin thus scored a point against the counter-revolutionary propaganda which claimed the communist regime was atheist. Amin also tried to increase his popularity with tribal groups, a feat Taraki had been unable or unwilling to achieve. In a speech to tribal elders Amin was defensive about the Western way he dressed; an official biography was published which depicted Amin in traditional Pashtun clothes. During his short stay in power, Amin became committed to establishing a collective leadership; when Taraki was ousted, Amin promised \"from now on there will be no one-man government...\"\n\nAttempting to pacify the population, Amin released a list of 18,000 people who had been executed, and blamed the executions on Taraki. The total number of arrested during Taraki's and Amin's combined reign number between 17,000 and 45,000. Amin was not liked by the Afghan people. During his rule, opposition to the communist regime increased, and the government lost control of the countryside. The state of the Afghan military deteriorated; due to desertions the number of military personnel in the Afghan army decreased from 100,000 in the immediate aftermath of the Saur Revolution, to somewhere between 50,000 and 70,000. Another problem Amin faced was the KGB's penetration of the PDPA, the military and the government bureaucracy. While Amin's position in Afghanistan was becoming more perilous by the day, his enemies who were exiled in the Soviet Union and the Eastern Bloc were agitating for his removal. Babrak Karmal, the Parchamite leader, met several leading Eastern Bloc figures during this period, and Mohammad Aslam Watanjar, Sayed Mohammad Gulabzoy and Assadullah Sarwari wanted to exact revenge upon Amin.\n\nWhen Amin became leader, he tried to reduce Afghanistan's dependence on the Soviet Union. To accomplish this, he aimed to balance Afghanistan's relations with the Soviet Union by strengthening relations with Pakistan and Iran. The Soviets were concerned when they received reports that Amin had met personally with Gulbuddin Hekmatyar, one of the leading anti-communists in Afghanistan. His general untrustworthiness and his unpopularity amongst Afghans made it more difficult for Amin to find new \"foreign patrons\". Amin's involvement in the death of Adolph Dubs, the American Ambassador to Afghanistan, strained his relations with the United States. He tried to improve relations by reestablishing contact, met with three different American chargé d'affaires, and was interviewed by an American correspondent. But this did not improve Afghanistan's standing in the eyes of the United States Government. After the third meeting with Amin, J. Bruce Amstutz, the American Ambassador to Afghanistan from 1979 to 1980, believed the wisest thing to do was to maintain \"a low profile, trying to avoid issues, and waiting to see what happens\". In early December 1979, the Ministry of Foreign Affairs proposed a joint summit meeting between Amin and Muhammad Zia-ul-Haq, the President of Pakistan. The Pakistanti Government, accepting a modified version of the offer, agreed to send Agha Shahi, the Pakistani foreign minister, to Kabul for talks. In the meanwhile, the Inter-Services Intelligence (ISI), Pakistani's secret police, continued to train Mujahideen fighters who opposed the communist regime.\n\nContrary to popular belief, the Soviet leadership headed by Leonid Brezhnev, Alexei Kosygin and the Politburo, were not eager to send troops to Afghanistan. The Soviet Politburo decisions were guided by a Special Commission on Afghanistan, which consisted of Yuri Andropov the KGB Chairman, Andrei Gromyko the Minister of Foreign Affairs, Defence Minister Dmitriy Ustinov, and Boris Ponomarev, the head of the International Department of the Central Committee. The Politburo was opposed to the removal of Taraki and his subsequent murder. According to Brezhnev, the General Secretary of the Central Committee of the Communist Party of the Soviet Union, \"Events developed so swiftly in Afghanistan that essentially there was little opportunity to somehow interfere in them. Right now our mission is to determine our further actions, so as to preserve our position in Afghanistan and to secure our influence there.\" Although Afghan–Soviet relations deteriorated during Amin's short stint in power, he was invited on an official visit to Moscow by Alexander Puzanov, the Soviet ambassador to Afghanistan, because of the Soviet leadership's satisfaction with his party and state-building policy. Not everything went as planned, and Andropov talked about \"the undesirable turn of events\" taking place in Afghanistan under Amin's rule. Andropov also brought up the ongoing political shift in Afghanistan under Amin; the Soviets were afraid that Amin would move Afghanistan's foreign policy from a pro-Soviet position to a pro-United States position. By early-to-mid December 1979, the Soviet leadership had established an alliance with Babrak Karmal and Assadullah Sarwari.\n\nAs it turned out, the relationship between Puzanov and Amin broke down. Amin started a smear campaign to discredit Puzanov. This in turn led to an assassination attempt against Amin, in which Puzanov participated. The situation was worsened by the KGB accusing Amin of misrepresenting the Soviet position on Afghanistan in the PDPA Central Committee and the Revolutionary Council. The KGB also noted an increase in anti-Soviet agitation by the government during Amin's rule, and harassment against Soviet citizens increased under Amin. A group of senior politicians reported to the Soviet Central Committee that it was necessary to do \"everything possible\" to prevent a change in political orientation in Afghanistan. However, the Soviet leadership did not advocate intervention at this time, and instead called for increasing its influence in the Amin leadership to expose his \"true intentions\". A Soviet Politburo assessment referred to Amin as \"a power-hungry leader who is distinguished by brutality and treachery\". Amongst the many sins they alleged were his \"insincerity and duplicity\" when dealing with the Soviet Union, creating fictitious accusations against PDPA-members who opposed him, indulging in a policy of nepotism, and his tendency to conduct a more \"balanced policy\" towards First World countries.\n\nBy the end of October the Special Commission on Afghanistan, which consisted of Andropov, Gromyko, Ustinov and Ponomarev, wanted to end the impression that the Soviet government supported Amin's leadership and policy. The KGB's First Chief Directorate was put under orders that something had to be done about Afghanistan, and several of its personnel were assembled to deal with the task. Andropov fought hard for Soviet intervention, saying to Brezhnev that Amin's policies had destroyed the military and the government's capabilities to handle the crisis by use of mass repression. The plan, according to Andropov, was to assemble a small force to intervene and remove Amin from power and replace him with Karmal. The Soviet Union declared its plan to intervene in Afghanistan on 12 December 1979, and the Soviet leadership initiated Operation Storm-333 (the first phase of the intervention) on 27 December 1979.\n\nAmin trusted the Soviet Union until the very end, despite the deterioration of official relations. When the Afghan intelligence service handed Amin a report that the Soviet Union would invade the country and topple him, Amin claimed the report was a product of imperialism. His view can be explained by the fact that the Soviet Union, after several months, finally gave in to Amin's demands and sent troops into Afghanistan to secure the PDPA government. Contrary to common Western belief, Amin was informed of the Soviet decision to send troops into Afghanistan. General Tukharinov, Commander of the 40th Army, met with Afghan Major General Babadzhan to talk about Soviet troop movements before the Soviet army's intervention. On 25 December Dmitriy Ustinov issued a formal order, stating \"The state frontier of the Democratic Republic of Afghanistan is to be crossed on the ground and in the air by forces of the 40th Army and the Air Force at 1500 hrs on 25 December\". This was the formal beginning of the Soviet intervention in Afghanistan.\n\nConcerned for his safety, Amin moved from the Presidential Palace, in the centre of Kabul, to the Tajbeg Palace, which had previously been the headquarters of the Central Army Corps of the Afghan military. The palace was formidable, with walls strong enough to withstand artillery fire. According to Rodric Braithwaite, \"its defences had been carefully and intelligently organised\". All roads to the palace had been mined, with the exception of one, which had heavy machine guns and artillery positioned to defend it. To make matters worse for the Soviets, the Afghans had established a second line of defence which consisted of seven posts, \"each manned by four sentries armed with a machine gun, a mortar, and automatic rifles\". The external defences of the palace were handled by the Presidential Guard, which consisted of 2,500 troops and three T-54 tanks. Several Soviet commanders involved in the assassination of Amin thought the plan to attack the palace was \"crazy\". Several soldiers hesitated, claiming, in contradiction of what their commanders Yuri Drozdov and Vasily Kolesnik had told them (they in turn had been informed by the Soviet leadership), it seemed strange that Amin, the leader of the PDPA government, was an American sympathiser (accused of being a \"CIA agent\" by the Soviets) and betrayed the Saur Revolution. Despite several objections, the plan to assassinate Amin went ahead.\n\nBefore resorting to killing Amin by brute force, the Soviets had tried to poison him (but nearly killed his nephew instead) and to kill him with a sniper shot on his way to work (this proved impossible as the Afghans had improved their security measures). They even tried to poison Amin just hours before the assault on the Presidential Palace. Amin had organised a lunch for party members to show guests his palace and to celebrate Ghulam Dastagir Panjsheri's return from Moscow. Panjsheri's return improved the mood even further; he boasted that the Soviet divisions had already crossed the border, and that he and Gromyko always kept in contact with each other. During the meal, Amin and several of his guests lost consciousness as they had been poisoned. Luckily for Amin, but unfortunately for the Soviets, he survived his encounter with death. Mikhail Talybov, a KGB agent, was given responsibility for the poisonings.\n\nThe assault on the palace began shortly afterward. During the attack Amin still believed the Soviet Union was on his side, and told his adjutant, \"The Soviets will help us\". The adjutant replied that it was the Soviets who were attacking them; Amin initially replied that this was a lie. Only after he tried but failed to contact the Chief of the General Staff, he muttered, \"I guessed it. It's all true\". There are various accounts of how Amin died, but the exact details have never been confirmed. Amin was either killed by a deliberate attack or died by a \"random burst of fire\". Amin's son was fatally wounded and died shortly after. His daughter was wounded, but survived. It was Gulabzoy who had been given orders to kill Amin and Watanjar who later confirmed his death.\n\n\n", "id": "14280", "title": "Hafizullah Amin"}
{"url": "https://en.wikipedia.org/wiki?curid=14282", "text": "Hubris\n\nHubris (, also hybris, from ancient Greek ) describes a personality quality of extreme or foolish pride or dangerous over-confidence. In its ancient Greek context, it typically describes behavior that defies the norms of behavior or challenges the gods, and which in turn brings about the downfall, or nemesis, of the perpetrator of hubris. \n\nThe adjectival form of the noun \"hubris\" is \"hubristic\". Hubris is usually perceived as a characteristic of an individual rather than a group, although the group the offender belongs to may unintentionally suffer consequences from the wrongful act. Hubris often indicates a loss of contact with reality and an overestimation of one's own competence, accomplishments or capabilities. Contrary to common expectations, hubris is not necessarily associated with high self-esteem but with highly fluctuating or variable self-esteem, and a gap between inflated self perception and a more modest reality.\n\nHubris is generally considered a sin in world religions. C. S. Lewis writes, in \"Mere Christianity\", that pride is the \"anti-God\" state, the position in which the ego and the self are directly opposed to God: \"Unchastity, anger, greed, drunkenness, and all that, are mere fleabites in comparison: it was through Pride that the devil became the devil: Pride leads to every other vice: it is the complete anti-God state of mind.\"\n\nIn ancient Greek, \"hubris\" referred to actions that shamed and humiliated the victim for the pleasure or gratification of the abuser. The term had a strong sexual connotation, and the shame reflected on the perpetrator as well.\n\nViolations of the law against hubris included what might today be termed assault and battery; sexual crimes; or the theft of public or sacred property. Two well-known cases are found in the speeches of Demosthenes, a prominent statesman and orator in ancient Greece. These two examples occurred when first Midias punched Demosthenes in the face in the theatre (\"Against Midias\"), and second when (in \"Against Conon\") a defendant allegedly assaulted a man and crowed over the victim. Yet another example of hubris appears in Aeschines' \"Against Timarchus\", where the defendant, Timarchus, is accused of breaking the law of hubris by submitting himself to prostitution and anal intercourse. Aeschines brought this suit against Timarchus to bar him from the rights of political office and his case succeeded.\n\nIn Ancient Athens, hubris was defined as the use of violence to shame the victim (this sense of hubris could also characterize rape). Aristotle defined hubris as shaming the victim, not because of anything that happened to the committer or might happen to the committer, but merely for that committer's own gratification:\nto cause shame to the victim, not in order that anything may happen to you, nor because anything has happened to you, but merely for your own gratification. Hubris is not the requital of past injuries; this is revenge. As for the pleasure in hubris, its cause is this: naive men think that by ill-treating others they make their own superiority the greater.\n\nCrucial to this definition are the ancient Greek concepts of honour (τιμή, \"timē\") and shame (αἰδώς, \"aidōs\"). The concept of honour included not only the exaltation of the one receiving honour, but also the shaming of the one overcome by the act of hubris. This concept of honour is akin to a zero-sum game. Rush Rehm simplifies this definition of hubris to the contemporary concept of \"insolence, contempt, and excessive violence\".\n\nIn Greek mythology, when a figure's hubris offends the pagan gods of ancient Greece, it is usually punished; examples of such hubristic, sinful humans include Icarus, Phaethon, Arachne, Salmoneus, Niobe, Cassiopeia, and Tereus.\n\nIn its modern usage, hubris denotes overconfident pride and arrogance. Hubris is often associated with a lack of humility. Sometimes a person's hubris is also associated with a lack of knowledge.The accusation of hubris often implies that suffering or punishment will follow, similar to the occasional pairing of hubris and nemesis in Greek mythology. The proverb \"pride goeth (goes) before destruction, a haughty spirit before a fall\" (from the biblical Book of Proverbs, 16:18) is thought to sum up the modern use of hubris. Hubris is also referred to as \"pride that blinds,\" as it often causes a committer of hubris to act in foolish ways that belie common sense. In other words, the modern definition may be thought of as, \"that pride that goes just before the fall.\"\n\nExamples of hubris are often found in literature, most famously in \"Paradise Lost\": John Milton's depiction of Lucifer (who attempts to force the other angels to worship him, is cast down to hell by God and the innocent angels, and proclaims: \"Better to reign in hell than serve in heaven.\") Victor in Mary Shelley's \"Frankenstein\" manifests hubris in his attempt to become a great scientist by creating life through technological means, but eventually regrets this previous desire. Marlowe's play \"Doctor Faustus\" portrays the eponymous character as a scholar whose arrogance and pride compel him to sign a deal with the Devil, and retain his haughtiness until his death and damnation, despite the fact that he could easily have repented had he chosen to do so. Chinua Achebe's novel \"Things Fall Apart\" has been called a modern Greek tragedy, and the main character Okonkwo is a classic tragic hero whose hubris leads to his downfall. Another example is the character Walter White from the TV show \"Breaking Bad\", whose entry into the criminal world gradually caused him to eradicate all moral boundaries (except for the will to protect his family) and perform heinous crimes in order to empower his ever growing meth empire. The characters Pride and Father in the popular manga Fullmetal Alchemist were inspired by the sin of hubris.\n\nOne notable example is the Battle of Little Big Horn, as General George Armstrong Custer was apocryphally reputed to have said there: \"Where did all those damned Indians come from?\"\n\nMore recently, in his two-volume biography of Adolf Hitler, historian Ian Kershaw uses both 'hubris' and 'nemesis' as titles. The first volume, \"Hubris\", describes Hitler's early life and rise to political power. The second, \"Nemesis\", gives details of Hitler's role in the Second World War, and concludes with his fall and suicide in 1945.\n\n\n", "id": "14282", "title": "Hubris"}
