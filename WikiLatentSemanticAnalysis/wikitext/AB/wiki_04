{"url": "https://en.wikipedia.org/wiki?curid=13699", "text": "Hobart\n\nHobart () is the capital and most populous city of the Australian island state of Tasmania. It is the least populated state capital in Australia. Founded in 1803 as a penal colony, Hobart is Australia's second oldest capital city after Sydney, New South Wales. The city is located in the state's south-east on the estuary of the Derwent River, making it the most southern of Australia's capital cities. Its harbour forms the second-deepest natural port in the world.\n\nIn June 2015, the city had a greater area population of approximately 221,000. Its skyline is dominated by the Kunanyi/Mount Wellington, and much of the city's waterfront consists of reclaimed land. It is the financial and administrative heart of Tasmania, serving as the home port for both Australian and French Antarctic operations and acting as a major tourist hub, with over 1.192 million visitors in 2011/2012. The metropolitan area is often referred to as \"Greater Hobart\", to differentiate it from the City of Hobart, one of the five local government areas that cover the city.\n\nThe first European settlement began in 1803 as a penal colony at Risdon Cove on the eastern shores of the Derwent River, amid British concerns over the presence of French explorers. In 1804 it was moved to a better location at the present site of Hobart at Sullivans Cove. The city, initially known as \"Hobart Town\" or \"Hobarton\", was named after Lord Hobart, the British secretary of state for war and the colonies.\n\nThe area's indigenous inhabitants were members of the semi-nomadic \"Mouheneener\" tribe. Violent conflict with the European settlers, and the effects of diseases brought by them, dramatically reduced the aboriginal population, which was rapidly replaced by free settlers and the convict population. Charles Darwin visited Hobart Town in February 1836 as part of the Beagle expedition. He writes of Hobart and the Derwent estuary in his \"Voyage of the Beagle\":\n...The lower parts of the hills which skirt the bay are cleared; and the bright yellow fields of corn, and dark green ones of potatoes, appear very luxuriant... I was chiefly struck with the comparative fewness of the large houses, either built or building. Hobart Town, from the census of 1835, contained 13,826 inhabitants, and the whole of Tasmania 36,505.\n\nThe Derwent River was one of Australia's finest deepwater ports and was the centre of the Southern Ocean whaling and sealing trades. The settlement rapidly grew into a major port, with allied industries such as shipbuilding.\n\nHobart Town became a city on 21 August 1842, and was renamed Hobart from the beginning of 1881.\n\nHobart is located on the estuary of the Derwent River in the state's south-east. Geologically Hobart is built predominantly on Jurassic dolerite around the foothills interspersed with smaller areas of Triassic siltstone and Permian mudstone.\nHobart extends along both sides of the Derwent River; on the western shore from the Derwent valley in the north through the flatter areas of Glenorchy which rests on older Triassic sediment and into the hilly areas of New Town, Lenah Valley. Both of these areas rest on the younger Jurassic dolerite deposits, before stretching into the lower areas such as the beaches of Sandy Bay in the south, in the Derwent estuary. South of the Derwent estuary lies Storm Bay and the Tasman Peninsula.\n\nThe Eastern Shore also extends from the Derwent valley area in a southerly direction hugging the Meehan Range in the east before sprawling into flatter land in suburbs such as Bellerive. These flatter areas of the eastern shore rest on far younger deposits from the Quaternary. From there the city extends in an easterly direction through the Meehan Range into the hilly areas of Rokeby and Oakdowns, before reaching into the tidal flatland area of Lauderdale.\n\nHobart has access to a number of beach areas including those in the Derwent estuary itself; Sandy Bay, Cornelian Bay, Nutgrove, Kingston, Bellerive, and Howrah Beaches as well as many more in Frederick Henry Bay such as; Seven Mile, Roaches, Cremorne, Clifton, and Goats Beaches.\n\nHobart has a mild temperate oceanic climate (). The highest temperature recorded was on 4 January 2013 and the lowest was on 25 June 1972 and 11 July 1981. Annually, Hobart receives 40.8 clear days. Compared to other major Australian cities, Hobart has the fewest daily average hours of sunshine, with 5.9 hours per day. However, during the summer it has the most hours of daylight of any Australian city, with 15.2 hours on the summer solstice.\n\nAlthough Hobart itself rarely receives snow during the winter (the city's geographic position keeps temperatures from plummeting far below zero), the adjacent kunanyi/Mount Wellington is often seen with a snowcap. Mountain snow covering has also been known to occur during the other seasons. During the 20th century, the city itself has received snowfalls at sea level on average only once every 15 years; however, outer suburbs lying higher on the slopes of Mount Wellington receive snow more often, owing to cold air masses arriving from Antarctica coupled with them resting at higher altitude. These snow-bearing winds often carry on through Tasmania and Victoria to the Snowy Mountains in northern Victoria and southern New South Wales.\n\nAt the 2011 census there were 211,656 people in the greater Hobart area and the City of Hobart local government area had a population of 48,703. According to the 2011 census, approximately 17.9% of greater Hobart's residents were born overseas, commonly the United Kingdom, New Zealand and China.\n\nThe most common occupation categories were professionals (21.6%), clerical and administrative workers (16.1%), technicians and trades workers (13.8%), managers (11.5%) and community and Personal Service Workers (10.6%). The median weekly household income was $869, compared with $1,027 nationally.\n\nIn the 2011 census, 58.6% of residents specified a Christian religion. Major religious affiliations were Anglican (26.2%), Catholic (20.3%), Uniting Church (3.4%), and Presbyterian and Reformed (1.9%). In addition, 29.3% specified \"No Religion\" and 8.6% did not answer.\n\nHobart has a small Mormon community of around 642 (2011), with meetinghouses in Glenorchy, Rosny, and Glen Huon. There is also a synagogue where the Jewish community, of around 111 (2001), or 0.05% of the Hobart population, worships. Hobart has a Bahá'í community, with a Bahá'í Centre of Learning, located within the city.\n\nIn 2013, Hillsong Church established a Hillsong Connect campus in Hobart.\n\nHobart is a busy seaport. Its economy is heavily reliant on the sea and it serves as the home port for the Antarctic activities of Australia and France. The port loads around 2,000 tonnes of Antarctic cargo a year for the Australian research vessel \"Aurora Australis.\" The city is also a hub for cruise ships during the summer months, with up to 40 such ships docking during the course of the season.\n\nThe city also supports many other industries. Major local employers include catamaran builder Incat, zinc refinery Nyrstar, Cascade Brewery and Cadbury's Chocolate Factory, Norske Skog and Wrest Point Casino. The city also supports a host of light industry manufacturers.\nHobart also supports a huge tourist industry. Visitors come to the city to explore its historic inner suburbs and nationally acclaimed restaurants and cafes, as well as its vibrant music and nightlife culture. Tourists also come to visit the massive weekly market in Salamanca Place, as well as to use the city as a base from which to explore the rest of Tasmania.\n\nThe last 15–20 years has also seen Hobart's wine industry thrive as many vineyards have developed in countryside areas outside of the city in the Coal River Wine Region and D'Entrecasteaux Channel, including Moorilla Estate at Berriedale one of the most awarded vineyards in Australia.\nHobart is an Antarctic gateway city, with geographical proximity to East Antarctica and the Southern Ocean. Infrastructure is provided by the port of Hobart for scientific research and cruise ships, and Hobart International Airport supports an Antarctic Airlink to Wilkins Runway at Casey Station. Hobart is a logistics point for the French icebreaker l'Astrolabe.\n\nHobart is the home port for the Australian and French Antarctic programs, and provides port services for other visiting Antarctic nations and Antarctic cruise ships. Antarctic and Southern Ocean expeditions are supported by a specialist cluster offering cold climate products, services and scientific expertise. The majority of these businesses and organisations are members of the Tasmanian polar network, supported in part by the Tasmanian State Government.\n\nTasmania has a high concentration of Antarctic and Southern Ocean scientists. Hobart is home to the following Antarctic and Southern Ocean scientific institutions:\n\nThe Royal Tasmanian Botanical Gardens is a popular recreation area a short distance from the city centre. It is the second-oldest Botanic Gardens in Australia and holds extensive significant plant collections.\n\nHadley's Orient Hotel, on Hobart's Murray Street, is the oldest continuously operating hotel in Australia.\n\nkunanyi/Mount Wellington, accessible by passing through Fern Tree, is the dominant feature of Hobart's skyline. Indeed, many descriptions of Hobart have used the phrase \"nestled amidst the foothills\", so undulating is the landscape. At 1,271 metres, the mountain has its own ecosystems, is rich in biodiversity and plays a large part in determining the local weather.\n\nThe Tasman Bridge is also a uniquely important feature of the city, connecting the two shores of Hobart and visible from many locations. The Hobart Synagogue is the oldest synagogue in Australia and a rare surviving example of an Egyptian Revival synagogue.\n\nHobart is known for its well-preserved historic architecture, much of it dating back to the Georgian and Victorian eras, giving the city a distinctly \"Old World\" feel. For locals, this became a source of discomfiture about the city's convict past, but is now a draw card for tourists. Regions within the city centre, such as Salamanca Place, contain many of the city's heritage-listed buildings. Historic homes and mansions also exist in the suburbs.\n\nKelly's Steps were built in 1839 by shipwright and adventurer James Kelly to provide a short-cut from Kelly Street and Arthur Circus in Battery Point to the warehouse and dockyards district of Salamanca Place. In 1835, John Lee Archer designed and oversaw the construction of the sandstone Customs House, facing Sullivans Cove. Completed in 1840, it was used as Tasmania's parliament house, and is now commemorated by a pub bearing the same name (built in 1844) which is frequented by yachtsmen after they have completed the Sydney to Hobart yacht race.\n\nHobart is also home to many historic churches. The Scots Church (formerly known as St Andrew's) was built in Bathurst Street from 1834–36, and a small sandstone building within the churchyard was used as the city's first Presbyterian Church. The Salamanca Place warehouses and the Theatre Royal were also constructed in this period. The Greek revival St George's Anglican Church in Battery Point was completed in 1838, and a classical tower, designed by James Blackburn, was added in 1847. St Joseph's was built in 1840. St David's Cathedral, Hobart's first cathedral, was consecrated in 1874.\n\nHobart has very few high rise buildings in comparison to other Australian cities. This is partly a result of height limits imposed due to Hobart's proximity to Derwent River and Mount Wellington.\n\nHobart is home to the Tasmanian Symphony Orchestra, which is resident at the Federation Concert Hall on the city's waterfront. It offers a year-round program of concerts and is thought to be one of the finest small orchestras in the world. Hobart also plays host to the University of Tasmania's acclaimed Australian International Symphony Orchestra Institute (AISOI) which brings pre-professional advanced young musicians to town from all over Australia and internationally. The AISOI plays host to a public concert season during the first two weeks of December every year focusing on large symphonic music. Like the Tasmanian Symphony Orchestra, the AISOI uses the Federation Concert Hall as its performing base.\n\nHobart is home to Australia's oldest theatre, the Theatre Royal, as well as the Playhouse theatre, the Backspace theatre and many smaller stage theatres. It also has three Village Cinema complexes, one each in Hobart CBD, Glenorchy and Rosny, with the possibility of a fourth being developed in Kingston. The State Cinema in North Hobart specialises in arthouse and foreign films.\n\nThe city has also long been home to a thriving classical, jazz, folk, punk, hip-hop, electro, metal and rock music scene. Internationally recognised musicians such as metal acts Striborg and Psycroptic, indie-electro bands The Paradise Motel and The Scientists of Modern Music, singer/songwriters Sacha Lucashenko (of The Morning After Girls), Michael Noga (of The Drones), and Monique Brumby, two-thirds of indie rock band Love of Diagrams, post punk band Sea Scouts, theremin player Miles Brown, blues guitarist Phil Manning (of blues-rock band Chain), power-pop group The Innocents are all successful expatriates. In addition, founding member of Violent Femmes, Brian Ritchie, now calls Hobart home, and has formed a local band, The Green Mist. Ritchie also curates the annual international arts festival MONA FOMA, held at Salamanca Place's waterfront venue, Princes Wharf, Shed No. 1. Hobart hosts many significant festivals including winter's landmark cultural event, the \"Festival of Voices\", Australia's premier festival celebration of voice, and Tasmania's biennial international arts festival Ten Days On The Island. Other festivals, including the \"Hobart Fringe Festival\", Hobart Summer Festival, Southern Roots Festival, the Falls Festival in Marion Bay and the Soundscape Festival also capitalise on Hobart's artistic communities.\n\nHobart is home to the Tasmanian Museum and Art Gallery. The Meadowbank Estate winery and restaurant features a floor mural by Tom Samek, part funded by the Federal Government. The Museum of Old and New Art (MONA) opened in 2011 to coincide with the third annual MONA FOMA festival. The multi-storey MONA gallery was built directly underneath the historic Sir Roy Grounds courtyard house, overlooking the Derwent River. This building serves as the entrance to the MONA Gallery.\n\nDesigned by the prolific architect Sir Roy Grounds, the 17-storey Wrest Point Hotel Casino in Sandy Bay, opened as Australia's first legal casino in 1973.\n\nThe city's nightlife primarily revolves around Salamanca Place, the waterfront area, Elizabeth St in North Hobart and Sandy Bay, but popular pubs, bars and nightclubs exist around the city as well. Major national and international music events are usually held at the Derwent Entertainment Centre, or the Casino. Popular restaurant strips include Elizabeth Street in North Hobart, and Salamanca Place near the waterfront. These include numerous ethnic restaurants including Chinese, Thai, Greek, Pakistani, Italian, Indian and Mexican. The major shopping street in the CBD is Elizabeth Street, with the pedestrianised Elizabeth Mall and the General Post Office.\n\nHobart is internationally famous among the yachting community as the finish of the Sydney to Hobart Yacht Race which starts in Sydney on Boxing Day (the day after Christmas Day). The arrival of the yachts is celebrated as part of the Hobart Summer Festival, a food and wine festival beginning just after Christmas and ending in mid-January. The Taste of Tasmania is a major part of the festival, where locals and visitors can taste fine local and international food and wine.\n\nThe city is the finishing point of the Targa Tasmania rally car event, which has been held annually in April since 1991.\n\nThe annual Tulip Festival at the Royal Tasmanian Botanical Gardens is a popular Spring celebration in the city.\n\nThe Australian Wooden Boat Festival is a biennial event held in Hobart celebrating wooden boats. It is held concurrently with the Royal Hobart Regatta, which began in 1830 and is therefore Tasmania's oldest surviving sporting event.\n\nMost of Hobart's sporting teams in national competitions are statewide teams rather than exclusively city teams.\n\nCricket is the most popular game of the city. The Tasmanian Tigers cricket team plays its home games at the Bellerive Oval on the Eastern Shore. A new team, Hobart Hurricanes represent the city in the Big Bash League. Bellerive Oval has been the breeding ground of some world class cricket players including the former Australia captain Ricky Ponting.\n\nDespite Australian rules football's huge popularity in the state of Tasmania, the state does not have a team in the Australian Football League. However, a bid for an Tasmanian AFL team is a popular topic among football fans. The State government is one of the potential sponsors of such a team. Local domestic club football is still played. Tasmanian State League football features five clubs from Hobart, and other leagues such as Southern Football League and the Old Scholars Football Association are also played each Winter.\n\nThe city has two local rugby league football teams (Hobart Tigers and South Hobart Storm) that compete in the Tasmanian Rugby League.\n\nTasmania is not represented by teams in the NRL, Super Rugby, netball, soccer, or basketball leagues. However, the \"Oasis Hobart Chargers\" team does represent Hobart in the South East Australian Basketball League. Besides the bid for an AFL club which was passed over in favour of a second Queensland team, despite several major local businesses and the Premier pioneering for a club, there is also a Hobart bid for entry into the A-League.\n\nHockey Tasmania has a men's team (the Tasmanian Tigers) and a women's team (the Van Demons) competing in the Australian Hockey League.\n\nThe city co-hosted the basketball FIBA Oceania Championship 1975.\n\nFive free-to-air television stations service Hobart:\nEach station broadcasts a primary channel and several multichannels.\n\nHobart is served by twenty-eight digital free-to-air television channels:\n\nThe majority of pay television services are provided by Foxtel via satellite, although other smaller pay television providers do service Hobart.\n\nCommercial radio stations licensed to cover the Hobart market include Triple M Hobart, Hit 100.9 and 7HO FM. Local community radio stations include Christian radio station Ultra106five, Edge Radio and 92FM which targets the wider community with specialist programmes. The five ABC radio networks available on analogue radio broadcast to Hobart via 936 ABC Hobart, Radio National, Triple J, NewsRadio and ABC Classic FM.\n\nHobart's major newspaper is \"The Mercury\", which was founded by John Davies in 1854 and has been continually published ever since. The paper is currently owned and operated by Rupert Murdoch's News Limited.\n\nThe Greater Hobart metropolitan area consists of five local government areas of which three, City of Hobart, City of Glenorchy and City of Clarence are designated as cities. Hobart also includes the urbanised local governments of the Municipality of Kingborough and Municipality of Brighton. Each local government services all the suburbs that are within its geographical boundaries and are responsible for their own urban area, up to a certain scale, and residential planning as well as waste management and mains water storage.\n\nMost citywide events such as the Taste of Tasmania and Hobart Summer Festival are funded by the Tasmanian State Government as a joint venture with the Hobart City Council. Urban planning of the Hobart CBD in particular the Heritage listed areas such as Sullivans Cove are also intensely scrutinised by State Government, which is operated out of Parliament House on the waterfront.\n\n\nHobart is home to the main campus of the University of Tasmania, located in Sandy Bay. On-site accommodation colleges include Christ College, Jane Franklin Hall and St John Fisher College. Other campuses are in Launceston and Burnie.\n\nThe G.H.A (Greater Hobart Area) contains 122 Primary, Secondary and Pretertiary (College) schools distributed throughout Clarence, Glenorchy and Hobart City Councils and Kingborough and Brighton Municipalities. These schools are made up of a mix of public, catholic, private and independent run, with the heaviest distribution lying in the more densely populated West around the Hobart city core. The city also maintains a large Polytechnics College campus (formerly TAFE Tasmania) for post-secondary studies in Trades and other non-university qualifications.\n\nThe only public transportation within the city of Hobart is via a network of Metro Tasmania buses funded by\nthe Tasmanian Government and a small number of private bus services. Like many large Australian cities, Hobart once operated passenger tram services, a trolleybus network consisting of six routes which operated until 1968. However, the tramway closed in the early 1960s. The tracks are still visible in the older streets of Hobart.\n\nSuburban passenger trains, run by the Tasmanian Government Railways, were closed in 1974 and the intrastate passenger service, the Tasman Limited, ceased running in 1978. Recently though there has been a push from the city, and increasingly from government, to establish a light rail network, intended to be fast, efficient, and eco-friendly, along existing tracks in a North South corridor; to help relieve the frequent jamming of traffic in Hobart CBD.\n\nThe main arterial routes within the urban area are the Brooker Highway to Glenorchy and the northern suburbs, the Tasman Bridge and Bowen Bridge across the river to Rosny and the Eastern Shore. The East Derwent Highway to Lindisfarne, Geilston Bay, and Northwards to Brighton, the South Arm Highway leading to Howrah, Rokeby, Lauderdale and Opossum Bay and the Southern Outlet south to Kingston and the D'Entrecasteaux Channel. Leaving the city, motorists can travel the Lyell Highway to the west coast, Midland Highway to Launceston and the north, Tasman Highway to the east coast, or the Huon Highway to the far south.\n\nFerry services from Hobart's Eastern Shore into the city were once a common form of public transportation, but with lack of government funding, as well as a lack of interest from the private sector, there has been the demise of a regular commuter ferry service – leaving Hobart's commuters relying solely on travel by automobiles and buses. There is however a water taxi service operating from the Eastern Shore into Hobart which provides an alternative to the Tasman Bridge.\n\nHobart is served by Hobart International Airport with flights to/from Melbourne (Qantas, Virgin Australia, Jetstar Airways and Tiger Airways Australia); Sydney (Qantas, Jetstar and Virgin); Brisbane (Virgin); Gold Coast (Jetstar); and Canberra (Virgin). The smaller Cambridge Aerodrome mainly serves small charter airlines offering local tourist flights. In the past decade, Hobart International Airport received a huge upgrade, with the airport now being a first class airport facility.\n\nIn 2009, it was announced that Hobart Airport would receive more upgrades, including a first floor, aerobridges (currently, passengers must walk on the tarmac) and shopping facilities. Possible new international flights to Asia and New Zealand, and possible new domestic flights to Darwin, Cairns and Perth have been proposed. A second runway, possibly to be constructed in the next 15 years, would assist with growing passenger numbers to Hobart. Hobart Control Tower may be renovated and fitted with new radar equipment, and the airport's carpark may be extended further. Also, new facilities will be built just outside the airport. A new service station, hotel and day care centre have already been built and the road leading to the airport has been maintained and re-sealed. In addition, Tony Abbott the former Prime minister of Australia promised in the lead up to the 2013 federal election that his government would provide the funding needed for an extension of the one and only runway at Hobart international. This would allow larger planes to land which could boost the economy.\n\n\n\n\n\n\n\n", "id": "13699", "title": "Hobart"}
{"url": "https://en.wikipedia.org/wiki?curid=13700", "text": "Hesiod\n\nHesiod ( or ; \"Hēsíodos\") was a Greek poet generally thought by scholars to have been active between 750 and 650 BC, around the same time as Homer. He is generally regarded as the first written poet in the Western tradition to regard himself as an individual persona with an active role to play in his subject. Ancient authors credited Hesiod and Homer with establishing Greek religious customs. Modern scholars refer to him as a major source on Greek mythology, farming techniques, early economic thought (he is sometimes considered history's first economist), archaic Greek astronomy and ancient time-keeping.\n\nThe dating of his life is a contested issue in scholarly circles and it is covered below in Dating. Epic narrative allowed poets like Homer no opportunity for personal revelations. However, Hesiod's extant work comprises didactic poems in which he went out of his way to let his audience in on a few details of his life. There are three explicit references in \"Works and Days\", as well as some passages in his \"Theogony\" that support inferences made by scholars. The former poem says that his father came from Cyme in Aeolis (on the coast of Asia Minor, a little south of the island Lesbos) and crossed the sea to settle at a hamlet, near Thespiae in Boeotia, named Ascra, \"a cursed place, cruel in winter, hard in summer, never pleasant\" (\"Works\", l. 640). Hesiod's patrimony there, a small piece of ground at the foot of Mount Helicon, occasioned lawsuits with his brother Perses, who seems, at first, to have cheated him of his rightful share thanks to corrupt authorities or \"kings\" but later became impoverished and ended up scrounging from the thrifty poet (\"Works\" l. 35, 396).\n\nUnlike their father, Hesiod was averse to sea travel, but he once crossed the narrow strait between the Greek mainland and Euboea to participate in funeral celebrations for one Athamas of Chalcis, and there won a tripod in a singing competition. He also describes a meeting between himself and the Muses on Mount Helicon, where he had been pasturing sheep when the goddesses presented him with a laurel staff, a symbol of poetic authority (\"Theogony\", ll. 22–35) Fanciful though the story might seem, the account has led ancient and modern scholars to infer that he did not play the lyre or that he was not professionally trained, or he would have been presented with a lyre instead.\n\nSome scholars have seen Perses as a literary creation, a foil for the moralizing that Hesiod develops in \"Works and Days\", but there are also arguments against that theory. For example, it is quite common for works of moral instruction to have an imaginative setting, as a means of getting the audience's attention, but it is difficult to see how Hesiod could have travelled around the countryside entertaining people with a narrative about himself if the account was known to be fictitious. Gregory Nagy, on the other hand, sees both \"Persēs\" (\"the destroyer\": \"perthō\") and \"Hēsiodos\" (\"he who emits the voice:\" \"hiēmi\" and \"audē\") as fictitious names for poetical personae. \n\nIt might seem unusual that Hesiod's father migrated from Asia Minor westwards to mainland Greece, the opposite direction to most colonial movements at the time, and Hesiod himself gives no explanation for it. However around 750 BC or a little later, there was a migration of seagoing merchants from his original home in Cyme in Asia Minor to Cumae in Campania (a colony they shared with Euboeans), and possibly his move west had something to do with that, since Euboea is not far from Boeotia, where he eventually established himself and his family. The family association with Cyme might explain his familiarity with eastern myths, evident in his poems, though the Greek world might have already developed its own versions of them.\n\nIn spite of Hesiod's complaints about poverty, life on his father's farm could not have been too uncomfortable if \"Works and Days\" is anything to judge by, since he describes the routines of prosperous yeomanry rather than peasants. His farmer employs a friend (l. 370) as well as servants (ll. 502, 573, 597, 608, 766), an energetic and responsible ploughman of mature years (ll. 469–71), a slave boy to cover the seed (ll. 441–6), a female servant to keep house (ll. 405, 602) and working teams of oxen and mules (ll. 405, 607f.). One modern scholar surmises that Hesiod may have learned about world geography, especially the catalogue of rivers in \"Theogony\" (ll. 337–45), listening to his father's accounts of his own sea voyages as a merchant The father probably spoke in the Aeolian dialect of Cyme but Hesiod probably grew up speaking the local Boeotian dialect. However, while his poetry features some Aeolisms there are no words that are certainly Boeotian—he composed in the main literary dialect of the time (Homer's dialect): Ionian.\n\nIt is probable that Hesiod wrote his poems down, or dictated them, rather than passed them on orally, as rhapsodes did—otherwise the pronounced personality that now emerges from the poems would surely have been diluted through oral transmission from one rhapsode to another. Pausanias asserted that Boeotians showed him an old tablet made of lead on which the Works were engraved. If he did write or dictate, it was perhaps as an aid to memory or because he lacked confidence in his ability to produce poems extempore, as trained rhapsodes could do. It certainly wasn't in a quest for immortal fame since poets in his era had no such notions. However, some scholars suspect the presence of large-scale changes in the text and attribute this to oral transmission. Possibly he composed his verses during idle times on the farm, in the spring before the May harvest or the dead of winter.\n\nThe personality behind the poems is unsuited to the kind of \"aristocratic withdrawal\" typical of a rhapsode but is instead \"argumentative, suspicious, ironically humorous, frugal, fond of proverbs, wary of women.\" He was in fact a misogynist of the same calibre as the later poet, Semonides. He resembles Solon in his preoccupation with issues of good versus evil and \"how a just and all-powerful god can allow the unjust to flourish in this life\". He resembles Aristophanes in his rejection of the idealised hero of epic literature in favour of an idealised view of the farmer. Yet the fact that he could eulogise kings in \"Theogony\" (ll. 80ff, 430, 434) and denounce them as corrupt in \"Works and Days\" suggests that he could resemble whichever audience he composed for.\n\nVarious legends accumulated about Hesiod and they are recorded in several sources: \n\nTwo different—yet early—traditions record the site of Hesiod's grave. One, as early as Thucydides, reported in Plutarch, the \"Suda\" and John Tzetzes, states that the Delphic oracle warned Hesiod that he would die in Nemea, and so he fled to Locris, where he was killed at the local temple to Nemean Zeus, and buried there. This tradition follows a familiar ironic convention: the oracle that predicts accurately after all. The other tradition, first mentioned in an epigram by Chersias of Orchomenus written in the 7th century BC (within a century or so of Hesiod's death) claims that Hesiod lies buried at Orchomenus, a town in Boeotia. According to Aristotle's \"Constitution of Orchomenus,\" when the Thespians ravaged Ascra, the villagers sought refuge at Orchomenus, where, following the advice of an oracle, they collected the ashes of Hesiod and set them in a place of honour in their \"agora\", next to the tomb of Minyas, their eponymous founder. Eventually they came to regard Hesiod too as their \"hearth-founder\" ( / \"oikistēs\"). Later writers attempted to harmonize these two accounts.\n\nGreeks in the late fifth and early 4th centuries BC considered their oldest poets to be Orpheus, Musaeus, Hesiod and Homer—in that order. Thereafter, Greek writers began to consider Homer earlier than Hesiod. Devotees of Orpheus and Musaeus were probably responsible for precedence being given to their two cult heroes and maybe the Homeridae were responsible in later antiquity for promoting Homer at Hesiod's expense.\n\nThe first known writers to locate Homer earlier than Hesiod were Xenophanes and Heraclides Ponticus, though Aristarchus of Samothrace was the first actually to argue the case. Ephorus made Homer a younger cousin of Hesiod, the 5th century BC historian Herodotus (\"Histories\", 2.53) evidently considered them near-contemporaries, and the 4th century BC sophist Alcidamas in his work \"Mouseion\" even brought them together for an imagined poetic \"agon\", which survives today as the \"Contest of Homer and Hesiod\". Most scholars today agree with Homer's priority but there are good arguments on either side.\n\nHesiod certainly predates the lyric and elegiac poets whose work has come down to the modern era. Imitations of his work have been observed in Alcaeus, Epimenides, Mimnermus, Semonides, Tyrtaeus and Archilochus, from which it has been inferred that the latest possible date for him is about 650 BC.\n\nAn upper limit of 750 BC is indicated by a number of considerations, such as the probability that his work was written down, the fact that he mentions a sanctuary at Delphi that was of little national significance before c. 750 BC (\"Theogony\" l. 499), and he lists rivers that flow into the Euxine, a region explored and developed by Greek colonists beginning in the 8th century BC. (\"Theogony\" 337–45).\n\nHesiod mentions a poetry contest at Chalcis in Euboea where the sons of one Amphidamas awarded him a tripod (\"Works and Days\" ll.654–662). Plutarch identified this Amphidamas with the hero of the Lelantine War between Chalcis and Eretria and he concluded that the passage must be an interpolation into Hesiod's original work, assuming that the Lelantine War was too late for Hesiod. Modern scholars have accepted his identification of Amphidamas but disagreed with his conclusion. The date of the war is not known precisely but estimates placing it around 730–705 BC, fit the estimated chronology for Hesiod. In that case, the tripod that Hesiod won might have been awarded for his rendition of \"Theogony\", a poem that seems to presuppose the kind of aristocratic audience he would have met at Chalcis.\n\nThree works have survived which are attributed to Hesiod by ancient commentators: \"Works and Days\", \"Theogony\", and \"Shield of Heracles\". Other works attributed to him are only found now in fragments. The surviving works and fragments were all written in the conventional metre and language of epic. However, the \"Shield of Heracles\" is now known to be spurious and probably was written in the sixth century BC. Many ancient critics also rejected \"Theogony\" (e.g., Pausanias 9.31.3), even though Hesiod mentions himself by name in that poem. \"Theogony\" and \"Works and Days\" might be very different in subject matter, but they share a distinctive language, metre, and prosody that subtly distinguish them from Homer's work and from the \"Shield of Heracles\" (see Hesiod's Greek below). Moreover, they both refer to the same version of the Prometheus myth. Yet even these authentic poems may include interpolations. For example, the first ten verses of the \"Works and Days\" may have been borrowed from an Orphic hymn to Zeus (they were recognised as not the work of Hesiod by critics as ancient as Pausanias).\nSome scholars have detected a proto-historical perspective in Hesiod, a view rejected by Paul Cartledge, for example, on the grounds that Hesiod advocates a not-forgetting without any attempt at verification. Hesiod has also been considered the father of gnomic verse. He had \"a passion for systematizing and explaining things\". Ancient Greek poetry in general had strong philosophical tendencies and Hesiod, like Homer, demonstrates a deep interest in a wide range of 'philosophical' issues, from the nature of divine justice to the beginnings of human society. Aristotle (\"Metaphysics\" 983b–987a) believed that the question of first causes may even have started with Hesiod (\"Theogony\" 116–53) and Homer (\"Iliad\" 14.201, 246).\n\nHe viewed the world from outside the charmed circle of aristocratic rulers, protesting against their injustices in a tone of voice that has been described as having a \"grumpy quality redeemed by a gaunt dignity\" but, as stated in the biography section, he could also change to suit the audience. This ambivalence appears to underlie his presentation of human history in \"Works and Days\", where he depicts a golden period when life was easy and good, followed by a steady decline in behaviour and happiness through the silver, bronze, and Iron Ages – except that he inserts a heroic age between the last two, representing its warlike men as better than their bronze predecessors. He seems in this case to be catering to two different world-views, one epic and aristocratic, the other unsympathetic to the heroic traditions of the aristocracy.\n\nThe \"Theogony\" is commonly considered Hesiod's earliest work. Despite the different subject matter between this poem and the \"Works and Days\", most scholars, with some notable exceptions, believe that the two works were written by the same man. As M.L. West writes, \"Both bear the marks of a distinct personality: a surly, conservative countryman, given to reflection, no lover of women or life, who felt the gods' presence heavy about him.\"\n\nThe \"Theogony\" concerns the origins of the world (cosmogony) and of the gods (theogony), beginning with Chaos, Gaia, Tartarus and Eros, and shows a special interest in genealogy. Embedded in Greek myth, there remain fragments of quite variant tales, hinting at the rich variety of myth that once existed, city by city; but Hesiod's retelling of the old stories became, according to Herodotus, the accepted version that linked all Hellenes.\n\nThe creation myth in Hesiod has long been held to have Eastern influences, such as the Hittite Song of Kumarbi and the Babylonian Enuma Elis. This cultural crossover would have occurred in the eighth and ninth century Greek trading colonies such as Al Mina in North Syria. (For more discussion, read Robin Lane Fox's \"Travelling Heroes\" and Walcot's \"Hesiod and the Near East.\")\n\nThe \"Works and Days\" is a poem of over 800 lines which revolves around two general truths: labour is the universal lot of Man, but he who is willing to work will get by. Scholars have interpreted this work against a background of agrarian crisis in mainland Greece, which inspired a wave of documented colonisations in search of new land. This poem is one of the earliest known musings on economic thought.\n\nThis work lays out the five Ages of Man, as well as containing advice and wisdom, prescribing a life of honest labour and attacking idleness and unjust judges (like those who decided in favour of Perses) as well as the practice of usury. It describes immortals who roam the earth watching over justice and injustice. The poem regards labor as the source of all good, in that both gods and men hate the idle, who resemble drones in a hive. In the horror of the triumph of violence over hard work and honor, verses describing the \"Golden Age\" present the social character and practice of nonviolent diet through agriculture and fruit-culture as a higher path of living sufficiently.\n\nIn addition to the \"Theogony\" and \"Works and Days\", numerous other poems were ascribed to Hesiod during antiquity. Modern scholarship has doubted their authenticity, and these works are generally referred to as forming part of the \"Hesiodic Corpus\" whether or not their authorship is accepted. The situation is summed up in this formulation by Glenn Most:\n\nOf these works forming the extended Hesiodic corpus, only the \"Shield of Heracles\" (, \"Aspis Hērakleous\") is transmitted intact via a medieval manuscript tradition.\n\nClassical authors also attributed to Hesiod a lengthy genealogical poem known as \"Catalogue of Women\" or \"Ehoiai\" (because sections began with the Greek words \"ē hoiē,\" \"Or like the one who ...\"). It was a mythological catalogue of the mortal women who had mated with gods, and of the offspring and descendants of these unions.\n\nSeveral additional hexameter poems were ascribed to Hesiod:\n\nIn addition to these works, the \"Suda\" lists an otherwise unknown \"dirge for Batrachus, [Hesiod's] beloved\".\n\n\nThe Roman bronze bust, the so-called \"Pseudo-Seneca,\" of the late first century BC found at Herculaneum is now thought not to be of Seneca the Younger. It has been identified by Gisela Richter as an imagined portrait of Hesiod. In fact, it has been recognized since 1813 that the bust was not of Seneca, when an inscribed herma portrait of Seneca with quite different features was discovered. Most scholars now follow Richter's identification.\n\nHesiod employed the conventional dialect of epic verse, which was Ionian. Comparisons with Homer, a native Ionian, can be unflattering. Hesiod's handling of the dactylic hexameter was not as masterful or fluent as Homer's and one modern scholar refers to his \"hobnailed hexameters\". His use of language and meter in \"Works and Days\" and \"Theogony\" distinguishes him also from the author of the \"Shield of Heracles\". All three poets, for example, employed digamma inconsistently, sometimes allowing it to affect syllable length and meter, sometimes not. The ratio of observance/neglect of digamma varies between them. The extent of variation depends on how the evidence is collected and interpreted but there is a clear trend, revealed for example in the following set of statistics.\nHesiod does not observe digamma as often as the others do. That result is a bit counter-intuitive since digamma was still a feature of the Boeotian dialect that Hesiod probably spoke, whereas it had already vanished from the Ionic vernacular of Homer. This anomaly can be explained by the fact that Hesiod made a conscious effort to compose like an Ionian epic poet at a time when digamma was not heard in Ionian speech, while Homer tried to compose like an older generation of Ionian bards, when it was heard in Ionian speech. There is also a significant difference in the results for \"Theogony\" and \"Works and Days\", but that is merely due to the fact that the former includes a catalog of divinities and therefore it makes frequent use of the definite article associated with digamma, oἱ.\n\nThough typical of epic, his vocabulary features some significant differences from Homer's. One scholar has counted 278 un-Homeric words in \"Works and Days\", 151 in \"Theogony\" and 95 in \"Shield of Heracles\". The disproportionate number of un-Homeric words in \"W & D\" is due to its un-Homeric subject matter. Hesiod's vocabulary also includes quite a lot of formulaic phrases that are not found in Homer, which indicates that he may have been writing within a different tradition.\n\n\n\n\n", "id": "13700", "title": "Hesiod"}
{"url": "https://en.wikipedia.org/wiki?curid=13702", "text": "Hebrew numerals\n\nThe system of Hebrew numerals is a quasi-decimal alphabetic numeral system using the letters of the Hebrew alphabet.\nThe system was adapted from that of the Greek numerals in the late 2nd century BC.\n\nThe current numeral system is also known as the \"Hebrew alphabetic numerals\" to contrast with earlier systems of writing numerals used in classical antiquity. These systems were inherited from usage in the Aramaic and Phoenician scripts, attested from c. 800 BC in the so-called Samaria ostraca and sometimes known as \"Hebrew-Aramaic numerals\", ultimately derived from the Egyptian Hieratic numerals.\n\nThe Greek system was adopted in Hellenistic Judaism and had been in use in Greece since about the 5th century BC.\nIn this system, there is no notation for zero, and the numeric values for individual letters are added together. Each unit (1, 2, ..., 9) is assigned a separate letter, each tens (10, 20, ..., 90) a separate letter, and the first four hundreds (100, 200, 300, 400) a separate letter. The later hundreds (500, 600, 700, 800 and 900) are represented by the sum of two or three letters representing the first four hundreds. To represent numbers from 1,000 to 999,999, the same letters are reused to serve as thousands, tens of thousands, and hundreds of thousands. Gematria (Jewish numerology) uses these transformations extensively.\n\nIn Israel today, the decimal system of Arabic numerals (ex. 0, 1, 2, 3, etc.) is used in almost all cases (money, age, date on the civil calendar). The Hebrew numerals are used only in special cases, such as when using the Hebrew calendar, or numbering a list (similar to a, b, c, d, etc.), much as Roman numerals are used in the West.\n\nNumbers in Hebrew from zero to one million. Hebrew alphabet are used to a limited extent to represent numbers, widely used on calendars. For other uses Arabic numerals are included. Cardinal and ordinal numbers must agree in gender with the noun they are describing. If there is no such noun (e.g. telephone numbers), the feminine form is used. For ordinal numbers greater than ten the cardinal is used and numbers above the value 20 have no gender.\n\nNote: For ordinal numbers greater than 10, cardinal numbers are used instead.\n\nNote: For numbers greater than 20, gender does not apply.\n\nCardinal and ordinal numbers must agree in gender (masculine or feminine; mixed groups are treated as masculine) with the noun they are describing. If there is no such noun (e.g. a telephone number or a house number in a street address), the feminine form is used. Ordinal numbers must also agree in number and definite status like other adjectives. The cardinal number precedes the noun (ex. shlosha yeladim), except for the number one which succeeds it (ex. yeled ehad). The number two is special - shnayim (m.) and shtayim (f.) become shney (m.) and shtey (f.) when followed by the noun they count. For ordinal numbers (numbers indicating position) greater than ten the cardinal is used.\n\nThe Hebrew numeric system operates on the additive principle in which the numeric values of the letters are added together to form the total. For example, 177 is represented as קעז which (from right to left) corresponds to 100 + 70 + 7 = 177.\n\nMathematically, this type of system requires 27 letters (1-9, 10-90, 100-900). In practice the last letter, \"tav\" (which has the value 400) is used in combination with itself and/or other letters from \"kof\" (100) onwards, to generate numbers from 500 and above. Alternatively, the 22-letter Hebrew numeral set is sometimes extended to 27 by using 5 \"sofit\" (final) forms of the Hebrew letters.\n\nBy convention, the numbers 15 and 16 are represented as ט״ו (9 + 6) and ט״ז (9 + 7), respectively, in order to refrain from using the two-letter combinations י-ה (10 + 5) and י-ו (10 + 6), which are alternate written forms for the Name of God in everyday writing. In the calendar, this manifests every full moon, since all Hebrew months start on a new moon.\n\nCombinations which would spell out words with negative connotations are sometimes avoided by switching the order of the letters. For instance, 744 which should be written as תשמ״ד (meaning \"you/it will be destroyed\") might instead be written as תשד״מ or תמש״ד (meaning \"end to demon\").\n\nThe Hebrew numeral system has sometimes been extended to include the five final letter forms——which are then used to indicate the numbers from 500 to 900.\n\nAlternate forms for 500 to 900 are—\n\nGershayim (U+05F4 in Unicode, and resembling a double quote mark) (sometimes erroneously referred to as \"merkha'ot\", which is Hebrew for double quote) are inserted before (to the right of) the last (leftmost) letter to indicate that the sequence of letters represents a number rather than a word. This is used in the case where a number is represented by two or more Hebrew numerals (\"e.g.,\" 28 → כ״ח).\n\nSimilarly, a single Geresh (U+05F3 in Unicode, and resembling a single quote mark) is appended after (to the left of) a single letter to indicate that the letter represents a number rather than a (one-letter) word. This is used in the case where a number is represented by a single Hebrew numeral (\"e.g.,\" 100 → ק׳).\n\nNote that Geresh and Gershayim merely indicate \"\"not a (normal) word.\"\" Context usually determines whether they indicate a number or something else (such as \"\"abbreviation\"\").\n\nAn alternative method found in old manuscripts and still found on modern-day tombstones is to put a dot above each letter of the number.\n\nIn print, Arabic numerals are employed in Modern Hebrew for most purposes. Hebrew numerals are used nowadays primarily for writing the days and years of the Hebrew calendar; for references to traditional Jewish texts (particularly for Biblical chapter and verse and for Talmudic folios); for bulleted or numbered lists (similar to \"A\", \"B\", \"C\", \"etc.\", in English); and in numerology (gematria).\n\nThousands are counted separately, and the thousands count precedes the rest of the number (to the \"right\", since Hebrew is read from right to left). There are no special marks to signify that the “count” is starting over with thousands, which can theoretically lead to ambiguity, although a single quote mark is sometimes used after the letter. When specifying years of the Hebrew calendar in the present millennium, writers usually omit the thousands (which is presently 5 []), but if they do not this is accepted to mean 5 * 1000, with no ambiguity. The current Israeli coinage includes the thousands.\n\n“Monday, 15 Adar 5764” (where 5764 = 5(×1000) + 400 + 300 + 60 + 4, and 15 = 9 + 6):\n\n“Thursday, 3 Nisan 5767” (where 5767 = 5(×1000) + 400 + 300 + 60 + 7):\n\nTo see how \"today's\" date in the Hebrew calendar is written, see, for example, Hebcal date converter.\n\n5780 (2019–20) = תש״פ\n\n5779 (2018–19) = תשע״ט\n\n5772 (2011–12) = תשע״ב\n\n5771 (2010–11) = תשע״א\n\n5770 (2009–10) = תש״ע\n\n5769 (2008–09) = תשס״ט\n\n5761 (2000–01) = תשס״א\n\n5760 (1999–00) = תש״ס\n\nThe Abjad numerals are equivalent to the Hebrew numerals up to 400. The Greek numerals differ from the Hebrew ones from 90 upwards because in the Greek alphabet there is no equivalent for \"Tsadi\" (צ).\n\n\n", "id": "13702", "title": "Hebrew numerals"}
{"url": "https://en.wikipedia.org/wiki?curid=13704", "text": "Hydroxy\n\nHydroxy can refer to:\n", "id": "13704", "title": "Hydroxy"}
{"url": "https://en.wikipedia.org/wiki?curid=13706", "text": "Hero\n\nA hero (masculine) or heroine (feminine) is a person or main character of a literary work who, in the face of danger, combats adversity through impressive feats of ingenuity, bravery or strength, often sacrificing his or her own personal concerns for some greater good.\n\nThe concept of the hero was first founded in classical literature. It is the main or revered character in heroic epic poetry celebrated through ancient legends of a people; often striving for military conquest and living by a continually flawed personal honor code. The definition of a hero has changed throughout time, and the Merriam Webster dictionary defines a hero as \"a person who is admired for great or brave acts or fine qualities\". Examples of heroes range from mythological figures, such as Gilgamesh, Achilles and Iphigenia, to historical figures, such as Joan of Arc, modern heroes like Florence Nightingale and Mahatma Gandhi to fantasy fictional heroes including Superman and Batman.\n\nThe word \"hero\" comes from the Greek ἥρως (\"hērōs\"), \"hero, warrior\", particularly one such as Heracles with divine ancestry or later given divine honors (literally \"protector\" or \"defender\"). Before the decipherment of Linear B the original form of the word was assumed to be *, \"hērōw-\"; R. S. P. Beekes has proposed a Pre-Greek origin.\n\nAccording to the \"American Heritage Dictionary of the English Language,\" the Indo-European root is \"*ser\" meaning \"to protect\". According to Eric Partridge in \"Origins,\" the Greek word \"Hērōs\" \"is akin to\" the Latin \"seruāre,\" meaning \"to safeguard\". Partridge concludes, \"The basic sense of both Hera and hero would therefore be 'protector'.\"\n\nThe word <nowiki>'hero' is used in English to refer either explicitly to male heros or as a gender neutral form (as opposed to the term heroine which designates only a female hero). The use of the male form 'hero' as a gender neutral substantive is a modern advent, as in Greek the term ἥρως (\"hērōs\"</nowiki>), was used exclusively to refer to the masculine form. As argued by Geneviève Dermenjian, Jacques Guilhaumou and Martine Lapied in \"Le Panthéon des Femmes Figures et Représentations des Héroines\", however, this supposedly gender neutral term in fact carries a strong implicit male bias. See also Gender neutrality in English.\n\nA classical hero is considered to be a \"warrior who lives and dies in the pursuit of honor\" and asserts his or her greatness by \"the brilliancy and efficiency with which they kill\". Each classical hero's life focuses on fighting, which occurs in war or during an epic quest. Classical heroes are commonly semi-divine and extraordinarily gifted, like Achilles, or, alternatively, are like Beowulf, evolving into heroic characters through their perilous circumstances. While these heroes are incredibly resourceful and skilled, they are often foolhardy, court disaster, risk their followers' lives for trivial matters, and behave arrogantly in a childlike manner. During classical times, people regarded heroes with the highest esteem and utmost importance, explaining their prominence within epic literature. The appearance of these mortal figures marks a revolution of audiences and writers turning away from immortal gods to mortal mankind, whose heroic moments of glory survive in the memory of their descendants, extending their legacy.\n\nHector was a Trojan prince and the greatest fighter for Troy in the Trojan War, which is known primarily through Homer's \"The Iliad\". Hector acted as leader of the Trojans and their allies in the defense of Troy, \"killing 31,000 Greek fighters,\" offers Hyginus. Hector was known not only for his courage but also for his noble and courtly nature. Indeed, Homer places Hector as peace-loving, thoughtful as well as bold, a good son, husband and father, and without darker motives. However, his familial values conflict greatly with his heroic aspirations in \"The Iliad,\" as he cannot be both the protector of Troy and a father to his child. Hector is ultimately betrayed by the gods when Athena appears disguised as his ally Deiphobus and convinces him to take on Achilles, leading to his death at the hands of a superior warrior.Achilles was a Greek Hero who was considered the most formidable military fighter in the entire Trojan War and the central character of \"The Iliad.\" He was the child of Thetis and Peleus, making him a demi-god. He wielded superhuman strength on the battlefield and was blessed with a close relationship to the Gods. Achilles famously refuses to fight after his dishonoring at the hands of Agamemnon, and only returns to the war due to unadulterated rage after Hector kills his close friend Patroclus. Achilles was known for uncontrollable rage that defined many of his bloodthirsty actions, such as defiling Hector's corpse by dragging it around the city of Troy. Achilles plays a tragic role in \"The Iliad\" brought about by constant de-humanization throughout the epic, having his \"menis\" (wrath) overpower his \"philos\" (love).\n\nHeroes in myth often had close but conflicted relationships with the gods. Thus Heracles's name means \"the glory of Hera\", even though he was tormented all his life by Hera, the Queen of the Gods. Perhaps the most striking example is the Athenian king Erechtheus, whom Poseidon killed for choosing Athena over him as the city's patron god. When the Athenians worshiped Erechtheus on the Acropolis, they invoked him as \"Poseidon Erechtheus\".\n\nFate, or destiny, plays a massive role in the stories of classical heroes. The classical hero's heroic significance stems from battlefield conquests, an inherently dangerous action. The gods in Greek Mythology, when interacting with the heroes, often foreshadow the hero's eventual death on the battlefield. Countless heroes and gods go to great lengths to alter their pre-destined fate, but with no success, as no immortal can change their prescribed outcomes by the three Fates. The most prominent example of this is found in \"Oedipus the King.\" After learning that his son, Oedipus, will end up killing him, the King of Thebes, Laius, takes huge steps to assure his son's death by removing him from the kingdom. But, Oedipus slays his father without an afterthought when he unknowingly encounters him in a dispute on the road many years later. The lack of recognition enabled Oedipus to slay his father, ironically further binding his father to his fate.\n\nStories of heroism may serve as moral examples. However, classical heroes often didn't embody the Christian notion of an upstanding, perfectly moral hero. For example, Achilles character-issues of hateful rage lead to merciless slaughter and his overwhelming pride lead to him only joining the Trojan War because he didn't want his soldiers to win all of the glory. Classical heroes, regardless of their morality, were placed in religion. In classical antiquity, cults that venerated deified heroes such as Heracles, Perseus, and Achilles played an important role in Ancient Greek religion. These ancient Greek hero cults worshipped heroes from oral epic tradition, with these heroes often bestowing blessings, especially healing ones, on individuals.\n\nThe concept of the \"Mythic Hero Archetype\" was first developed by Lord Raglan in his 1936 book, \"The Hero, A Study in Tradition, Myth and Drama\". It is a set of 22 common traits that he said were shared by many heroes in various cultures, myths and religions throughout history and around the world. Raglan argued that the higher the score, the more likely the figure is mythical.\n\nThe concept of a story archetype of the standard monomythical \"hero's quest\" that was reputed to be pervasive across all cultures is somewhat controversial. Expounded mainly by Joseph Campbell in his 1949 work \"The Hero with a Thousand Faces\", it illustrates several uniting themes of hero stories that hold similar ideas of what a hero represents, despite vastly different cultures and beliefs. The monomyth or Hero's Journey consists of three separate stages including the Departure, Initiation, and Return. Within these stages there are several archetypes that the hero or heroine may follow including the call to adventure (which they may initially refuse), supernatural aid, proceeding down a road of trials, achieving a realization about themselves (or an apotheosis), and attaining the freedom to live through their quest or journey. Campbell offered examples of stories with similar themes such as Krishna, Buddha, Apollonius of Tyana, and Jesus. One of the themes he explores is the androgynous hero, who combines male and female traits, like Bodhisattva: \"The first wonder to be noted here is the androgynous character of the Bodhisattva: masculine Avalokiteshvara, feminine Kwan Yin.\" In his 1968 book, \"The Masks of God: Occidental Mythology\", Campbell writes \"It is clear that, whether accurate or not as to biographical detail, the moving legend of the Crucified and Risen Christ was fit to bring a new warmth, immediacy, and humanity, to the old motifs of the beloved Tammuz, Adonis, and Osiris cycles.\"\n\nVladimir Propp, in his analysis of the Russian fairy tale, concluded that a fairy tale had only eight \"dramatis personæ\", of which one was the hero, and his analysis has been widely applied to non-Russian folklore. The actions that fall into such a hero's sphere include:\nPropp distinguished between \"seekers\" and \"victim-heroes\". A villain could initiate the issue by kidnapping the hero or driving him out; these were victim-heroes. On the other hand, an antagonist could rob the hero, or kidnap someone close to him, or, without the villain's intervention, the hero could realize that he lacked something and set out to find it; these heroes are seekers. Victims may appear in tales with seeker heroes, but the tale does not follow them both.\n\nNo history can be written without consideration of the lengthy list of recipients of national medals for bravery, populated by firefighters, policemen and policewomen, ambulance medics and ordinary have-a-go heroes. These persons risked their lives to try to save or protect the lives of others: for example, the Canadian Cross of Valour (C.V.) \"recognizes acts of the most conspicuous courage in circumstances of extreme peril\"; examples of recipients are Mary Dohey and David Gordon Cheverie.\n\nThe philosopher Hegel gave a central role to the \"hero\", personalized by Napoleon, as the incarnation of a particular culture's \"Volksgeist\", and thus of the general \"Zeitgeist\". Thomas Carlyle's 1841 \"On Heroes, Hero Worship and the Heroic in History\" also accorded a key function to heroes and great men in history. Carlyle centered history on the biography of a few central individuals such as Oliver Cromwell or Frederick the Great. His heroes were political and military figures, the founders or topplers of states. His history of great men included geniuses good and, perhaps for the first time in historical study, evil.\n\nExplicit defenses of Carlyle's position were rare in the second part of the 20th century. Most in the philosophy of history school contend that the motive forces in history can best be described only with a wider lens than the one that Carlyle used for his portraits. For example, Karl Marx argued that history was determined by the massive social forces at play in \"class struggles\", not by the individuals by whom these forces are played out. After Marx, Herbert Spencer wrote at the end of the 19th century: \"You must admit that the genesis of the great man depends on the long series of complex influences which has produced the race in which he appears, and the social state into which that race has slowly grown...Before he can remake his society, his society must make him.\" As Michel Foucault pointed out in his analysis of societal communication and debate, history was mainly the \"science of the sovereign\", until its inversion by the \"historical and political popular discourse\", but these intellectuals attempt to relieve themselves of responsibility for their own actions, so that they can be considered unreliable sources at best .\n\nModern examples of the typical hero are Minnie Vautrin, Norman Bethune, Alan Turing, Raoul Wallenberg, Mother Theresa, Nelson Mandela, and Aung San Suu Kyi.\n\nThe Annales School, led by Lucien Febvre, Marc Bloch and Fernand Braudel, would contest the exaggeration of the role of individual subjects in history. Indeed, Braudel distinguished various time scales, one accorded to the life of an individual, another accorded to the life of a few human generations, and the last one to civilizations, in which geography, economics and demography play a role considerably more decisive than that of individual subjects.\n\nAmong noticeable events in the studies of the role of the hero and Great man in history one should mention Sydney Hook's book (1943) \"The Hero in History\". In the second half of the twentieth century such male-focused theory has been contested, among others by feminists writers such as Judith Fetterley in \"The Resisting Reader\" (1977) and literary theorist Nancy K. Miller, \"The Heroine's Text: Readings in the French and English Novel, 1722-1782\".\n\nIn the epoch of globalization an individual can still change the development of the country and of the whole world so this gives reasons to some scholars to suggest returning to the problem of the role of the hero in history from the viewpoint of modern historical knowledge and using up-to-date methods of historical analysis.\n\nWithin the frameworks of developing counterfactual history, attempts are made to examine some hypothetical scenarios of historical development. The hero attracts much attention because most of those scenarios are based on the suppositions: what would have happened if this or that historical individual had or had not been alive.\n\nIf the term heroine exists, hero is often the predominantly used term even though its neutrality can be put into question. The definitions of the heroine often refer back to the one of the hero, but sometimes insinuates that their deeds are of less value, were obtained only thanks to their love of god or a country or of a man. Therefore, implying that an external explanation for the extraordinary nature of her deeds is needed to justify them. The warrior women is considered unholy, unnatural. These figures tend to be erased because they don't fit in the feminine values they are supposed to represent.\nActs of heroism coming from women are acceptable, during specific time, like when men are at war, during times of crisis, but they are otherwise often seen as suspicious. Moreover, women are often not individualized, but praised as a group for heroic deeds. Women in the military were often subordinated to tasks less likely to be praised than armed combat, and are rather praised for their courage as a general force, nurses during wartime are a good example of this phenomenon.\nIf their story gets told, they are made to fit in the acceptable script. Their story is told in a way as to match the expectations of femininity ex: maternal love, compassion, fidelity, resistance,\ndefense. Etc. So the set of strengths in which a heroine could historically express her value are overall not the same and perceived as less valuable than their masculine counterpart.\n\n\"In general, the cultural repertoire of heroic stories requires different qualities for each gender. The contrast of the ideal narrative line pits the autonomous ego-enhancing hero single-handedly and single-heartedly progressing toward a goal versus the long-suffering, selfless, socially embedded heroine, being moved in many directions, lacking the tenacious loyalty demanded of a quest.\"\nIf they get mentioned in history, the way their story is told also differs from their male counterpart, they are generally portrayed as young and beautiful, their actions are limited to a short time\nlapse in opposition to the possibility of a long heroic career for male heroes, underlying feelings that led to their heroic acts are underlined, overall less details about their life are kept and\nemphasis is put over their tragic death. Not to forget that heroes and heroines are part of a social construct, their history is told and changes throughout history to serve different purposes of memory, propaganda according to diverse social, political or religious evolutions.\n\nThe word \"hero\" or \"heroine\", in modern times, is sometimes used to describe the protagonist or the love interest of a story, a usage which can conflict with the superhuman expectations of heroism. A classic example is Anna Karenina, the lead character in the novel of the same title by Leo Tolstoy. In modern literature the hero is more and more a problematic concept. In 1848, for example, William Makepeace Thackeray gave \"Vanity Fair\" the subtitle \"A Novel without a Hero\", and imagined a world in which no sympathetic character was to be found. \"Vanity Fair\" is a satirical representation of the absence of truly moral heroes in the modern world. The story focuses on the characters Emmy Sedley and Becky Sharpe (the latter as the clearly defined anti-hero), with the plot focused on the eventual marriage of these two characters to rich men, revealing character flaws as the story progresses. Even the most sympathetic characters, like Captain Dobbin, are susceptible to weakness, as he is often narcissistic and melancholy.\n\nThe larger-than-life hero is a more common feature of fantasy (particularly in comic-books and epic fantasy) than more realist works. However, these larger-than life figures remain prevalent in society. The superhero genre is a multibillion-dollar industry that includes comic books, movies, toys and video games. Superheroes usually possess extraordinary talents and powers that no living human could ever emulate. The superhero stories often pit a super villain against the hero, with the hero fighting the crime caused by the super villain. Examples of long-running superheroes include Superman and Wonder Woman.\n\nSocial psychology has begun paying attention to heroes and heroism. Zeno Franco and Philip Zimbardo point out differences between heroism and altruism, and they offer evidence that observers' perceptions of unjustified risk plays a role, above and beyond risk type, in determining the ascription of heroic status.\n\nAn evolutionary psychology explanation for heroic risk-taking is that it is a costly signal demonstrating the ability of the hero. It can be seen as one form of altruism for which there are also several other evolutionary explanations.\n\nRoma Chatterji has suggested that the hero or more generally protagonist is first and foremost a symbolic representation of the person who is experiencing the story while reading, listening or watching; thus the relevance of the hero to the individual relies a great deal on how much similarity there is between the two. One reason for the hero-as-self interpretation of stories and myths is the human inability to view the world from any perspective but a personal one.\n\n\n", "id": "13706", "title": "Hero"}
{"url": "https://en.wikipedia.org/wiki?curid=13711", "text": "Hydroxide\n\nHydroxide is a diatomic anion with chemical formula OH. It consists of an oxygen and hydrogen atom held together by a covalent bond, and carries a negative electric charge. It is an important but usually minor constituent of water. It functions as a base, a ligand, a nucleophile and a catalyst. The hydroxide ion forms salts, some of which dissociate in aqueous solution, liberating solvated hydroxide ions. Sodium hydroxide is a multi-million-ton per annum commodity chemical. A hydroxide attached to a strongly electropositive center may itself ionize, liberating a hydrogen cation (H), making the parent compound an acid.\n\nThe corresponding electrically neutral compound •HO is the hydroxyl radical. The corresponding covalently-bound group –OH of atoms is the hydroxyl group.\nHydroxide ion and hydroxyl group are nucleophiles and can act as a catalyst in organic chemistry.\n\nMany inorganic substances which bear the word \"hydroxide\" in their names are not ionic compounds of the hydroxide ion, but covalent compounds which contain hydroxyl groups.\n\nThe hydroxide ion is a natural part of water, because of the self-ionization reaction:\nThe equilibrium constant for this reaction, defined as \nhas a value close to 10 at 25 °C, so the concentration of hydroxide ions in pure water is close to 10 mol∙dm, in order to satisfy the equal charge constraint. The pH of a solution is equal to the decimal cologarithm of the hydrogen cation concentration; the pH of pure water is close to 7 at ambient temperatures. The concentration of hydroxide ions can be expressed in terms of pOH, which is close to 14 − pH, so pOH of pure water is also close to 7. Addition of a base to water will reduce the hydrogen cation concentration and therefore increase the hydroxide ion concentration (increase pH, decrease pOH) even if the base does not itself contain hydroxide. For example, ammonia solutions have a pH greater than 7 due to the reaction NH + H , which results in a decrease in hydrogen cation concentration and an increase in hydroxide ion concentration. pOH can be kept at a nearly constant value with various buffer solutions.\n\nIn aqueous solution the hydroxide ion is a base in the Brønsted–Lowry sense as it can accept a proton from a Brønsted–Lowry acid to form a water molecule. It can also act as a Lewis base by donating a pair of electrons to a Lewis acid. In aqueous solution both hydrogen and hydroxide ions are strongly solvated, with hydrogen bonds between oxygen and hydrogen atoms. Indeed, the bihydroxide ion has been characterized in the solid state. This compound is centrosymmetric and has a very short hydrogen bond (114.5 pm) that is similar to the length in the bifluoride ion (114 pm). In aqueous solution the hydroxide ion forms strong hydrogen bonds with water molecules. A consequence of this is that concentrated solutions of sodium hydroxide have high viscosity due to the formation of an extended network of hydrogen bonds as in hydrogen fluoride solutions.\n\nIn solution, exposed to air, the hydroxide ion reacts rapidly with atmospheric carbon dioxide, acting as an acid, to form, initially, the bicarbonate ion.\nThe equilibrium constant for this reaction can be specified either as a reaction with dissolved carbon dioxide or as a reaction with carbon dioxide gas (see carbonic acid for values and details). At neutral or acid pH, the reaction is slow, but is catalyzed by the enzyme carbonic anhydrase, which effectively creates hydroxide ions at the active site.\n\nSolutions containing the hydroxide ion attack glass. In this case, the silicates in glass are acting as acids. Basic hydroxides, whether solids or in solution, are stored in airtight plastic containers.\n\nThe hydroxide ion can function as a typical electron-pair donor ligand, forming such complexes as [Al(OH)]. It is also often found in mixed-ligand complexes of the type [ML(OH)], where L is a ligand. The hydroxide ion often serves as a bridging ligand, donating one pair of electrons to each of the atoms being bridged. As illustrated by [Pb(OH)], metal hydroxides are often written in a simplified format. It can even act as a 3-electron-pair donor, as in the tetramer [PtMe(OH)].\n\nWhen bound to a strongly electron-withdrawing metal centre, hydroxide ligands tend to ionise into oxide ligands. For example, the bichromate ion [HCrO] dissociates according to\nwith a p\"K\" of about 5.9.\n\nThe infrared spectra of compounds containing the OH functional group have strong absorption bands in the region centered around 3500 cm. The high frequency of molecular vibration is a consequence of the small mass of the hydrogen atom as compared to the mass of the oxygen atom and this makes detection of hydroxyl groups by infrared spectroscopy relatively easy. A band due to an OH group tends to be sharp. However, the band width increases when the OH group is involved in hydrogen bonding. A water molecule has an HOH bending mode at about 1600 cm, so the absence of this band can be used to distinguish an OH group from a water molecule.\n\nWhen the OH group is bound to a metal ion in a coordination complex, an M−OH bending mode can be observed. For example, in [Sn(OH)] it occurs at 1065 cm. The bending mode for a bridging hydroxide tends to be at a lower frequency as in [(bipyridine)Cu(OH)Cu(bipyridine)] (955 cm). M−OH stretching vibrations occur below about 600 cm. For example, the tetrahedral ion [Zn(OH)] has bands at 470 cm (Raman-active, polarized) and 420 cm (infrared). The same ion has a (HO)–Zn–(OH) bending vibration at 300 cm.\n\nSodium hydroxide solutions, also known as lye and caustic soda, are used in the manufacture of pulp and paper, textiles, drinking water, soaps and detergents, and as a drain cleaner. Worldwide production in 2004 was approximately 60 million tonnes. The principal method of manufacture is the chlor-alkali process.\n\nSolutions containing the hydroxide ion are generated when a salt of a weak acid is dissolved in water. Sodium carbonate is used as an alkali, for example, by virtue of the hydrolysis reaction\nAlthough the base strength of sodium carbonate solutions is lower than a concentrated sodium hydroxide solution, it has the advantage of being a solid. It is also manufactured on a vast scale (42 million tonnes in 2005) by the Solvay process. An example of the use of sodium carbonate as an alkali is when washing soda (another name for sodium carbonate) acts on insoluble esters, such as triglycerides, commonly known as fats, to hydrolyze them and make them soluble.\n\nBauxite, a basic hydroxide of aluminium, is the principal ore from which the metal is manufactured. Similarly, goethite (α-FeO(OH)) and lepidocrocite (γ-FeO(OH)), basic hydroxides of iron, are among the principal ores used for the manufacture of metallic iron. Numerous other uses can be found in the articles on individual hydroxides.\n\nAside from NaOH and KOH, which enjoy very large scale applications, the hydroxides of the other alkali metals also are useful. Lithium hydroxide is a strong base, with a p\"K\" of −0.36. Lithium hydroxide is used in breathing gas purification systems for spacecraft, submarines, and rebreathers to remove carbon dioxide from exhaled gas.\nThe hydroxide of lithium is preferred to that of sodium because of its lower mass. Sodium hydroxide, potassium hydroxide and the hydroxides of the other alkali metals are also strong bases.\n\nBeryllium hydroxide Be(OH) is amphoteric. The hydroxide itself is insoluble in water, with a solubility product log \"K\"* of −11.7. Addition of acid gives soluble hydrolysis products, including the trimeric ion [Be(OH)(HO)], which has OH groups bridging between pairs of beryllium ions making a 6-membered ring. At very low pH the aqua ion [Be(HO)] is formed. Addition of hydroxide to Be(OH) gives the soluble tetrahydroxo anion [Be(OH)].\n\nThe solubility in water of the other hydroxides in this group increases with increasing atomic number. Magnesium hydroxide Mg(OH) is a strong base as are the hydroxides of the heavier alkaline earths, calcium hydroxide, strontium hydroxide and barium hydroxide. A solution/suspension of calcium hydroxide is known as limewater and can be used to test for the weak acid carbon dioxide. The reaction Ca(OH) + CO Ca + + OH illustrates the strong basicity of calcium hydroxide. Soda lime, which is a mixture of NaOH and Ca(OH), is used as a CO absorbent.\n\nThe simplest hydroxide of boron B(OH), known as boric acid, is an acid. Unlike the hydroxides of the alkali and alkaline earth hydroxides, it does not dissociate in aqueous solution. Instead, it reacts with water molecules acting as a Lewis acid, releasing protons.\nA variety of oxyanions of boron are known, which, in the protonated form, contain hydroxide groups.\n\nAluminium hydroxide Al(OH) is amphoteric and dissolves in alkaline solution.\nIn the Bayer process for the production of pure aluminium oxide from bauxite minerals this equilibrium is manipulated by careful control of temperature and alkali concentration. In the first phase, aluminium dissolves in hot alkaline solution as but other hydroxides usually present in the mineral, such as iron hydroxides, do not dissolve because they are not amphoteric. After removal of the insolubles, the so-called red mud, pure aluminium hydroxide is made to precipitate by reducing the temperature and adding water to the extract, which, by diluting the alkali, lowers the pH of the solution. Basic aluminium hydroxide AlO(OH), which may be present in bauxite, is also amphoteric.\n\nIn mildly acidic solutions the hydroxo complexes formed by aluminium are somewhat different from those of boron, reflecting the greater size of Al(III) vs. B(III). The concentration of the species [Al(OH)] is very dependent on the total aluminium concentration. Various other hydroxo complexes are found in crystalline compounds. Perhaps the most important is the basic hydroxide AlO(OH), a polymeric material known by the names of the mineral forms boehmite or diaspore, depending on crystal structure. Gallium hydroxide, indium hydroxide and thallium(III) hydroxides are also amphoteric. Thallium(I) hydroxide is a strong base.\n\nCarbon forms no simple hydroxides. The hypothetical compound C(OH) (orthocarbonic acid or methanetetraol) is unstable in aqueous solution:\nCarbon dioxide is also known as carbonic anhydride, meaning that it forms by dehydration of carbonic acid HCO (OC(OH)).\n\nSilicic acid is the name given to a variety of compounds with a generic formula [SiO(OH)]. \"Orthosilicic acid\" has been identified in very dilute aqueous solution. It is a weak acid with p\"K\" = 9.84, p\"K\" = 13.2 at 25 °C. It is usually written as HSiO but the formula SiO(OH) is generally accepted . Other silicic acids such as \"metasilicic acid\" (HSiO), \"disilicic acid\" (HSiO), and \"pyrosilicic acid\" (HSiO) have been characterized. These acids also have hydroxide groups attached to the silicon; the formulas suggest that these acids are protonated forms of polyoxyanions.\n\nFew hydroxo complexes of germanium have been characterized. Tin(II) hydroxide Sn(OH) was prepared in anhydrous media. When tin(II) oxide is treated with alkali the pyramidal hydroxo complex is formed. When solutions containing this ion are acidified the ion [Sn(OH)] is formed together with some basic hydroxo complexes. The structure of [Sn(OH)] has a triangle of tin atoms connected by bridging hydroxide groups. Tin(IV) hydroxide is unknown but can be regarded as the hypothetical acid from which stannates, with a formula [Sn(OH)], are derived by reaction with the (Lewis) basic hydroxide ion.\n\nHydrolysis of Pb in aqueous solution is accompanied by the formation of various hydroxo-containing complexes, some of which are insoluble. The basic hydroxo complex [PbO(OH)] is a cluster of six lead centres with metal–metal bonds surrounding a central oxide ion. The six hydroxide groups lie on the faces of the two external Pb tetrahedra. In strongly alkaline solutions soluble plumbate ions are formed, including [Pb(OH)].\n\nIn the higher oxidation states of the pnictogens, chalcogens, halogens, and noble gases there are oxoacids in which the central atom is attached to oxide ions and hydroxide ions. Examples include phosphoric acid HPO, and sulfuric acid HSO. In these compounds one or more hydroxide groups can dissociate with the liberation of hydrogen cations as in a standard Brønsted–Lowry acid. Many oxoacids of sulfur are known and all feature OH groups that can dissociate.\n\nTelluric acid is often written with the formula HTeO·2HO but is better described structurally as Te(OH).\n\n\"Ortho\"-periodic acid can lose all its protons, eventually forming the periodate ion [IO]. It can also be protonated in strongly acidic conditions to give the octahedral ion [I(OH)], completing the isoelectronic series, [E(OH)], E = Sn, Sb, Te, I; \"z\" = −2, −1, 0, +1. Other acids of iodine(VII) that contain hydroxide groups are known, in particular in salts such as the \"meso\"periodate ion that occurs in K[IO(OH)]·8HO.\n\nAs is common outside of the alkali metals, hydroxides of the elements in lower oxidation states are complicated. For example, phosphorous acid HPO predominantly has the structure OP(H)(OH), in equilibrium with a small amount of P(OH).\n\nThe oxoacids of chlorine, bromine and iodine have the formula OA(OH) where \"n\" is the oxidation number: +1, +3, +5 or +7, and A = Cl, Br or I. The only oxoacid of fluorine is F(OH), hypofluorous acid. When these acids are neutralized the hydrogen atom is removed from the hydroxide group.\n\nThe hydroxides of the transition metals and post-transition metals usually have the metal in the +2 (M = Mn, Fe, Co, Ni, Cu, Zn) or +3 (M = Fe, Ru, Rh, Ir) oxidation state. None are soluble in water, and many are poorly defined. One complicating feature of the hydroxides is their tendency to undergo further condensation to the oxides, a process called olation. Hydroxides of metals in the +1 oxidation state are also poorly defined or unstable. For example, silver hydroxide Ag(OH) decomposes spontaneously to the oxide (AgO). Copper(I) and gold(I) hydroxides are also unstable, although stable adducts of CuOH and AuOH are known. The polymeric compounds M(OH) and M(OH) are in general prepared by increasing the pH of an aqueous solutions of the corresponding metal cations until the hydroxide precipitates out of solution. On the converse, the hydroxides dissolve in acidic solution. Zinc hydroxide Zn(OH) is amphoteric, forming the zincate ion Zn(OH) in strongly alkaline solution.\n\nNumerous mixed ligand complexes of these metals with the hydroxide ion exist. In fact these are in general better defined than the simpler derivatives. Many can be made by deprotonation of the corresponding metal aquo complex.\n\nVanadic acid HVO shows similarities with phosphoric acid HPO though it has a much more complex vanadate oxoanion chemistry. Chromic acid HCrO, has similarities with sulfuric acid HSO; for example, both form acid salts A[HMO]. Some metals, e.g. V, Cr, Nb, Ta, Mo, W, tend to exist in high oxidation states. Rather than forming hydroxides in aqueous solution, they convert to oxo clusters by the process of olation, forming polyoxometalates.\n\nIn some cases the products of partial hydrolysis of metal ion, described above, can be found in crystalline compounds. A striking example is found with zirconium(IV). Because of the high oxidation state, salts of Zr are extensively hydrolyzed in water even at low pH. The compound originally formulated as ZrOCl·8HO was found to be the chloride salt of a tetrameric cation [Zr(OH)(HO)] in which there is a square of Zr ions with two hydroxide groups bridging between Zr atoms on each side of the square and with four water molecules attached to each Zr atom.\n\nThe mineral malachite is a typical example of a basic carbonate. The formula, CuCO(OH) shows that it is halfway between copper carbonate and copper hydroxide. Indeed, in the past the formula was written as CuCO·Cu(OH). The crystal structure is made up of copper, carbonate and hydroxide ions. The mineral atacamite is an example of a basic chloride. It has the formula, CuCl(OH). In this case the composition is nearer to that of the hydroxide than that of the chloride CuCl·3Cu(OH). Copper forms hydroxy phosphate (libethenite), arsenate (olivenite), sulfate (brochantite) and nitrate compounds. White lead is a basic lead carbonate, (PbCO)·Pb(OH), which has been used as a white pigment because of its opaque quality, though its use is now restricted because it can be a source for lead poisoning.\n\nThe hydroxide ion appears to rotate freely in crystals of the heavier alkali metal hydroxides at higher temperatures so as to present itself as a spherical ion, with an effective ionic radius of about 153 pm. Thus, the high-temperature forms of KOH and NaOH have the sodium chloride structure, which gradually freezes in a monocinically distorted sodium chloride structure at temperatures below about 300 °C. The OH groups still rotate even at room temperature around their symmetry axes and, therefore, cannot be detected by X-ray diffraction. The room-temperature form of NaOH has the thallium iodide structure. LiOH, however, has a layered structure, made up of tetrahedral Li(OH) and (OH)Li units. This is consistent with the weakly basic character of LiOH in solution, indicating that the Li–OH bond has much covalent character.\n\nThe hydroxide ion displays cylindrical symmetry in hydroxides of divalent metals Ca, Cd, Mn, Fe, and Co. For example, magnesium hydroxide Mg(OH) (brucite) crystallizes with the cadmium iodide layer structure, with a kind of close-packing of magnesium and hydroxide ions.\n\nThe amphoteric hydroxide Al(OH) has four major crystalline forms: gibbsite (most stable), bayerite, nordstrandite and doyleite.\nAll these polymorphs are built up of double layers of hydroxide ions – the aluminium atoms on two-thirds of the octahedral holes between the two layers – and differ only in the stacking sequence of the layers. The structures are similar to the brucite structure. However, whereas the brucite structure can be described as a close-packed structure in gibbsite the OH groups on the underside of one layer rest on the groups of the layer below. This arrangement led to the suggestion that there are directional bonds between OH groups in adjacent layers. This is an unusual form of hydrogen bonding since the two hydroxide ion involved would be expected to point away from each other. The hydrogen atoms have been located by neutron diffraction experiments on α-AlO(OH) (diaspore). The O–H–O distance is very short, at 265 pm; the hydrogen is not equidistant between the oxygen atoms and the short OH bond makes an angle of 12° with the O–O line. A similar type of hydrogen bond has been proposed for other amphoteric hydroxides, including Be(OH), Zn(OH) and Fe(OH)\n\nA number of mixed hydroxides are known with stoichiometry AM(OH), AM(OH) and AM(OH). As the formula suggests these substances contain M(OH) octahedral structural units. Layered double hydroxides may be represented by the formula [MM(OH)](X)·\"y\"HO. Most commonly, \"z\" = 2, and M = Ca, Mg, Mn, Fe, Co, Ni, Cu or Zn; hence \"q\" = \"x\".\n\nPotassium hydroxide and sodium hydroxide are two well-known reagents in organic chemistry.\n\nThe hydroxide ion may act as a base catalyst. The base abstracts a proton from a weak acid to give an intermediate that goes on to react with another reagent. Common substrates for proton abstraction are alcohols, phenols, amines and carbon acids. The p\"K\" value for dissociation of a C–H bond is extremely high, but the pK alpha hydrogens of a carbonyl compound are about 3 log units lower. Typical p\"K\" values are 16.7 for acetaldehyde and 19 for acetone. Dissociation can occur in the presence of a suitable base.\nThe base should have a p\"K\" value not less than about 4 log units smaller or the equilibrium will lie almost completely to the left.\n\nThe hydroxide ion by itself is not a strong enough base, but it can be converted in one by adding sodium hydroxide to ethanol\nto produce the ethoxide ion. The pK for self-dissociation of ethanol is about 16 so the alkoxide ion is a strong enough base The addition of an alcohol to an aldehyde to form a hemiacetal is an example of a reaction that can be catalyzed by the presence of hydroxide. Hydroxide can also act as a Lewis-base catalyst.\n\nThe hydroxide ion is intermediate in nucleophilicity between the fluoride ion F, and the amide ion . The hydrolysis of an ester\nalso known as saponification is an example of a nucleophilic acyl substitution with the hydroxide ion acting as a nucleophile. In this case the leaving group is an alkoxide ion, which immediately removes a proton from a water molecule to form an alcohol. In the manufacture of soap, sodium chloride is added to salt out the sodium salt of the carboxylic acid; this is an example of the application of the common-ion effect.\n\nOther cases where hydroxide can act as a nucleophilic reagent are amide hydrolysis, the Cannizzaro reaction, nucleophilic aliphatic substitution, nucleophilic aromatic substitution and in elimination reactions. The reaction medium for KOH and NaOH is usually water but with a phase-transfer catalyst the hydroxide anion can be shuttled into an organic solvent as well, for example in the generation of dichlorocarbene.\n\n", "id": "13711", "title": "Hydroxide"}
{"url": "https://en.wikipedia.org/wiki?curid=13713", "text": "H. R. Giger\n\nHans Rudolf \"H.R.\" Giger ( ; ; 5 February 1940 – 12 May 2014) was a Swiss surrealist painter, whose style was adapted for many forms of media, including record-albums, furniture and tattoo-art.\n\nThe Zurich-based artist was best known for airbrush images of humans and machines linked together in a cold 'biomechanical' relationship. Later he abandoned airbrush work for pastels, markers or ink. He was part of the special effects team that won an Academy Award for design work on the film \"Alien\". In Switzerland there are two theme-bars that reflect his interior designs, and his work is on permanent display at the H.R. Giger Museum at Gruyères.\n\nGiger was born in 1940 in Chur, capital city of Graubünden, the largest and easternmost Swiss canton. His father, a pharmacist, viewed art as a \"breadless profession\" and strongly encouraged him to enter pharmacy, Giger recalled. He moved to Zürich in 1962, where he studied Architecture and industrial design at the School of Applied Arts until 1970.\n\nGiger's first success was when H. H. Kunz, co-owner of Switzerland's first poster publishing company, printed and distributed Giger's first posters, beginning in 1969.\n\nGiger's style and thematic execution were influential. He was part of the special effects team that won an Academy Award for Best Achievement in Visual Effects for their design work on the film \"Alien\". His design for the Alien was inspired by his painting \"Necronom IV\" and earned him an Oscar in 1980. His books of paintings, particularly \"Necronomicon\" and \"Necronomicon II\" (1985) and the frequent appearance of his art in \"Omni\" magazine continued his rise to international prominence. Giger was admitted to the Science Fiction and Fantasy Hall of Fame in 2013. He is also well known for artwork on several music recording albums including \"Brain Salad Surgery\" of Emerson, Lake & Palmer and Deborah Harry's \"KooKoo\".\n\nIn 1998 Giger acquired the Château St. Germain in Gruyères, Switzerland, and it now houses the H.R. Giger Museum, a permanent repository of his work.\n\nGiger had a relationship with Swiss actress Li Tobler until she committed suicide in 1975. Li's image appears in many of his paintings. He married Mia Bonzanigo in 1979; they divorced a year and a half later.\n\nThe artist lived and worked in Zürich with his second wife, Carmen Maria Scheifele Giger, who is the Director of the H.R. Giger Museum. \n\nOn 12 May 2014, Giger died in a hospital in Zürich after having suffered injuries in a fall.\n\nIn addition to his awards, Giger was recognized by a variety of festivals and institutions. On the one year anniversary of his death, the Museum of Arts and Design in New York City staged the series \"The Unseen Cinema of HR Giger\" in May 2015.\n\n\"\", a biographical documentary by Belinda Sallin, debuted 27 September 2014 in Zurich, Switzerland.\n\nGiger started with small ink drawings before progressing to oil paintings. For most of his career, Giger had worked predominantly in airbrush, creating monochromatic canvasses depicting surreal, nightmarish dreamscapes. However, he then largely abandoned large airbrush works in favor of works with pastels, markers or ink.\n\nGiger's most distinctive stylistic innovation was that of a representation of human bodies and machines in a cold, interconnected relationship, he described as \"biomechanical\". His main influences were painters Dado, Ernst Fuchs and Salvador Dalí. He met Salvador Dalí, to whom he was introduced by painter Robert Venosa. Giger was also influenced by the work of the sculptor Stanislas Szukalski, and by the painters Austin Osman Spare and Mati Klarwein. He was also a personal friend of Timothy Leary. Giger studied interior and industrial design at the School of Commercial Art in Zurich (from 1962 to 1965) and made his first paintings as a means of art therapy.\n\nGiger directed a number of films, including \"Swiss Made\" (1968), \"Tagtraum\" (1973), \"Giger's Necronomicon\" (1975) and \"Giger's Alien\" (1979).\n\nGiger created furniture designs, particularly the Harkonnen Capo Chair for a film of the novel \"Dune\" that was to be directed by Alejandro Jodorowsky. Many years later, David Lynch directed the film, using only rough concepts by Giger. Giger had wished to work with Lynch, as he stated in one of his books that Lynch's film \"Eraserhead\" was closer than even Giger's own films to realizing his vision.\n\nGiger applied his biomechanical style to interior design. One \"Giger Bar\" appeared in Tokyo, but the realization of his designs were a great disappointment to him, since the Japanese organization behind the venture did not wait for his final designs, and instead used Giger's rough preliminary sketches. For that reason; Giger disowned the Tokyo Giger Bar. The two Giger Bars in his native Switzerland (in Gruyères and Chur), however, were built under Giger's close supervision and they accurately reflect his original concepts. At The Limelight in Manhattan, Giger's artwork was licensed to decorate the VIP room, the uppermost chapel of the landmarked church, but it was never intended to be a permanent installation and bore no similarity to the Giger Bars in Switzerland. The arrangement was terminated after two years when the Limelight closed. As of 2009 only the two authentic Swiss Giger Bars remain.\n\nGiger's art has greatly influenced tattooists and fetishists worldwide. Under a licensing deal Ibanez guitars released an H. R. Giger signature series: the Ibanez ICHRG2, an Ibanez Iceman, features \"NY City VI\", the Ibanez RGTHRG1 has \"NY City XI\" printed on it, the S Series SHRG1Z has a metal-coated engraving of \"Biomechanical Matrix\" on it, and a 4-string SRX bass, SRXHRG1, has \"N.Y. City X\" on it.\n\nGiger is often referred to in popular culture, especially in science fiction and cyberpunk. William Gibson (who wrote an early script for \"Alien 3\") seems particularly fascinated: A minor character in \"Virtual Light\", Lowell, is described as having \"New York XXIV\" tattooed across his back, and in \"Idoru\" a secondary character, Yamazaki, describes the buildings of nanotech Japan as Giger-esque.\n\n\n\n\n\n\n", "id": "13713", "title": "H. R. Giger"}
{"url": "https://en.wikipedia.org/wiki?curid=13714", "text": "Hispaniola\n\nHispaniola (Spanish: \"La Española\"; Latin: \"Hispaniola\"; Taíno: \"Haiti\") is the 22nd-largest island in the world, located in the Caribbean island group, the Greater Antilles. It is the second largest island in the Caribbean after Cuba, and the tenth most populous island in the world.\n\nTwo sovereign nations share the island. The Dominican Republic, at , is nearly twice as large as its neighbour, Haiti, with an area of . The only other shared island in the Caribbean is Saint Martin, which is shared between France (Saint-Martin) and the Netherlands (Sint Maarten).\n\nHispaniola is the site of the first permanent European settlement in the Americas, founded by Christopher Columbus on his voyages in 1492 and 1493.\n\nThe island was called by various names by its native people, the Taíno Amerindians. No known Taíno texts survive, hence, historical evidence for those names comes to us through three Spanish historians: Pietro Martyr d‘Anghiera, Bartolomé de las Casas, and Gonzalo Fernández de Oviedo. Fernández de Oviedo and de las Casas both recorded that the island was called \"Haiti\" (\"Mountainous Land\") by the Taíno. D'Anghiera added another name, \"Quizqueia\" (supposedly \"Mother of all Lands\"), but later research shows that the word does not seem to derive from the original Arawak Taíno language. (\"Quisqueya\" is today mostly used in the Dominican Republic.) Although the Taínos' use of \"Haiti\" is verified, and the name was used by all three historians, evidence suggests that it probably was not the Taíno name of the whole island, but only for a region (now known as \"Los Haitises\") in the northeastern section of the present-day Dominican Republic.\n\nWhen Columbus took possession of the island in 1492, he named it \"Insula Hispana\", meaning \"the Spanish Island\" in Latin and \"La Isla Española\", meaning \"the Spanish Island\", in Spanish. De las Casas shortened the name to \"Española\", and when d‘Anghiera detailed his account of the island in Latin, he rendered its name as \"Hispaniola\". In the oldest documented map of the island, created by Andrés de Morales, Los Haitises is labeled \"Montes de Haití\" (\"Haiti Mountains\"), and de las Casas apparently named the whole island Haiti on the basis of that particular region, as d'Anghiera states that the name of one part was given to the whole island.\n\nDue to Taíno, Spanish and French influences on the island, historically the whole island was often referred to as \"Haiti,\" \"Hayti,\" \"Santo Domingo,\" \"St. Domingue,\" or \"San Domingo.\" The colonial terms \"Saint-Domingue\" and \"Santo Domingo\" are sometimes still applied to the whole island, though these names refer, respectively, to the colonies that became Haiti and the Dominican Republic. Since Anghiera's literary work was translated into English and French soon after being written, the name \"Hispaniola\" became the most frequently used term in English-speaking countries for the island in scientific and cartographic works. In 1918, the United States occupation government, presided by Harry Shepard Knapp, obliged the use of the name Hispaniola on the island, and recommended the use of that name to the National Geographic Society. Only relatively recently has the term \"Hispaniola\" come into wide use as a name for the geographic unit.\n\nThe name \"Haïti\" was adopted by Haitian revolutionary Jean-Jacques Dessalines in 1804, as the official name of independent Saint-Domingue, as a tribute to the Amerindian predecessors. It was also adopted as the official name of independent Santo Domingo, as the \"Republic of Spanish Haiti,\" a state that existed from November 1821 until its forced annexation by Haiti in February 1822.\n\nChristopher Columbus inadvertently landed on the island during his first voyage across the Atlantic in 1492, where his flagship, the \"Santa Maria\", sank after running aground on December 25. A contingent of men were left at an outpost christened La Navidad, on the north coast of present-day Haiti. On his return the following year, following the destruction of La Navidad by the local population, Columbus quickly established a second compound farther east in present-day Dominican Republic, La Isabela.\n\nThe island was inhabited by the Taíno, one of the indigenous Arawak peoples. The Taino were at first tolerant of Columbus and his crew, and helped him to construct La Navidad on what is now Môle-Saint-Nicolas, Haiti, in December 1492. European colonization of the island began in earnest the following year, when 1,300 men arrived from Spain under the watch of Bartolomeo Columbus. In 1496 the town of \"Nueva Isabela\" was founded. After being destroyed by a hurricane, it was rebuilt on the opposite side of the Ozama River and called Santo Domingo. It is the oldest permanent European settlement in the Americas.\nThe Taíno population of the island rapidly died, 90% from new infectious diseases, to which they had no immunity.\nHarsh enslavement by Spanish colonists resulted in the death of most of the remainder. In 1503 the colony began to import African slaves, believing them more capable of performing physical labor. The natives had no immunity to European diseases, including smallpox, and entire tribes were destroyed. From an estimated initial population of 250,000 in 1492, 14,000 Arawaks survived in 1517.\n\nIn 1574, a census taken of the Greater Antilles reported 1,000 Spaniards and 12,000 African slaves on Hispaniola.\n\nAs Spain conquered new regions on the mainland of the Americas, its interest in Hispaniola waned, and the colony’s population grew slowly. By the early 17th century, the island and its smaller neighbors (notably Tortuga) became regular stopping points for Caribbean pirates. In 1606, the government of Philip III ordered all inhabitants of Hispaniola to move close to Santo Domingo, to avoid interaction with pirates. Rather than secure the island, his action meant that French, English and Dutch pirates established their own bases on the abandoned north and west coasts of the island.\nIn 1665, French colonization of the island was officially recognized by King Louis XIV. The French colony was given the name Saint-Domingue. In the 1697 Treaty of Ryswick, Spain formally ceded the western third of the island to France. Saint-Domingue quickly came to overshadow the east in both wealth and population. Nicknamed the \"Pearl of the Antilles,\" it became the richest and most prosperous colony in the West Indies, with a system of human enslavement used to grow and harvest sugar cane, during a time when demand for sugar was high in Europe. Slavery kept prices low and profit was maximized at the expense of human lives. It was an important port in the Americas for goods and products flowing to and from France and Europe.\n\nWith the treaty of Peace of Basel, revolutionary France emerged as a major European power. In the second 1795 Treaty of Basel (July 22), Spain ceded the eastern two-thirds of the island of Hispaniola, later to become the Dominican Republic. French settlers had begun to colonize some areas in the Spanish side of the territory.\n\nEuropean colonists often died young due to tropical fevers, as well as from violent slave resistance in the late eighteenth century. When the French Revolution abolished slavery in the colonies on February 4, 1794, it was a European first, and when Napoleon reimposed slavery in 1802 it led to a major upheaval by the emancipated black slaves.\n\nThousands of the French Troops sent by Napoleon to reestablished slavery succumbed to yellow fever during the summer months, and more than half of the French army died because of disease. After the French removed the surviving 7,000 troops in late 1803, the leaders of the revolution declared western Hispaniola the new nation of independent Haiti in early 1804. France continued to rule Spanish Santo Domingo. In 1805, Haitian troops of general Henri Christophe tried to conquer all of Hispaniola. They invaded Santo Domingo and sacked the towns of Santiago de los Caballeros and Moca, killing most of their residents, but news of a French fleet sailing towards Haiti forced general Christophe to return to Haiti, leaving the Eastern Spanish side of the island in French hands. In 1808, following Napoleon's invasion of Spain, the criollos of Santo Domingo revolted against French rule and, with the aid of the United Kingdom (Spain's ally) returned Santo Domingo to Spanish control.\n\nFearing the influence of a society that had successfully fought and won against their enslavers, the United States and European powers refused to recognize Haiti, the second republic in the western hemisphere. France demanded a high payment for compensation to slaveholders who lost their property, and Haiti was saddled with unmanageable debt for decades. It became one of the poorest countries in the Americas, while the Dominican Republic gradually has developed into the largest economy of Central America and the Caribbean.\n\nHispaniola is the second-largest island in the Caribbean (after Cuba), with an area of , of which is under the sovereignty of the Dominican Republic occupying the eastern portion and under the sovereignty of Haiti occupying the western portion.\n\nThe island of Cuba lies to the northwest across the Windward Passage; to the southwest lies Jamaica, separated by the Jamaica Channel. Puerto Rico lies east of Hispaniola across the Mona Passage. The Bahamas and Turks and Caicos Islands lie to the north. Its westernmost point is known as Cap Carcasse.\n\nCuba, Hispaniola, Jamaica, and Puerto Rico are collectively known as the Greater Antilles.\n\nThe island has five major mountain ranges: The Central Range, known in the Dominican Republic as the \"Cordillera Central\", spans the central part of the island, extending from the south coast of the Dominican Republic into northwestern Haiti, where it is known as the \"Massif du Nord\". This mountain range boasts the highest peak in the Antilles, Pico Duarte at above sea level. The \"Cordillera Septentrional\" runs parallel to the Central Range across the northern end of the Dominican Republic, extending into the Atlantic Ocean as the Samaná Peninsula. The \"Cordillera Central\" and \"Cordillera Septentrional\" are separated by the lowlands of the Cibao Valley and the Atlantic coastal plains, which extend westward into Haiti as the \"Plaine du Nord\" (Northern Plain). The lowest of the ranges is the \"Cordillera Oriental\", in the eastern part of the country.\n\nThe \"Sierra de Neiba\" rises in the southwest of the Dominican Republic, and continues northwest into Haiti, parallel to the \"Cordillera Central\", as the \"Montagnes Noires\", \"Chaîne des Matheux\" and the \"Montagnes du Trou d'Eau\". \"The Plateau Central\" lies between the \"Massif du Nord\" and the \"Montagnes Noires\", and the \"Plaine de l‘Artibonite\" lies between the \"Montagnes Noires\" and the \"Chaîne des Matheux\", opening westward toward the Gulf of Gonâve, the largest gulf of the Antilles.\n\nThe southern range begins in the southwestern most Dominican Republic as the Sierra de Bahoruco, and extends west into Haiti as the Massif de la Selle and the Massif de la Hotte, which form the mountainous spine of Haiti’s southern peninsula. Pic de la Selle is the highest peak in the southern range, the third highest peak in the Antilles and consequently the highest point in Haiti, at above sea level. A depression runs parallel to the southern range, between the southern range and the \"Chaîne des Matheux\"-\"Sierra de Neiba\". It is known as the \"Plaine du Cul-de-Sac\" in Haiti, and Haiti’s capital Port-au-Prince lies at its western end. The depression is home to a chain of salt lakes, including Lake Azuei in Haiti and Lake Enriquillo in the Dominican Republic.\n\nThe island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic.\n\nThere are many bird species in Hispaniola, and the island's amphibian species are also diverse.\n\nThe island has four distinct ecoregions. The Hispaniolan moist forests ecoregion covers approximately 50% of the island, especially the northern and eastern portions, predominantly in the lowlands but extending up to elevation. The Hispaniolan dry forests ecoregion occupies approximately 20% of the island, lying in the rain shadow of the mountains in the southern and western portion of the island and in the Cibao valley in the center-north of the island. The Hispaniolan pine forests occupy the mountainous 15% of the island, above elevation. The flooded grasslands and savannas ecoregion in the south central region of the island surrounds a chain of lakes and lagoons in which the most notable include that of Lake Azuei and Trou Caïman in Haiti and the nearby Lake Enriquillo in the Dominican Republic.\nIn Haiti, deforestation has long been cited by scientists as a source of ecological crisis; the timber industry dates back to French colonial rule.\n\nHaiti has seen a dramatic reduction of forests due to the excessive and increasing use of charcoal as fuel for cooking. Recent in-depth studies of satellite imagery and environmental analysis regarding forest classification conclude an accurate estimate of approximately 30% tree cover, a stark decrease in the 60% forest cover in 1925. Despite recent in-depth studies, the notoriously unsubstantiated 2% forest cover estimate has been widely circulated in media and in discourse concerning the country. Despite the drastic underestimation of Haiti's forest cover, the country has been significantly deforested over the last 50 years, resulting in the desertification of portions of the Haitian territory.\n\nIn the Dominican Republic the forest cover has increased. In 2003 the Dominican forest cover had been reduced to 32% of the territory, but in 2011 the trend towards reducing reverts to increase forest cover by eight percentage points to stand at nearly 40% of territory. The success of the Dominican forest growth is due to several Dominican government policies and private organizations for the purpose, and a strong educational campaign that has resulted in increased awareness on the Dominican people of the importance of forests for their welfare and in other forms of life on the island.\n\nOwing to its mountainous topography, Hispaniola’s climate shows considerable variation over short distances, and is the most varied of all the Antilles.\n\nExcept in the Northern Hemisphere summer season, the predominant winds over Hispaniola are the northeast trade winds. As in Jamaica and Cuba, these winds deposit their moisture on the northern mountains, and create a distinct rain shadow on the southern coast, where some areas receive as little as of rainfall, and have semi-arid climates. Annual rainfall under also occurs on the southern coast of Haiti’s northwest peninsula and in the central Azúa region of the \"Plaine du Cul-de-Sac\". In these regions, moreover, there is generally little rainfall outside hurricane season from August to October, and droughts are by no means uncommon when hurricanes do not come.\nOn the northern coast, in contrast, rainfall may peak between December and February, though some rain falls in all months of the year. Annual amounts typically range from on the northern coastal lowlands; there is probably much more in the Cordillera Septentrional, though no data exist.\n\nThe interior of Hispaniola, along with the southeastern coast centred around Santo Domingo, typically receives around per year, with a distinct wet season from May to October. Usually, this wet season has two peaks: one around May, the other around the hurricane season. In the interior highlands, rainfall is much greater, around per year, but with a similar pattern to that observed in the central lowlands.\n\nAs is usual for tropical islands, variations of temperature are much less marked than rainfall variations, and depend only on altitude. Lowland Hispaniola is generally oppressively hot and humid, with temperatures averaging . with high humidity during the daytime, and around at night. At higher altitudes, temperatures fall steadily, so that frosts occur during the dry season on the highest peaks, where maxima are no higher than .\n\nThe island is divided into two sovereign states: the Dominican Republic, which occupies most of the island and is the heir to the Spanish province of Santo Domingo; and the Republic of Haiti which occupies the western third of the island.\n\nThe Dominican Republic is a Hispanophone nation of approximately 10 million people. Spanish is spoken by all Dominicans as a primary language. Roman Catholicism is the official and dominant religion.\n\nHaiti is a Francophone nation of roughly 10 million people. Although French is spoken as a primary language by the educated and wealthy minority, virtually the entire population speaks Haitian Creole, one of several French-derived creole languages. Roman Catholicism is the dominant religion, practiced by more than half the population, although in some cases in combination with Haitian Vodou faith. Another 25% of the populace belong to Protestant churches. Haiti emerged as the first Black republic in the world.\n\nThe ethnic composition of the Dominican population is 73% mixed (African/European), 16% white and 11% black.\n\nThe ethnic composition of Haiti is estimated to be 95% black, 5% white and mixed.\n\nThe island has the largest economy in the Greater Antilles, however most of the economic development is found in the Dominican Republic, the Dominican economy being nearly 800% larger than the Haitian economy.\n\nThe estimated annual per capita income is US$1,300 in Haiti and US$8,200 in Dominican Republic.\n\nThe divergence between the level of economic development between Haiti and Dominican Republic makes its border the higher contrast of all western land borders and is evident that the Dominican Republic has one of the highest migration issues in the Americas.\n\nChristopher Columbus, noting \"the land and trees resembled those of Spain, and that the sailors caught in their nets many fish like those of Spain...named it Espanola on Sunday, December 9th.\" One of the first inhabitants he came across on this island was \"a girl wearing only a gold nose plug.\" Columbus later learned that the \"land of gold was farther east.\" Soon the Tainos were trading pieces of gold for hawk's bells with their cacique declaring the gold came from Cibao. Traveling further east from Navidad, Columbus came across the Yaque del Norte River, which he named Rio de Oro because its \"sands abound in gold dust.\"\n\nOn Columbus' return during his second voyage he learned it was the cacique Caonabo, \"lord of the mines\", who had massacred his settlement at Navidad. While Columbus established a new settlement at La Isabela on Jan. 1494, he sent Alonso de Ojeda and 15 men to search for the \"mines of Cibao.\" After a six-day journey, Ojeda came across an area \"very rich in gold\", in which the \"Indians took gold out of a brook...and many other streams in that province.\" Columbus himself visited the mines of Cibao on 12 March 1494. He constructed the Fort of Santo Tomas, present day Janico, with Captain Pedro Margarit in command of 56 men.\n\nOn 24 March 1495, Columbus with his ally Guacanagarix, embarked on a war of revenge against Caonabo, capturing him and his family while \"killing many Indians and capturing others.\" Afterwards, \"every person of fourteen years of age or upward was to pay a large hawk's bell of gold dust.\"\n\nMiguel Diaz and Francisco de Garay discovered large gold nuggets on the lower Haina River in 1496. These San Cristobal mines were later known as the Minas Viejas mines. Then, in 1499, the first major discovery of gold was made in the cordillera central, which led to a mining boom. By 1501, Columbus' cousin Giovanni Colombo, had discovered gold near Buenaventura, the deposits were later known as Minas Nuevas. Two major mining areas resulted, one along San Cristobal-Buenaventura, and another in Cibao within the La Vega-Cotuy-Bonao triangle, while Santiago de los Caballeros, Concepcion, and Bonao became mining towns. The gold rush of 1500-1508 ensued. \n\nUnder Nicolás de Ovando y Cáceres' governorship, the Indians were made to work in the gold mines, \"where they were grossly overworked, mistreated, and underfed,\" according to Pons. By 1503, the Spanish Crown legalized the distribution of Indians to work the mines as part of the encomienda system. According to Pons, \"Once the Indians entered the mines, hunger and disease literally wiped them out.\" By 1508 the Indian population of about 400,000 was reduced to 60,000, and by 1514, only 26,334 remained. About half were located in the mining towns of Concepcion, Santiago, Santo Domingo, and Buenaventura. The repartimiento of 1514 accelerated emigration of the Spanish colonists, coupled with the exhaustion of the mines. In 1516, a smallpox epidemic killed an additional 8,000, of the remaining 11,000 Indians, in one month. By 1519, according to Pons, \"Both the gold economy and the Indian population became extinct at the same time.\"\n\nHowever, writing in 1860, Courtney observed, the island is \"one immense gold field\", of which the early Spaniards had \"scarcely began to be developed.\" Additionally, \"The gold is still found in the Cibao regions as of old.\" By 1919, Condit and Ross noted \"the greater part of the Republic is covered by concessions granted by the government for mining minerals of diverse sorts.\" Besides gold, these minerals included silver, manganese, copper, magnetite, iron and nickel.\n\nMining operations in 2016 have taken advantage of the volcanogenic massive sulfide ore deposits, (VMS), around Maimón. To the northeast, the Pueblo Viejo Gold Mine was operated by state-owned Rosario Dominicana from 1975 until 1991. In 2009, Pueblo Viejo Dominicana Corporation, formed by Barrick Gold and Goldcorp, started open-pit mining operations of the Monte Negro and Moore oxide deposits. The mined ore is processed with gold cyanidation. Pyrite and sphalerite are the main sulfide minerals found in the 120 m thick volcanic conglomerates and agglomerates, which constitute the world's second largest sulphidation gold deposit.\n\nBetween Bonao and Maimon, Falconbridge Dominicana has been mining nickel laterites since 1971. The Cerro de Maimon copper/gold open-pit mine southeast of Maimon has been operated by Perilya since 2006. Copper is extracted from the sulfide ores, while gold and silver are extracted from both the sulfide and the oxide ores. Processing is via froth flotation and cyanidation. The ore is located in the VMS Early Cretaceous Maimon Formation. Goethite enriched with gold and silver is found in the 30 m thick oxide cap. Below that cap is a supergene zone containing pyrite, chalcopyrite, and sphalerite. Below the supergene zone is found the unaltered massive sulphide mineralization.\n\n\n", "id": "13714", "title": "Hispaniola"}
{"url": "https://en.wikipedia.org/wiki?curid=13717", "text": "Halle Berry\n\nHalle Maria Berry (born Maria Halle Berry; August 14, 1966) is an American actress, film producer, and former fashion model. She won an Academy Award for Best Actress in 2002 for her performance in the romantic drama \"Monster's Ball\" (2001), which made her the only black woman to win a Best Actress Academy Award to date, as of 2016.\n\nBerry was one of the highest paid actresses in Hollywood during the 2000s and has been involved in the production of several of the films in which she performed. She is also a Revlon spokesmodel. Before becoming an actress, she started modeling and entered several beauty contests, finishing as the 1st runner-up in the Miss USA Pageant and coming in 6th place in the Miss World Pageant in 1986. Her breakthrough film role was in the romantic comedy \"Boomerang\" (1992), alongside Eddie Murphy, which led to roles in films such as the comedy \"The Flintstones\" (1994), the political comedy-drama \"Bulworth\" (1998) and the television film \"Introducing Dorothy Dandridge\" (1999), for which she won the Emmy Award and Golden Globe Award for Best Actress, among many other awards.\n\nIn addition to her Academy Award win, Berry garnered high-profile roles in the 2000s such as Storm in the \"X-Men\" film series (beginning in 2000), the action crime thriller \"Swordfish\" (2001), and the spy film \"Die Another Day\" (2002), where she played Bond Girl Jinx. She then appeared in the \"X-Men\" sequels, \"X2: X-Men United\" (2003) and \"\" (2006). In the 2010s, she appeared in movies such as the science fiction film \"Cloud Atlas\" (2012), the crime thriller \"The Call\" (2013) and \"\" (2014).\n\nDivorced from baseball player David Justice and singer-songwriter Eric Benét, Berry has a daughter by model Gabriel Aubry, and a son by actor Olivier Martinez.\n\nBerry was born Maria Halle Berry; her name was legally changed to Halle Maria Berry at age five. Her parents selected her middle name from Halle's Department Store, which was then a local landmark in her birthplace of Cleveland, Ohio. Her mother, Judith Ann (née Hawkins), who is of English and German ancestry, was a psychiatric nurse. Her father, Jerome Jesse Berry, was an African-American hospital attendant in the psychiatric ward where her mother worked; he later became a bus driver. Berry's maternal grandmother, Nellie Dicken, was born in Sawley, Derbyshire, England, while her maternal grandfather, Earl Ellsworth Hawkins, was born in Ohio. Berry's parents divorced when she was four years old; she and her older sister, Heidi Berry-Henderson, were raised exclusively by their mother.\n\nBerry has said in published reports that she has been estranged from her father since her childhood, noting in 1992, \"I haven't heard from him since [he left]. Maybe he's not alive.\" Her father was very abusive to her mother. Berry has recalled witnessing her mother being beaten daily, kicked down stairs and hit in the head with a wine bottle.\n\nBerry graduated from Bedford High School where she was a cheerleader, honor student, editor of the school newspaper and prom queen. She worked in the children's department at Higbee's Department store. She then studied at Cuyahoga Community College. In the 1980s, she entered several beauty contests, winning Miss Teen All American in 1985 and Miss Ohio USA in 1986. She was the 1986 Miss USA first runner-up to Christy Fichtner of Texas. In the Miss USA 1986 pageant interview competition, she said she hoped to become an entertainer or to have something to do with the media. Her interview was awarded the highest score by the judges. She was the first African-American Miss World entrant in 1986, where she finished sixth and Trinidad and Tobago's Giselle Laronde was crowned Miss World. According to the \"Current Biography Yearbook\", Berry \"...pursued a modeling career in Chicago... Berry's first weeks in New York were less than auspicious: She slept in a homeless shelter and then in a YMCA\".\n\nIn 1989, Berry moved to New York City to pursue her acting ambitions. During her early time there, she ran out of money and had to live briefly in a homeless shelter.\n\nLater in 1989, her situation improved and she was cast in the role of model Emily Franklin in the short-lived ABC television series \"Living Dolls\", which was shot in New York and was a spin-off of the hit series \"Who's the Boss?\". During the taping of \"Living Dolls\", she lapsed into a coma and was diagnosed with Type 1 diabetes. After the cancellation of \"Living Dolls\", she moved to Los Angeles. She went on to have a recurring role on the long-running primetime serial \"Knots Landing\".\nHer film debut was in a small role for Spike Lee's \"Jungle Fever\" (1991), in which she played Vivian, a drug addict. That same year, Berry had her first co-starring role in \"Strictly Business\". In 1992, Berry portrayed a career woman who falls for the lead character played by Eddie Murphy in the romantic comedy \"Boomerang\". The following year, she caught the public's attention as a headstrong biracial slave in the TV adaptation of \"\", based on the book by Alex Haley. Berry was in the live-action \"Flintstones\" movie playing the part of \"Sharon Stone\", a sultry secretary who seduced Fred Flintstone.\n\nBerry tackled a more serious role, playing a former drug addict struggling to regain custody of her son in \"Losing Isaiah\" (1995), starring opposite Jessica Lange. She portrayed Sandra Beecher in \"Race the Sun\" (1996), which was based on a true story, shot in Australia, and co-starred alongside Kurt Russell in \"Executive Decision\". Beginning in 1996, she was a Revlon spokeswoman for seven years and renewed her contract in 2004.\n\nShe starred alongside Natalie Deselle Reid in the 1997 comedy film \"B*A*P*S\". In 1998, Berry received praise for her role in \"Bulworth\" as an intelligent woman raised by activists who gives a politician (Warren Beatty) a new lease on life. The same year, she played the singer Zola Taylor, one of the three wives of pop singer Frankie Lymon, in the biopic \"Why Do Fools Fall in Love\". In the 1999 HBO biopic \"Introducing Dorothy Dandridge\", she portrayed the first black woman to be nominated for a Best Actress Academy Award, and it was to Berry a heart-felt project that she introduced, co-produced and fought intensely for it to come through. Berry's performance was recognized with several awards, including an Emmy and a Golden Globe.\n\nBerry portrayed the mutant superhero Storm in the film adaptation of the comic book series \"X-Men\" (2000) and its sequels, \"X2: X-Men United\" (2003), \"\" (2006) and \"\" (2014). In 2001, Berry appeared in the film \"Swordfish\", which featured her first topless scene. At first, she refused to be filmed topless in a sunbathing scene, but she changed her mind when Warner Brothers raised her fee substantially. The brief flash of her breasts added $500,000 to her fee. Berry considered these stories to be rumors and was quick to deny them. After turning down numerous roles that required nudity, she said she decided to make \"Swordfish\" because her then-husband, Eric Benét, supported her and encouraged her to take risks.\n\nShe appeared as Leticia Musgrove, the troubled wife of an executed murderer (Sean Combs), in the 2001 feature film \"Monster's Ball\". Her performance was awarded the National Board of Review and the Screen Actors Guild best-actress prizes; in an interesting coincidence she became the first woman of color to win the Academy Award for Best Actress (earlier in her career, she portrayed Dorothy Dandridge, the first African American to be nominated for Best Actress, and who was born at the same hospital as Berry, in Cleveland, Ohio). The NAACP issued the statement: \"Congratulations to Halle Berry and Denzel Washington for giving us hope and making us proud. If this is a sign that Hollywood is finally ready to give opportunity and judge performance based on skill and not on skin color then it is a good thing.\" This role generated controversy. Her graphic nude love scene with a racist character played by co-star Billy Bob Thornton was the subject of much media chatter and discussion among African Americans. Many in the African-American community were critical of Berry for taking the part. Berry responded: \"I don't really see a reason to ever go that far again. That was a unique movie. That scene was special and pivotal and needed to be there, and it would be a really special script that would require something like that again.\"\n\nBerry asked for a higher fee for Revlon advertisements after winning the Academy Award. Ron Perelman, the cosmetics firm's chief, congratulated her, saying how happy he was that she modeled for his company. She replied, \"Of course, you'll have to pay me more.\" Perelman stalked off in a rage. Her win at the Academy Awards led to two famous \"Oscar moments.\" In accepting her award, she gave an acceptance speech honoring previous black actresses who had never had the opportunity. She said, \"This moment is so much bigger than me. This is for every nameless, faceless woman of colour who now has a chance tonight because this door has been opened.\"\n\nAs Bond girl Giacinta 'Jinx' Johnson in the 2002 blockbuster \"Die Another Day\", Berry recreated a scene from \"Dr. No\", emerging from the surf to be greeted by James Bond as Ursula Andress had 40 years earlier. Lindy Hemming, costume designer on \"Die Another Day\", had insisted that Berry wear a bikini and knife as a homage. Berry has said of the scene: \"It's splashy\", \"exciting\", \"sexy\", \"provocative\" and \"it will keep me still out there after winning an Oscar\". The bikini scene was shot in Cadiz; the location was reportedly cold and windy, and footage has been released of Berry wrapped in thick towels in between takes to try to stay warm. According to an ITV news poll, Jinx was voted the fourth toughest girl on screen of all time. Berry was hurt during filming when debris from a smoke grenade flew into her eye. It was removed in a 30-minute operation. After Berry won the Academy Award, rewrites were commissioned to give her more screentime for \"X2\".\n\nShe starred in the psychological thriller \"Gothika\" opposite Robert Downey, Jr. in November 2003, during which she broke her arm in a scene with Downey, who twisted her arm too hard. Production was halted for eight weeks. It was a moderate hit at the United States box office, taking in $60 million; it earned another $80 million abroad. Berry appeared in the nu metal band Limp Bizkit's music video for \"Behind Blue Eyes\" for the motion picture soundtrack for the film. The same year, she was named #1 in \"FHM\"'s 100 Sexiest Women in the World poll.\n\nBerry received $12.5 million for the title role in the film \"Catwoman\", a $100 million movie; it grossed $17 million on its first weekend. She was awarded a Worst Actress Razzie Award in 2005 for this role. She appeared at the ceremony to accept the award in person (making her the third person, and second actor, ever to do so) with a sense of humor, considering it an experience of the \"rock bottom\" in order to be \"at the top\". Holding the Academy Award in one hand and the Razzie in the other she said, \"I never in my life thought that I would be here, winning a Razzie. It's not like I ever aspired to be here, but thank you. When I was a kid, my mother told me that if you could not be a good loser, then there's no way you could be a good winner.\" The Fund for Animals praised Berry's compassion towards cats and for squelching rumors that she was keeping a Bengal tiger from the sets of Catwoman as a \"pet.\"\n\nHer next film appearance was in the Oprah Winfrey-produced ABC television movie \"Their Eyes Were Watching God\" (2005), an adaptation of Zora Neale Hurston's novel, in which Berry portrayed Janie Crawford, a free-spirited woman whose unconventional sexual mores upset her 1920s contemporaries in a small community. She was nominated for an Emmy for this TV film. Meanwhile, she voiced the character of Cappy, one of the many mechanical beings in the animated feature \"Robots\" (2005).\n\nBerry is involved in production of films and television. She served as executive producer on \"Introducing Dorothy Dandridge\" in 1999, and \"Lackawanna Blues\" in 2005. In 2007, Berry both produced and starred in the thriller \"Perfect Stranger\" with Bruce Willis, and starred in \"Things We Lost in the Fire\" with Benicio del Toro, the first film in which she worked with a female director, Danish Susanne Bier, giving her a new feeling of \"thinking the same way\", which she appreciated. Berry then starred in the film \"Frankie and Alice\", in which she plays Frankie Murdoch, a young multiracial American women with dissociative identity disorder struggling against her alter personality to retain her true self. She was awarded the African-American Film Critics Association Award for Best Actress and was nominated for the Golden Globe Award for Best Actress – Motion Picture Drama.\n\nBerry was one of the highest-paid actresses in Hollywood during the 2000s, earning an estimated $10 million per film. On April 3, 2007, she was awarded a star on the Hollywood Walk of Fame in front of the Kodak Theatre at 6801 Hollywood Boulevard for her contributions to the film industry.\n\nAs of 2013, Berry's worldwide box office gross has been more than USD$3.3 billion. In 2011, she appeared in \"New Year's Eve\". She played one of the leads in the film \"Cloud Atlas\", which was released in October 2012.\n\nOn October 4, 2013, Berry signed on to star in the CBS drama series \"Extant\". Berry played Molly Woods, an astronaut who struggles to reconnect with her husband and android son after spending 13 months in space. The show premiered on July 9, 2014. She served as a co-executive producer on the series.\n\nBerry has served for many years as the face of Revlon cosmetics and as the face of Versace. The Coty Inc. fragrance company signed Berry to market her debut fragrance in March 2008. Berry was delighted, saying that she had created her own fragrances at home by mixing scents.\n\nIn March 2014 Berry launched a new production company, 606 Films, with producing partner Elaine Goldsmith-Thomas. It is named after the Anti-Paparazzi Bill, SB 606, that the actress pushed for and which was signed into law by California Governor Jerry Brown in the fall of 2013. The new company emerged as part of a deal for Berry to star in the CBS sci-fi drama series \"Extant\". 606 Films is housed within CBS.\n\nIn February 2000, Berry was involved in a traffic collision and left the scene. She was charged with misdemeanor hit-and-run. \n\nBerry dated Chicago dentist John Ronan from March 1989 to October 1991. In November 1993, Ronan sued Berry for $80,000 in what he claimed were unpaid loans to help launch her career. Berry contended that the money was a gift, and a judge dismissed the case because Ronan did not list Berry as a debtor when he filed for bankruptcy in 1992.\n\nAccording to Berry, a beating from a former abusive boyfriend during the filming of \"The Last Boy Scout\" punctured her eardrum and caused her to lose eighty percent of her hearing in her left ear. Berry has never named the abuser but has said that he is someone well known in Hollywood.\n\nBerry first saw baseball player David Justice on TV playing in an MTV celebrity baseball game in February 1992. When a reporter from Justice's hometown of Cincinnati told her that Justice was a fan, Berry gave her phone number to the reporter to give to Justice. Berry married Justice shortly after midnight on January 1, 1993.\n\nFollowing their separation in February 1996, Berry stated publicly that she was so depressed that she considered taking her own life. Berry and Justice were officially divorced on June 24, 1997.\n\nBerry married her second husband, singer-songwriter Eric Benét, on January 24, 2001, following a two-year courtship. but by early October 2003 they had separated, with the divorce finalized on January 3, 2005. Benét underwent treatment for sex addiction in 2002.\n\nIn November 2005, Berry began dating French Canadian model Gabriel Aubry, whom she met at a Versace photoshoot. Berry gave birth to their daughter in March 2008. On April 30, 2010, Berry and Aubry announced their separation.\n\nAfter their 2010 separation, Berry and Aubry became involved in a highly publicized custody battle, centered primarily on Berry's desire to move with their daughter from Los Angeles, where Berry and Aubry resided, to France, the home of French actor Olivier Martinez, whom Berry had started dating in 2010 after they met while filming \"Dark Tide\" in South Africa. Aubry objected to the move on the grounds that it would interfere with their joint custody arrangement. In November 2012, a judge denied Berry's request to move the couple's daughter to France in light of Aubry's objections. Less than two weeks later, on November 22, 2012, Aubry and Martinez were both treated at a hospital for injuries after engaging in a physical altercation at Berry's residence. Martinez performed a citizen's arrest on Aubry, and because it was considered a domestic violence incident, was granted a temporary emergency protective order preventing Aubry from coming within 100 yards of Berry, Martinez, the child with whom he shares custody with Berry, until November 29, 2012. In turn, Aubry obtained a temporary restraining order against Martinez on November 26, 2012, asserting that the fight began when Martinez threatened to kill Aubry if he did not allow the couple to move to France. Leaked court documents included photos showing significant injuries to Aubry's face, which were widely displayed in the media.\n\nOn November 29, 2012, Berry's lawyer announced that Berry and Aubry had reached an amicable custody agreement in court. In June 2014, a Superior Court ruling called for Berry to pay Aubry $16,000 a month in child support (around 200k/year) as well as a retroactive payment of $115,000 and a sum of $300,000 for Aubry's attorney fees.\n\nBerry and Martinez confirmed their engagement in March 2012, and married in France on July 13, 2013. In October 2013, Berry gave birth to their son. After two years of marriage, in 2015 the couple announced they were divorcing. The divorce became final in December 2016.\n\nAlong with Pierce Brosnan, Cindy Crawford, Jane Seymour, Dick Van Dyke, Téa Leoni, and Daryl Hannah, Berry successfully fought in 2006 against the Cabrillo Port Liquefied Natural Gas facility that was proposed off the coast of Malibu. Berry said, \"I care about the air we breathe, I care about the marine life and the ecosystem of the ocean.\" In May 2007, Governor Arnold Schwarzenegger vetoed the facility. Hasty Pudding Theatricals gave her its 2006 \"Woman of The Year\" award.\n\nBerry took part in a nearly 2,000-house cell-phone bank campaign for Barack Obama in February 2008. In April 2013, she appeared in a video clip for Gucci's \"Chime for Change\" campaign that aims to raise funds and awareness of women's issues in terms of education, health, and justice. In August 2013, Berry testified alongside Jennifer Garner before the California State Assembly's Judiciary Committee in support of a bill that would protect celebrities' children from harassment by photographers. The bill passed in September.\n\nBerry was ranked No. 1 on \"People\" \"50 Most Beautiful People in the World\" list in 2003 after making the top ten seven times and appeared No. 1 on \"FHM\" \"100 Sexiest Women in the World\" the same year. She was named \"Esquire\" magazine's \"Sexiest Woman Alive\" in October 2008, about which she stated: \"I don't know exactly what it means, but being 42 and having just had a baby, I think I'll take it.\" \"Men's Health\" ranked her at No. 35 on their \"100 Hottest Women of All-Time\" list. In 2009, she was voted #23 on \"Empire\"'s 100 Sexiest Film Stars. The same year, rapper Hurricane Chris released a song entitled \"Halle Berry (She's Fine)\", extolling Berry's beauty and sex appeal. At the age of 42 (in 2008), she was named the “Sexiest Black Woman” by Access Hollywood’s TV One Access survey.\n\nBorn to an African-American father and a white mother, Berry has stated that her biracial background was \"painful and confusing\" when she was a young woman, and she made the decision early on to identify as a black woman because she knew that was how she would be perceived.\n\n\n", "id": "13717", "title": "Halle Berry"}
{"url": "https://en.wikipedia.org/wiki?curid=13722", "text": "Robert Koch\n\n\"\n\nRobert Heinrich Hermann Koch (; ; 11 December 1843 – 27 May 1910) was a celebrated German physician and pioneering microbiologist. As the founder of modern bacteriology, he is known for his role in identifying the specific causative agents of tuberculosis, cholera, and anthrax and for giving experimental support for the concept of infectious disease. In addition to his innovative studies on these diseases, Koch created and improved laboratory technologies and techniques in the field of microbiology, and made key discoveries in public health. His research led to the creation of Koch’s postulates, a series of four generalized principles linking specific microorganisms to specific diseases that remain today the \"gold standard\" in medical microbiology. As a result of his groundbreaking research on tuberculosis, Koch received the Nobel Prize in Physiology or Medicine in 1905.\n\nRobert Koch was born in Clausthal, Hanover, Germany, on 11 December 1843, to Hermann Koch and Mathilde Julie Henriette Biewand. Koch excelled in academics from an early age. Before entering school in 1848, he had taught himself how to read and write. He graduated from high school in 1862, having excelled in science and maths. At the age of 19, Koch entered the University of Göttingen, studying natural science. However, after three semesters, Koch decided to change his area of study to medicine, as he aspired to be a physician. During his fifth semester of medical school, Jacob Henle, an anatomist who had published a theory of contagion in 1840, asked him to participate in his research project on uterine nerve structure. In his sixth semester, Koch began to conduct research at the Physiological Institute, where he studied succinic acid secretion. This would eventually form the basis of his dissertation. In January 1866, Koch graduated from medical school, earning honors of the highest distinction. In July 1867, Koch married Emma Adolfine Josephine Fraatz, and the two had a daughter, Gertrude, in 1868. Several years after his graduation in 1866, he worked as a surgeon in the Franco-Prussian War, and following his service, worked as a physician in Wollstein, Posen. From 1885 to 1890, he served as an administrator and professor at Berlin University. Koch’s marriage to Emma Fraatz ended in 1893, and later that same year, he married actress Hedwig Freiberg. Koch suffered a heart attack on 9 April 1910, and never made a complete recovery. On 27 May, only three days after giving a lecture on his tuberculosis research at the Prussian Academy of Sciences, Robert Koch died in Baden-Baden at the age of 66. Following his death, the Institute named its establishment after him in his honour. He was irreligious.\n\nRobert Koch is widely known for his work with anthrax, discovering the causative agent of the fatal disease to be \"Bacillus anthracis\". Koch discovered the formation of spores in anthrax bacteria that could remain dormant under specific conditions. However, under optimal conditions, the spores were activated and caused disease. To determine this causative agent, he dry-fixed bacterial cultures onto glass slides, used dyes to stain the cultures, and observed them through a microscope. Koch’s work with anthrax is notable in that he was the first to link a specific microorganism with a specific disease, rejecting the idea of spontaneous generation and supporting the germ theory of disease.\n\nKoch accepted a position as government advisor with the Imperial Department of Health in 1880. During his time as government advisor, he published a report in which he stated the importance of pure cultures in isolating disease-causing organisms and explained the necessary steps to obtain these cultures, methods which are summarized in Koch’s four postulates. Koch’s discovery of the causative agent of anthrax led to the formation of a generic set of postulates which can be used in the determination of the cause of most infectious diseases. These postulates, which not only outlined a method for linking cause and effect of an infectious disease but also established the significance of laboratory culture of infectious agents, are listed here:\n\nKoch began conducting research on microorganisms in a laboratory connected to his patient examination room. Koch’s early research in this laboratory proved to yield one of his major contributions to the field of microbiology, as it was there that he developed the technique of growing bacteria. Koch's second postulate calls for the isolation and growth of a selected pathogen in pure laboratory culture. In an attempt to grow bacteria, Koch began to use solid nutrients such as potato slices. Through these initial experiments, Koch observed individual colonies of identical, pure cells. Coming to the conclusion that potato slices were not suitable media for all organisms, Koch later began to use nutrient solutions with gelatin. However, he soon realized that gelatin, like potato slices, was not the optimal medium for bacterial growth, as it did not remain solid at 37 °C, the ideal temperature for growth of most human pathogens. As suggested to him by Walther and Fanny Hesse, Koch began to utilize agar to grow and isolate pure cultures, as this polysaccharide remains solid at 37 °C, is not degraded by most bacteria, and results in a transparent medium.\n\nKoch next turned his attention to cholera, and began to conduct research in Egypt in the hopes of isolating the causative agent of the disease. However, he was not able to complete the task before the epidemic in Egypt ended, and subsequently traveled to India to continue with the study. In India, Koch was indeed able to determine the causative agent of cholera, isolating \"Vibrio cholerae\". The bacterium had originally been isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.\n\nDuring his time as the government advisor with the Imperial Department of Health in Berlin in the 1880s, Robert Koch became interested in tuberculosis research. At the time, it was widely believed that tuberculosis was an inherited disease. However, Koch was convinced that the disease was caused by a bacterium and was infectious, and tested his four postulates using guinea pigs. Through these experiments, he found that his experiments with tuberculosis satisfied all four of his postulates. In 1882, he published his findings on tuberculosis, in which he reported the causative agent of the disease to be the slow-growing \"Mycobacterium tuberculosis\". His work with this disease won Koch the Nobel Prize in Physiology and Medicine in 1905. Additionally, Koch's research on tuberculosis, along with his studies on tropical diseases, won him the Prussian Order Pour le Merite in 1906 and the Robert Koch medal, established to honour the greatest living physicians, in 1908.\n\nIn addition to being awarded a Nobel Prize, Koch was elected a Foreign Member of the Royal Society (ForMemRS) in 1897. His microbial postulates are named in his honour, Koch's postulates.\n\nKoch's name is one of twenty-three, from the fields of hygiene and tropical medicine, featured on the frieze of the London School of Hygiene & Tropical Medicine building in Keppel Street, Bloomsbury.\n\nA large marble statue of Koch stands in a small park known as Robert Koch Platz, just north of the Charity Hospital, in the Mitte section of Berlin.\n\n\n", "id": "13722", "title": "Robert Koch"}
{"url": "https://en.wikipedia.org/wiki?curid=13726", "text": "Hogshead\n\nA hogshead (abbreviated \"Hhd\", plural \"Hhds\") is a large cask of liquid (or, less often, of a food commodity). More specifically, it refers to a specified volume, measured in either imperial or US customary measures, primarily applied to alcoholic beverages, such as wine, ale, or cider.\n\nA tobacco hogshead was used in British and American colonial times to transport and store tobacco. It was a very large wooden barrel. A standardized hogshead measured long and in diameter at the head (at least , depending on the width in the middle). Fully packed with tobacco, it weighed about .\n\nA wine hogshead contains about .\n\nThe \"Oxford English Dictionary\" (OED) notes that the hogshead was first standardized by an act of Parliament in 1423, though the standards continued to vary by locality and content. For example, the OED cites an 1897 edition of \"Whitaker's Almanack\", which specified the number of gallons of wine in a hogshead varying by type of wine: claret (presumably) , port , sherry ; and Madeira . The \"American Heritage Dictionary\" claims that a hogshead can consist of anything from (presumably) .\n\nEventually, a hogshead of wine came to be , while a hogshead of beer or ale is 54 gallons (250 L if old beer/ale gallons, 245 L if imperial).\n\nA hogshead was also used as unit of measurement for sugar in Louisiana for most of the 19th century. Plantations were listed in sugar schedules as having produced \"x\" number of hogsheads of sugar or molasses. A hogshead was also used for the measurement of herring fished for sardines in Blacks Harbour, New Brunswick.\n\nThe etymology of hogshead is uncertain. According to English philologist Walter William Skeat (1835–1912), the origin is to be found in the name for a cask or liquid measure appearing in various forms in several Germanic languages, in Dutch oxhooft (modern okshoofd), Danish oxehoved, Old Swedish oxhufvod, etc. The word should therefore be \"oxhead\", \"hogshead\" being a mere corruption. It has been suggested that the name arose from the branding of such a measure with the head of an ox.\n\nA hogshead of Madeira wine was approximately equal to 45–48 gallons (0.205–0.218) m.\n\nA hogshead of brandy was approximately equal to 56–61 gallons (0.255–0.277) m.\n\n", "id": "13726", "title": "Hogshead"}
{"url": "https://en.wikipedia.org/wiki?curid=13727", "text": "Huallaga\n\nHuallaga may refer to:\n\n", "id": "13727", "title": "Huallaga"}
{"url": "https://en.wikipedia.org/wiki?curid=13729", "text": "Honda\n\nHonda has been the world's largest motorcycle manufacturer since 1959, as well as the world's largest manufacturer of internal combustion engines measured by volume, producing more than 14 million internal combustion engines each year. Honda became the second-largest Japanese automobile manufacturer in 2001. Honda was the eighth largest automobile manufacturer in the world behind General Motors, Volkswagen Group, Toyota, Hyundai Motor Group, Ford, Nissan, and PSA Peugeot Citroën in 2011.\n\nHonda was the first Japanese automobile manufacturer to release a dedicated luxury brand, Acura, in 1986. Aside from their core automobile and motorcycle businesses, Honda also manufactures garden equipment, marine engines, personal watercraft and power generators, and other products. Since 1986, Honda has been involved with artificial intelligence/robotics research and released their ASIMO robot in 2000. They have also ventured into aerospace with the establishment of GE Honda Aero Engines in 2004 and the Honda HA-420 HondaJet, which began production in 2012. Honda has three joint-ventures in China (Honda China, Dongfeng Honda, and Guangqi Honda).\n\nIn 2013, Honda invested about 5.7% (US$6.8 billion) of its revenues in research and development. Also in 2013, Honda became the first Japanese automaker to be a net exporter from the United States, exporting 108,705 Honda and Acura models, while importing only 88,357.\n\nThroughout his life, Honda's founder, Soichiro Honda had an interest in automobiles. He worked as a mechanic at the Art Shokai garage, where he tuned cars and entered them in races. In 1937, with financing from his acquaintance Kato Shichirō, Honda founded Tōkai Seiki (Eastern Sea Precision Machine Company) to make piston rings working out of the Art Shokai garage. After initial failures, Tōkai Seiki won a contract to supply piston rings to Toyota, but lost the contract due to the poor quality of their products. After attending engineering school without graduating, and visiting factories around Japan to better understand Toyota's quality control processes, by 1941 Honda was able to mass-produce piston rings acceptable to Toyota, using an automated process that could employ even unskilled wartime laborers.\n\nTōkai Seiki was placed under control of the Ministry of Commerce and Industry (called the Ministry of Munitions after 1943) at the start of World War II, and Soichiro Honda was demoted from president to senior managing director after Toyota took a 40% stake in the company. Honda also aided the war effort by assisting other companies in automating the production of military aircraft propellers. The relationships Honda cultivated with personnel at Toyota, Nakajima Aircraft Company and the Imperial Japanese Navy would be instrumental in the postwar period. A US B-29 bomber attack destroyed Tōkai Seiki's Yamashita plant in 1944, and the Itawa plant collapsed in the 13 January 1945 Mikawa earthquake, and Soichiro Honda sold the salvageable remains of the company to Toyota after the war for ¥450,000, and used the proceeds to found the Honda Technical Research Institute in October 1946.\n\nWith a staff of 12 men working in a shack, they built and sold improvised motorized bicycles, using a supply of 500 two-stroke \"50 cc\" Tohatsu war surplus radio generator engines. When the engines ran out, Honda began building their own copy of the Tohatsu engine, and supplying these to customers to attach to their bicycles. This was the Honda A-Type, nicknamed the Bata Bata for the sound the engine made. In 1949, the Honda Technical Research Institute was liquidated for 1,000,000, or about 5,000 today; these funds were used to incorporate Honda Motor Co., Ltd. At about the same time Honda hired engineer Kihachiro Kawashima, and Takeo Fujisawa who provided indispensable business and marketing expertise to complement Soichiro Honda's technical bent. The close partnership between Soichiro Honda and Fujisawa lasted until they stepped down together in October 1973.\n\nThe first complete motorcycle, with both the frame and engine made by Honda, was the 1949 D-Type, the first Honda to go by the name Dream. Honda Motor Company grew in a short time to become the world's largest manufacturer of motorcycles by 1964.\n\nThe first production automobile from Honda was the T360 mini pick-up truck, which went on sale in August 1963. Powered by a small 356-cc straight-4 gasoline engine, it was classified under the cheaper Kei car tax bracket. The first production car from Honda was the S500 sports car, which followed the T360 into production in October 1963. Its chain-driven rear wheels pointed to Honda's motorcycle origins.\n\nOver the next few decades, Honda worked to expand its product line and expanded operations and exports to numerous countries around the world. In 1986, Honda introduced the successful Acura brand to the American market in an attempt to gain ground in the luxury vehicle market. The year 1991 saw the introduction of the Honda NSX supercar, the first all-aluminum monocoque vehicle that incorporated a mid-engine V6 with variable-valve timing.\n\nCEO Tadashi Kume was succeeded by Nobuhiko Kawamoto in 1990. Kawamoto was selected over Shoichiro Irimajiri, who oversaw the successful establishment of Honda of America Manufacturing, Inc. in Marysville, Ohio. Both Kawamoto and Irimajiri shared a friendly rivalry within Honda, and Irimajiri would resign in 1992 due to health issues.\n\nFollowing the death of Soichiro Honda and the departure of Irimajiri, Honda found itself quickly being outpaced in product development by other Japanese automakers and was caught off-guard by the truck and sport utility vehicle boom of the 1990s, all which took a toll on the profitability of the company. Japanese media reported in 1992 and 1993 that Honda was at serious risk of an unwanted and hostile takeover by Mitsubishi Motors, who at the time was a larger automaker by volume and flush with profits from their successful Pajero and Diamante.\n\nKawamoto acted quickly to change Honda's corporate culture, rushing through market-driven product development that resulted in recreational vehicles such as the first generation Odyssey and the CR-V, and a refocusing away from some of the numerous sedans and coupes that were popular with Honda's engineers but not with the buying public. The most shocking change to Honda came when Kawamoto ended Honda's successful participation in Formula One after the 1992 season, citing costs in light of the takeover threat from Mitsubishi as well as the desire to create a more environmentally-friendly company image.\n\nLater, 1995 gave rise to the Honda Aircraft Company with the goal of producing jet aircraft under Honda's name.\n\nOn 23 February 2015, Honda announced that CEO and President Takanobu Ito would step down and be replaced by Takahiro Hachigo by June; additional retirements by senior managers and directors were expected.\n\nHonda is headquartered in Minato, Tokyo, Japan. Their shares trade on the Tokyo Stock Exchange and the New York Stock Exchange, as well as exchanges in Osaka, Nagoya, Sapporo, Kyoto, Fukuoka, London, Paris and Switzerland.\n\nThe company has assembly plants around the globe. These plants are located in China, the United States, Pakistan, Canada, England, Japan, Belgium, Brazil, México, New Zealand, Malaysia, Indonesia, India, Philippines, Thailand, Vietnam, Turkey, Taiwan, Perú and Argentina. As of July 2010, 89 percent of Honda and Acura vehicles sold in the United States were built in North American plants, up from 82.2 percent a year earlier. This shields profits from the yen's advance to a 15-year high against the dollar.\n\nHonda's Net Sales and Other Operating Revenue by Geographical Regions in 2007\n\nAmerican Honda Motor Company is based in Torrance, California. Honda Racing Corporation (HRC) is Honda's motorcycle racing division. Honda Canada Inc. is headquartered in Markham, Ontario, their manufacturing division, Honda of Canada Manufacturing, is based in Alliston, Ontario. Honda has also created joint ventures around the world, such as Honda Siel Cars and Hero Honda Motorcycles in India, Guangzhou Honda and Dongfeng Honda in China, Boon Siew Honda in Malaysia and Honda Atlas in Pakistan.\n\nFollowing the Japanese earthquake and tsunami in March 2011 Honda announced plans to halve production at its UK plants. The decision was made to put staff at the Swindon plant on a 2-day week until the end of May as the manufacturer struggled to source supplies from Japan. It's thought around 22,500 cars were produced during this period.\n\nHonda's global lineup consists of the Fit, Civic, Accord, Insight, CR-V, CR-Z, Legend and two versions of the Odyssey, one for North America, and a smaller vehicle sold internationally. An early proponent of developing vehicles to cater to different needs and markets worldwide, Honda's lineup varies by country and may have vehicles exclusive to that region. A few examples are the latest Honda Odyssey minivan and the Ridgeline, Honda's first light-duty uni-body pickup truck. Both were designed and engineered primarily in North America and are produced there. Other example of exclusive models includes the Honda Civic five-door hatchback sold in Europe.\n\nHonda's automotive manufacturing ambitions can be traced back to 1963, with the Honda T360, a kei car truck built for the Japanese market. This was followed by the two-door roadster, the Honda S500 also introduced in 1963. In 1965, Honda built a two-door commercial delivery van, called the Honda L700. Honda's first four-door sedan was not the Accord, but the air-cooled, four-cylinder, gasoline-powered Honda 1300 in 1969. The Civic was a hatchback that gained wide popularity internationally, but it wasn't the first two-door hatchback built. That was the Honda N360, another \"Kei car\" that was adapted for international sale as the N600. The Civic, which appeared in 1972 and replaced the N600 also had a smaller sibling that replaced the air-cooled N360, called the Honda Life that was water-cooled.\n\nThe Honda Life represented Honda's efforts in competing in the \"kei\" car segment, offering sedan, delivery van and small pick-up platforms on a shared chassis. The Life StepVan had a novel approach that, while not initially a commercial success, appears to be an influence in vehicles with the front passengers sitting behind the engine, a large cargo area with a flat roof and a liftgate installed in back, and utilizing a transversely installed engine with a front-wheel-drive powertrain.\n\nAs Honda entered into automobile manufacturing in the late 1960s, where Japanese manufacturers such as Toyota and Nissan had been making cars since before WWII, it appears that Honda instilled a sense of doing things a little differently than its Japanese competitors. Its mainstay products, like the Accord and Civic (with the exception of its USA-market 1993–97 Passport which was part of a vehicle exchange program with Isuzu (part of the Subaru-Isuzu joint venture)), have always employed front-wheel-drive powertrain implementation, which is currently a long held Honda tradition. Honda also installed new technologies into their products, first as optional equipment, then later standard, like anti lock brakes, speed sensitive power steering, and multi-port fuel injection in the early 1980s. This desire to be the first to try new approaches is evident with the creation of the first Japanese luxury chain Acura, and was also evident with the all aluminum, mid-engined sports car, the Honda NSX, which also introduced variable valve timing technology, Honda calls VTEC.\n\nThe Civic is a line of compact cars developed and manufactured by Honda. In North America, the Civic is the second-longest continuously running nameplate from a Japanese manufacturer; only its perennial rival, the Toyota Corolla, introduced in 1968, has been in production longer. The Civic, along with the Accord and Prelude, comprised Honda's vehicles sold in North America until the 1990s, when the model lineup was expanded. Having gone through several generational changes, the Civic has become larger and more upmarket, and it currently slots between the Fit and Accord.\n\nHonda produces Civic hybrid, a hybrid electric vehicle that competes with the Toyota Prius, and also produces the Insight and CR-Z.\n\nIn 2008, Honda increased global production to meet demand for small cars and hybrids in the U.S. and emerging markets. The company shuffled U.S. production to keep factories busy and boost car output, while building fewer minivans and sport utility vehicles as light truck sales fell.\n\nIts first entrance into the pickup segment, the light duty Ridgeline, won Truck of the Year from \"Motor Trend\" magazine in 2006. Also in 2006, the redesigned Civic won Car of the Year from the magazine, giving Honda a rare double win of Motor Trend honors. Honda's 9th generation Civic also won the Car of the Year award based on a public survey held by PakWheels\n\nIt is reported that Honda plans to increase hybrid sales in Japan to more than 20% of its total sales in fiscal year 2011, from 14.8% in previous year.\n\nFive of United States Environmental Protection Agency's top ten most fuel-efficient cars from 1984 to 2010 comes from Honda, more than any other automakers. The five models are: 2000–2006 Honda Insight ( combined), 1986–1987 Honda Civic Coupe HF ( combined), 1994–1995 Honda Civic hatchback VX ( combined), 2006– Honda Civic Hybrid ( combined), and 2010– Honda Insight ( combined). The ACEEE has also rated the Civic GX as the greenest car in America for seven consecutive years.\n\nHonda is the largest motorcycle manufacturer in Japan and has been since it started production in 1955.\nAt its peak in 1982, Honda manufactured almost three million motorcycles annually. By 2006 this figure had reduced to around 550,000 but was still higher than its three domestic competitors.\n\nDuring the 1960s, when it was a small manufacturer, Honda broke out of the Japanese motorcycle market and began exporting to the U.S. Working with the advertising agency Grey Advertising, Honda created an innovative marketing campaign, using the slogan \"You meet the nicest people on a Honda.\" In contrast to the prevailing negative stereotypes of motorcyclists in America as tough, antisocial rebels, this campaign suggested that Honda motorcycles were made for the everyman. The campaign was hugely successful; the ads ran for three years, and by the end of 1963 alone, Honda had sold 90,000 motorcycles.\n\nTaking Honda's story as an archetype of the smaller manufacturer entering a new market already occupied by highly dominant competitors, the story of their market entry, and their subsequent huge success in the U.S. and around the world, has been the subject of some academic controversy. Competing explanations have been advanced to explain Honda's strategy and the reasons for their success.\n\nThe first of these explanations was put forward when, in 1975, Boston Consulting Group (BCG) was commissioned by the UK government to write a report explaining why and how the British motorcycle industry had been out-competed by its Japanese competitors. The report concluded that the Japanese firms, including Honda, had sought a very high scale of production (they had made a large number of motorbikes) in order to benefit from economies of scale and learning curve effects. It blamed the decline of the British motorcycle industry on the failure of British managers to invest enough in their businesses to profit from economies of scale and scope.\nThe second explanation was offered in 1984 by Richard Pascale, who had interviewed the Honda executives responsible for the firm's entry into the U.S. market. As opposed to the tightly focused strategy of low cost and high scale that BCG accredited to Honda, Pascale found that their entry into the U.S. market was a story of \"miscalculation, serendipity, and organizational learning\" – in other words, Honda's success was due to the adaptability and hard work of its staff, rather than any long term strategy. For example, Honda's initial plan on entering the US was to compete in large motorcycles, around 300 cc. Honda's motorcycles in this class suffered performance and reliability problems when ridden the relatively long distances of the US highways. When the team found that the scooters they were using to get themselves around their U.S. base of San Francisco attracted positive interest from consumers that they fell back on selling the Super Cub instead.\n\nThe most recent school of thought on Honda's strategy was put forward by Gary Hamel and C. K. Prahalad in 1989. Creating the concept of core competencies with Honda as an example, they argued that Honda's success was due to its focus on leadership in the technology of internal combustion engines. For example, the high power-to-weight ratio engines Honda produced for its racing bikes provided technology and expertise which was transferable into mopeds. Honda's entry into the U.S. motorcycle market during the 1960s is used as a case study for teaching introductory strategy at business schools worldwide.\n\nProduction started in 1953 with H-type engine (prior to motorcycle).<br>\nHonda power equipment reached record sales in 2007 with 6.4 million units. By 2010 this figure had decreased to 4,7 million units. Cumulative production of power products has exceeded 85 million units (as of September 2008).\n\nHonda power equipment includes:\nHonda engines powered the entire 33-car starting field of the 2010 Indianapolis 500 and for the fifth consecutive race, there were no engine-related retirements during the running of the Memorial Day Classic.\n\nIn the 1980s Honda developed the GY6 engine for use in motor scooters. Although no longer manufactured by Honda it is still commonly used in many Chinese, Korean and Taiwanese light vehicles.\n\nHonda, despite being known as an engine company, has never built a V8 for passenger vehicles. In the late 1990s, the company resisted considerable pressure from its American dealers for a V8 engine (which would have seen use in top-of-the-line Honda SUVs and Acuras), with American Honda reportedly sending one dealer a shipment of V8 beverages to silence them. Honda considered starting V8 production in the mid-2000s for larger Acura sedans, a new version of the high end NSX sports car (which previously used DOHC V6 engines with VTEC to achieve its high power output) and possible future ventures into the American full-size truck and SUV segment for both the Acura and Honda brands, but this was cancelled in late 2008, with Honda citing environmental and worldwide economic conditions as reasons for the termination of this project.\n\nASIMO is the part of Honda's Research & Development robotics program. It is the eleventh in a line of successive builds starting in 1986 with Honda E0 moving through the ensuing Honda E series and the Honda P series. Weighing 54 kilograms and standing 130 centimeters tall, ASIMO resembles a small astronaut wearing a backpack, and can walk on two feet in a manner resembling human locomotion, at up to . ASIMO is the world's only humanoid robot able to ascend and descend stairs independently. However, human motions such as climbing stairs are difficult to mimic with a machine, which ASIMO has demonstrated by taking two plunges off a staircase.\n\nHonda's robot ASIMO (see below) as an R&D project brings together expertise to create a robot that walks, dances and navigates steps.\n2010 marks the year Honda has developed a machine capable of reading a user's brainwaves to move ASIMO. The system uses a helmet covered with electroencephalography and near-infrared spectroscopy sensors that monitor electrical brainwaves and cerebral blood flow—signals that alter slightly during the human thought process. The user thinks of one of a limited number of gestures it wants from the robot, which has been fitted with a Brain Machine Interface.\n\nHonda has also pioneered new technology in its HA-420 HondaJet, manufactured by its subsidiary Honda Aircraft Company, which allows new levels of reduced drag, increased aerodynamics and fuel efficiency thus reducing operating costs.\n\nHonda has also built a downhill racing bicycle known as the Honda RN-01. It is not available for sale to the public. The bike has a gearbox, which replaces the standard derailleur found on most bikes.\n\nHonda has hired several people to pilot the bike, among them Greg Minnaar. The team is known as Team G Cross Honda.\n\nHonda also builds all-terrain vehicles (ATV).\n450r\n400ex\n300ex\n250r\n\nHonda's solar cell subsidiary company Honda Soltec (Headquarters: Kikuchi-gun, Kumamoto; President and CEO: Akio Kazusa) started sales throughout Japan of thin-film solar cells for public and industrial use on 24 October 2008, after selling solar cells for residential use since October 2007. Honda announced in the end of October 2013 that Honda Soltec would cease the business operation except for support for existing customers in Spring 2014 and the subsidiary would be dissolved.\n\nHonda has been active in motorsports, like Motorcycle Grand Prix, Superbike racing and others.\n\nHonda entered Formula One as a constructor for the first time in the 1964 season at the German Grand Prix with Ronnie Bucknum at the wheel. 1965 saw the addition of Richie Ginther to the team, who scored Honda's first point at the Belgian Grand Prix, and Honda's first win at the Mexican Grand Prix. 1967 saw their next win at the Italian Grand Prix with John Surtees as their driver. In 1968, Jo Schlesser was killed in a Honda RA302 at the French Grand Prix. This racing tragedy, coupled with their commercial difficulties selling automobiles in the United States, prompted Honda to withdraw from all international motorsport that year.\n\nAfter a learning year in 1965, Honda-powered Brabhams dominated the 1966 French Formula Two championship in the hands of Jack Brabham and Denny Hulme. As there was no European Championship that season, this was the top F2 championship that year. In the early 1980s Honda returned to F2, supplying engines to Ron Tauranac's Ralt team. Tauranac had designed the Brabham cars for their earlier involvement. They were again extremely successful. In a related exercise, John Judd's Engine Developments company produced a turbo \"Brabham-Honda\" engine for use in IndyCar racing. It won only one race, in 1988 for Bobby Rahal at Pocono.\n\nHonda returned to Formula One in 1983, initially with another Formula Two partner, the Spirit team, before switching abruptly to Williams in 1984. In the late 1980s and early 1990s, Honda powered cars won six consecutive Formula One Constructors Championships. WilliamsF1 won the crown in 1986 and 1987. Honda switched allegiance again in 1988. New partners Team McLaren won the title in 1988, 1989, 1990 and 1991. Honda withdrew from Formula One at the end of 1992, although the related Mugen-Honda company maintained a presence up to the end of 1999, winning four races with Ligier and Jordan Grand Prix.\n\nHonda debuted in the CART IndyCar World Series as a works supplier in 1994. The engines were far from competitive at first, but after development, the company powered six consecutive drivers championships. In 2003, Honda transferred its effort to the rival IRL IndyCar Series with Ilmor as joint development until 2006. In 2004, Honda-powered cars overwhelmingly dominated the IndyCar Series, winning 14 of 16 IndyCar races, including the Indianapolis 500, and claimed the IndyCar Series Manufacturers' Championship, Drivers' Championship and Rookie of the Year titles. From 2006 to 2011, Honda was the lone engine supplier for the IndyCar Series, including the Indianapolis 500. In the 2006 Indianapolis 500, for the first time in Indianapolis 500 history, the race was run without a single engine problem. Since 2012, HPD has constructed turbocharged V-6 engines for its IndyCar effort.\n\nDuring 1998, Honda considered returning to Formula One with their own team. The project was aborted after the death of its technical director, Harvey Postlethwaite. Honda instead came back as an official engine supplier to British American Racing (BAR) and Jordan Grand Prix. Honda bought a stake in the BAR team in 2004 before buying the team outright at the end of 2005, becoming a constructor for the first time since the 1960s. Honda won the 2006 Hungarian Grand Prix with driver Jenson Button.\n\nIt was announced on 5 December 2008, that Honda would be exiting Formula One with immediate effect due to the 2008 global economic crisis. The team was sold to former team principal Ross Brawn, renamed Brawn GP and subsequently Mercedes.\n\nHonda became an official works team in the British Touring Car Championship in 2010.\n\nHonda made an official announcement on 16 May 2013 that it will re-enter Formula One racing in 2015 as an engine supplier to the McLaren team.\n\nHonda Racing Corporation (HRC) was formed in 1982. The company combines participation in motorcycle races throughout the world with the development of high potential racing machines. Its racing activities are an important source for the creation of leading edge technologies used in the development of Honda motorcycles. HRC also contributes to the advancement of motorcycle sports through a range of activities that include sales of production racing motorcycles, support for satellite teams, and rider education programs.\n\nSoichiro Honda, being a race driver himself, could not stay out of international motorsport. In 1959, Honda entered five motorcycles into the Isle of Man TT race, the most prestigious motorcycle race in the world. While always having powerful engines, it took until 1961 for Honda to tune their chassis well enough to allow Mike Hailwood to claim their first Grand Prix victories in the 125 and 250 cc classes. Hailwood would later pick up their first Senior TT wins in 1966 and 1967. Honda's race bikes were known for their \"sleek & stylish design\" and exotic engine configurations, such as the 5-cylinder, 22,000 rpm, 125 cc bike and their 6-cylinder 250 cc and 297 cc bikes.\n\nIn 1979, Honda returned to Grand Prix motorcycle racing with the monocoque-framed, four-stroke NR500. The FIM rules limited engines to four cylinders, so the NR500 had non-circular, 'race-track', cylinders, each with 8 valves and two connecting rods, in order to provide sufficient valve area to compete with the dominant two-stroke racers. Unfortunately, it seemed Honda tried to accomplish too much at one time and the experiment failed. For the 1982 season, Honda debuted their first two-stroke race bike, the NS500 and in , Honda won their first 500 cc Grand Prix World Championship with Freddie Spencer. Since then, Honda has become a dominant marque in motorcycle Grand Prix racing, winning a plethora of top level titles with riders such as Mick Doohan and Valentino Rossi. Honda also head the number of wins at the Isle of Man TT having notched up 227 victories in the solo classes and Sidecar TT, including Ian Hutchinson's clean sweep at the 2010 races. The outright lap record on the Snaefell Mountain Course is also held by Honda, set at the 2015 TT by John McGuinness at an average speed of on a Honda CBR1000RR.\n\nIn the Motocross World Championship, Honda has claimed six world championships. In the World Enduro Championship, Honda has captured eight titles, most recently with Stefan Merriman in 2003 and with Mika Ahola from 2007 to 2010. In motorcycle trials, Honda has claimed three world championships with Belgian rider Eddy Lejeune.\n\nThe Honda Civic GX was for a long time the only purpose-built natural gas vehicle (NGV) commercially available in some parts of the U.S. The Honda Civic GX first appeared in 1998 as a factory-modified Civic LX that had been designed to run exclusively on compressed natural gas. The car looks and drives just like a contemporary Honda Civic LX, but does not run on gasoline. In 2001, the Civic GX was rated the cleanest-burning internal combustion engine in the world by the U.S. Environmental Protection Agency (EPA).\n\nFirst leased to the City of Los Angeles, in 2005, Honda started offering the GX directly to the public through factory trained dealers certified to service the GX. Before that, only fleets were eligible to purchase a new Civic GX. In 2006, the Civic GX was released in New York, making it the second state where the consumer is able to buy the car.\n\nIn June 2015, Honda announced its decision to phase out the commercialization of natural-gas powered vehicles to focus on the development of a new generation of electrified vehicles such as hybrids, plug-in electric cars and hydrogen-powered fuel cell vehicles. Since 2008, Honda has sold about 16,000 natural-gas vehicles, mainly to taxi and commercial fleets.\n\nHonda's Brazilian subsidiary launched flexible-fuel versions for the Honda Civic and Honda Fit in late 2006. As other Brazilian flex-fuel vehicles, these models run on any blend of hydrous ethanol (E100) and E20-E25 gasoline. Initially, and in order to test the market preferences, the carmaker decided to produce a limited share of the vehicles with flex-fuel engines, 33 percent of the Civic production and 28 percent of the Fit models. Also, the sale price for the flex-fuel version was higher than the respective gasoline versions, around US$1,000 premium for the Civic, and US$650 for the Fit, despite the fact that all other flex-fuel vehicles sold in Brazil had the same tag price as their gasoline versions. In July 2009, Honda launched in the Brazilian market its third flexible-fuel car, the Honda City.\n\nDuring the last two months of 2006, both flex-fuel models sold 2,427 cars against 8,546 gasoline-powered automobiles, jumping to 41,990 flex-fuel cars in 2007, and reaching 93,361 in 2008. Due to the success of the flex versions, by early 2009 a hundred percent of Honda's automobile production for the Brazilian market is now flexible-fuel, and only a small percentage of gasoline version is produced in Brazil for exports.\n\nIn March 2009, Honda launched in the Brazilian market the first flex-fuel motorcycle in the world. Produced by its Brazilian subsidiary Moto Honda da Amazônia, the CG 150 Titan Mix is sold for around US$2,700.\n\nIn late 1999, Honda launched the first commercial hybrid electric car sold in the U.S. market, the Honda Insight, just one month before the introduction of the Toyota Prius, and initially sold for US$20,000. The first-generation Insight was produced from 2000 to 2006 and had a fuel economy of for the EPA's highway rating, the most fuel-efficient mass-produced car at the time. Total global sales for the Insight amounted to only around 18,000 vehicles. Cumulative global sales reached 100,000 hybrids in 2005 and 200,000 in 2007.\n\nHonda introduced the second-generation Insight in Japan in February 2009, and released it in other markets through 2009 and in the U.S. market in April 2009. At $19,800 as a five-door hatchback it will be the least expensive hybrid available in the U.S.\nSince 2002, Honda has also been selling the Honda Civic Hybrid (2003 model) in the U.S. market. It was followed by the Honda Accord Hybrid, offered in model years 2005 through 2007. Sales of the Honda CR-Z began in Japan in February 2010, becoming Honda's third hybrid electric car in the market. , Honda was producing around 200,000 hybrids a year in Japan.\n\nSales of the Fit Hybrid began in Japan in October 2010, at the time, the lowest price for a gasoline-hybrid electric vehicle sold in the country. The European version, called Honda Jazz Hybrid, was released in early 2011. During 2011 Honda launched three hybrid models available only in Japan, the Fit Shuttle Hybrid, Freed Hybrid and Freed Spike Hybrid.\n\nHonda's cumulative global hybrid sales passed the 1 million unit milestone at the end of September 2012, 12 years and 11 months after sales of the first generation Insight began in Japan November 1999. A total of 187,851 hybrids were sold worldwide in 2013, and 158,696 hybrids during the first six months of 2014. , Honda has sold more than 1.35 million hybrids worldwide.\n\nIn Takanezawa, Japan, on 16 June 2008, Honda Motors produced the first assembly-line FCX Clarity, a hybrid hydrogen fuel cell vehicle. More efficient than a gas-electric hybrid vehicle, the FCX Clarity combines hydrogen and oxygen from ordinary air to generate electricity for an electric motor. In July 2014 Honda announced the end of production of the Honda FCX Clarity for the 2015 model.\n\nThe vehicle itself does not emit any pollutants and its only by products are heat and water. The FCX Clarity also has an advantage over gas-electric hybrids in that it does not use an internal combustion engine to propel itself. Like a gas-electric hybrid, it uses a lithium ion battery to assist the fuel cell during acceleration and capture energy through regenerative braking, thus improving fuel efficiency. The lack of hydrogen filling stations throughout developed countries will keep production volumes low. Honda will release the vehicle in groups of 150. California is the only U.S. market with infrastructure for fueling such a vehicle, though the number of stations is still limited. Building more stations is expensive, as the California Air Resources Board (CARB) granted $6.8 million for four H2 fueling stations, costing $1.7 million USD each.\n\nHonda views hydrogen fuel cell vehicles as the long term replacement of piston cars, not battery cars.\n\nThe all-electric Honda EV Plus was introduced in 1997 as a result of CARB's zero-emissions vehicle mandate and was available only for leasing in California. The EV plus was the first battery electric vehicle from a major automaker with non-lead–acid batteries The EV Plus had an all-electric range of . Around 276 units were sold in the U.S. and production ended in 1999.\n\nThe all-electric Honda Fit EV was introduced in 2012 and has a range of . The all-electric car was launched in the U.S. to retail customers in July 2012 with initial availability limited to California and Oregon. Production is limited to only 1,100 units over the first three years. A total of 1,007 units have been leased in the U.S. through September 2014. The Fit EV was released in Japan through leasing to local government and corporate customers in August 2012. Availability in the Japanese market is limited to 200 units during its first two years. In July 2014 Honda announced the end of production of the Fit EV for the 2015 model.\n\nThe Honda Accord Plug-in Hybrid was introduced in 2013 and has an all-electric range of Sales began in the U.S. in January 2013 and the plug-in hybrid is available only in California and New York. A total of 835 units have been sold in the U.S. through September 2014. The Accord PHEV was introduced in Japan in June 2013 and is available only for leasing, primarily to corporations and government agencies.\n\nStarting in 1978, Honda in Japan decided to diversify its sales distribution channels, and created Honda Verno, which sold established products with a higher content of standard equipment and a more sporting nature. The establishment of \"Honda Verno\" coincided with its new sports compact, called the Honda Prelude. Later, the Honda Vigor, the Honda Ballade, and the Honda Quint were added to \"Honda Verno\" stores. This approach was implemented due to efforts in place by rival Japanese automakers Toyota and Nissan.\n\nAs sales progressed, Honda created two more sales channels, called Honda Clio in 1984, and Honda Primo in 1985. The \"Honda Clio\" chain sold products that were traditionally associated with Honda dealerships before 1978, like the Honda Accord, and \"Honda Primo\" sold the Honda Civic, kei cars, such as the Honda Today, superminis like the Honda Capa, along with other Honda products, such as farm equipment, lawn mowers, portable generators, marine equipment, plus motorcycles and scooters like the Honda Super Cub. A styling tradition was established when \"Honda Primo\" and \"Clio\" began operations, in that all \"Verno\" products had the rear license plate installed in the rear bumper, while \"Primo\" and \"Clio\" products had the rear license plate installed on the trunk lid or rear door for minivans.\n\nAs time progressed and sales began to diminish partly due to the collapse of the Japanese \"bubble economy\", \"supermini\" and \"kei\" vehicles that were specific to \"Honda Primo\" were \"badge engineered\" and sold at the other two sales channels, thereby providing smaller vehicles that sold better at both \"Honda Verno\" and \"Honda Clio\" locations. As of March 2006, the three sales chains were discontinued, with the establishment of \"Honda Cars\" dealerships. While the network was disbanded, some Japanese Honda dealerships still use the network names, offering all Japanese market Honda cars at all locations.\n\nHonda sells genuine accessories through a separate retail chain called \"\" for both their motorcycle, scooter and automobile products. In cooperation with corporate \"keiretsu\" partner Pioneer, Honda sells an aftermarket line of audio and in-car navigation equipment that can be installed in any vehicle under the brand name , which is available at Honda Access locations as well as Japanese auto parts retailers, such as Autobacs. Buyers of used vehicles are directed to a specific Honda retail chain that sells only used vehicles called \".\"\n\nIn the spring of 2012, Honda in Japan introduced \"Honda Cars Small Store\" (Japanese) which is devoted to compact cars like the Honda Fit, and \"kei\" vehicles like the Honda N-One and Honda S660 roadster.\n\nPrelude, Integra, CR-X, Vigor, Saber, Ballade, Quint, Crossroad, Element, NSX, HR-V, Mobilio Spike, S2000, CR-V, That's, MDX, Rafaga, Capa, and the Torneo\n\nAccord, Legend, Inspire, Avancier, S-MX, Lagreat, Stepwgn, Elysion, Stream, Odyssey (int'l), Domani, Concerto, Accord Tourer, Logo, Fit, Insight, That's, Mobilio, and the City\n\nCivic, Life, Acty, Vamos, Hobio, Ascot, Ascot Innova, Torneo, Civic Ferio, Freed, Mobilio, Orthia, Capa, Today, Z, and the Beat\n\nIn 2003, Honda released its \"Cog\" advertisement in the UK and on the Internet. To make the ad, the engineers at Honda constructed a Rube Goldberg Machine made entirely out of car parts from a Honda Accord Touring. To the chagrin of the engineers at Honda, all the parts were taken from two of only six hand-assembled pre-production models of the Accord. The advertisement depicted a single cog which sets off a chain of events that ends with the Honda Accord moving and Garrison Keillor speaking the tagline, \"Isn't it nice when things just... work?\" It took 606 takes to get it perfect.\n\nIn 2004, they produced the \"Grrr\" advert, usually immediately followed by a shortened version of the 2005 \"Impossible Dream\" advert.\n\nIn December 2005, Honda released \"The Impossible Dream\" a two-minute panoramic advertisement filmed in New Zealand, Japan and Argentina which illustrates the founder's dream to build performance vehicles. While singing the song \"Impossible Dream\", a man reaches for his racing helmet, leaves his trailer on a minibike, then rides a succession of vintage Honda vehicles: a motorcycle, then a car, then a powerboat, then goes over a waterfall only to reappear piloting a hot air balloon, with Garrison Keillor saying \"I couldn't have put it better myself\" as the song ends. The song is from the 1960s musical \"Man Of La Mancha\", sung by Andy Williams.\n\nIn 2006, Honda released its \"Choir\" advertisement, for the UK and the internet. This had a 60-person choir who sang the car noises as film of the Honda Civic are shown.\n\nIn the mid to late 2000s in the United States, during model close-out sales for the current year before the start of the new model year, Honda's advertising has had an animated character known simply as Mr. Opportunity, voiced by Rob Paulsen. The casual looking man talked about various deals offered by Honda and ended with the phrase \"I'm Mr. Opportunity, and I'm knockin'\", followed by him \"knocking\" on the television screen or \"thumping\" the speaker at the end of radio ads. In addition, commercials for Honda's international hatchback, the Jazz, are parodies of well-known pop culture images such as Tetris and Thomas The Tank Engine.\n\nIn late 2006, Honda released an ad with ASIMO exploring a museum, looking at the exhibits with almost childlike wonderment (spreading out its arms in the aerospace exhibit, waving hello to an astronaut suit that resembles him, etc.), while Garrison Keillor ruminates on progress. It concludes with the tagline: \"More forwards please\".\n\nHonda also sponsored ITV's coverage of Formula One in the UK for 2007. However they had announced that they would not continue in 2008 due to the sponsorship price requested by ITV being too high.\n\nIn May 2007, focuses on their strengths in racing and the use of the Red H badge – a symbol of what is termed as \"Hondamentalism\". The campaign highlights the lengths that Honda engineers go to in order to get the most out of an engine, whether it is for bikes, cars, powerboats – even lawnmowers. Honda released its Hondamentalism campaign. In the TV spot, Garrison Keillor says, \"An engineer once said to build something great is like swimming in honey\", while Honda engineers in white suits walk and run towards a great light, battling strong winds and flying debris, holding on to anything that will keep them from being blown away. Finally one of the engineers walks towards a red light, his hand outstretched. A web address is shown for the Hondamentalism website. The digital campaign aims to show how visitors to the site share many of the Hondamentalist characteristics.\n\nAt the beginning of 2008, Honda released – the \"Problem Playground\". The advert outlines Honda's environmental responsibility, demonstrating a hybrid engine, more efficient solar panels and the FCX Clarity, a hydrogen powered car. The 90 second advert has large scale puzzles, involving Rubik's Cubes, large shapes and a 3-dimensional puzzle.\n\nOn 29 May 2008, Honda, in partnership with Channel 4, broadcast a live advertisement. It showed skydivers jumping from an aeroplane over Spain and forming the letters H, O, N, D and A in mid-air. This live advertisement is generally agreed to be the first of its kind on British television. The advert lasted three minutes.\n\nIn 2009, American Honda released the \"Dream the Impossible\" documentary series, a collection of 5–8 minute web vignettes that focus on the core philosophies of Honda. Current short films include \"Failure: The Secret to Success\", \"Kick Out the Ladder\" and \"Mobility 2088\". They have Honda employees as well as Danica Patrick, Christopher Guest, Ben Bova, Chee Pearlman, Joe Johnston and Orson Scott Card. The film series plays at dreams.honda.com.\n\nIn Australia, Honda advertised heavily during most motor racing telecasts, and was the official sponsor of the 2006 FIA Formula 1 telecast on broadcaster channel \"Ten\". In fact, it was the only manufacturer involved in the 2006 Indy Racing League season. In a series of adverts promoting the history of Honda's racing heritage, Honda claimed it \"built\" cars that won 72 Formula 1 Grand Prix. Skeptics have accused Honda of interpreting its racing history rather liberally, saying that virtually all of the 72 victories were achieved by Honda \"powered\" (engined) machines, whereas the cars themselves were designed and built by Lotus F1, Williams F1, and McLaren F1 teams, respectively. However, former and current staff of the McLaren F1 team have reiterated that Honda contributed more than just engines and provided various chassis, tooling, and aerodynamic parts as well as funding.\n\nThe late F1 driver Ayrton Senna stated that Honda probably played the most significant role in his three world championships. He had immense respect for founder, Soichiro Honda, and had a good relationship with Nobuhiko Kawamoto, the chairman of Honda at that time. Senna once called Honda \"the greatest company in the world\".\n\nAs part of its marketing campaign, Honda is an official partner and sponsor of the National Hockey League, the Anaheim Ducks of the NHL, and the arena named after it: Honda Center. Honda also sponsors The Honda Classic golf tournament and is a sponsor of Major League Soccer. The \"Honda Player of the Year\" award is presented in United States soccer. The \"Honda Sports Award\" is given to the best female athlete in each of twelve college sports in the United States. One of the twelve Honda Sports Award winners is chosen to receive the Honda-Broderick Cup, as \"Collegiate Woman Athlete of the Year.\"\n\nHonda will be sponsoring La Liga club Valencia CF starting from 2014–15 season. Valencia CF will carry \"Honda Cars Valencia\" insignia on their football kits. \n\nHonda has been a presenting sponsor of the Los Angeles Marathon since 2010 in a three-year sponsorship deal, with winners of the LA Marathon receiving a free Honda Accord. Since 1989, the Honda Campus All-Star Challenge has been a quizbowl tournament for Historically black colleges and universities.\n\n\n\n", "id": "13729", "title": "Honda"}
{"url": "https://en.wikipedia.org/wiki?curid=13730", "text": "Handball\n\nHandball (also known as team handball or Olympic handball) is a team sport in which two teams of seven players each (six outfield players and a goalkeeper) pass a ball using their hands with the aim of throwing it into the goal of the other team. A standard match consists of two periods of 30 minutes, and the team that scores more goals wins.\n\nModern handball is played on a court , with a goal in the middle of each end. The goals are surrounded by a zone where only the defending goalkeeper is allowed; goals must be scored by throwing the ball from outside the zone or while \"jumping\" into it. The sport is usually played indoors, but outdoor variants exist in the forms of field handball and Czech handball (which were more common in the past) and beach handball. The game is fast and high-scoring: professional teams now typically score between 20 and 35 goals each, though lower scores were not uncommon until a few decades ago. Body contact is permitted by the defenders trying to stop the attackers from approaching the goal.\n\nThe game was codified at the end of the 19th century in northern Europe and Germany. The modern set of rules was published in 1917 in Germany, and had several revisions since. The first international games were played under these rules for men in 1925 and for women in 1930. Men's handball was first played at the 1936 Summer Olympics in Berlin as outdoors, and the next time at the 1972 Summer Olympics in Munich as indoors, and has been an Olympic sport since. Women's team handball was added at the 1976 Summer Olympics.\n\nThe International Handball Federation was formed in 1946 and, , has 197 member federations. The sport is most popular in the countries of continental Europe, which have won all medals but one in the men's world championships since 1938, and all medals in the women's world championships until 2013, when Brazil broke the series. The game also enjoys popularity in the Far East, North Africa and parts of South America.\n\nThere is evidence of ancient Roman women playing a version of handball called \"expulsim ludere\". There are records of handball-like games in medieval France, and among the Inuit in Greenland, in the Middle Ages. By the 19th century, there existed similar games of \"håndbold\" from Denmark, \"házená\" in the Czech Republic, \"hádzaná\" in Slovakia, \"handbol\" in Ukraine, and \"torball\" in Germany.\n\nThe team handball game of today was codified at the end of the 19th century in northern Europe—primarily in Denmark, Germany, Norway and Sweden. The first written set of team handball rules was published in 1906 by the Danish gym teacher, lieutenant and Olympic medalist Holger Nielsen from Ordrup grammar school, north of Copenhagen. The modern set of rules was published on 29 October 1917 by Max Heiser, Karl Schelenz, and Erich Konigh from Germany. After 1919 these rules were improved by Karl Schelenz. The first international games were played under these rules, between Germany and Belgium by men in 1925 and between Germany and Austria by women in 1930.\n\nIn 1926, the Congress of the International Amateur Athletics Federation nominated a committee to draw up international rules for field handball. The International Amateur Handball Federation was formed in 1928 and later the International Handball Federation was formed in 1946.\n\nMen's field handball was played at the 1936 Summer Olympics in Berlin. During the next several decades, indoor handball flourished and evolved in the Scandinavian countries. The sport re-emerged onto the world stage as team handball for the 1972 Summer Olympics in Munich. Women's team handball was added at the 1976 Summer Olympics in Montreal. Due to its popularity in the region, the Eastern European countries that refined the event became the dominant force in the sport when it was reintroduced.\n\nThe International Handball Federation organised the men's world championship in 1938 and every four (sometimes three) years from World War II to 1995. Since the 1995 world championship in Iceland, the competition has been held every two years. The women's world championship has been held since 1957. The IHF also organizes women's and men's junior world championships. By July 2009, the IHF listed 166 member federations - approximately 795,000 teams and 19 million players.\n\nThe rules are laid out in the IHF's set of rules.\n\nTwo teams of seven players (six field players plus one goalkeeper) take the field and attempt to score points by putting the game ball into the opposing team's goal. In handling the ball, players are subject to the following restrictions:\n\n\nNotable scoring opportunities can occur when attacking players jump into the goal area. For example, an attacking player may catch a pass while launching inside the goal area, and then shoot or pass before touching the floor. \"Doubling\" occurs when a diving attacking player passes to another diving team-mate.\n\nHandball is played on a court , with a goal in the centre of each end. The goals are surrounded by a near-semicircular area, called the zone or the crease, defined by a line six meters from the goal. A dashed near-semicircular line nine metres from the goal marks the free-throw line. Each line on the court is part of the area it encompasses. This implies that the middle line belongs to both halves at the same time.\n\nEach goal has a circle clearance area of three meters in width and two meters in height. It must be securely bolted either to the floor or the wall behind.\n\nThe goal posts and the crossbar must be made out of the same material (e.g., wood or aluminium) and feature a quadratic cross section with sides of . The three sides of the beams visible from the playing field must be painted alternatingly in two contrasting colors which both have to contrast against the background. The colors on both goals must be the same.\n\nEach goal must feature a net. This must be fastened in such a way that a ball thrown into the goal does not leave or pass the goal under normal circumstances. If necessary, a second net may be clasped to the back of the net on the inside.\n\nThe goals are surrounded by the crease. This area is delineated by two quarter circles with a radius of six metres around the far corners of each goal post and a connecting line parallel to the goal line. Only the defending goalkeeper is allowed inside this zone. However, the court players may catch and touch the ball in the air within it as long as the player starts his jump outside the zone and releases the ball before he lands (landing inside the perimeter is allowed in this case as long as the ball has been released).\n\nIf a player without the ball contacts the ground inside the goal perimeter, or the line surrounding the perimeter, he must take the most direct path out of it. However, should a player cross the zone in an attempt to gain an advantage (e.g., better position) their team cedes the ball. Similarly, violation of the zone by a defending player is penalized only if they do so in order to gain an advantage in defending.\n\nOutside of one long edge of the playing field to both sides of the middle line are the substitution areas for each team. The areas usually contain the benches as seating opportunities. Team officials, substitutes, and suspended players must wait within this area. The area always lies to the same side as the team's own goal. During half-time, substitution areas are swapped. Any player entering or leaving the play must cross the substitution line which is part of the side line and extends from the middle line to the team's side.\n\n A standard match for all teams of at least age 16 has two 30-minute halves with a 10- to 15-minute halftime break. At half-time, teams switch sides of the court as well as benches. For youths the length of the halves is reduced—25 minutes at ages 12 to 15, and 20 minutes at ages 8 to 11; though national federations of some countries may differ in their implementation from the official guidelines.\n\nIf a decision must be reached in a particular match (e.g., in a tournament) and it ends in a draw after regular time, there are at maximum two overtimes, each consisting of two straight 5-minute periods with a one-minute break in between. Should these not decide the game either, the winning team is determined in a penalty shootout (best-of-five rounds; if still tied, extra rounds afterwards until won by one team).\n\nThe referees may call \"timeout\" according to their sole discretion; typical reasons are injuries, suspensions, or court cleaning. Penalty throws should trigger a timeout only for lengthy delays, such as a change of the goalkeeper.\n\nSince 2012, teams can call 3 \"team timeouts\" per game (up to two per half), which last one minute each. This right may only be invoked by team in ball possession. Team representatives must show a green card marked with a black \"T\" on the timekeeper's desk. The timekeeper then immediately interrupts the game by sounding an acoustic signal and stops the time. Before that, it was one per half. For purpose of calling timeouts, overtime and shootouts are extensions of the second half.\n\nA handball match is led by two equal referees. Some national bodies allow games with only a single referee in special cases like illness on short notice. Should the referees disagree on any occasion, a decision is made on mutual agreement during a short timeout; or, in case of punishments, the more severe of the two comes into effect. The referees are obliged to make their decisions \"on the basis of their observations of facts\". Their judgements are final and can be appealed against only if not in compliance with the rules.\n\nThe referees position themselves in such a way that the team players are confined between them. They stand diagonally aligned so that each can observe one side line. Depending on their positions, one is called \"field referee\" and the other \"goal referee\". These positions automatically switch on ball turnover. They physically exchange their positions approximately every 10 minutes (long exchange), and change sides every five minutes (short exchange).\n\nThe IHF defines 18 hand signals for quick visual communication with players and officials. The signal for warning or disqualification is accompanied by a yellow or red card, respectively. The referees also use whistle blows to indicate infractions or to restart the play.\n\nThe referees are supported by a \"scorekeeper\" and a \"timekeeper\" who attend to formal things such as keeping track of goals and suspensions, or starting and stopping the clock, respectively. They also keep an eye on the benches and notify the referees on substitution errors. Their desk is located between the two substitution areas.\n\nEach team consists of seven players on court and seven substitute players on the bench. One player on the court must be the designated goalkeeper, differing in his clothing from the rest of the field players. Substitution of players can be done in any number and at any time during game play. An exchange takes place over the substitution line. A prior notification of the referees is not necessary.\n\nSome national bodies, such as the Deutsche Handball Bund (DHB, \"German Handball Federation\"), allow substitution in junior teams only when in ball possession or during timeouts. This restriction is intended to prevent early specialization of players to offence or defence.\n\nField players are allowed to touch the ball with any part of their bodies above and including the knee. As in several other team sports, a distinction is made between catching and dribbling. A player who is in possession of the ball may stand stationary for only three seconds, and may take only three steps. They must then either shoot, pass, or dribble the ball. Taking more than three steps at any time is considered travelling, and results in a turnover. A player may dribble as many times as they want (though, since passing is faster, it is the preferred method of attack), as long as during each dribble the hand contacts only the top of the ball. Therefore, carrying is completely prohibited, and results in a turnover. After the dribble is picked up, the player has the right to another three seconds or three steps. The ball must then be passed or shot, as further holding or dribbling will result in a \"double dribble\" turnover and a free throw for the other team. Other offensive infractions that result in a turnover include charging and setting an illegal screen. Carrying the ball into the six-meter zone results either in ball possession by the goalkeeper (by attacker) or turnover (by defender).\n\nOnly the goalkeepers are allowed to move freely within the goal perimeter, although they may not cross the goal perimeter line while carrying or dribbling the ball. Within the zone, they are allowed to touch the ball with all parts of their bodies, including their feet, with a defensive aim (for other actions, they are subject to the same restrictions as the field players). The goalkeepers may participate in the normal play of their teammates. They may be substituted by a regular field player if their team elects to use this scheme in order to outnumber the defending players. Earlier, this field player become the designated goalkeeper on the court; and had to wear some vest or bib to be identified as such. That shirt had to be equal in colour and form to the goalkeeper's shirt, to avoid confusion. A rule change meant to make the game more offensive now allows any player to substitute with the goalkeeper. The new rule resembles the one used in ice hockey. This rule was first used in the women's world championship in December 2015 and has since been used by the men's European championship in January 2016 and by both genders in the Olympic tournament in Rio in 2016.\n\nIf either goalkeeper deflects the ball over the outer goal line, their team stays in possession of the ball, in contrast to other sports like football. The goalkeeper resumes the play with a throw from within the zone (\"goalkeeper throw\"). Passing to one's own goalkeeper results in a turnover. In a penalty shot, throwing the ball against the head of a goalkeeper who is not moving is punished by a direct disqualification (\"red card\").\n\nOutside of own D-zone, the goalkeeper is treated as a current field player, and has to follow field players' rules; holding or tackling an opponent player outside the area results in a direct disqualification. The goalkeeper may not return to the area with the ball.\n\nEach team is allowed to have a maximum of four team officials seated on the benches. An official is anybody who is neither player nor substitute. One official must be the designated representative who is usually the team manager. Since 2012, representatives can call up to 3 team timeouts (up to twice per half), and may address the scorekeeper, timekeeper, and referees (before that, it was once per half); overtime and shootouts are considered extensions of the second half. Other officials typically include physicians or managers. Neither official is allowed to enter the playing court without the permission of the referees.\n\n The ball is spherical and must be made either of leather or a synthetic material. It is not allowed to have a shiny or slippery surface. As the ball is intended to be operated by a single hand, its official sizes vary depending on age and gender of the participating teams.\nThe referees may award a special throw to a team. This usually happens after certain events such as scored goals, off-court balls, turnovers and timeouts. All of these special throws require the thrower to obtain a certain position, and pose restrictions on the positions of all other players. Sometimes the execution must wait for a whistle blow by the referee.\n\n\n\n\n\nPenalties are given to players, in progressive format, for fouls that require more punishment than just a free-throw. Actions directed mainly at the opponent and not the ball (such as reaching around, holding, pushing, tripping, and jumping into opponent) as well as contact from the side, from behind a player or impeding the opponent's counterattack are all considered illegal and are subject to penalty. Any infraction that prevents a clear scoring opportunity will result in a seven-meter penalty shot.\n\nTypically the referee will give a warning yellow card for an illegal action; but, if the contact was particularly dangerous, like striking the opponent in the head, neck or throat, the referee can forego the warning for an immediate two-minute suspension. A player can get only one warning before receiving a two-minute suspension. One player is only permitted two two-minute suspensions; after the third time, they will be shown the red card.\n\nA red card results in an ejection from the game and a two-minute penalty for the team. A player may receive a red card directly for particularly rough penalties. For instance, any contact from behind during a fast break is now being treated with a red card. A red-carded player has to leave the playing area completely. A player who is disqualified may be substituted with another player after the two-minute penalty is served. A coach or official can also be penalized progressively. Any coach or official who receives a two-minute suspension will have to pull out one of their players for two minutes; however, the player is not the one punished, and can be substituted in again, as the penalty consists of the team playing with a one player less than the opposing team.\n\nAfter referees award the ball to the opponents for whatever reason, the player currently in possession of the ball has to lay it down quickly, or else face a two-minute suspension. Also, gesticulating or verbally questioning the referee's order, as well as arguing with the officials' decisions, will normally result in a two-minute suspension. If the suspended player protests further, does not walk straight off the field to the bench, or if the referee deems the tempo deliberately slow, the player can be given an additional two-minute suspension. Illegal substitution (outside of the dedicated area, or if the replacement player enters too early) is also punishable by a two-minute suspension.\n\nPlayers are typically referred to by the position they are playing. The positions are always denoted from the view of the respective goalkeeper, so that a defender on the right opposes an attacker on the left. However, not all of the following positions may be occupied depending on the formation or potential suspensions.\n\n\nSometimes, the offense uses formations with two Pivot players.\n\nThere are a lot of variations in defensive formations. Usually, they are described as n:m formations, where n is the amount of players defending at the goal line, m the amount of players defending more offensive. Exceptions are the 3:2:1 defense and n+m formation (e.g. 5+1), where m players defend some offensive player in man coverage (instead of the usual zone coverage).\n\n\nAttacks are played with all field players on the side of the defenders. Depending on the speed of the attack, one distinguishes between three attack \"waves\" with a decreasing chance of success:\n\n\n\nThe third wave evolves into the normal offensive play when all defenders not only reach the zone, but gain their accustomed positions. Some teams then substitute specialised offence players. However, this implies that these players must play in the defence should the opposing team be able to switch quickly to offence. The latter is another benefit for fast playing teams.\n\nIf the attacking team does not make sufficient progress (eventually releasing a shot on goal), the referees can call passive play (since about 1995, the referee gives a passive warning some time before the actual call by holding one hand up in the air, signalling that the attacking team should release a shot soon), turning control over to the other team. A shot on goal or an infringement leading to a yellow card or two-minute penalty will mark the start of a new attack, causing the hand to be taken down; but a shot blocked by the defense or a normal free throw will not. If it were not for this rule, it would be easy for an attacking team to stall the game indefinitely, as it is difficult to intercept a pass without at the same time conceding dangerous openings towards the goal.\n\nThe usual formations of the defense are 6–0, when all the defense players line up between the and lines to form a wall; the 5–1, when one of the players cruises outside the perimeter, usually targeting the center forwards while the other 5 line up on the line; and the less common 4–2 when there are two such defenders out front. Very fast teams will also try a 3–3 formation which is close to a switching man-to-man style. The formations vary greatly from country to country, and reflect each country's style of play. 6–0 is sometimes known as \"flat defense\", and all other formations are usually called \"offensive defense\".\n\nHandball teams are usually organised as clubs. On a national level, the clubs are associated in federations which organize matches in leagues and tournaments.\n\nThe International Handball Federation (IHF) is the administrative and controlling body for international handball. Handball is an Olympic sport played during the Summer Olympics. \n\nThe IHF organizes world championships, held in uneven years, with separate competitions for men and women.\nThe IHF World Men's Handball Championship 2015 title holders are France.\nThe IHF 2015 Women’s World Championship title holders are Norway.\n\nThe IHF is composed of five continental federations: Asian Handball Federation, African Handball Confederation, Pan-American Team Handball Federation, European Handball Federation and Oceania Handball Federation. These federations organize continental championships held every other second year. Handball is played during the Pan American Games, All-Africa Games, and Asian Games. It is also played at the Mediterranean Games. In addition to continental competitions between national teams, the federations arrange international tournaments between club teams.\n\n\n\nThe current worldwide attendance record for seven-a-side handball was set on September 6, 2014, during a neutral venue German league game between HSV Hamburg and the Mannheim-based Rhein-Neckar Lions. The matchup drew 44,189 spectators to Commerzbank Arena in Frankfurt, exceeding the previous record of 36,651 set at Copenhagen's Parken Stadium during the 2011 Danish Cup final.\n\nHandball events have been selected as a main motif in numerous collectors' coins. One of the recent samples is the €10 Greek Handball commemorative coin, minted in 2003 to commemorate the 2004 Summer Olympics. On the coin, the modern athlete directs the ball in his hands towards his target, while in the background the ancient athlete is just about to throw a ball, in a game known as cheirosphaira, in a representation taken from a black-figure pottery vase of the Archaic period.\n\nThe most recent commemorative coin featuring handball is the British 50 pence coin, part of the series of coins commemorating the London 2012 Olympic Games. \n\nNotes\n", "id": "13730", "title": "Handball"}
{"url": "https://en.wikipedia.org/wiki?curid=13733", "text": "Hilbert's basis theorem\n\nIn mathematics, specifically commutative algebra, Hilbert's basis theorem says that a polynomial ring over a Noetherian ring is Noetherian.\n\nIf formula_1 a ring, let formula_2 denote the ring of polynomials in the indeterminate formula_3 over formula_1. Hilbert proved that if formula_1 is \"not too large\", in the sense that if formula_1 is Noetherian, the same must be true for formula_2. Formally,\n\nHilbert's Basis Theorem. If formula_1 is a Noetherian ring, then formula_2 is a Noetherian ring.\n\nCorollary. If formula_1 is a Noetherian ring, then formula_11 is a Noetherian ring.\n\nThis can be translated into algebraic geometry as follows: every algebraic set over a field can be described as the set of common roots of finitely many polynomial equations. proved the theorem (for the special case of polynomial rings over a field) in the course of his proof of finite generation of rings of invariants. \n\nHilbert produced an innovative proof by contradiction using mathematical induction; his method does not give an algorithm to produce the finitely many basis polynomials for a given ideal: it only shows that they must exist. One can determine basis polynomials using the method of Gröbner bases.\n\nRemark. We will give two proofs, in both only the \"left\" case is considered, the proof for the right case is similar.\n\nSuppose formula_14 were a non-finitely generated left-ideal. Then by recursion (using the axiom of dependent choice) there is a sequence formula_15 of polynomials such that if formula_16 is the left ideal generated by formula_17 then formula_18 in formula_19 is of minimal degree. It is clear that formula_20 is a non-decreasing sequence of naturals. Let formula_21 be the leading coefficient of formula_18 and let formula_23 be the left ideal in formula_1 generated by formula_25. Since formula_1 is Noetherian the chain of ideals formula_27 must terminate. Thus formula_28 for some integer formula_29. So in particular, \n\nNow consider \n\nwhose leading term is equal to that of formula_32; moreover, formula_33. However, formula_34, which means that formula_35 has degree less than formula_32, contradicting the minimality.\n\nLet formula_14 be a left-ideal. Let formula_23 be the set of leading coefficients of members of formula_39. This is obviously a left-ideal over formula_1, and so is finitely generated by the leading coefficients of finitely many members of formula_39; say formula_42. Let formula_43 be the maximum of the set formula_44, and let formula_45 be the set of leading coefficients of members of formula_39, whose degree is formula_47. As before, the formula_45 are left-ideals over formula_1, and so are finitely generated by the leading coefficients of finitely many members of formula_39, say \n\nwith degrees formula_47. Now let formula_53 be the left-ideal generated by \n\nWe have formula_55 and claim also formula_56. Suppose for the sake of contradiction this is not so. Then let formula_57 be of minimal degree, and denote its leading coefficient by formula_58.\n\nThus our claim holds, and formula_74 which is finitely generated.\n\nNote that the only reason we had to split into two cases was to ensure that the powers of formula_3 multiplying the factors, were non-negative in the constructions.\n\nLet formula_1 be a Noetherian commutative ring. Hilbert's basis theorem has some immediate corollaries. \n\n\nThe Mizar project has completely formalized and automatically checked a proof of Hilbert's basis theorem in the HILBASIS file.\n\n", "id": "13733", "title": "Hilbert's basis theorem"}
{"url": "https://en.wikipedia.org/wiki?curid=13734", "text": "Heterocyclic compound\n\nA heterocyclic compound or ring structure is a cyclic compound that has atoms of at least two different elements as members of its ring(s). Heterocyclic chemistry is the branch of organic chemistry dealing with the synthesis, properties, and applications of these heterocycles. \n\nExamples of heterocyclic compounds include all of the nucleic acids, the majority of drugs, most biomass (cellulose and related materials), and many natural and synthetic dyes.\n\nAlthough heterocyclic compounds may be inorganic, most contain at least one carbon. While atoms that are neither carbon nor hydrogen are normally referred to in organic chemistry as heteroatoms, this is usually in comparison to the all-carbon backbone. But this does not prevent a compound such as borazine (which has no carbon atoms) from being labelled \"heterocyclic\". IUPAC recommends the Hantzsch-Widman nomenclature for naming heterocyclic compounds.\n\nHeterocyclic compounds can be usefully classified based on their electronic structure. The saturated heterocycles behave like the acyclic derivatives. Thus, piperidine and tetrahydrofuran are conventional amines and ethers, with modified steric profiles. Therefore, the study of heterocyclic chemistry focuses especially on unsaturated derivatives, and the preponderance of work and applications involves unstrained 5- and 6-membered rings. Included are pyridine, thiophene, pyrrole, and furan. Another large class of heterocycles are fused to benzene rings, which for pyridine, thiophene, pyrrole, and furan are quinoline, benzothiophene, indole, and benzofuran, respectively. Fusion of two benzene rings gives rise to a third large family of compounds, respectively the acridine, dibenzothiophene, carbazole, and dibenzofuran. The unsaturated rings can be classified according to the participation of the heteroatom in the pi system.\n\nHeterocycles with three atoms in the ring are more reactive because of ring strain. Those containing one heteroatom are, in general, stable. Those with two heteroatoms are more likely to occur as reactive intermediates.<br>\nCommon 3-membered heterocycles with \"one\" heteroatom are:\n\nThose with \"two\" heteroatoms include:\n\nCompounds with one heteroatom:\n\nCompounds with two heteroatoms:\n\nWith heterocycles containing five atoms, the unsaturated compounds are frequently more stable because of aromaticity.\n\nFive-membered rings with \"one\" heteroatom:\n\nThe 5-membered ring compounds containing \"two\" heteroatoms, at least one of which is nitrogen, are collectively called the azoles. Thiazoles and isothiazoles contain a sulfur and a nitrogen atom in the ring. Dithiolanes have two sulfur atoms.\n\nA large group of 5-membered ring compounds with \"three\" heteroatoms also exists. One example is dithiazoles that contain two sulfur and a nitrogen atom.\n\nFive-member ring compounds with \"four\" heteroatoms:\n\nWith 5-heteroatoms, the compound may be considered inorganic rather than heterocyclic. \n\nSix-membered rings with a \"single\" heteroatom:\n\nWith \"two\" heteroatoms:\n\nWith three heteroatoms:\n\nWith four heteroatoms:\n\nWith five heteroatoms:\nThe hypothetical compound with six nitrogen heteroatoms would be hexazine.\n\nWith 7-membered rings, the heteroatom must be able to provide an empty pi orbital (e.g., boron) for \"normal\" aromatic stabilization to be available; otherwise, homoaromaticity may be possible. Compounds with one heteroatom include:\n\nThose with two heteroatoms include:\n\nHeterocyclic rings systems that are formally derived by fusion with other rings, either carbocyclic or heterocyclic, have a variety of common and systematic names. For example, with the benzo-fused unsaturated nitrogen heterocycles, pyrrole provides indole or isoindole depending on the orientation. The pyridine analog is quinoline or isoquinoline. For azepine, benzazepine is the preferred name. Likewise, the compounds with two benzene rings fused to the central heterocycle are carbazole, acridine, and dibenzoazepine.\n\nThe history of heterocyclic chemistry began in the 1800s, in step with the development of organic chemistry. Some noteworthy developments:\n1818: Brugnatelli isolates alloxan from uric acid\n1832: Dobereiner produces furfural (a furan) by treating starch with sulfuric acid\n1834: Runge obtains pyrrole (\"fiery oil\") by dry distillation of bones\n1906: Friedlander synthesizes indigo dye, allowing synthetic chemistry to displace a large agricultural industry\n1936: Treibs isolates chlorophyl derivatives from crude oil, explaining the biological origin of petroleum.\n1951: Chargaff's rules are described, highlighting the role of heterocyclic compounds (purines and pyrimidines) in the genetic code.\n\nHeterocyclic compounds are pervasive in many areas of life sciences and technology. Many drugs are heterocyclic compounds.\n\n", "id": "13734", "title": "Heterocyclic compound"}
{"url": "https://en.wikipedia.org/wiki?curid=13742", "text": "Hero Wars\n\nHero Wars is the name for the fantasy role-playing game published by Issaries, Inc.\n\nThe first-edition rulebook, \"Hero Wars\", was published in 2000.\n\nLike \"RuneQuest\", \"Hero Wars\" is set in the world of Glorantha but the rules system is designed for more epic games; there is no real relationship between the \"RuneQuest\" game and \"Hero Wars\" except for the setting of Glorantha.\n\n\"Hero Wars\" uses an innovative resolution system capable of fulfilling either simulationist or narrativist play with no modification. Some players found the abstraction of the system problematic, while others have found it to have been a better match for the mythic vision of Glorantha.\n\nThe first-edition core books had serious quality issues as the publisher did not have sufficient funding to complete production.\n\nBy 2002 the line had expanded to\n\n\nThe game's extensively revised second edition was published in 2003 as \"HeroQuest\". The \"Hero Wars\" products are highly compatible, and readily convertible.\n\n", "id": "13742", "title": "Hero Wars"}
{"url": "https://en.wikipedia.org/wiki?curid=13743", "text": "Harry Connick Jr.\n\nJoseph Harry Fowler Connick Jr. (born September 11, 1967) is an American singer, big band leader, talk show host and actor. He has sold over 28million albums worldwide. Connick is ranked among the top60 best-selling male artists in the United States by the Recording Industry Association of America, with 16million in certified sales. He has had seven top20 US albums, and ten number-one US jazz albums, earning more number-one albums than any other artist in US jazz chart history.\n\nConnick's best-selling album in the United States is his Christmas album \"When My Heart Finds Christmas\" (1993). His highest-charting album is his release \"Only You\" (2004), which reached No.5 in the US and No.6 in Britain. He has won three Grammy Awards and two Emmy Awards. He played Grace's husband, Leo Markus, on the NBC sitcom \"Will & Grace\" from 2002 to 2006.\n\nConnick began his acting career as a tail gunner in the World War II film \"Memphis Belle\" (1990). He played a serial killer in \"Copycat\" (1995), before being cast as a fighter pilot in the blockbuster \"Independence Day\" (1996). Connick's first role as a leading man was in \"Hope Floats\" (1998) with Sandra Bullock. His first thriller film since \"Copycat\" came in the film \"Basic\" (2003) with John Travolta. Additionally, he played the violent ex-husband in \"Bug\", before two romantic comedies, \"P.S. I Love You\" (2007), and the leading man in \"New in Town\" (2009) with Renée Zellweger. In 2011, he appeared in the family film \"Dolphin Tale\" as Dr. Clay Haskett and in the sequel \"Dolphin Tale 2\" (2014).\n\nHarry Connick Jr. was born and raised in New Orleans, Louisiana. His mother, Anita Frances (née Levy; later Livingston; May 22, 1926 – July 1981), was a lawyer and judge in New Orleans and, later, a Louisiana Supreme Court justice. His father, Joseph Harry Fowler Connick Sr., was the district attorney of Orleans Parish from 1973 to 2003. His parents also owned a record store. Connick's father is a Catholic of Irish, English, and German ancestry. Connick's mother, who died from ovarian cancer, was Jewish (her parents had emigrated from Minsk and Vienna, respectively). Connick has a sister, Suzanna; the siblings were raised in the Lakeview neighborhood of New Orleans. Connick is a first cousin of both Jefferson Parish District Attorney, Paul Connick, and State Representative Patrick Connick (of Harvey, Jefferson Parish).\n\nConnick's musical talents soon came to the fore when he started learning the keyboards at age three, playing publicly at age five, and recording with a local jazz band at ten. When Connick was nine years old, he performed the Piano Concerto No. 3 Opus 37 of Beethoven with the New Orleans Symphony Orchestra (now the Louisiana Philharmonic), and later played a duet with Eubie Blake at the Royal Orleans Esplanade Lounge in New Orleans. The song was \"I'm Just Wild About Harry\". This was recorded for a Japanese documentary called \"Jazz Around the World\". The clip was also shown in a Bravo special, called \"Worlds of Harry Connick, Junior.\" in 1999. His musical talents were developed at the New Orleans Center for Creative Arts and under the tutelage of Ellis Marsalis Jr. and James Booker.\n\nConnick attended Jesuit High School, Isidore Newman School, Lakeview School, and the New Orleans Center for Creative Arts, all in New Orleans. Following an unsuccessful attempt to study jazz academically, and having given recitals in the classical and jazz piano programs at Loyola University, Connick moved to the 92nd Street YMHA in New York City to study at Hunter College and the Manhattan School of Music, where a Columbia Records executive Sr. V.P. of A&R, Dr. George Butler, persuaded him to sign with that label. His first record for the label, \"Harry Connick Jr.\", was a mainly instrumental album of standards. He soon acquired a reputation in jazz because of extended stays at high-profile New York venues. His next album, \"20\", featured his vocals and added to this reputation.\n\nWith Connick's reputation growing, director Rob Reiner asked him to provide a soundtrack for his romantic comedy, \"When Harry Met Sally...\" (1989), starring Meg Ryan and Billy Crystal. The soundtrack consisted of several standards, including \"It Had to Be You\", \"Let's Call the Whole Thing Off\" and \"Don't Get Around Much Anymore\", and achieved double-platinum status in the United States. He won his first Grammy Award for Best Jazz Male Vocal Performance for his work on the soundtrack.\n\nConnick made his screen debut in \"Memphis Belle\" (1990), about a B-17 Flying Fortress bomber crew in World War II. In that year he began a two-year world tour. In addition, he released two albums in July 1990: the instrumental jazz trio album \"Lofty's Roach Souffle\" and a big-band album of mostly original songs titled \"We Are in Love\", which also went double platinum. \"We Are in Love\" earned him his second consecutive Grammy for Best Jazz Male Vocal.\n\n\"Promise Me You'll Remember\", his contribution to the \"Godfather III\" soundtrack, was nominated for both an Academy Award and a Golden Globe Award in 1991. In a year of recognition, he was also nominated for an Emmy Award for Best Performance in a Variety Special for his PBS special \"Swingin' Out Live\", which was also released as a video. In October 1991, he released his third consecutive multi-platinum album, \"Blue Light, Red Light\", on which he wrote and arranged the songs. Also in October 1991, he starred in \"Little Man Tate\", directed by Jodie Foster, playing the friend of a child prodigy who goes to college.\n\nIn November 1992, Connick released \"25\", a solo piano collection of standards that again went platinum. He also re-released the album \"Eleven\". Connick contributed \"A Wink and a Smile\" to the \"Sleepless in Seattle\" soundtrack, released in 1993. His multi-platinum album of holiday songs, \"When My Heart Finds Christmas\", was the best-selling Christmas album in 1993.\n\nIn 1994, Connick decided to branch out. He released \"She\", an album of New Orleans funk that also went platinum. In addition, he released a song called \"(I Could Only) Whisper Your Name\" for the soundtrack of \"The Mask\", starring Jim Carrey, which is his most successful single in the United States to date.\n\nConnick took his funk music on a tour of the United Kingdom in 1994, an effort that did not please some of his fans, who were expecting a jazz crooner. Connick also took his funk music to the People's Republic of China in 1995, playing at the \"Shanghai Center Theatre\". The performance was televised live in China for what became known as the Shanghai Gumbo special. In his third film \"Copycat\", Connick played a serial killer. Released in 1995, \"Copycat\" also starred Holly Hunter and Sigourney Weaver. The following year, he released his second funk album, \"Star Turtle\", which did not sell as well as previous albums, although it did reach No. 38 on the charts. However, he appeared in the most successful movie of 1996, \"Independence Day\", with Will Smith and Jeff Goldblum.\n\nFor his 1997 release \"To See You\", Connick recorded original love songs, touring the United States and Europe with a full symphony orchestra backing him and his piano in each city. As part of his tour, he played at the Nobel Peace Prize Concert in Oslo, Norway, with his final concert of that tour in Paris being recorded for a Valentine's Day special on PBS in 1998. He also continued his film career, starring in \"Excess Baggage\" (1997) opposite Alicia Silverstone and Benicio del Toro.\n\nIn May 1998, he had his first leading role in director Forest Whitaker's \"Hope Floats\", with Sandra Bullock as his female lead. He released \"Come By Me\", his first album of big band music in eight years in 1999, and embarked on a world tour visiting the United States, Europe, Japan and Australia. In addition, he provided the voice of Dean McCoppin in the animated film \"The Iron Giant\".\n\nConnick wrote the score for Susan Stroman's Broadway musical \"Thou Shalt Not\", based on Émile Zola's novel \"Thérèse Raquin\", in 2000; it premiered in 2001. His music and lyrics earned a Tony Award nomination. He was also the narrator of the film \"My Dog Skip\", released in that year.\n\nIn March 2001, Connick starred in a television production of \"South Pacific\" with Glenn Close, televised on the ABC network. He also starred in his twelfth movie, \"Mickey\", featuring a screenplay by John Grisham that same year. In October 2001, he again released two albums: \"Songs I Heard\", featuring big band re-workings of children's show themes, and \"30\", featuring Connick on piano with guest appearances by several other musical artists. \"Songs I Heard\" won Connick another Grammy for Best Traditional Pop Album and he toured performing songs from the album, holding matinees at which each parent had to be accompanied by a child.\n\nIn 2002, he received a for a \"system and method for coordinating music display among players in an orchestra.\" Connick appeared as Grace Adler's boyfriend (and later husband) Leo Markus on the NBC sitcom \"Will & Grace\" from 2002 to 2006.\n\nIn July 2003, Connick released his first instrumental album in fifteen years, \"Other Hours Connick on Piano Volume 1\". It was released on Branford Marsalis' new label Marsalis Music and led to a short tour of nightclubs and small theaters. Connick appeared in the film \"Basic\". In October 2003, he released his second Christmas album, \"Harry for the Holidays\", which went gold and reached No. 12 on the \"Billboard\" 200 albums chart. He also had a television special on NBC featuring Whoopi Goldberg, Nathan Lane, Marc Anthony and Kim Burrell. \"Only You\", his seventeenth album for Columbia Records, was released in February 2004. A collection of 1950s and 1960s ballads, \"Only You\", went top ten on both sides of the Atlantic and was certified gold in the United States in March 2004. The \"Only You\" tour with big band went on in America, Australia and a short trip to Asia. \"Harry for the Holidays\" was certified platinum in November 2004. A music DVD \"Harry Connick Jr.\"Only You\" in Concert\" was released in March 2004, after it had first aired as a \"Great Performances\" special on PBS. The special won him an Emmy Award for Outstanding Music Direction. The DVD received a Gold & Platinum Music VideoLong Form awards from the RIAA in November 2005.\n\nAn animated holiday special, \"The Happy Elf\", aired on NBC in December 2005, with Connick as the composer, the narrator, and one of the executive producers. Shortly after, it was released on DVD. The holiday special was based on his original song \"The Happy Elf\", from his 2003 album \"Harry for the Holidays\". Another album from Marsalis Music was recorded in 2005, \"\", a duo album with Harry Connick Jr. on piano together with Branford Marsalis on saxophone. A music DVD, \"A Duo Occasion\", was filmed at the Ottawa International Jazz Festival 2005 in Canada, and released in November 2005.\n\nHe appeared in another episode of NBC sitcom \"Will & Grace\" in November 2005, and appeared in an additional three episodes in 2006.\n\n\"Bug\", a film directed by William Friedkin, is a psychological thriller filmed in 2005, starring Connick, Ashley Judd, and Michael Shannon. The film was released in 2007. He starred in the Broadway revival of \"The Pajama Game\", produced by the Roundabout Theater Company, along with Michael McKean and Kelli O'Hara, at the \"American Airlines Theatre\" in 2006. It ran from February 23 to June 17, 2006, including five benefit performances running from June 13 to 17. The \"Pajama Game\" cast recording was nominated for a Grammy, after being released as part of Connick's double disc album Harry on Broadway, Act I.\n\nHe hosted The Weather Channel's miniseries \"100 Biggest Weather Moments\" which aired in 2007. He was part of the documentary , released in November 2007. He sat in on piano on Bob French's 2007 album \"Marsalis Music Honors Series: Bob French\". He appeared in the film \"P.S. I Love You\", released in December 2007. A third album in the \"Connick on Piano\" series, \"Chanson du Vieux Carré\" was released in 2007, and Connick received two Grammy nominations for the track \"Ash Wednesday\", for the Grammy awards in 2008. \"Chanson du Vieux Carré\" was released simultaneously with the album \"Oh, My NOLA\". Connick toured North America and Europe in 2007, and toured Asia and Australia in 2008, as part of his My New Orleans Tour. Connick did the arrangements for, wrote a couple of songs, and sang a duet on Kelli O'Hara's album that was released in May 2008. He was also the featured singer at the Concert of Hope immediately preceding Pope Benedict XVI's Mass at Yankee Stadium in April 2008. He had the starring role of Dr. Dennis Slamon in the Lifetime television film \"Living Proof\" (2008). His third Christmas album, \"What a Night!\", was released in November 2008.\n\nHarry has a vast knowledge of musical genres and vocalist, even Gospel music. One of his favorite Gospel artists is Stella Award winner and Grammy nominated artist, Kim Burrell of Houston, Texas. \"And when Harry Connick Jr. assembled a symphony orchestra for Pope Benedict XVI's appearance at Yankee Stadium in 2008, he wanted Burrell on vocals\"\n\nThe film \"New in Town\" starring Connick and Renée Zellweger, began filming in January 2008, and was released in January 2009. Connick's album \"Your Songs\" was released on CD, September 22, 2009. In contrast to Connick's previous albums, this album is a collaboration with a record company producer, the multiple Grammy Award winning music executive Clive Davis.\n\nConnick starred in the Broadway revival of \"On a Clear Day You Can See Forever\", which opened at the St. James Theatre in November 2011 in previews.\n\nConnick appeared on May 4, 2010 episode of \"American Idol\" season 9, where he acted as a mentor for the top 5 finalists. He appeared again the next night on 5 May to perform \"And I Love Her\".\n\nOn January 6, 2012, NBC president Robert Greenblatt announced at the Television Critics Association winter press tour that Harry Connick Junior had been cast in a four-episode arc of NBC's long-running legal drama, \"\" as new Executive ADA, David Haden, a dedicated, straight-shooting prosecutor who is assigned a case with Detective Benson (Mariska Hargitay).\n\nOn June 11, 2013, Connick released a new album of all original music titled \"Every Man Should Know\". Connick debuted the title track live on May 2, 2013 episode of \"American Idol\" and appeared on \"The Ellen DeGeneres Show\" the following week to discuss his new project. A 2013 US summer tour was announced in support of the album.\n\nConnick returned to \"American Idol\" to mentor the top four of season 12. He performed \"Every Man Should Know\" on the results show the following night.\n\nOn September 3, 2013, the officials of \"American Idol\" officially announced that Connick would be a part of the judging panel for season 13 alongside former judge Jennifer Lopez and returning judge Keith Urban.\n\n\"Angels Sing\", a family Christmas movie released by Lionsgate, afforded Connick an onscreen collaboration with fellow music legend Willie Nelson. The two wrote a special song exclusively for the movie. Shot in Austin, TX, Angels Sing features actor/musicians Connie Britton, Lyle Lovett, and Kris Kristofferson and is directed by Tim McCanlies, who previously worked with Connick in The Iron Giant.\n\nA one-hour weekday daytime talk show both starring and named \"Harry\", debuted on September 12, 2016.\n\nThe following musicians have toured as the Harry Connick Jr. Big Band since its inception in 1990:\n\nConnick, a New Orleans native, is a founder of the Krewe of Orpheus, a music-based New Orleans krewe, taking its name from Orpheus of classical mythology. The Krewe of Orpheus parades on St. Charles Avenue and Canal Street in New Orleans on Lundi Gras (Fat Monday)the day before Mardi Gras (Fat Tuesday).\n\nOn September 2, 2005, Connick helped to organize, and appeared in, the NBC-sponsored live telethon concert, \"A Concert for Hurricane Relief\", for relief in the wake of Hurricane Katrina. He spent several days touring the city to draw attention to the plight of citizens stranded at the Ernest N. Morial Convention Center and other places. At the concert he paired with host Matt Lauer, and entertainers including Tim McGraw, Faith Hill, Kanye West, Mike Myers and John Goodman.\n\nOn September 6, 2005, Connick was made honorary chair of Habitat for Humanity's Operation Home Delivery, a long-term rebuilding plan for families victimized by Hurricane Katrina in New Orleans and along the Gulf Coast. His actions in New Orleans earned him a Jefferson Award for Public Service.\n\nConnick's album \"Oh, My NOLA\", and \"\" were released in 2007, with a following tour called the My New Orleans Tour.\n\nConnick and Branford Marsalis devised an initiative to help restore New Orleans' musical heritage. Habitat for Humanity and New Orleans Area Habitat for Humanity, working with Connick and Marsalis announced December 6, 2005, plans for a Musicians' Village in New Orleans. The Musicians' Village includes Habitat-constructed homes, with an \"Ellis Marsalis Center for Music\", as the area's centerpiece. The Habitat-built homes provide musicians, and anyone else who qualifies, the opportunity to buy decent, affordable housing.\n\nIn 2012, Connick and Marsalis received the S. Roger Horchow Award for Greatest Public Service by a Private Citizen, an award given out annually by Jefferson Awards.\n\nOn April 16, 1994, Connick married former \"Victoria's Secret\" model Jill Goodacre, originally from Texas, at the St. Louis Cathedral, New Orleans. Jill is the daughter of sculptor Glenna Goodacre, originally from Lubbock, and now Santa Fe, New Mexico. The song \"Jill\", on the album \"Blue Light, Red Light\" (1991) is about her. They have three daughters: Georgia Tatum (born April 17, 1996), Sarah Kate (born September 12, 1997), and Charlotte (born June 26, 2002). The family currently resides in New Orleans, Louisiana, and New York City. Connick is a practicing Roman Catholic.\n\nConnick is a supporter of hometown NFL franchise New Orleans Saints. He was caught on camera at the Super Bowl XLIV, which the Saints won, in Miami by the television crew of \"The Ellen DeGeneres Show\" during the post-game celebrations. Ellen's mother Betty was on the sidelines watching the festivities when she spotted Connick in the stands sporting a Drew Brees jersey.\n\nIn December 1992, he was charged with bringing a gun to the security checkpoint in an airport. Connick was arrested by the Port Authority Police in 1992 and charged with having a 9mm pistol in his possession at JFK International Airport. After spending a day in jail, he agreed to make a public-service television commercial warning against breaking gun laws. The court agreed to drop all charges if Connick stayed out of trouble for six months.\n\n\n\n", "id": "13743", "title": "Harry Connick Jr."}
{"url": "https://en.wikipedia.org/wiki?curid=13744", "text": "List of humorists\n\nA humorist is a person who writes or performs humorous material.\n\nA humorist is usually distinct from a stand-up comedian. For people who are primarily stand-ups, see list of stand-up comedians.\n\nNotable humorists include:\n", "id": "13744", "title": "List of humorists"}
{"url": "https://en.wikipedia.org/wiki?curid=13746", "text": "Hydrostatic shock\n\nHydrostatic shock or hydraulic shock is a term which describes the observation that a penetrating projectile can produce remote wounding and incapacitating effects in living targets through a hydraulic effect in their liquid-filled tissues, in addition to local effects in tissue caused by direct impact. Just as force applied by a pump in a hydraulic circuit is transmitted throughout the circuit because of the near incompressibility of the liquid, so the kinetic energy of a bullet can sometimes send a shock wave through the body, transferring physical shock to tissues whose physiologic function may be disrupted by it (especially in the circulatory or nervous systems). (Other kinds of shock, namely circulatory and psychological, may follow, but mechanical shock is the immediate disruptor.) There is scientific evidence that hydrostatic shock can produce remote neural damage and produce incapacitation more quickly than blood loss effects. In arguments about the differences in stopping power between calibers and between cartridge models, proponents of cartridges that are \"light and fast\" (such as the 9×19mm Parabellum) versus cartridges that are \"slow and heavy\" (such as the .45 ACP) often refer to this phenomenon.\n\nHuman autopsy results have demonstrated brain hemorrhaging from fatal hits to the chest, including cases with handgun bullets. Thirty-three cases of fatal penetrating chest wounds by a single bullet were selected from a much larger set by excluding all other traumatic factors, including past history.\nIt has often been asserted that hydrostatic shock and other descriptions of remote wounding effects are nothing but myths. Correspondence in the journal, \"Neurosurgery\", reviews the published evidence and concludes that the phenomenon is well-established.\nIn the scientific literature, the first discussion of pressure waves created when a bullet hits a living target is presented by E. Harvey Newton and his research group at Princeton University in 1947:\n\nFrank Chamberlin, a World War II trauma surgeon and ballistics researcher, noted remote pressure wave effects. Col. Chamberlin described what he called “explosive effects” and “hydraulic reaction” of bullets in tissue. \"...liquids are put in motion by ‘shock waves’ or hydraulic effects... with liquid filled tissues, the effects and destruction of tissues extend in all directions far beyond the wound axis\". He avoided the ambiguous use of the term “shock” because it can refer to either a specific kind of pressure wave associated with explosions and supersonic projectiles or to a medical condition in the body.\n\nCol. Chamberlin recognized that many theories have been advanced in wound ballistics. During World War II he commanded an 8,500-bed hospital center that treated over 67,000 patients during the fourteen months that he operated it. P.O. Ackley estimates that 85% of the patients were suffering from gunshot wounds. Col. Chamberlin spent many hours interviewing patients as to their reactions to bullet wounds. He conducted many live animal experiments after his tour of duty. On the subject of wound ballistics theories, he wrote:\nOther World War II era scientists noted remote pressure wave effects in the peripheral nerves. There was support for the idea of remote neural effects of ballistic pressure waves in the medical and scientific communities, but the phrase \"’hydrostatic shock’\" and similar phrases including “shock” were used mainly by gunwriters (such as Jack O'Conner) and the small arms industry (such as Roy Weatherby, and Federal “Hydrashock.”)\n\nDr. Martin Fackler, a Vietnam-era trauma surgeon, wound ballistics researcher, a Colonel in the U.S. Army and the head of the Wound Ballistics Laboratory for the U.S. Army’s Medical Training Center, Letterman Institute, claimed that hydrostatic shock had been disproved and that the assertion that a pressure wave plays a role in injury or incapacitation is a myth. Others expressed similar views.\n\nDr. Fackler based his argument on the lithotriptor, a tool commonly used to break up kidney stones. The lithotriptor uses sonic pressure waves which are stronger than those caused by most handgun bullets, yet it produces no damage to soft tissues whatsoever. Hence, Fackler argued, ballistic pressure waves cannot damage tissue either.\n\nDr. Fackler claimed that a study of rifle bullet wounds in Vietnam (Wound Data and Munitions Effectiveness Team) found “no cases of bones being broken, or major vessels torn, that were not hit by the penetrating bullet. In only two cases, an organ that was not hit (but was within a few cm of the projectile path), suffered some disruption.” Dr. Fackler cited a personal communication with R. F. Bellamy. However, Bellamy’s published findings the following year estimated that 10% of fractures in the data set might be due to indirect injuries, and one specific case is described in detail (pp. 153–154). In addition, the published analysis documents five instances of abdominal wounding in cases where the bullet did not penetrate the abdominal cavity (pp. 149–152), a case of lung contusion resulting from a hit to the shoulder (pp. 146–149), and a case of indirect effects on the central nervous system (p. 155). Fackler's critics argue that Fackler's evidence does not contradict distant injuries, as Fackler claimed, but the WDMET data from Vietnam actually provides supporting evidence for it.\n\nA summary of the debate was published in 2009 as part of a \"Historical Overview of Wound Ballistics Research.\"\n\nThe Wound Data and Munitions Effectiveness Team (WDMET) gathered data on wounds sustained during the Vietnam War. In their analysis of this data published in the Textbook of Military Medicine, Ronald Bellamy and Russ Zajtchuck point out a number of cases which seem to be examples of distant injuries. Bellamy and Zajtchuck describe three mechanisms of distant wounding due to pressure transients: 1) stress waves 2) shear waves and 3) a vascular pressure impulse.\n\nAfter citing Harvey's conclusion that “stress waves probably do not cause any tissue damage” (p. 136), Bellamy and Zajtchuck express their view that Harvey's interpretation might not be definitive because they write “the possibility that stress waves from a penetrating projectile might also cause tissue damage cannot be ruled out.” (p. 136) The WDMET data includes a case of a lung contusion resulting from a hit to the shoulder. The caption to Figure 4-40 (p. 149) says, “The pulmonary injury may be the result of a stress wave.” They describe the possibility that a hit to a soldier's trapezius muscle caused temporary paralysis due to “the stress wave passing through the soldier's neck indirectly [causing] cervical cord dysfunction.” (p. 155)\n\nIn addition to stress waves, Bellamy and Zajtchuck describe shear waves as a possible mechanism of indirect injuries in the WDMET data. They estimate that 10% of bone fractures in the data may be the result of indirect injuries, that is, bones fractured by the bullet passing close to the bone without a direct impact. A Chinese experiment is cited which provides a formula estimating how pressure magnitude decreases with distance. Together with the difference between strength of human bones and strength of the animal bones in the Chinese experiment, Bellamy and Zajtchuck use this formula to estimate that assault rifle rounds “passing within a centimeter of a long bone might very well be capable of causing an indirect fracture.” (p. 153) Bellamy and Zajtchuck suggest the fracture in Figures 4-46 and 4-47 is likely an indirect fracture of this type. Damage due to shear waves extends to even greater distances in abdominal injuries in the WDMET data. Bellamy and Zajtchuck write, “The abdomen is one body region in which damage from indirect effects may be common.” (p. 150) Injuries to the liver and bowel shown in Figures 4-42 and 4-43 are described, “The damage shown in these examples extends far beyond the tissue that is likely to direct contact with the projectile.” (p. 150)\n\nIn addition to providing examples from the WDMET data for indirect injury due to propagating shear and stress waves, Bellamy and Zajtchuck expresses an openness to the idea of pressure transients propagating via blood vessels can cause indirect injuries. “For example, pressure transients arising from an abdominal gunshot wound might propagate through the vena cavae and jugular venous system into the cranial cavity and cause a precipitous rise in intracranial pressure there, with attendant transient neurological dysfunction.” (p. 154) However, no examples of this injury mechanism are presented from the WDMET data. However, the authors suggest the need for additional studies writing, “Clinical and experimental data need to be gathered before such indirect injuries can be confirmed.” Distant injuries of this nature were later confirmed in the experimental data of Swedish and Chinese researchers, in the clinical findings of Krajsa and in autopsy findings from Iraq.\n\nAn 8-month study in Iraq performed in 2010 and published in 2011 reports on autopsies of 30 gunshot victims struck with high-velocity (greater than 2500 fps) rifle bullets. In all 30 cases, autopsies revealed injuries distant from the main wound channel due to hydrostatic shock. The authors determined that the lungs and chest are the most susceptible to distant wounding, followed by the abdomen. The authors conclude:\n\nA shock wave can be created when fluid is rapidly displaced by an explosive or projectile. Tissue behaves similarly enough to water that a sonic pressure wave can be created by a bullet impact, generating pressures in excess of .\n\nDuncan MacPherson, a former member of the International Wound Ballistics Association and author of the book, Bullet Penetration, claimed that shock waves cannot result from bullet impacts with tissue. In contrast, Brad Sturtevant, a leading researcher in shock wave physics at Caltech for many decades, found that shock waves can result from handgun bullet impacts in tissue. Other sources indicate that ballistic impacts can create shock waves in tissue.\n\nBlast and ballistic pressure waves have physical similarities. Prior to wave reflection, they both are characterized by a steep wave front followed by a nearly exponential decay at close distances. They have similarities in how they cause neural effects in the brain. In tissue, both types of pressure waves have similar magnitudes, duration, and frequency characteristics. Both have been shown to cause damage in the hippocampus. It has been hypothesized that both reach the brain from the thoracic cavity via major blood vessels.\n\nFor example, Ibolja Cernak, a leading researcher in blast wave injury at the Applied Physics Laboratory at Johns Hopkins University, hypothesized, \"alterations in brain function following blast exposure are induced by kinetic energy transfer of blast overpressure via great blood vessels in abdomen and thorax to the central nervous system.\" This hypothesis is supported by observations of neural effects in the brain from localized blast exposure focused on the lungs in experiments in animals.\n\n“Hydrostatic shock” expresses the idea that organs can be damaged by the pressure wave in addition to damage from direct contact with the penetrating projectile. If one interprets the \"shock\" in the term \"hydrostatic shock\" to refer to the physiological effects rather than the physical wave characteristics, the question of whether the pressure waves satisfy the definition of “shock wave” is unimportant, and one can consider the weight of scientific evidence and various claims regarding the possibility of a ballistic pressure wave to create tissue damage and incapacitation in living targets.\n\nA number of papers describe the physics of ballistic pressure waves created when a high-speed projectile enters a viscous medium. These results show that ballistic impacts produce pressure waves that propagate at close to the speed of sound.\n\nLee et al. present an analytical model showing that unreflected ballistic pressure waves are well approximated by an exponential decay, which is similar to blast pressure waves. Lee et al. note the importance of the energy transfer:\n\nThe rigorous calculations of Lee et al. require knowing the drag coefficient and frontal area of the penetrating projectile at every instant of the penetration. Since this is not generally possible with expanding handgun bullets, Courtney and Courtney developed a model for estimating the peak pressure waves of handgun bullets from the impact energy and penetration depth in ballistic gelatin. This model agrees with the more rigorous approach of Lee et al. for projectiles where they can both be applied. For expanding handgun bullets, the peak pressure wave magnitude is proportional to the bullet’s kinetic energy divided by the penetration depth.\n\nGoransson et al. were the first contemporary researchers to present compelling evidence for remote cerebral effects of extremity bullet impact. They observed changes in EEG readings from pigs shot in the thigh. A follow-up experiment by Suneson et al. implanted high-speed pressure transducers into the brain of pigs and demonstrated that a significant pressure wave reaches the brain of pigs shot in the thigh. These scientists observed apnea, depressed EEG readings, and neural damage in the brain caused by the distant effects of the ballistic pressure wave originating in the thigh.\n\nThe results of Suneson et al. were confirmed and expanded upon by a later experiment in dogs\nwhich \"confirmed that distant effect exists in the central nervous system after a high-energy missile impact to an extremity. A high-frequency oscillating pressure wave with large amplitude and short duration was found in the brain after the extremity impact of a high-energy missile . . .\" Wang et al. observed significant damage in both the hypothalamus and hippocampus regions of the brain due to remote effects of the ballistic pressure wave.\n\nIn a study of a handgun injury, Sturtevant found that pressure waves from a bullet impact in the torso can reach the spine and that a focusing effect from concave surfaces can concentrate the pressure wave on the spinal cord producing significant injury. This is consistent with other work showing remote spinal cord injuries from ballistic impacts.\n\nRoberts et al. present both experimental work and finite element modeling showing that there can be considerable pressure wave magnitudes in the thoracic cavity for handgun projectiles stopped by a Kevlar vest. For example, an 8 gram projectile at 360 m/s impacting a NIJ level II vest over the sternum can produce an estimated pressure wave level of nearly 2.0 MPa (280 psi) in the heart and a pressure wave level of nearly 1.5 MPa (210 psi) in the lungs. Impacting over the liver can produce an estimated pressure wave level of 2.0 MPa (280 psi) in the liver.\n\nThe work of Courtney et al. supports the role of a ballistic pressure wave in incapacitation and injury. The work of Suneson et al. and Courtney et al. suggest that remote neural effects can occur with levels of energy transfer possible with handguns, about . Using sensitive biochemical techniques, the work of Wang et al. suggests even lower impact energy thresholds for remote neural injury to the brain. In analysis of experiments of dogs shot in the thigh they report highly significant (p < 0.01), easily detectable neural effects in the hypothalamus and hippocampus with energy transfer levels close to . Wang et al. reports less significant (p < 0.05) remote effects in the hypothalamus with energy transfer just under .\n\nEven though Wang et al. document remote neural damage for low levels of energy transfer, roughly , these levels of neural damage are probably too small to contribute to rapid incapacitation. Courtney and Courtney believe that remote neural effects only begin to make significant contributions to rapid incapacitation for ballistic pressure wave levels above (corresponds to transferring roughly in of penetration) and become easily observable above (corresponds to transferring roughly in of penetration). Incapacitating effects in this range of energy transfer are consistent with observations of remote spinal injuries, observations of suppressed EEGs and apnea in pigs and with observations of incapacitating effects of ballistic pressure waves without a wound channel.\n\nThe scientific literature contains significant other findings regarding injury mechanisms of ballistic pressure waves. Ming et al. found that ballistic pressure waves can break bones. Tikka et al. reports abdominal pressure changes produced in pigs hit in one thigh. Akimov et al. report on injuries to the nerve trunk from gunshot wounds to the extremities.\n\nThe FBI recommends that loads intended for self-defense and law enforcement applications meet a minimum penetration requirement of in ballistic gelatin and explicitly advises against selecting rounds based on hydrostatic shock effects.\n\nIn self-defense, military, and law enforcement communities, opinions vary regarding the importance of remote wounding effects in ammunition design and selection. In his book on hostage rescuers, Leroy Thompson discusses the importance of hydrostatic shock in choosing a specific design of .357 Magnum and 9×19mm Parabellum bullets. In \"Armed and Female\", Paxton Quigley explains that hydrostatic shock is the real source of “stopping power.” Jim Carmichael, who served as shooting editor for Outdoor Life magazine for 25 years, believes that hydrostatic shock is important to “a more immediate disabling effect” and is a key difference in the performance of .38 Special and .357 Magnum hollow point bullets. In “The search for an effective police handgun,” Allen Bristow describes that police departments recognize the importance of hydrostatic shock when choosing ammunition. A research group at West Point suggests handgun loads with at least of energy and of penetration and recommends:\n\nA number of law enforcement and military agencies have adopted the 5.7×28mm cartridge. These agencies include the Navy SEALs and the Federal Protective Service branch of the ICE. In contrast, some defense contractors, law enforcement analysts, and military analysts say that hydrostatic shock is an unimportant factor when selecting cartridges for a particular use because any incapacitating effect it may have on a target is difficult to measure and inconsistent from one individual to the next. This is in contrast to factors such as proper shot placement and massive blood loss which are almost always eventually incapacitating for nearly every individual.\n\nHydrostatic shock is commonly considered as a factor in the selection of hunting ammunition. Peter Capstick explains that hydrostatic shock may have value for animals up to the size of white-tailed deer, but the ratio of energy transfer to animal weight is an important consideration for larger animals. If the animal’s weight exceeds the bullet’s energy transfer, penetration in an undeviating line to a vital organ is a much more important consideration than energy transfer and hydrostatic shock. Jim Carmichael, in contrast, describes evidence that hydrostatic shock can affect animals as large as Cape Buffalo in the results of a carefully controlled study carried out by veterinarians in a buffalo culling operation.\n\nDr. Randall Gilbert describes hydrostatic shock as an important factor in bullet performance on whitetail deer, “When it [a bullet] enters a whitetail’s body, huge accompanying shock waves send vast amounts of energy through nearby organs, sending them into arrest or shut down.” Dave Ehrig expresses the view that hydrostatic shock depends on impact velocities above per second. Sid Evans explains the performance of the Nosler Partition bullet and Federal Cartridge Company’s decision to load this bullet in terms of the large tissue cavitation and hydrostatic shock produced from the frontal diameter of the expanded bullet. The North American Hunting Club suggests big game cartridges that create enough hydrostatic shock to quickly bring animals down.\n\n\nTerminal Ballistics Research http://www.ballisticstudies.com/Knowledgebase.html\n", "id": "13746", "title": "Hydrostatic shock"}
{"url": "https://en.wikipedia.org/wiki?curid=13749", "text": "Hadith\n\nA hadith ( or ; , plural: ahadith, , ) is one of various reports describing the words, actions, or habits of the Islamic prophet Muhammad. The term comes from Arabic meaning a \"report\", \"account\" or \"narrative\". Hadith are second only to the Quran in developing Islamic jurisprudence, and regarded as important tools for understanding the Quran and commentaries (\"tafsir\") written on it. Some important elements of traditional Islam, such as the five salat prayers, are mentioned in hadith.\n\nThe hadith literature is based on spoken reports that were in circulation in society after the death of Muhammad. Unlike the Qur'an the hadiths were not quickly and concisely compiled during and immediately after Muhammad's life. Hadith were evaluated and gathered into large collections during the 8th and 9th centuries, generations after the death of Muhammad, after the end of the era of the \"rightful\" Rashidun Caliphate, over from where Muhammad lived.\n\nEach hadith consists of two parts, the \"isnad\" (Arabic: 'support'), or the chain of transmitters through which a scholar traced the \"matn\", or text, of a hadith back to the Prophet. Individual hadith are classified by Muslim clerics and jurists as \"sahih\" (\"authentic\"), \"hasan\" (\"good\") or \"da'if\" (\"weak\"). However, there is no overall agreement: different groups and different individual scholars may classify a hadith differently. \n\nDifferent branches of Islam (Sunni, Shia, Ibadi) as well as the Ahmadiyya refer to different collections of hadith, and the relatively small sect of Quranists reject the authority of any of the hadith collections.\n\nIn Arabic, the noun ' ( '  ) means \"report\", \"account\", or \"narrative\". Its Arabic plural is ʾaḥādīth (أحاديث) (). \"Hadith\" also refers to the speech of a person.\n\nIn Islamic terminology, according to Juan Campo, the term \"hadith\" refers to reports of statements or actions of Muhammad, or of his tacit approval or criticism of something said or done in his presence, though some sources (Khaled Abou El Fadl) limit hadith to verbal reports and include the deeds of Muhammad and reports about his companions only in the \"Sunnah\".\n\nClassical hadith specialist Ibn Hajar al-Asqalani says that the intended meaning of \"hadith\" in religious tradition is something attributed to Muhammad but that is not found in the Quran. Other associated words possess similar meanings including: \"khabar\" (news, information) often refers to reports about Muhammad, but sometimes refers to traditions about his companions and their successors from the following generation; conversely, \"athar\" (trace, vestige) usually refers to traditions about the companions and successors, though sometimes connotes traditions about Muhammad. The word \"sunnah\" (custom) is also used in reference to a normative custom of Muhammad or the early Muslim community.\n\nThe two major aspects of a hadith are the text of the report (the \"matn\"), which contains the actual narrative, and the chain of narrators (the \"isnad\"), which documents the route by which the report has been transmitted. The isnad was an effort to document that a hadith had actually come from Muhammad, and Muslim scholars from the eighth century until today have never ceased repeating the mantra \"The isnad is part of the religion - if not for the isnad, whoever wanted could say whatever they wanted.\" The \"isnad\" means literally 'support', and it is so named due to the reliance of the hadith specialists upon it in determining the authenticity or weakness of a hadith. The \"isnad\" consists of a chronological list of the narrators, each mentioning the one from whom they heard the hadith, until mentioning the originator of the \"matn\" along with the \"matn\" itself.\n\nThe first people to hear hadith were the companions who preserved it and then conveyed it to those after them. Then the generation following them received it, thus conveying it to those after them and so on. So a companion would say, \"I heard the Prophet say such and such.\" The Follower would then say, \"I heard a companion say, 'I heard the Prophet.'\" The one after him would then say, \"I heard someone say, 'I heard a Companion say, 'I heard the Prophet...\"\" and so on.\n\nDifferent branches of Islam refer to different collections of hadith, though the same incident may be found in hadith in different collections:\n\nSome minor groups, collectively known as Quranists, reject the authority of the hadith collections.\n\nThe hadith also had a profound and controversial influence on moulding the commentaries (\"tafsir\") of the Quran. The earliest commentary of the Quran known as Tafsir Ibn Abbas is sometimes attributed to the companion Ibn Abbas, but this is rejected by scholars.The hadith were used in forming the basis of Shariah. Much of early Islamic history available today is also based on the hadith and is challenged for lack of basis in primary source material and contradictions based on secondary material available.\n\nTraditions of the life of Muhammad and the early history of Islam were passed down mostly orally for more than a hundred years after Muhammad's death in AD 632. Muslim historians say that Caliph Uthman ibn Affan (the third khalifa (caliph) of the Rashidun Empire, or third successor of Muhammad, who had formerly been Muhammad's secretary), is generally believed to urge Muslims to record the hadith just as Muhammad suggested to some of his followers to write down his words and actions.\n\nUthman's labours were cut short by his assassination, at the hands of aggrieved soldiers, in 656. No sources survive directly from this period so we are dependent on what later writers tell us about this period.\n\nAccording to British historian of Arab world Alfred Guillaume, it is \"certain\" that \"several small collections\" of hadith were \"assembled in Umayyad times.\"\n\nIn 851 the rationalist Mu`tazila school of thought fell from favor in the Abbasid Caliphate. The Mu`tazila, for whom the \"judge of truth ... was human reason,\" had clashed with traditionists who looked to the literal meaning of the Quran and hadith for truth. While the Quran had been officially compiled and approved, hadiths had not. \nOne result was the number of hadiths began \"multiplying in suspiciously direct correlation to their utility\" to the quoter of the hadith (Traditionists quoted hadith warning against listening to human opinion instead of Sharia; Hanafites quoted a hadith stating that \"In my community there will rise a man called Abu Hanifa [the Hanafite founder] who will be its guiding light\". In fact one agreed upon hadith warned that, \"There will be forgers, liars who will bring you hadiths which neither you nor your forefathers have heard, Beware of them.\" In addition the number of hadith grew enormously. While Malik ibn Anas had attributed just 1720 statements or deeds to the Muhammad, it was no longer unusual to find people who had collected a hundred times that number of hadith.\n\nFaced with a huge corpus of miscellaneous traditions supported differing views on a variety of controversial matters—some of them flatly contradicting each other—Islamic scholars of the Abbasid sought to authenticate hadith. Scholars had to decide which hadith were to be trusted as authentic and which had been invented for political or theological purposes. To do this, they used a number of techniques which Muslims now call the science of hadith.\n\nSunni and Shia hadith collections differ because scholars from the two traditions differ as to the reliability of the narrators and transmitters. Narrators who took the side of Abu Bakr and Umar rather than Ali, in the disputes over leadership that followed the death of Muhammad, are seen as unreliable by the Shia; narrations sourced to Ali and the family of Muhammad, and to their supporters, are preferred. Sunni scholars put trust in narrators, such as Aisha, whom Shia reject. Differences in hadith collections have contributed to differences in worship practices and shari'a law and have hardened the dividing line between the two traditions.\n\nIn the Sunni tradition, the number of such texts is ten thousand plus or minus a few thousand. But if, say, ten companions record a text reporting a single incident in the life of the prophet, hadith scholars can count this as ten hadiths. So Musnad Ahmad, for example, has over 30,000 hadiths—but this count includes texts that are repeated in order to record slight variations within the text or within the chains of narrations. Identifying the narrators of the various texts, comparing their narrations of the same texts to identify both the soundest reporting of a text and the reporters who are most sound in their reporting occupied experts of hadith throughout the 2nd century. In the 3rd century of Islam (from 225/840 to about 275/889), hadith experts composed brief works recording a selection of about two- to five-thousand such texts which they felt to have been most soundly documented or most widely referred to in the Muslim scholarly community. The 4th and 5th century saw these six works being commented on quite widely. This auxiliary literature has contributed to making their study the place of departure for any serious study of hadith. In addition, Bukhari and Muslim in particular, claimed that they were collecting only the soundest of sound hadiths. These later scholars tested their claims and agreed to them, so that today, they are considered the most reliable collections of hadith. Toward the end of the 5th century, Ibn al-Qaisarani formally standardized the Sunni canon into six pivotal works, a delineation which remains to this day.\n\nOver the centuries, several different categories of collections came into existence. Some are more general, like the \"muṣannaf\", the \"muʿjam\", and the \"jāmiʿ\", and some more specific, either characterized by the topics treated, like the \"sunan\" (restricted to legal-liturgical traditions), or by its composition, like the \"arbaʿīniyyāt\" (collections of forty hadiths).\n\nShi'a Muslims do not use the six major hadith collections followed by the Sunni, as they do not trust many of the Sunni narrators and transmitters. They have their own extensive hadith literature. The best-known hadith collections are The Four Books, which were compiled by three authors who are known as the 'Three Muhammads'. The Four Books are: \"Kitab al-Kafi\" by Muhammad ibn Ya'qub al-Kulayni al-Razi (329 AH), \"Man la yahduruhu al-Faqih\" by Muhammad ibn Babuya and \"Al-Tahdhib\" and \"Al-Istibsar\" both by Shaykh Muhammad Tusi. Shi'a clerics also make use of extensive collections and commentaries by later authors.\n\nUnlike Sunnis, Shia do not consider any of their hadith collections to be sahih (authentic) in their entirety. Therefore, every individual hadith in a specific collection must be investigated separately to determine its authenticity.\n\nThe mainstream sects consider hadith to be essential supplements to, and clarifications of, the Quran, Islam's holy book, as well as for clarifying issues pertaining to Islamic jurisprudence. Ibn al-Salah, a hadith specialist, described the relationship between hadith and other aspect of the religion by saying: \"It is the science most pervasive in respect to the other sciences in their various branches, in particular to jurisprudence being the most important of them.\" \"The intended meaning of 'other sciences' here are those pertaining to religion,\" explains Ibn Hajar al-Asqalani, \"Quranic exegesis, hadith, and jurisprudence. The science of hadith became the most pervasive due to the need displayed by each of these three sciences. The need hadith has of its science is apparent. As for Quranic exegesis, then the preferred manner of explaining the speech of God is by means of what has been accepted as a statement of Muhammad. The one looking to this is in need of distinguishing the acceptable from the unacceptable. Regarding jurisprudence, then the jurist is in need of citing as an evidence the acceptable to the exception of the later, something only possible utilizing the science of hadith.\"\n\nHadith studies use a number of methods of evaluation developed by early Muslim scholars in determining the veracity of reports attributed to Muhammad. This is achieved by analyzing the text of the report, the scale of the report's transmission, the routes through which the report was transmitted, and the individual narrators involved in its transmission. On the basis of these criteria, various classifications were devised for hadith. The earliest comprehensive work in hadith studies was Abu Muhammad al-Ramahurmuzi's \"al-Muhaddith al-Fasil\", while another significant work was al-Hakim al-Naysaburi's \"Ma‘rifat ‘ulum al-hadith\". Ibn al-Salah's \"ʻUlum al-hadith\" is considered the standard classical reference on hadith studies.\n\nBy means of hadith terminology, hadith are categorized as \"ṣaḥīḥ\" (sound, authentic), \"ḍaʿīf\" (weak), or \"mawḍūʿ\" (fabricated). Other classifications used also include: \"ḥasan\" (good), which refers to an otherwise \"ṣaḥīḥ\" report suffering from minor deficiency, or a weak report strengthened due to numerous other corroborating reports; and \"munkar\" (denounced) which is a report that is rejected due to the presence of an unreliable transmitter contradicting another more reliable narrator. Both \"sahīh\" and \"hasan\" reports are considered acceptable for usage in Islamic legal discourse. Classifications of hadith may also be based upon the scale of transmission. Reports that pass through many reliable transmitters at each point in the \"isnad\" up until their collection and transcription are known as \"mutawātir\". These reports are considered the most authoritative as they pass through so many different routes that collusion between all of the transmitters becomes an impossibility. Reports not meeting this standard are known as \"aahad\", and are of several different types.\n\nSome hadith are also called \"hadith qudsi\" (sacred hadith), like Ziyarat Ashura. It is a sub-category of hadith which some Muslims regard as the words of God (Arabic: Allah). According to as-Sayyid ash-Sharif al-Jurjani, the hadith qudsi differ from the Quran in that the former are \"expressed in Muhammad's words\", whereas the latter are the \"direct words of God\". However, note that a \"hadith qudsi\" is not necessarily \"sahih\", it can also be \"da‘if\" or even \"mawdu‘\".\n\nAn example of a \"hadith qudsi\" is the hadith of Abu Hurairah who said that Muhammad said:\nWhen God decreed the Creation He pledged Himself by writing in His book which is laid down with Him: My mercy prevails over My wrath.\n\nAnother area of focus in the study of hadith is biographical analysis (\"‘ilm al-rijāl\", lit. \"science of people\"), in which details about the transmitter are scrutinized. This includes analyzing their date and place of birth; familial connections; teachers and students; religiosity; moral behaviour; literary output; their travels; as well as their date of death. Based upon these criteria, the reliability (\"thiqāt\") of the transmitter is assessed. Also determined is whether the individual was actually able to transmit the report, which is deduced from their contemporaneity and geographical proximity with the other transmitters in the chain. Examples of biographical dictionaries include: Abd al-Ghani al-Maqdisi's \"Al-Kamal fi Asma' al-Rijal\", Ibn Hajar al-Asqalani's \"Tahdhīb al-Tahdhīb\" and al-Dhahabi's \"Tadhkirat al-huffaz\".\n\nThe major points of criticism of the Hadith literature is based in questions regarding its authenticity, as well as theological/philosophical critiques. Muslim scholars questioned the Hadith literature throughout its history, with Western academics also becoming active in the field later on.\n\n\n\n\n", "id": "13749", "title": "Hadith"}
{"url": "https://en.wikipedia.org/wiki?curid=13755", "text": "Hull (watercraft)\n\nThe hull is the watertight body of a ship or boat. Above the hull is the superstructure and/or deckhouse, where present. The line where the hull meets the water surface is called the waterline.\n\nThe structure of the hull varies depending on the vessel type. In a typical modern steel ship, the structure consists of watertight and non-tight decks, major transverse and watertight (and also sometimes non-tight or longitudinal) members called bulkheads, intermediate members such as girders, stringers and webs, and minor members called ordinary transverse frames, frames, or longitudinals, depending on the structural arrangement. The uppermost continuous deck may be called the \"upper deck\", \"weather deck\", \"spar deck\", \"main deck\", or simply \"deck\". The particular name given depends on the context—the type of ship or boat, the arrangement, or even where it sails. Not all hulls are decked (for instance a dinghy).\n\nIn a typical wooden sailboat, the hull is constructed of wooden planking, supported by transverse frames (often referred to as ribs) and bulkheads, which are further tied together by longitudinal stringers or ceiling. Often but not always there is a centerline longitudinal member called a keel. In fiberglass or composite hulls, the structure may resemble wooden or steel vessels to some extent, or be of a monocoque arrangement. In many cases, composite hulls are built by sandwiching thin fiber-reinforced skins over a lightweight but reasonably rigid core of foam, balsa wood, impregnated paper honeycomb or other material.\nThe shape of the hull is entirely dependent upon the needs of the design. Shapes range from a nearly perfect box in the case of scow barges, to a needle-sharp surface of revolution in the case of a racing multihull sailboat. The shape is chosen to strike a balance between cost, hydrostatic considerations (accommodation, load carrying and stability), hydrodynamics (speed, power requirements, and motion and behavior in a seaway) and special considerations for the ship's role, such as the rounded bow of an icebreaker or the flat bottom of a landing craft.\n\nHulls come in many varieties and can have composite shape, (e.g., a fine entry forward and inverted bell shape aft), but are grouped primarily as follows:\n\nAfter this they can be categorized as:\nThe hull is supported exclusively or predominantly by buoyancy. Vessels that have this type of hull travel through the water at a limited rate that is defined by the waterline length. They are often, though not always, heavier than planing types.\n\nThe planing hull form is configured to develop positive dynamic pressure so that its draft decreases with increasing speed. The dynamic lift reduces the wetted surface and therefore also the drag. They are sometimes flat-bottomed, sometimes V-bottomed and more rarely, round-bilged. The most common form is to have at least one chine, which makes for more efficient planing and can throw spray down. Planing hulls are more efficient at higher speeds, although they still require more energy to achieve these speeds. An effective planing hull must be as light as possible with flat surfaces that are consistent with good sea keeping. Sail boats that plane must also sail efficiently in displacement mode in light winds.\n\nTthe hull form is capable of developing a moderate amount of dynamic lift; however, most of the vessel's weight is still supported through buoyancy.\n\nAt present, the most widely used form is the round bilge hull.\n\nIn the inverted bell shape of the hull, with a smaller payload the waterline cross-section is less, hence the resistance is less and the speed is higher. With a higher payload the outward bend provides smoother performance in waves. As such, the inverted bell shape is a popular form used with planing hulls.\n\nA chined hull consists of straight, smooth, tall, long, or short plates, timbers or sheets of ply, which are set at an angle to each other when viewed in transverse section. The traditional chined hull is a simple hull shape because it works with only straight planks bent into a curve. These boards are often bent lengthwise. Plywood chined boats made of 8'x4' sheets have most bend along the long axis of the sheet. Only thin ply 3–6 mm can easily be shaped into a compound bend. Most home-made constructed boats are chined hull boats. Mass-produced chine powerboats are usually made of sprayed chop strand fibreglass over a wooden mold. The Cajun \"pirogue\" is an example of a craft with hard chines. Benefits of this type of hull is the low production cost and the (usually) fairly flat bottom, making the boat faster at planing. Sail boats with chined hull make use of a dagger board or keel.\n\nChined hulls can be divided up into three shapes:\n\nEach of these chine hulls has its own unique characteristics and use. The flat bottom hull has high initial stability but high drag. To counter the high drag hull forms are narrow and sometimes severely tapered at bow and stern. This leads to poor stability when heeled in a sail boat. This is often countered by using heavy interior ballast on sailing versions. They are best suited to sheltered inshore waters. Early racing power boats were fine forward and flat aft. This produced maximum lift and a smooth,fast ride in flat water but this hull form is easily unsettled in waves. The multi chine hull approximates a curved hull form. It has less drag than a flat bottom boat. Multi chines are more complex to build but produce a more seaworthy hull form. They are usually displacement hulls. V or arc bottom chine boats have a Vshape between 6and 23degrees. This is called the deadrise angle. The flatter shape of a 6degrees hull will plane with less wind or a lower horse power engine but will pound more in waves. The deep Vform (between 18and 23degrees) is only suited to high power planing boats. They require more powerful engines to lift the boat onto the plane but give a faster smoother ride in waves.\nDisplacement chined hulls have more wetted surface area, hence more drag, than an equivalent round hull form, for any given displacement.\n\nSmooth curve hulls are hulls which use, just like the curved hulls, a sword or an attached keel.\n\nSemi round bilge hulls are somewhat less round. The advantage of the semi-round is that it is a nice middle between the S-bottom and chined hull. Typical examples of a semi-round bilge hull can be found in the Centaur and Laser cruising dinghies.\n\nS-bottom hulls are hulls shaped like an \"s\" . In the s-bottom, the hull runs smooth to the keel. As there are no sharp corners in the fuselage. Boats with this hull have a fixed keel, or a \"kielmidzwaard\" (literally \"keel with sword\"). This is a short fixed keel, with a swing keel inside. Examples of cruising dinghies that use this s-shape are the Yngling and Randmeer.\n\n\nHull forms are defined as follows:\n\nRafts have a hull of sorts, however, hulls of the earliest design are thought to have each consisted of a hollowed out tree bole: in effect the first canoes. Hull form then proceeded to the coracle shape and on to more sophisticated forms as the science of naval architecture advanced.\n\n\n\n", "id": "13755", "title": "Hull (watercraft)"}
{"url": "https://en.wikipedia.org/wiki?curid=13756", "text": "Hymn\n\nA hymn is a type of song, usually religious, specifically written for the purpose of adoration or prayer, and typically addressed to a deity or deities, or to a prominent figure or personification. The word \"hymn\" derives from Greek (\"hymnos\"), which means \"a song of praise\". A writer of hymns is known as a hymnodist. The singing of hymns is called hymnody. Collections of hymns are known as hymnals or hymn books. Hymns may or may not include instrumental accompaniment.\n\nAlthough most familiar to speakers of English in the context of Christian churches, hymns are also a fixture of other world religions, especially on the Indian subcontinent. Hymns also survive from antiquity, especially from Egyptian and Greek cultures. Some of the oldest surviving examples of notated music are hymns with Greek texts.\n\nAncient hymns include the Egyptian \"Great Hymn to the Aten\", composed by Pharaoh Akhenaten; the \"Vedas\", a collection of hymns in the tradition of Hinduism; and the Psalms, a collection of songs from Judaism. The Western tradition of hymnody begins with the Homeric Hymns, a collection of ancient Greek hymns, the oldest of which were written in the 7th century BC, praising deities of the ancient Greek religions. Surviving from the 3rd century BC is a collection of six literary hymns () by the Alexandrian poet Callimachus.\n\nPatristic writers began applying the term , or \"hymnus\" in Latin, to Christian songs of praise, and frequently used the word as a synonym for \"psalm\".\n\nOriginally modeled on the Psalms and other poetic passages (commonly referred to as \"canticles\") in the Scriptures, Christian hymns are generally directed as praise to the Christian God. Many refer to Jesus Christ either directly or indirectly.\n\nSince the earliest times, Christians have sung \"psalms and hymns and spiritual songs\", both in private devotions and in corporate worship (; ; ; ; ; ; ; cf. ; ).\n\nOne definition of a hymn is \"...a lyric poem, reverently and devotionally conceived, which is designed to be sung and which expresses the worshipper's attitude toward God or God's purposes in human life. It should be simple and metrical in form, genuinely emotional, poetic and literary in style, spiritual in quality, and in its ideas so direct and so immediately apparent as to unify a congregation while singing it.\"\n\nChristian hymns are often written with special or seasonal themes and these are used on holy days such as Christmas, Easter and the Feast of All Saints, or during particular seasons such as Advent and Lent. Others are used to encourage reverence for the Holy Bible or to celebrate Christian practices such as the eucharist or baptism. Some hymns praise or address individual saints, particularly the Blessed Virgin Mary; such hymns are particularly prevalent in Catholicism, Eastern Orthodoxy and to some extent High Church Anglicanism.\n\nA writer of hymns is known as a hymnodist, and the practice of singing hymns is called \"hymnody\"; the same word is used for the collectivity of hymns belonging to a particular denomination or period (e.g. \"nineteenth century Methodist hymnody\" would mean the body of hymns written and/or used by Methodists in the 19th century). A collection of hymns is called a \"hymnal\" or \"hymnary\". These may or may not include music. A student of hymnody is called a \"hymnologist\", and the scholarly study of hymns, hymnists and hymnody is hymnology. The music to which a hymn may be sung is a hymn tune. \n\nIn many Evangelical churches, traditional songs are classified as hymns while more contemporary worship songs are not considered hymns. The reason for this distinction is unclear, but according to some it is due to the radical shift of style and devotional thinking that began with the Jesus movement and Jesus music.\n\nIn ancient and medieval times, stringed instruments such as the harp, lyre and lute were used with psalms and hymns.\n\nSince there is a lack of musical notation in early writings, the actual musical forms in the early church can only be surmised. During the Middle Ages a rich hymnody developed in the form of Gregorian chant or plainsong. This type was sung in unison, in one of eight church modes, and most often by monastic choirs. While they were written originally in Latin, many have been translated; a familiar example is the 4th century \"Of the Father's Heart Begotten\" sung to the 11th century plainsong \"Divinum Mysterium\".\n\nLater hymnody in the Western church introduced four-part vocal harmony as the norm, adopting major and minor keys, and came to be led by organ and choir. It shares many elements with classical music.\n\nToday, except for choirs, more musically inclined congregations and \"a cappella\" congregations, hymns are typically sung in unison. In some cases complementary full settings for organ are also published, in others organists and other accompanists are expected to transcribe the four-part vocal score for their instrument of choice.\n\nTo illustrate Protestant usage, in the traditional services and liturgies of the Methodist churches, which are based upon Anglican practice, hymns are sung (often accompanied by an organ) during the processional to the altar, during the receiving of communion, during the recessional, and sometimes at other points during the service. These hymns can be found in a common book such as the United Methodist Hymnal. The Doxology is also sung after the tithes and offerings are brought up to the altar.\n\nContemporary Christian worship, as often found in Evangelicalism and Pentecostalism, may include the use of contemporary worship music played with electric guitars and the drum kit, sharing many elements with rock music.\n\nOther groups of Christians have historically excluded instrumental accompaniment, citing the absence of instruments in worship by the church in the first several centuries of its existence, and adhere to an unaccompanied \"a cappella\" congregational singing of hymns. These groups include the 'Brethren' (often both 'Open' and 'Exclusive'), the Churches of Christ, Mennonites, Primitive Baptists, and certain Reformed churches, although during the last century or so, several of these, such as the Free Church of Scotland have abandoned this stance.\n\nEastern Christianity (the Eastern Orthodox, Oriental Orthodox and Eastern Catholic churches) have a very rich and ancient hymnographical tradition. \n\nEastern chant is almost always a cappella, and instrumental accompaniment is rare. The central form of chant in the Eastern Orthodoxy is Byzantine Chant, which is used to chant all forms of liturgical worship. Exceptions include the Coptic Orthodox tradition which makes use of the cymbals and the Triangle (musical instrument), the Indian Orthodox (Malankara Orthodox Syrian Church) which makes use of the organ and the Ethiopian Orthodox Tewahedo Church, which also uses drums, cymbals and other instruments on certain occasions.\n\nThomas Aquinas, in the introduction to his commentary on the Psalms, defined the Christian hymn thus: \"\"Hymnus est laus Dei cum cantico; canticum autem exultatio mentis de aeternis habita, prorumpens in vocem\".\" (\"A hymn is the praise of God with song; a song is the exultation of the mind dwelling on eternal things, bursting forth in the voice.\")\n\nThe Protestant Reformation resulted in two conflicting attitudes towards hymns. One approach, the regulative principle of worship, favoured by many Zwinglians, Calvinists and some radical reformers, considered anything that was not directly authorised by the Bible to be a novel and Catholic introduction to worship, which was to be rejected. All hymns that were not direct quotations from the Bible fell into this category. Such hymns were banned, along with any form of instrumental musical accompaniment, and organs were removed from churches. Instead of hymns, biblical psalms were chanted, most often without accompaniment, to very basic melodies. This was known as exclusive psalmody. Examples of this may still be found in various places, including in some of the Presbyterian churches of western Scotland. \nThe other Reformation approach, the normative principle of worship, produced a burst of hymn writing and congregational singing. Martin Luther is notable not only as a reformer, but as the author of many hymns including \"Ein feste Burg ist unser Gott\" (\"A Mighty Fortress Is Our God\"), which is sung today even by Catholics, and \"Gelobet seist du, Jesu Christ\" (\"Praise be to You, Jesus Christ\") for Christmas. Luther and his followers often used their hymns, or chorales, to teach tenets of the faith to worshipers. The first Protestant hymnal was published in Bohemia in 1532 by the Unitas Fratrum. Count Zinzendorf, the Lutheran leader of the Moravian Church in the 18th century wrote some 2,000 hymns. The earlier English writers tended to paraphrase biblical texts, particularly Psalms; Isaac Watts followed this tradition, but is also credited as having written the first English hymn which was not a direct paraphrase of Scripture.\nWatts (1674–1748), whose father was an Elder of a dissenter congregation, complained at age 16, that when allowed only psalms to sing, the faithful could not even sing about their Lord, Christ Jesus. His father invited him to see what he could do about it; the result was Watts' first hymn, \"Behold the glories of the Lamb\".\nFound in few hymnals today, the hymn has eight stanzas in common meter and is based on Revelation 5:6, 8, 9, 10, 12.\n\nRelying heavily on Scripture, Watts wrote metered texts based on New Testament passages that brought the Christian faith into the songs of the church. Isaac Watts has been called \"the father of English hymnody\", but Erik Routley sees him more as \"the liberator of English hymnody\", because his hymns, and hymns like them, moved worshipers beyond singing only Old Testament psalms, inspiring congregations and revitalizing worship.\n\nLater writers took even more freedom, some even including allegory and metaphor in their texts.\n\nCharles Wesley's hymns spread Methodist theology, not only within Methodism, but in most Protestant churches. He developed a new focus: expressing one's personal feelings in the relationship with God as well as the simple worship seen in older hymns. Wesley wrote:\n\nWesley's contribution, along with the Second Great Awakening in America led to a new style called gospel, and a new explosion of sacred music writing with Fanny Crosby, Lina Sandell, Philip Bliss, Ira D. Sankey, and others who produced testimonial music for revivals, camp meetings, and evangelistic crusades. The tune style or form is technically designated \"gospel songs\" as distinct from hymns. Gospel songs generally include a refrain (or chorus) and usually (though not always) a faster tempo than the hymns. As examples of the distinction, \"Amazing Grace\" is a hymn (no refrain), but \"How Great Thou Art\" is a gospel song. During the 19th century the gospel-song genre spread rapidly in Protestantism and, to a lesser but still definite extent, in Roman Catholicism; the gospel-song genre is unknown in the worship \"per se\" by Eastern Orthodox churches, which rely exclusively on traditional chants (a type of hymn).\n\nThe Methodist Revival of the 18th century created an explosion of hymn-writing in Welsh, which continued into the first half of the 19th century. The most prominent names among Welsh hymn-writers are William Williams Pantycelyn and Ann Griffiths. The second half of the 19th century witnessed an explosion of hymn tune composition and choir singing in Wales. \n\nAlong with the more classical sacred music of composers ranging from Mozart to Monteverdi, the Catholic Church continued to produce many popular hymns such as Lead, Kindly Light, Silent Night, O Sacrament Divine and Faith of our Fathers. \n\nMany churches today use contemporary worship music which includes a range of styles often influenced by popular music. This often leads to some conflict between older and younger congregants (see contemporary worship). This is not new; the Christian pop music style began in the late 1960s and became very popular during the 1970s, as young hymnists sought ways in which to make the music of their religion relevant for their generation.\n\nThis long tradition has resulted in a wide variety of hymns. Some modern churches include within hymnody the traditional hymn (usually describing God), contemporary worship music (often directed to God) and gospel music (expressions of one's personal experience of God). This distinction is not perfectly clear; and purists remove the second two types from the classification as hymns. It is a matter of debate, even sometimes within a single congregation, often between revivalist and traditionalist movements.\n\nAfrican-Americans developed a rich hymnody from spirituals during times of slavery to the modern, lively black gospel style. The first influences of African American Culture into hymns came from Slave Songs of the United States a collection of slave hymns complied by William Francis Allen who had difficulty pinning them down from the oral tradition, and though he succeeded, he points out the awe inspiring effect of the hymns when sung in by their originators.\n\nHymn writing, composition, performance and the publishing of Christian hymnals was prolific in the 19th-century and was often linked to the abolitionist movement by many hymn writers. Surprisingly, Stephen Foster wrote a number of hymns that were used during church services during this era of publishing.\n\nThomas Symmes spread throughout churches a new idea of how to sing hymns, in which anyone could sing a hymn any way they felt led to; this idea was opposed by the views of Symmes' colleagues who felt it was \"like Five Hundred different Tunes roared out at the same time\". William Billings, a singing school teacher, created the first tune book with only American born compositions. Within his books, Billings did not put as much emphasis on \"common measure\" which was the typical way hymns were sung, but he attempted \"to have a Sufficiency in each measure\". Boston's Handel and Haydn Society aimed at raising the level of church music in America, publishing their \"Collection of Church Music\". In the late 19th century Ira D. Sankey and Dwight L. Moody developed the relatively new subcategory of gospel hymns.\n\nThe meter indicates the number of syllables for the lines in each stanza of a hymn. This provides a means of marrying the hymn's text with an appropriate hymn tune for singing. In practice many hymns conform to one of a relatively small number of meters (syllable count and stress patterns). Care must be taken, however, to ensure that not only the metre of words and tune match, but also the stresses on the words in each line. Technically speaking an iambic tune, for instance, cannot be used with words of, say, trochaic metre.\n\nThe meter is often denoted by a row of figures besides the name of the tune, such as \"87.87.87\", which would inform the reader that each verse has six lines, and that the first line has eight syllables, the second has seven, the third line eight, etc. The meter can also be described by initials; L.M. indicates long meter, which is 88.88 (four lines, each eight syllables long); S.M. is short meter (66.86); C.M. is common metre (86.86), while D.L.M., D.S.M. and D.C.M. (the \"D\" stands for double) are similar to their respective single meters except that they have eight lines in a verse instead of four.\n\nAlso, if the number of syllables in one verse differ from another verse in the same hymn (e.g., the hymn \"I Sing a Song of the Saints of God\"), the meter is called Irregular.\n\nThe Sikh holy book, the Guru Granth Sahib Ji ( ), is a collection of hymns (Shabad) or \"Gurbani\" describing the qualities of God and why one should meditate on God's name. The \"Guru Granth Sahib\" is divided by their musical setting in different ragas into fourteen hundred and thirty pages known as \"Angs\" (limbs) in Sikh tradition. Guru Gobind Singh (1666–1708), the tenth guru, after adding Guru Tegh Bahadur's bani to the Adi Granth affirmed the sacred text as his successor, elevating it to \"Guru Granth Sahib\". The text remains the holy scripture of the Sikhs, regarded as the teachings of the Ten Gurus. The role of Guru Granth Sahib, as a source or guide of prayer, is pivotal in Sikh worship.\n\n\n\nThe links below are restricted to either material that is historical or resources that are non-denominational or inter-denominational. Denomination-specific resources are mentioned from the relevant denomination-specific articles. \n", "id": "13756", "title": "Hymn"}
{"url": "https://en.wikipedia.org/wiki?curid=13758", "text": "History of physics\n\nPhysics (from the Ancient Greek φύσις \"physis\" meaning \"nature\") is the fundamental branch of science that developed out of the study of nature and philosophy known, until around the end of the 19th century, as \"natural philosophy\". Today, physics is ultimately defined as the study of matter, energy and the relationships between them. Physics is, in some senses, the oldest and most basic pure science; its discoveries find applications throughout the natural sciences, since matter and energy are the basic constituents of the natural world. The other sciences are generally more limited in their scope and may be considered branches that have split off from physics to become sciences in their own right. Physics today may be divided loosely into classical physics and modern physics.\n\nElements of what became physics were drawn primarily from the fields of astronomy, optics, and mechanics, which were methodologically united through the study of geometry. These mathematical disciplines began in antiquity with the Babylonians and with Hellenistic writers such as Archimedes and Ptolemy. Ancient philosophy, meanwhile – including what was called \"physics\" – focused on explaining nature through ideas such as Aristotle's four types of \"cause\".\n\nThe move towards a rational understanding of nature began at least since the Archaic period in Greece (650–480 BCE) with the Pre-Socratic philosophers. The philosopher Thales of Miletus (7th and 6th centuries BCE), dubbed \"the Father of Science\" for refusing to accept various supernatural, religious or mythological explanations for natural phenomena, proclaimed that every event had a natural cause. Thales also made advancements in 580 BCE by suggesting that water is the basic element, experimenting with the attraction between magnets and rubbed amber and formulating the first recorded cosmologies. Anaximander, famous for his proto-evolutionary theory, disputed the Thales' ideas and proposed that rather than water, a substance called \"apeiron\" was the building block of all matter. Around 500 BCE, Heraclitus proposed that the only basic law governing the Universe was the principle of change and that nothing remains in the same state indefinitely. This observation made him one of the first scholars in ancient physics to address the role of time in the universe, a key and sometimes contentious concept in modern and present-day physics. The early physicist Leucippus (fl. first half of the 5th century BCE) adamantly opposed the idea of direct divine intervention in the universe, proposing instead that natural phenomena had a natural cause. Leucippus and his student Democritus were the first to develop the theory of atomism, the idea that everything is composed entirely of various imperishable, indivisible elements called atoms.\n\nDuring the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy slowly developed into an exciting and contentious field of study. Aristotle (, \"Aristotélēs\") (384 – 322 BCE), a student of Plato, promoted the concept that observation of physical phenomena could ultimately lead to the discovery of the natural laws governing them. Aristotle's writings cover physics, metaphysics, poetry, theater, music, logic, rhetoric, linguistics, politics, government, ethics, biology and zoology. He wrote the first work which refers to that line of study as \"Physics\" – in the 4th century BCE, Aristotle founded the system known as Aristotelian physics. He attempted to explain ideas such as motion (and gravity) with the theory of four elements. Aristotle believed that all matter was made up of aether, or some combination of four elements: earth, water, air, and fire. According to Aristotle, these four terrestrial elements are capable of inter-transformation and move toward their natural place, so a stone falls downward toward the center of the cosmos, but flames rise upward toward the circumference. Eventually, Aristotelian physics became enormously popular for many centuries in Europe, informing the scientific and scholastic developments of the Middle Ages. It remained the mainstream scientific paradigm in Europe until the time of Galileo Galilei and Isaac Newton.\n\nEarly in Classical Greece, knowledge that the Earth is spherical (\"round\") was common. Around 240 BCE, as the result a seminal experiment, Eratosthenes (276–194 BCE) accurately estimated its circumference. In contrast to Aristotle's geocentric views, Aristarchus of Samos (; c.310 – c.230 BCE) presented an explicit argument for a heliocentric model of the Solar system, i.e. for placing the Sun, not the Earth, at its centre. Seleucus of Seleucia, a follower of Aristarchus' heliocentric theory, stated that the Earth rotated around its own axis, which, in turn, revolved around the Sun. Though the arguments he used were lost, Plutarch stated that Seleucus was the first to prove the heliocentric system through reasoning.\n\nIn the 3rd century BCE, the Greek mathematician Archimedes of Syracuse ( (287–212 BCE) – generally considered to be the greatest mathematician of antiquity and one of the greatest of all time – laid the foundations of hydrostatics, statics and calculated the underlying mathematics of the lever. A leading scientist of classical antiquity, Archimedes also developed elaborate systems of pulleys to move large objects with a minimum of effort. The Archimedes' screw underpins modern hydroengineering, and his machines of war helped to hold back the armies of Rome in the First Punic War. Archimedes even tore apart the arguments of Aristotle and his metaphysics, pointing out that it was impossible to separate mathematics and nature and proved it by converting mathematical theories into practical inventions. Furthermore, in his work \"On Floating Bodies\", around 250 BCE, Archimedes developed the law of buoyancy, also known as Archimedes' Principle. In mathematics, Archimedes used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers. He also developed the principles of equilibrium states and centers of gravity, ideas that would influence the well known scholars, Galileo, and Newton.\n\nHipparchus (190–120 BCE), focusing on astronomy and mathematics, used sophisticated geometrical techniques to map the motion of the stars and planets, even predicting the times that Solar eclipses would happen. In addition, he added calculations of the distance of the Sun and Moon from the Earth, based upon his improvements to the observational instruments used at that time. Another of the most famous of the early physicists was Ptolemy (90–168 CE), one of the leading minds during the time of the Roman Empire. Ptolemy was the author of several scientific treatises, at least three of which were of continuing importance to later Islamic and European science. The first is the astronomical treatise now known as the \"Almagest\" (in Greek, Ἡ Μεγάλη Σύνταξις, \"The Great Treatise\", originally Μαθηματικὴ Σύνταξις, \"Mathematical Treatise\"). The second is the \"Geography\", which is a thorough discussion of the geographic knowledge of the Greco-Roman world.\n\nMuch of the accumulated knowledge of the ancient world was lost. Even of the works of the better known thinkers, few fragments survived. Although he wrote at least fourteen books, almost nothing of Hipparchus' direct work survived. Of the 150 reputed Aristotelian works, only 30 exist, and some of those are \"little more than lecture notes\".\n\nImportant physical and mathematical traditions also existed in ancient Chinese and Indian sciences.\n\nIn Indian philosophy, Maharishi Kanada was the first to systematically develop a theory of atomism around 200 BCE though some authors have allotted him an earlier era in the 6th century BCE. It was further elaborated by the Buddhist atomists Dharmakirti and Dignāga during the 1st millennium CE. Pakudha Kaccayana, a 6th-century BCE Indian philosopher and contemporary of Gautama Buddha, had also propounded ideas about the atomic constitution of the material world. These philosophers believed that other elements (except ether) were physically palpable and hence comprised minuscule particles of matter. The last minuscule particle of matter that could not be subdivided further was termed Parmanu. These philosophers considered the atom to be indestructible and hence eternal. The Buddhists thought atoms to be minute objects unable to be seen to the naked eye that come into being and vanish in an instant. The Vaisheshika school of philosophers believed that an atom was a mere point in space. Indian theories about the atom are greatly abstract and enmeshed in philosophy as they were based on logic and not on personal experience or experimentation. In Indian astronomy, Aryabhata's \"Aryabhatiya\" (499 CE) proposed the Earth's rotation, while Nilakantha Somayaji (1444–1544) of the Kerala school of astronomy and mathematics proposed a semi-heliocentric model resembling the Tychonic system.\n\nThe study of magnetism in Ancient China dates back to the 4th century BCE. (in the \"Book of the Devil Valley Master\"), A main contributor to this field was Shen Kuo (1031–1095), a polymath and statesman who was the first to describe the magnetic-needle compass used for navigation, as well as establishing the concept of true north. In optics, Shen Kuo independently developed a camera obscura.\n\nIn the 5th to 15th centuries, scientific progress occurred in the Muslim world. Many classic works in Latin and Greek were translated into Arabic. Ibn Sīnā (980–1037), known as \"Avicenna\", was a polymath from Bukhara (now in present-day Uzbekistan) responsible for important contributions to physics, optics, philosophy and medicine. He is most famous for writing \"The Canon of Medicine\", a text that was used to teach student doctors in Europe until the 1600s.\n\nImportant contributions were made by Ibn al-Haytham (965–1040), a mathematician from Basra (in present-day Iraq) considered one of the founders of modern optics. Ptolemy and Aristotle theorised that light either shone from the eye to illuminate objects or that light emanated from objects themselves, whereas al-Haytham (known by the Latin name Alhazen) suggested that light travels to the eye in rays from different points on an object. The works of Ibn al-Haytham and Abū Rayhān Bīrūnī, a Persian scientist, eventually passed on to Western Europe where they were studied by scholars such as Roger Bacon and Witelo. Omar Khayyám (1048–1131), a Persian scientist, calculated the length of a solar year and was only out by a fraction of a second when compared to our modern day calculations. He used this to compose a calendar considered more accurate than the Gregorian calendar that came along 500 years later. He is classified as one of the world's first great science communicators, said, for example to have convinced a Sufi theologian that the world turns on an axis.\n\nNasir al-Din al-Tusi (1201–1274), a Persian astronomer and mathematician who died in Baghdad, authored the \"Treasury of Astronomy\", a remarkably accurate table of planetary movements that reformed the existing planetary model of Roman astronomer Ptolemy by describing a uniform circular motion of all planets in their orbits. This work led to the later discovery, by one of his students, that planets actually have an elliptical orbit. Copernicus later drew heavily on the work of al-Din al-Tusi and his students, but without acknowledgment. The gradual chipping away of the Ptolemaic system paved the way for the revolutionary idea that the Earth actually orbited the Sun (heliocentrism).\n\nAwareness of ancient works re-entered the West through translations from Arabic to Latin. Their re-introduction, combined with Judeo-Islamic theological commentaries, had a great influence on Medieval philosophers such as Thomas Aquinas. Scholastic European scholars, who sought to reconcile the philosophy of the ancient classical philosophers with Christian theology, proclaimed Aristotle the greatest thinker of the ancient world. In cases where they didn't directly contradict the Bible, Aristotelian physics became the foundation for the physical explanations of the European Churches. Quantification became a core element of medieval physics.\n\nBased on Aristotelian physics, Scholastic physics described things as moving according to their essential nature. Celestial objects were described as moving in circles, because perfect circular motion was considered an innate property of objects that existed in the uncorrupted realm of the celestial spheres. The theory of impetus, the ancestor to the concepts of inertia and momentum, was developed along similar lines by medieval philosophers such as John Philoponus and Jean Buridan. Motions below the lunar sphere were seen as imperfect, and thus could not be expected to exhibit consistent motion. More idealized motion in the \"sublunary\" realm could only be achieved through artifice, and prior to the 17th century, many did not view artificial experiments as a valid means of learning about the natural world. Physical explanations in the sublunary realm revolved around tendencies. Stones contained the element earth, and earthly objects tended to move in a straight line toward the centre of the earth (and the universe in the Aristotelian geocentric view) unless otherwise prevented from doing so.\n\nDuring the 16th and 17th centuries, a large advancement of scientific progress known as the Scientific revolution took place in Europe. Dissatisfaction with older philosophical approaches had begun earlier and had produced other changes in society, such as the Protestant Reformation, but the revolution in science began when natural philosophers began to mount a sustained attack on the Scholastic philosophical program and supposed that mathematical descriptive schemes adopted from such fields as mechanics and astronomy could actually yield universally valid characterizations of motion and other concepts.\n\nA breakthrough in astronomy was made by Polish astronomer Nicolaus Copernicus (1473–1543) when, in 1543, he proposed a heliocentric model of the Solar system, ostensibly as a means to render tables charting planetary motion more accurate and to simplify their production. In heliocentric models of the Solar system, the Earth orbits the Sun along with other bodies in Earth's galaxy, a contradiction according to the Greek-Egyptian astronomer Ptolemy (2nd century CE; see above), whose system placed the Earth at the center of the Universe and had been accepted for over 1,400 years. The Greek astronomer Aristarchus of Samos (c.310 – c.230 BCE) had suggested that the Earth revolves around the Sun, but Copernicus' theory was the first to be accepted as a valid scientific possibility. Copernicus' book presenting the theory (\"De revolutionibus orbium coelestium\", \"On the Revolutions of the Celestial Spheres\") was published just before his death in 1543 and, as it is now generally considered to mark the beginning of modern astronomy, is also considered to mark the beginning of the Scientific revolution. Copernicus' new perspective, along with the accurate observations made by Tycho Brahe, enabled German astronomer Johannes Kepler (1571–1630) to formulate his laws regarding planetary motion that remain in use today.\n\nThe Italian mathematician, astronomer, and physicist Galileo Galilei (1564–1642) was the central figure in the Scientific revolution and famous for his support for Copernicanism, his astronomical discoveries, empirical experiments and his improvement of the telescope. As a mathematician, Galileo's role in the university culture of his era was subordinated to the three major topics of study: law, medicine, and theology (which was closely allied to philosophy). Galileo, however, felt that the descriptive content of the technical disciplines warranted philosophical interest, particularly because mathematical analysis of astronomical observations – notably, Copernicus' analysis of the relative motions of the Sun, Earth, Moon, and planets – indicated that philosophers' statements about the nature of the universe could be shown to be in error. Galileo also performed mechanical experiments, insisting that motion itself – regardless of whether it was produced \"naturally\" or \"artificially\" (i.e. deliberately) – had universally consistent characteristics that could be described mathematically.\n\nGalileo's early studies at the University of Pisa were in medicine, but he was soon drawn to mathematics and physics. At 19, he discovered (and, subsequently, verified) the isochronal nature of the pendulum when, using his pulse, he timed the oscillations of a swinging lamp in Pisa's cathedral and found that it remained the same for each swing regardless of the swing's amplitude. He soon became known through his invention of a hydrostatic balance and for his treatise on the center of gravity of solid bodies. While teaching at the University of Pisa (1589–92), he initiated his experiments concerning the laws of bodies in motion that brought results so contradictory to the accepted teachings of Aristotle that strong antagonism was aroused. He found that bodies do not fall with velocities proportional to their weights. The famous story in which Galileo is said to have dropped weights from the Leaning Tower of Pisa is apocryphal, but he did find that the path of a projectile is a parabola and is credited with conclusions that anticipated Newton's laws of motion (e.g. the notion of inertia). Among these is what is now called Galilean relativity, the first precisely formulated statement about properties of space and time outside three-dimensional geometry.\n\nGalileo has been called the \"father of modern observational astronomy\", the \"father of modern physics\", the \"father of science\", and \"the father of modern science\". According to Stephen Hawking, \"Galileo, perhaps more than any other single person, was responsible for the birth of modern science.\" As religious orthodoxy decreed a geocentric or Tychonic understanding of the Solar system, Galileo's support for heliocentrism provoked controversy and he was tried by the Inquisition. Found \"vehemently suspect of heresy\", he was forced to recant and spent the rest of his life under house arrest.\n\nThe contributions that Galileo made to observational astronomy include the telescopic confirmation of the phases of Venus; his discovery, in 1609, of Jupiter's four largest moons (subsequently given the collective name of the \"Galilean moons\"); and the observation and analysis of sunspots. Galileo also pursued applied science and technology, inventing, among other instruments, a military compass. His discovery of the Jovian moons was published in 1610 and enabled him to obtain the position of mathematician and philosopher to the Medici court. As such, he was expected to engage in debates with philosophers in the Aristotelian tradition and received a large audience for his own publications such as the \"Discourses and Mathematical Demonstrations Concerning Two New Sciences\" (published abroad following his arrest for the publication of \"Dialogue Concerning the Two Chief World Systems\") and \"The Assayer\". Galileo's interest in experimenting with and formulating mathematical descriptions of motion established experimentation as an integral part of natural philosophy. This tradition, combining with the non-mathematical emphasis on the collection of \"experimental histories\" by philosophical reformists such as William Gilbert and Francis Bacon, drew a significant following in the years leading up to and following Galileo's death, including Evangelista Torricelli and the participants in the Accademia del Cimento in Italy; Marin Mersenne and Blaise Pascal in France; Christiaan Huygens in the Netherlands; and Robert Hooke and Robert Boyle in England.\n\nThe French philosopher René Descartes (1596–1650) was well-connected to, and influential within, the experimental philosophy networks of the day. Descartes had a more ambitious agenda, however, which was geared toward replacing the Scholastic philosophical tradition altogether. Questioning the reality interpreted through the senses, Descartes sought to re-establish philosophical explanatory schemes by reducing all perceived phenomena to being attributable to the motion of an invisible sea of \"corpuscles\". (Notably, he reserved human thought and God from his scheme, holding these to be separate from the physical universe). In proposing this philosophical framework, Descartes supposed that different kinds of motion, such as that of planets versus that of terrestrial objects, were not fundamentally different, but were merely different manifestations of an endless chain of corpuscular motions obeying universal principles. Particularly influential were his explanations for circular astronomical motions in terms of the vortex motion of corpuscles in space (Descartes argued, in accord with the beliefs, if not the methods, of the Scholastics, that a vacuum could not exist), and his explanation of gravity in terms of corpuscles pushing objects downward.\n\nDescartes, like Galileo, was convinced of the importance of mathematical explanation, and he and his followers were key figures in the development of mathematics and geometry in the 17th century. Cartesian mathematical descriptions of motion held that all mathematical formulations had to be justifiable in terms of direct physical action, a position held by Huygens and the German philosopher Gottfried Leibniz, who, while following in the Cartesian tradition, developed his own philosophical alternative to Scholasticism, which he outlined in his 1714 work, \"The Monadology\". Descartes has been dubbed the 'Father of Modern Philosophy', and much subsequent Western philosophy is a response to his writings, which are studied closely to this day. In particular, his \"Meditations on First Philosophy\" continues to be a standard text at most university philosophy departments. Descartes' influence in mathematics is equally apparent; the Cartesian coordinate system — allowing algebraic equations to be expressed as geometric shapes in a two-dimensional coordinate system — was named after him. He is credited as the father of analytical geometry, the bridge between algebra and geometry, important to the discovery of calculus and analysis.\n\nThe late 17th and early 18th centuries saw the achievements of the greatest figure of the Scientific revolution: Cambridge University physicist and mathematician Sir Isaac Newton (1642-1727), considered by many to be the greatest and most influential scientist who ever lived. Newton, a fellow of the Royal Society of England, combined his own discoveries in mechanics and astronomy to earlier ones to create a single system for describing the workings of the universe. Newton formulated three laws of motion and the law of universal gravitation, the latter of which could be used to explain the behavior not only of falling bodies on the earth but also planets and other celestial bodies in the heavens. To arrive at his results, Newton invented one form of an entirely new branch of mathematics: calculus (also invented independently by Gottfried Leibniz), which was to become an essential tool in much of the later development in most branches of physics. Newton's findings were set forth in his \"Philosophiæ Naturalis Principia Mathematica\" (\"Mathematical Principles of Natural Philosophy\"), the publication of which in 1687 marked the beginning of the modern period of mechanics and astronomy.\n\nNewton was able to refute the Cartesian mechanical tradition that all motions should be explained with respect to the immediate force exerted by corpuscles. Using his three laws of motion and law of universal gravitation, Newton removed the idea that objects followed paths determined by natural shapes and instead demonstrated that not only regularly observed paths, but all the future motions of any body could be deduced mathematically based on knowledge of their existing motion, their mass, and the forces acting upon them. However, observed celestial motions did not precisely conform to a Newtonian treatment, and Newton, who was also deeply interested in theology, imagined that God intervened to ensure the continued stability of the solar system.\n\nNewton's principles (but not his mathematical treatments) proved controversial with Continental philosophers, who found his lack of metaphysical explanation for movement and gravitation philosophically unacceptable. Beginning around 1700, a bitter rift opened between the Continental and British philosophical traditions, which were stoked by heated, ongoing, and viciously personal disputes between the followers of Newton and Leibniz concerning priority over the analytical techniques of calculus, which each had developed independently. Initially, the Cartesian and Leibnizian traditions prevailed on the Continent (leading to the dominance of the Leibnizian calculus notation everywhere except Britain). Newton himself remained privately disturbed at the lack of a philosophical understanding of gravitation, while insisting in his writings that none was necessary to infer its reality. As the 18th century progressed, Continental natural philosophers increasingly accepted the Newtonians' willingness to forgo ontological metaphysical explanations for mathematically described motions.\n\nNewton built the first functioning reflecting telescope and developed a theory of color, published in \"Opticks\", based on the observation that a prism decomposes white light into the many colours forming the visible spectrum. While Newton explained light as being composed of tiny particles, a rival theory of light which explained its behavior in terms of waves was presented in 1690 by Christiaan Huygens. However, the belief in the mechanistic philosophy coupled with Newton's reputation meant that the wave theory saw relatively little support until the 19th century. Newton also formulated an empirical law of cooling, studied the speed of sound, investigated power series, demonstrated the generalised binomial theorem and developed a method for approximating the roots of a function. His work on infinite series was inspired by Simon Stevin's decimals. Most importantly, Newton showed that the motions of objects on Earth and of celestial bodies are governed by the same set of natural laws, which were neither capricious nor malevolent. By demonstrating the consistency between Kepler's laws of planetary motion and his own theory of gravitation, Newton also removed the last doubts about heliocentrism. By bringing together all the ideas set forth during the Scientific revolution, Newton effectively established the foundation for modern society in mathematics and science.\n\nOther branches of physics also received attention during the period of the Scientific revolution. William Gilbert, court physician to Queen Elizabeth I, published an important work on magnetism in 1600, describing how the earth itself behaves like a giant magnet. Robert Boyle (1627–91) studied the behavior of gases enclosed in a chamber and formulated the gas law named for him; he also contributed to physiology and to the founding of modern chemistry. Another important factor in the scientific revolution was the rise of learned societies and academies in various countries. The earliest of these were in Italy and Germany and were short-lived. More influential were the Royal Society of England (1660) and the Academy of Sciences in France (1666). The former was a private institution in London and included such scientists as John Wallis, William Brouncker, Thomas Sydenham, John Mayow, and Christopher Wren (who contributed not only to architecture but also to astronomy and anatomy); the latter, in Paris, was a government institution and included as a foreign member the Dutchman Huygens. In the 18th century, important royal academies were established at Berlin (1700) and at St. Petersburg (1724). The societies and academies provided the principal opportunities for the publication and discussion of scientific results during and after the scientific revolution. In 1690, James Bernoulli showed that the cycloid is the solution to the tautochrone problem; and the following year, in 1691, Johann Bernoulli showed that a chain freely suspended from two points will form a catenary, the curve with the lowest possible center of gravity available to any chain hung between two fixed points. He then showed, in 1696, that the cycloid is the solution to the brachistochrone problem.\n\nA precursor of the engine was designed by the German scientist Otto von Guericke who, in 1650, designed and built the world's first vacuum pump and created the world's first ever vacuum known as the Magdeburg hemispheres experiment. He was driven to make a vacuum to disprove Aristotle's long-held supposition that 'Nature abhors a vacuum'. Shortly thereafter, Irish physicist and chemist Boyle had learned of Guericke's designs and in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed the pressure-volume correlation for a gas: \"PV\" = \"k\", where \"P\" is pressure, \"V\" is volume and \"k\" is a constant: this relationship is known as Boyle's Law. In that time, air was assumed to be a system of motionless particles, and not interpreted as a system of moving molecules. The concept of thermal motion came two centuries later. Therefore, Boyle's publication in 1660 speaks about a mechanical concept: the air spring. Later, after the invention of the thermometer, the property temperature could be quantified. This tool gave Gay-Lussac the opportunity to derive his law, which led shortly later to the ideal gas law. But, already before the establishment of the ideal gas law, an associate of Boyle's named Denis Papin built in 1679 a bone digester, which is a closed vessel with a tightly fitting lid that confines steam until a high pressure is generated.\n\nLater designs implemented a steam release valve to keep the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and cylinder engine. He did not however follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time. Hence, prior to 1698 and the invention of the Savery Engine, horses were used to power pulleys, attached to buckets, which lifted water out of flooded salt mines in England. In the years to follow, more variations of steam engines were built, such as the Newcomen Engine, and later the Watt Engine. In time, these early engines would eventually be utilized in place of horses. Thus, each engine began to be associated with a certain amount of \"horse power\" depending upon how many horses it had replaced. The main problem with these first engines was that they were slow and clumsy, converting less than 2% of the input fuel into useful work. In other words, large quantities of coal (or wood) had to be burned to yield only a small fraction of work output. Hence the need for a new science of engine dynamics was born.\n\nDuring the 18th century, the mechanics founded by Newton was developed by several scientists as more mathematicians learned calculus and elaborated upon its initial formulation. The application of mathematical analysis to problems of motion was known as rational mechanics, or mixed mathematics (and was later termed classical mechanics).\n\nIn 1714, Brook Taylor derived the fundamental frequency of a stretched vibrating string in terms of its tension and mass per unit length by solving a differential equation. The Swiss mathematician Daniel Bernoulli (1700–1782) made important mathematical studies of the behavior of gases, anticipating the kinetic theory of gases developed more than a century later, and has been referred to as the first mathematical physicist. In 1733, Daniel Bernoulli derived the fundamental frequency and harmonics of a hanging chain by solving a differential equation. In 1734, Bernoulli solved the differential equation for the vibrations of an elastic bar clamped at one end. Bernoulli's treatment of fluid dynamics and his examination of fluid flow was introduced in his 1738 work \"Hydrodynamica\".\n\nRational mechanics dealt primarily with the development of elaborate mathematical treatments of observed motions, using Newtonian principles as a basis, and emphasized improving the tractability of complex calculations and developing of legitimate means of analytical approximation. A representative contemporary textbook was published by Johann Baptiste Horvath. By the end of the century analytical treatments were rigorous enough to verify the stability of the solar system solely on the basis of Newton's laws without reference to divine intervention—even as deterministic treatments of systems as simple as the three body problem in gravitation remained intractable. In 1705, Edmond Halley predicted the periodicity of Halley's Comet, William Herschel discovered Uranus in 1781, and Henry Cavendish measured the gravitational constant and determined the mass of the Earth in 1798. In 1783, John Michell suggested that some objects might be so massive that not even light could escape from them.\n\nIn 1739, Leonhard Euler solved the ordinary differential equation for a forced harmonic oscillator and noticed the resonance phenomenon. In 1742, Colin Maclaurin discovered his uniformly rotating self-gravitating spheroids. In 1742, Benjamin Robins published his \"New Principles in Gunnery\", establishing the science of aerodynamics. British work, carried on by mathematicians such as Taylor and Maclaurin, fell behind Continental developments as the century progressed. Meanwhile, work flourished at scientific academies on the Continent, led by such mathematicians as Bernoulli, Euler, Lagrange, Laplace, and Legendre. In 1743, Jean le Rond d'Alembert published his \"Traite de Dynamique\", in which he introduced the concept of generalized forces for accelerating systems and systems with constraints, and applied the new idea of virtual work to solve dynamical problem, now known as D'Alembert's principle, as a rival to Newton's second law of motion. In 1747, Pierre Louis Maupertuis applied minimum principles to mechanics. In 1759, Euler solved the partial differential equation for the vibration of a rectangular drum. In 1764, Euler examined the partial differential equation for the vibration of a circular drum and found one of the Bessel function solutions. In 1776, John Smeaton published a paper on experiments relating power, work, momentum and kinetic energy, and supporting the conservation of energy. In 1788, Joseph Louis Lagrange presented Lagrange's equations of motion in \"Mécanique Analytique\", in which the whole of mechanics was organized around the principle of virtual work. In 1789, Antoine Lavoisier states the law of conservation of mass. The rational mechanics developed in the 18th century received a brilliant exposition in both Lagrange's 1788 work and the \"Celestial Mechanics\" (1799–1825) of Pierre-Simon Laplace.\n\nDuring the 18th century, thermodynamics was developed through the theories of weightless \"imponderable fluids\", such as heat (\"caloric\"), electricity, and phlogiston (which was rapidly overthrown as a concept following Lavoisier's identification of oxygen gas late in the century). Assuming that these concepts were real fluids, their flow could be traced through a mechanical apparatus or chemical reactions. This tradition of experimentation led to the development of new kinds of experimental apparatus, such as the Leyden Jar; and new kinds of measuring instruments, such as the calorimeter, and improved versions of old ones, such as the thermometer. Experiments also produced new concepts, such as the University of Glasgow experimenter Joseph Black's notion of latent heat and Philadelphia intellectual Benjamin Franklin's characterization of electrical fluid as flowing between places of excess and deficit (a concept later reinterpreted in terms of positive and negative charges). Franklin also showed that lightning is electricity in 1752.\n\nThe accepted theory of heat in the 18th century viewed it as a kind of fluid, called caloric; although this theory was later shown to be erroneous, a number of scientists adhering to it nevertheless made important discoveries useful in developing the modern theory, including Joseph Black (1728–99) and Henry Cavendish (1731–1810). Opposed to this caloric theory, which had been developed mainly by the chemists, was the less accepted theory dating from Newton's time that heat is due to the motions of the particles of a substance. This mechanical theory gained support in 1798 from the cannon-boring experiments of Count Rumford (Benjamin Thompson), who found a direct relationship between heat and mechanical energy.\n\nWhile it was recognized early in the 18th century that finding absolute theories of electrostatic and magnetic force akin to Newton's principles of motion would be an important achievement, none were forthcoming. This impossibility only slowly disappeared as experimental practice became more widespread and more refined in the early years of the 19th century in places such as the newly established Royal Institution in London. Meanwhile, the analytical methods of rational mechanics began to be applied to experimental phenomena, most influentially with the French mathematician Joseph Fourier's analytical treatment of the flow of heat, as published in 1822. Joseph Priestley proposed an electrical inverse-square law in 1767, and Charles-Augustin de Coulomb introduced the inverse-square law of electrostatics in 1798.\n\nAt the end of the century, the members of the French Academy of Sciences had attained clear dominance in the field. At the same time, the experimental tradition established by Galileo and his followers persisted. The Royal Society and the French Academy of Sciences were major centers for the performance and reporting of experimental work. Experiments in mechanics, optics, magnetism, static electricity, chemistry, and physiology were not clearly distinguished from each other during the 18th century, but significant differences in explanatory schemes and, thus, experiment design were emerging. Chemical experimenters, for instance, defied attempts to enforce a scheme of abstract Newtonian forces onto chemical affiliations, and instead focused on the isolation and classification of chemical substances and reactions.\n\nIn 1800, Alessandro Volta invented the electric battery (known of the voltaic pile) and thus improved the way electric currents could also be studied. A year later, Thomas Young demonstrated the wave nature of light—which received strong experimental support from the work of Augustin-Jean Fresnel—and the principle of interference. In 1813, Peter Ewart supported the idea of the conservation of energy in his paper \"On the measure of moving force\". In 1820, Hans Christian Ørsted found that a current-carrying conductor gives rise to a magnetic force surrounding it, and within a week after Ørsted's discovery reached France, André-Marie Ampère discovered that two parallel electric currents will exert forces on each other. In 1821, William Hamilton began his analysis of Hamilton's characteristic function. In 1821, Michael Faraday built an electricity-powered motor, while Georg Ohm stated his law of electrical resistance in 1826, expressing the relationship between voltage, current, and resistance in an electric circuit. A year later, botanist Robert Brown discovered Brownian motion: pollen grains in water undergoing movement resulting from their bombardment by the fast-moving atoms or molecules in the liquid. In 1829, Gaspard Coriolis introduced the terms of work (force times distance) and kinetic energy with the meanings they have today.\n\nIn 1831, Faraday (and independently Joseph Henry) discovered the reverse effect, the production of an electric potential or current through magnetism – known as electromagnetic induction; these two discoveries are the basis of the electric motor and the electric generator, respectively. In 1834, Carl Jacobi discovered his uniformly rotating self-gravitating ellipsoids. In 1834, John Russell observed a nondecaying solitary water wave (soliton) in the Union Canal near Edinburgh and used a water tank to study the dependence of solitary water wave velocities on wave amplitude and water depth. In 1835, William Hamilton stated Hamilton's canonical equations of motion. In the same year, Gaspard Coriolis examined theoretically the mechanical efficiency of waterwheels, and deduced the Coriolis effect. In 1841, Julius Robert von Mayer, an amateur scientist, wrote a paper on the conservation of energy but his lack of academic training led to its rejection. In 1842, Christian Doppler proposed the Doppler effect. In 1847, Hermann von Helmholtz formally stated the law of conservation of energy. In 1851, Léon Foucault showed the Earth's rotation with a huge pendulum (Foucault pendulum).\n\nThere were important advances in continuum mechanics in the first half of the century, namely formulation of laws of elasticity for solids and discovery of Navier–Stokes equations for fluids.\n\nIn the 19th century, the connection between heat and mechanical energy was established quantitatively by Julius Robert von Mayer and James Prescott Joule, who measured the mechanical equivalent of heat in the 1840s. In 1849, Joule published results from his series of experiments (including the paddlewheel experiment) which show that heat is a form of energy, a fact that was accepted in the 1850s. The relation between heat and energy was important for the development of steam engines, and in 1824 the experimental and theoretical work of Sadi Carnot was published. Carnot captured some of the ideas of thermodynamics in his discussion of the efficiency of an idealized engine. Sadi Carnot's work provided a basis for the formulation of the first law of thermodynamics—a restatement of the law of conservation of energy—which was stated around 1850 by William Thomson, later known as Lord Kelvin, and Rudolf Clausius. Lord Kelvin, who had extended the concept of absolute zero from gases to all substances in 1848, drew upon the engineering theory of Lazare Carnot, Sadi Carnot, and Émile Clapeyron–as well as the experimentation of James Prescott Joule on the interchangeability of mechanical, chemical, thermal, and electrical forms of work—to formulate the first law.\n\nKelvin and Clausius also stated the second law of thermodynamics, which was originally formulated in terms of the fact that heat does not spontaneously flow from a colder body to a hotter. Other formulations followed quickly (for example, the second law was expounded in Thomson and Peter Guthrie Tait's influential work \"Treatise on Natural Philosophy\") and Kelvin in particular understood some of the law's general implications. The second Law was the idea that gases consist of molecules in motion had been discussed in some detail by Daniel Bernoulli in 1738, but had fallen out of favor, and was revived by Clausius in 1857. In 1850, Hippolyte Fizeau and Léon Foucault measured the speed of light in water and find that it is slower than in air, in support of the wave model of light. In 1852, Joule and Thomson demonstrated that a rapidly expanding gas cools, later named the Joule–Thomson effect or Joule–Kelvin effect. Hermann von Helmholtz puts forward the idea of the heat death of the universe in 1854, the same year that Clausius established the importance of \"dQ/T\" (Clausius's theorem) (though he did not yet name the quantity).\n\nIn 1859, James Clerk Maxwell discovered the distribution law of molecular velocities. Maxwell showed that electric and magnetic fields are propagated outward from their source at a speed equal to that of light and that light is one of several kinds of electromagnetic radiation, differing only in frequency and wavelength from the others. In 1859, Maxwell worked out the mathematics of the distribution of velocities of the molecules of a gas. The wave theory of light was widely accepted by the time of Maxwell's work on the electromagnetic field, and afterward the study of light and that of electricity and magnetism were closely related. In 1864 James Maxwell published his papers on a dynamical theory of the electromagnetic field, and stated that light is an electromagnetic phenomenon in the 1873 publication of Maxwell's \"Treatise on Electricity and Magnetism\". This work drew upon theoretical work by German theoreticians such as Carl Friedrich Gauss and Wilhelm Weber. The encapsulation of heat in particulate motion, and the addition of electromagnetic forces to Newtonian dynamics established an enormously robust theoretical underpinning to physical observations.\n\nThe prediction that light represented a transmission of energy in wave form through a \"luminiferous ether\", and the seeming confirmation of that prediction with Helmholtz student Heinrich Hertz's 1888 detection of electromagnetic radiation, was a major triumph for physical theory and raised the possibility that even more fundamental theories based on the field could soon be developed. Experimental confirmation of Maxwell's theory was provided by Hertz, who generated and detected electric waves in 1886 and verified their properties, at the same time foreshadowing their application in radio, television, and other devices. In 1887, Heinrich Hertz discovered the photoelectric effect. Research on the electromagnetic waves began soon after, with many scientists and inventors conducting experiments on their properties. In the mid to late 1890s Guglielmo Marconi developed a radio wave based wireless telegraphy system (see invention of radio).\n\nThe atomic theory of matter had been proposed again in the early 19th century by the chemist John Dalton and became one of the hypotheses of the kinetic-molecular theory of gases developed by Clausius and James Clerk Maxwell to explain the laws of thermodynamics. The kinetic theory in turn led to the statistical mechanics of Ludwig Boltzmann (1844–1906) and Josiah Willard Gibbs (1839–1903), which held that energy (including heat) was a measure of the speed of particles. Interrelating the statistical likelihood of certain states of organization of these particles with the energy of those states, Clausius reinterpreted the dissipation of energy to be the statistical tendency of molecular configurations to pass toward increasingly likely, increasingly disorganized states (coining the term \"entropy\" to describe the disorganization of a state). The statistical versus absolute interpretations of the second law of thermodynamics set up a dispute that would last for several decades (producing arguments such as \"Maxwell's demon\"), and that would not be held to be definitively resolved until the behavior of atoms was firmly established in the early 20th century. In 1902, James Jeans found the length scale required for gravitational perturbations to grow in a static nearly homogeneous medium.\n\nAt the end of the 19th century, physics had evolved to the point at which classical mechanics could cope with highly complex problems involving macroscopic situations; thermodynamics and kinetic theory were well established; geometrical and physical optics could be understood in terms of electromagnetic waves; and the conservation laws for energy and momentum (and mass) were widely accepted. So profound were these and other developments that it was generally accepted that all the important laws of physics had been discovered and that, henceforth, research would be concerned with clearing up minor problems and particularly with improvements of method and measurement. However, around 1900 serious doubts arose about the completeness of the classical theories—the triumph of Maxwell's theories, for example, was undermined by inadequacies that had already begun to appear—and their inability to explain certain physical phenomena, such as the energy distribution in blackbody radiation and the photoelectric effect, while some of the theoretical formulations led to paradoxes when pushed to the limit. Prominent physicists such as Hendrik Lorentz, Emil Cohn, Ernst Wiechert and Wilhelm Wien believed that some modification of Maxwell's equations might provide the basis for all physical laws. These shortcomings of classical physics were never to be resolved and new ideas were required. At the beginning of the 20th century a major revolution shook the world of physics, which led to a new era, generally referred to as modern physics.\n\nIn the 19th century, experimenters began to detect unexpected forms of radiation: Wilhelm Röntgen caused a sensation with his discovery of X-rays in 1895; in 1896 Henri Becquerel discovered that certain kinds of matter emit radiation on their own accord. In 1897, J. J. Thomson discovered the electron, and new radioactive elements found by Marie and Pierre Curie raised questions about the supposedly indestructible atom and the nature of matter. Marie and Pierre coined the term \"radioactivity\" to describe this property of matter, and isolated the radioactive elements radium and polonium. Ernest Rutherford and Frederick Soddy identified two of Becquerel's forms of radiation with electrons and the element helium. Rutherford identified and named two types of radioactivity and in 1911 interpreted experimental evidence as showing that the atom consists of a dense, positively charged nucleus surrounded by negatively charged electrons. Classical theory, however, predicted that this structure should be unstable. Classical theory had also failed to explain successfully two other experimental results that appeared in the late 19th century. One of these was the demonstration by Albert A. Michelson and Edward W. Morley—known as the Michelson–Morley experiment—which showed there did not seem to be a preferred frame of reference, at rest with respect to the hypothetical luminiferous ether, for describing electromagnetic phenomena. Studies of radiation and radioactive decay continued to be a preeminent focus for physical and chemical research through the 1930s, when the discovery of nuclear fission opened the way to the practical exploitation of what came to be called \"atomic\" energy.\n\nIn 1905 a young, 26-year-old German physicist (then a Bern patent clerk) named Albert Einstein (1879–1955), showed how measurements of time and space are affected by motion between an observer and what is being observed. To say that Einstein's radical theory of relativity revolutionized science is no exaggeration. Although Einstein made many other important contributions to science, the theory of relativity alone represents one of the greatest intellectual achievements of all time. Although the concept of relativity was not introduced by Einstein, his major contribution was the recognition that the speed of light in a vacuum is constant, i.e. the same for all observers, and an absolute physical boundary for motion. This does not impact a person's day-to-day life since most objects travel at speeds much slower than light speed. For objects travelling near light speed, however, the theory of relativity shows that clocks associated with those objects will run more slowly and that the objects shorten in length according to measurements of an observer on Earth. Einstein also derived the famous equation, \"E\" = \"mc\", which expresses the equivalence of mass and energy.\n\nEinstein argued that the speed of light was a constant in all inertial reference frames and that electromagnetic laws should remain valid independent of reference frame—assertions which rendered the ether \"superfluous\" to physical theory, and that held that observations of time and length varied relative to how the observer was moving with respect to the object being measured (what came to be called the \"special theory of relativity\"). It also followed that mass and energy were interchangeable quantities according to the equation \"E\"=\"mc\". In another paper published the same year, Einstein asserted that electromagnetic radiation was transmitted in discrete quantities (\"quanta\"), according to a constant that the theoretical physicist Max Planck had posited in 1900 to arrive at an accurate theory for the distribution of blackbody radiation—an assumption that explained the strange properties of the photoelectric effect.\n\nThe special theory of relativity is a formulation of the relationship between physical observations and the concepts of space and time. The theory arose out of contradictions between electromagnetism and Newtonian mechanics and had great impact on both those areas. The original historical issue was whether it was meaningful to discuss the electromagnetic wave-carrying \"ether\" and motion relative to it and also whether one could detect such motion, as was unsuccessfully attempted in the Michelson–Morley experiment. Einstein demolished these questions and the ether concept in his special theory of relativity. However, his basic formulation does not involve detailed electromagnetic theory. It arises out of the question: \"What is time?\" Newton, in the \"Principia\" (1686), had given an unambiguous answer: \"Absolute, true, and mathematical time, of itself, and from its own nature, flows equably without relation to anything external, and by another name is called duration.\" This definition is basic to all classical physics.\n\nEinstein had the genius to question it, and found that it was incomplete. Instead, each \"observer\" necessarily makes use of his or her own scale of time, and for two observers in relative motion, their time-scales will differ. This induces a related effect on position measurements. Space and time become intertwined concepts, fundamentally dependent on the observer. Each observer presides over his or her own space-time framework or coordinate system. There being no absolute frame of reference, all observers of given events make different but equally valid (and reconcilable) measurements. What remains absolute is stated in Einstein's relativity postulate: \"The basic laws of physics are identical for two observers who have a constant relative velocity with respect to each other.\"\n\nSpecial Relativity had a profound effect on physics: started as a rethinking of the theory of electromagnetism, it found a new symmetry law of nature, now called \"Poincaré symmetry\", that replaced the old Galilean (see above) symmetry.\n\nSpecial Relativity exerted another long-lasting effect on dynamics. Although initially it was credited with the \"unification of mass and energy\", it became evident that relativistic dynamics established a firm \"distinction\" between rest mass, which is an invariant (observer independent) property of a particle or system of particles, and the energy and momentum of a system. The latter two are separately conserved in all situations but not invariant with respect to different observers. The term \"mass\" in particle physics underwent a semantic change, and since the late 20th century it almost exclusively denotes the rest (or \"invariant\") mass. See mass in special relativity for additional discussion.\n\nBy 1916, Einstein was able to generalize this further, to deal with all states of motion including non-uniform acceleration, which became the general theory of relativity. In this theory Einstein also specified a new concept, the curvature of space-time, which described the gravitational effect at every point in space. In fact, the curvature of space-time completely replaced Newton's universal law of gravitation. According to Einstein, gravitational force in the normal sense is a kind of illusion caused by the geometry of space. The presence of a mass causes a curvature of space-time in the vicinity of the mass, and this curvature dictates the space-time path that all freely-moving objects must follow. It was also predicted from this theory that light should be subject to gravity - all of which was verified experimentally. This aspect of relativity explained the phenomena of light bending around the sun, predicted black holes as well as properties of the Cosmic microwave background radiation — a discovery rendering fundamental anomalies in the classic Steady-State hypothesis. For his work on relativity, the photoelectric effect and blackbody radiation, Einstein received the Nobel Prize in 1921.\n\nThe gradual acceptance of Einstein's theories of relativity and the quantized nature of light transmission, and of Niels Bohr's model of the atom created as many problems as they solved, leading to a full-scale effort to reestablish physics on new fundamental principles. Expanding relativity to cases of accelerating reference frames (the \"general theory of relativity\") in the 1910s, Einstein posited an equivalence between the inertial force of acceleration and the force of gravity, leading to the conclusion that space is curved and finite in size, and the prediction of such phenomena as gravitational lensing and the distortion of time in gravitational fields.\n\nAlthough relativity resolved the electromagnetic phenomena conflict demonstrated by Michelson and Morley, a second theoretical problem was the explanation of the distribution of electromagnetic radiation emitted by a black body; experiment showed that at shorter wavelengths, toward the ultraviolet end of the spectrum, the energy approached zero, but classical theory predicted it should become infinite. This glaring discrepancy, known as the ultraviolet catastrophe, was solved by the new theory of quantum mechanics. Quantum mechanics is the theory of atoms and subatomic systems. Approximately the first 30 years of the 20th century represent the time of the conception and evolution of the theory. The basic ideas of quantum theory were introduced in 1900 by Max Planck (1858–1947), who was awarded the Nobel Prize for Physics in 1918 for his discovery of the quantified nature of energy. The quantum theory (which previously relied in the \"correspondence\" at large scales between the quantized world of the atom and the continuities of the \"classical\" world) was accepted when the Compton Effect established that light carries momentum and can scatter off particles, and when Louis de Broglie asserted that matter can be seen as behaving as a wave in much the same way as electromagnetic waves behave like particles (wave–particle duality).\n\nIn 1905, Einstein used the quantum theory to explain the photoelectric effect, and in 1913 the Danish physicist Niels Bohr used the same constant to explain the stability of Rutherford's atom as well as the frequencies of light emitted by hydrogen gas. The quantized theory of the atom gave way to a full-scale quantum mechanics in the 1920s. New principles of a \"quantum\" rather than a \"classical\" mechanics, formulated in matrix-form by Werner Heisenberg, Max Born, and Pascual Jordan in 1925, were based on the probabilistic relationship between discrete \"states\" and denied the possibility of causality. Quantum mechanics was extensively developed by Heisenberg, Wolfgang Pauli, Paul Dirac, and Erwin Schrödinger, who established an equivalent theory based on waves in 1926; but Heisenberg's 1927 \"uncertainty principle\" (indicating the impossibility of precisely and simultaneously measuring position and momentum) and the \"Copenhagen interpretation\" of quantum mechanics (named after Bohr's home city) continued to deny the possibility of fundamental causality, though opponents such as Einstein would metaphorically assert that \"God does not play dice with the universe\". The new quantum mechanics became an indispensable tool in the investigation and explanation of phenomena at the atomic level. Also in the 1920s, the Indian scientist Satyendra Nath Bose's work on photons and quantum mechanics provided the foundation for Bose–Einstein statistics, the theory of the Bose–Einstein condensate.\n\nFermions are particles \"like electrons and nucleons\" and are the usual constituents of matter. Fermi–Dirac statistics later found numerous other uses, from astrophysics (see Degenerate matter) to semiconductor design.\n\nAs the philosophically inclined continued to debate the fundamental nature of the universe, quantum theories continued to be produced, beginning with Paul Dirac's formulation of a relativistic quantum theory in 1928. However, attempts to quantize electromagnetic theory entirely were stymied throughout the 1930s by theoretical formulations yielding infinite energies. This situation was not considered adequately resolved until after World War II ended, when Julian Schwinger, Richard Feynman and Sin-Itiro Tomonaga independently posited the technique of renormalization, which allowed for an establishment of a robust quantum electrodynamics (QED).\n\nMeanwhile, new theories of fundamental particles proliferated with the rise of the idea of the quantization of fields through \"exchange forces\" regulated by an exchange of short-lived \"virtual\" particles, which were allowed to exist according to the laws governing the uncertainties inherent in the quantum world. Notably, Hideki Yukawa proposed that the positive charges of the nucleus were kept together courtesy of a powerful but short-range force mediated by a particle with a mass between that of the electron and proton. This particle, the \"pion\", was identified in 1947 as part of what became a slew of particles discovered after World War II. Initially, such particles were found as ionizing radiation left by cosmic rays, but increasingly came to be produced in newer and more powerful particle accelerators.\n\nOutside particle physics, significant advances of the time were:\n\nEinstein deemed that all fundamental interactions in nature can be explained in a single theory. Unified field theories were numerous attempts to \"merge\" several interactions. One of formulations of such theories (as well as field theories in general) is a \"gauge theory\", a generalization of the idea of symmetry. Eventually the Standard Model (see below) succeeded in unification of strong, weak, and electromagnetic interactions. All attempts to unify gravitation with something else failed.\nThe interaction of these particles by scattering and decay provided a key to new fundamental quantum theories. Murray Gell-Mann and Yuval Ne'eman brought some order to these new particles by classifying them according to certain qualities, beginning with what Gell-Mann referred to as the \"Eightfold Way\". While its further development, the quark model, at first seemed inadequate to describe strong nuclear forces, allowing the temporary rise of competing theories such as the S-Matrix, the establishment of quantum chromodynamics in the 1970s finalized a set of fundamental and exchange particles, which allowed for the establishment of a \"standard model\" based on the mathematics of gauge invariance, which successfully described all forces except for gravitation, and which remains generally accepted within its domain of application.\n\nThe Standard Model groups the electroweak interaction theory and quantum chromodynamics into a structure denoted by the gauge group SU(3)×SU(2)×U(1). The formulation of the unification of the electromagnetic and weak interactions in the standard model is due to Abdus Salam, Steven Weinberg and, subsequently, Sheldon Glashow. Electroweak theory was later confirmed experimentally (by observation of neutral weak currents), and distinguished by the 1979 Nobel Prize in Physics.\n\nSince the 1970s, fundamental particle physics has provided insights into early universe cosmology, particularly the Big Bang theory proposed as a consequence of Einstein's general theory of relativity. However, starting in the 1990s, astronomical observations have also provided new challenges, such as the need for new explanations of galactic stability (\"dark matter\") and the apparent acceleration in the expansion of the universe (\"dark energy\").\n\nWhile accelerators have confirmed most aspects of the Standard Model by detecting expected particle interactions at various collision energies, no theory reconciling general relativity with the Standard Model has yet been found, although supersymmetry and string theory were believed by many theorists to be a promising avenue forward. The Large Hadron Collider, however, which began operating in 2008, has failed to find any evidence whatsoever that is supportive of supersymmetry and string theory.\n\nCosmology may be said to have become a serious research question with the publication of Einstein's General Theory of Relativity in 1916 [1915?] although it did not enter the scientific mainstream until the period known as the \"Golden age of general relativity\".\n\nAbout a decade later, in the midst of what was dubbed the \"Great Debate\", Hubble and Slipher discovered the expansion of universe in the 1920s measuring the redshifts of Doppler spectra from galactic nebulae. Using Einstein's general relativity, Lemaître and Gamow formulated what would become known as the big bang theory. A rival, called the steady state theory was devised by Hoyle, Gold, Narlikar and Bondi.\n\nCosmic background radiation was verified in the 1960s by Penzias and Wilson, and this discovery favoured the big bang at the expense of the steady state scenario. Later work was by Smoot et al. (1989), among other contributors, using data from the Cosmic Background explorer (CoBE) and the Wilkinson Microwave Anisotropy Probe (WMAP) satellites that refined these observations. The 1980s (the same decade of the COBE measurements) also saw the proposal of inflation theory by Guth.\n\nRecently the problems of dark matter and dark energy have risen to the top of the cosmology agenda.\n\nOn July 4, 2012, physicists working at CERN's Large Hadron Collider announced that they had discovered a new subatomic particle greatly resembling the Higgs boson, a potential key to an understanding of why elementary particles have mass and indeed to the existence of diversity and life in the universe. For now, some physicists are calling it a \"Higgslike\" particle. Joe Incandela, of the University of California, Santa Barbara, said, \"It's something that may, in the end, be one of the biggest observations of any new phenomena in our field in the last 30 or 40 years, going way back to the discovery of quarks, for example.\" Michael Turner, a cosmologist at the University of Chicago and the chairman of the physics center board, said:\n\nPeter Higgs was one of six physicists, working in three independent groups, who, in 1964, invented the notion of the Higgs field (\"cosmic molasses\"). The others were Tom Kibble of Imperial College, London; Carl Hagen of the University of Rochester; Gerald Guralnik of Brown University; and François Englert and Robert Brout, both of Université libre de Bruxelles.\n\nAlthough they have never been seen, Higgslike fields play an important role in theories of the universe and in string theory. Under certain conditions, according to the strange accounting of Einsteinian physics, they can become suffused with energy that exerts an antigravitational force. Such fields have been proposed as the source of an enormous burst of expansion, known as inflation, early in the universe and, possibly, as the secret of the dark energy that now seems to be speeding up the expansion of the universe.\n\nWith increased accessibility to and elaboration upon advanced analytical techniques in the 19th century, physics was defined as much, if not more, by those techniques than by the search for universal principles of motion and energy, and the fundamental nature of matter. Fields such as acoustics, geophysics, astrophysics, aerodynamics, plasma physics, low-temperature physics, and solid-state physics joined optics, fluid dynamics, electromagnetism, and mechanics as areas of physical research. In the 20th century, physics also became closely allied with such fields as electrical, aerospace and materials engineering, and physicists began to work in government and industrial laboratories as much as in academic settings. Following World War II, the population of physicists increased dramatically, and came to be centered on the United States, while, in more recent decades, physics has become a more international pursuit than at any time in its previous history.\n\nThe following is a gallery of highly influential and important figures in the history of physics. For a list that includes even more people, see list of physicists. \n\n\n\n", "id": "13758", "title": "History of physics"}
{"url": "https://en.wikipedia.org/wiki?curid=13761", "text": "Hydrofoil\n\nA hydrofoil is a lifting surface, or foil, that operates in water. They are similar in appearance and purpose to aerofoils used by aeroplanes. Boats that use hydrofoil technology are also simply termed hydrofoils. As a hydrofoil craft gains speed, the hydrofoils lift the boat's hull out of the water, decreasing drag and allowing greater speeds.\n\nThe hydrofoil usually consists of a wing like structure mounted on struts below the hull, or across the keels of a catamaran in a variety of boats (see illustration). As a hydrofoil equipped watercraft increases in speed, the hydrofoil elements below the hull(s) develop enough lift to raise the hull out of the water, which greatly reduces hull drag. This provides a corresponding increase in speed and fuel efficiency.\n\nWider adoption of hydrofoils is prevented by the increased complexity of building and maintaining them. Hydrofoils are generally prohibitively more expensive than conventional watercraft. However, the design is simple enough that there are many human-powered hydrofoil designs. Amateur experimentation and development of the concept is popular.\n\nSince air and water are governed by similar fluid equations—albeit with different levels of viscosity, density, and compressibility—the hydrofoil and airfoil (both types of foil) create lift in identical ways. The foil shape moves smoothly through the water, deflecting the flow downward, which, following the Euler equations, exerts an upward force on the foil. This turning of the water creates higher pressure on the bottom of the foil and reduced pressure on the top. This pressure difference is accompanied by a velocity difference, via Bernoulli's principle, so the resulting flowfield about the foil has a higher average velocity on one side than the other.\n\nWhen used as a lifting element on a hydrofoil boat, this upward force lifts the body of the vessel, decreasing drag and increasing speed. The lifting force eventually balances with the weight of the craft, reaching a point where the hydrofoil no longer lifts out of the water but remains in equilibrium. Since wave resistance and other impeding forces such as various types of drag (physics) on the hull are eliminated as the hull lifts clear, turbulence and drag act increasingly on the much smaller surface area of the hydrofoil, and decreasingly on the hull, creating a marked increase in speed.\n\nEarly hydrofoils used V-shaped foils. Hydrofoils of this type are known as \"surface-piercing\" since portions of the V-shape hydrofoils rise above the water surface when foilborne. Some modern hydrofoils use fully submerged inverted T-shape foils. Fully submerged hydrofoils are less subject to the effects of wave action, and, therefore, more stable at sea and more comfortable for crew and passengers. This type of configuration, however, is not self-stabilizing. The angle of attack on the hydrofoils must be adjusted continuously to changing conditions, a control process performed by sensors, a computer, and active surfaces.\n\nItalian inventor Enrico Forlanini began work on hydrofoils in 1898 and used a \"ladder\" foil system. Forlanini obtained patents in Britain and the United States for his ideas and designs.\n\nBetween 1899 and 1901, British boat designer John Thornycroft worked on a series of models with a stepped hull and single bow foil. In 1909 his company built the full scale long boat, \"Miranda III\". Driven by a engine, it rode on a bowfoil and flat stern. The subsequent \"Miranda IV\" was credited with a speed of .\n\nA March 1906 Scientific American article by American hydrofoil pioneer William E. Meacham explained the basic principle of hydrofoils. Alexander Graham Bell considered the invention of the hydroplane a very significant achievement, and after reading the article began to sketch concepts of what is now called a hydrofoil boat. With his chief engineer Casey Baldwin, Bell began hydrofoil experiments in the summer of 1908. Baldwin studied the work of the Italian inventor Enrico Forlanini and began testing models based on those designs, which led to the development of hydrofoil watercraft. During Bell's world tour of 1910–1911, Bell and Baldwin met with Forlanini in Italy, where they rode in his hydrofoil boat over Lake Maggiore. Baldwin described it as being as smooth as flying.\n\nOn returning to Bell's large laboratory at his Beinn Bhreagh estate near Baddeck, Nova Scotia, they experimented with a number of designs, culminating in Bell's \"HD-4\". Using Renault engines, a top speed of was achieved, accelerating rapidly, taking waves without difficulty, steering well and showing good stability. Bell's report to the United States Navy permitted him to obtain two 260 kW (350 hp) engines. On 9 September 1919 the \"HD-4\" set a world marine speed record of , which stood for two decades. A full-scale replica of the \"HD-4\" is viewable at the Alexander Graham Bell National Historic Site museum in Baddeck, Nova Scotia.\n\nIn the early 1950s an English couple built the \"White Hawk\", a jet-powered hydrofoil water craft, in an attempt to beat the absolute water speed record. However, in tests, \"White Hawk\" could barely top the record breaking speed of the 1919 \"HD-4\". The designers had faced an engineering phenomenon that limits the top speed of even modern hydrofoils: cavitation disturbs the lift created by the foils as they move through the water at speed above , bending the lifting foil.\n\nGerman engineer Hanns von Schertel worked on hydrofoils prior to and during World War II in Germany. After the war, the Russians captured Schertel's team. As Germany was not authorized to build fast boats, Schertel went to Switzerland, where he established the Supramar company. In 1952, Supramar launched the first commercial hydrofoil, PT10 \"Freccia d'Oro\" (Golden Arrow), in Lake Maggiore, between Switzerland and Italy. The PT10 is of surface-piercing type, it can carry 32 passengers and travel at . In 1968, the Bahraini born banker Hussain Najadi acquired the Supramar AG and expanded its operations into Japan, Hong Kong, Singapore, the UK, Norway and the US. General Dynamics of the United States became its licensee, and the Pentagon awarded its first R&D naval research project in the field of supercavitation. Hitachi Shipbuilding of Osaka, Japan, was another licensee of Supramar, as well as many leading ship owners and shipyards in the OECD countries.\n\nFrom 1952 to 1971, Supramar designed many models of hydrofoils: PT20, PT50, PT75, PT100 and PT150. All are of surface-piercing type, except the PT150 combining a surface-piercing foil forward with a fully submerged foil in the aft location. Over 200 of Supramar's design were built, most of them by Rodriquez in Sicily, Italy.\n\nDuring the same period the Soviet Union experimented extensively with hydrofoils, constructing hydrofoil river boats and ferries with streamlined designs during the cold war period and into the 1980s. Such vessels include the Raketa (1957) type, followed by the larger Meteor type and the smaller Voskhod type. One of the most successful Soviet designer/inventor in this area was Rostislav Alexeyev, who some consider the 'father' of the modern hydrofoil due to his 1950's era high speed hydrofoil designs. Later, circa 1970's, Alexeyev combined his hydrofoil experience with the surface effect principle to create the Ekranoplan.\n\nIn 1961, SRI International issued a study on \"The Economic Feasibility of Passenger Hydrofoil Craft in US Domestic and Foreign Commerce\". Commercial use of hydrofoils in the US first appeared in 1961 when two commuter vessels were commissioned by Harry Gale Nye, Jr.'s North American Hydrofoils to service the route from Atlantic Highlands, New Jersey to the financial district of Lower Manhattan.\n\nA 17-ton German craft \"VS-6 Hydrofoil\" was designed and constructed in 1940, completed in 1941 for use as a mine layer, it was tested in the Baltic Sea, producing speeds of 47 knots. Tested against a standard E-boat over the next three years it performed well but was not brought into production. Being faster it could carry a higher payload and was capable of travelling over minefields but was prone to damage and noisier.\n\nIn Canada during World War II, Baldwin worked on an experimental smoke laying hydrofoil (later called the Comox Torpedo) that was later superseded by other smoke-laying technology and an experimental target-towing hydrofoil. The forward two foil assemblies of what is believed to be the latter hydrofoil were salvaged in the mid-1960s from a derelict hulk in Baddeck, Nova Scotia by Colin MacGregor Stevens. These were donated to the Maritime Museum in Halifax, Nova Scotia. The Canadian Armed Forces built and tested a number of hydrofoils (e.g., Baddeck and two vessels named Bras d'Or), which culminated in the high-speed anti-submarine hydrofoil HMCS Bras d'Or in the late 1960s. However, the program was cancelled in the early 1970s due to a shift away from anti-submarine warfare by the Canadian military. The Bras d'Or was a surface-piercing type that performed well during her trials, reaching a maximum speed of .\n\nThe USSR introduced several hydrofoil-based fast attack craft into their navy, principally:\n\nThe US Navy began experiments with hydrofoils in the mid-1950s by funding a sailing vessel that used hydrofoils to reach speeds in the 30 mph range. The \"XCH-4\" (officially, \"Experimental Craft, Hydrofoil No. 4\"), designed by William P. Carl, exceeded speeds of and was mistaken for a seaplane due to its shape. The US Navy implemented a small number of combat hydrofoils, such as the \"Pegasus\" class, from 1977 through 1993. These hydrofoils were fast and well armed, and were capable of sinking all but the largest surface vessels.\n\nThe Italian Navy has used six hydrofoils of the \"Sparviero\" class since the late 1970s. These were armed with a 76 mm gun and two missiles, and were capable of speeds up to . Three similar boats were built for the Japan Maritime Self-Defense Force.\n\nThe French experimental sail powered hydrofoil \"Hydroptère\" is the result of a research project that involves advanced engineering skills and technologies. In September 2009, the \"Hydroptère\" set new sailcraft world speed records in the 500 m category, with a speed of and in the one nautical mile (1.9 km) category with a speed of .\n\nAnother trimaran sailboat is the Windrider Rave. The Rave is a commercially available , two person, hydrofoil trimaran, capable of reaching speeds of . The boat was designed by Jim Brown.\n\nThe Moth dinghy has evolved into some radical foil configurations.\n\nHobie Sailboats produced a production foiling trimaran, the Hobie Trifoiler, the fastest production sailboat. Trifoilers have clocked speeds upward of thirty knots.\n\nA new kayak design, called Flyak, has hydrofoils that lift the kayak enough to significantly reduce drag, allowing speeds of up to . Some surfers have developed surfboards with hydrofoils called foilboards, specifically aimed at surfing big waves further out to sea.\n\nSoviet-built Voskhods are one of the most successful passenger hydrofoil designs. Manufactured in Russia and Ukraine, they are in service in more than 20 countries. The most recent model, Voskhod-2M FFF, also known as Eurofoil, was built in Feodosiya for the Dutch public transport operator Connexxion.\n\nThe Boeing 929 is widely used in Asia for passenger services between the many islands of Japan, between Hong Kong and Macau and on the Korean peninsula.\n\nCurrent operators of hydrofoils include:\n\n\n\nSee also the history of Condor Ferries, which operated six hydrofoil ferries over a 29-year period between the Channel Islands, south coast of England and Saint-Malo.\n\nHydrofoils had their peak in popularity in the 1960s and 70s. Since then there has been a steady decline in their use and popularity for leisure, military and commercial passenger transport use. There are a number of reasons for this:\n\n\n", "id": "13761", "title": "Hydrofoil"}
{"url": "https://en.wikipedia.org/wiki?curid=13763", "text": "Henri Chopin\n\nHenri Chopin (18 June 1922 – 3 January 2008) was an avant-garde poet and musician.\n\nHenri Chopin was born in Paris,18 June 1922, one of three brothers, and the son of an accountant. Both his siblings died during the war. One was shot by a German soldier the day after an armistice was declared in Paris, the other while sabotaging a train (Acquaviva 2008).\n\nChopin was a French practitioner of concrete and sound poetry, well known throughout the second half of the 20th century. His work, though iconoclastic, remained well within the historical spectrum of poetry as it moved from a spoken tradition to the printed word and now back to the spoken word again (Wendt 1996, 112). He created a large body of pioneering recordings using early tape recorders, studio technologies and the sounds of the manipulated human voice. His emphasis on sound is a reminder that language stems as much from oral traditions as from classic literature, of the relationship of balance between order and chaos.\n\nChopin is significant above all for his diverse spread of creative achievement, as well as for his position as a focal point of contact for the international arts. As poet, painter, graphic artist and designer, typographer, independent publisher, filmmaker, broadcaster and arts promoter, Chopin's work is a barometer of the shifts in European media between the 1950s and the 1970s. \nIn 1964 he created \"OU\", one of the most notable reviews of the second half of the 20th century, and he ran it until 1974. \"OU\"'s contributors included William S. Burroughs, Brion Gysin, Gil J Wolman, François Dufrêne, Bernard Heidsieck, John Furnival, Tom Phillips, and the Austrian sculptor, writer and Dada pioneer Raoul Hausmann.\n\nHis books included \"Le Dernier Roman du Monde\" (1971), \"Portrait des 9\" (1975), \"The Cosmographical Lobster\" (1976), \"Poésie Sonore Internationale\" (1979), \"Les Riches Heures de l'Alphabet\" (1992) and \"Graphpoemesmachine\" (2006). Henri also created many graphic works on his typewriter: the typewriter poems (also known as dactylopoèmes) feature in international art collections such as those of Francesco Conz in Verona, the Morra Foundation in Naples and Ruth and Marvin Sackner in Miami, and have been the subject of Australian, British and French retrospectives (Acquaviva 2008).\n\nHis publication and design of the classic audio-visual magazines \"Cinquième Saison\" and \"OU\" between 1958 and 1974, each issue containing recordings as well as texts, images, screenprints and multiples, brought together international contemporary writers and artists such as members of Lettrisme and Fluxus, Jiri Kolar, Ian Hamilton Finlay, Tom Phillips, Brion Gysin, William S. Burroughs and many others, as well as bringing the work of survivors from earlier generations such as Raoul Hausmann and Marcel Janco to a fresh audience.\n\nFrom 1968 to 1986 Henri Chopin lived in Ingatestone, Essex, but with the death of his wife Jean in 1985, he moved back to France.\n\nIn 2001 with his health failing, he returned to England, living with his daughter and family at Dereham, Norfolk (Acquaviva 2008).\n\nChopin's \"poesie sonore\" aesthetics included a deliberate cultivation of a \"barbarian\" approach in production, using raw or crude sound manipulations to explore the area between\ndistortion and intelligibility. He avoided high-quality, professional recording machines, preferring to use very basic equipment and \"bricolage\" methods, such as sticking matchsticks in the erase heads of a second-hand tape recorder, or manually interfering with the tape path (Wendt 1985, 16–17).\n\n\n\n\n\n\n", "id": "13763", "title": "Henri Chopin"}
{"url": "https://en.wikipedia.org/wiki?curid=13764", "text": "Hassium\n\nHassium is a chemical element with symbol Hs and atomic number 108, named after the German state of Hesse. It is a synthetic element (an element that can be created in a laboratory but is not found in nature) and radioactive; the most stable known isotope, Hs, has a half-life of approximately 9.7 seconds, although an unconfirmed metastable state, Hs, may have a longer half-life of about 130 seconds. More than 100 atoms of hassium have been synthesized to date.\n\nIn the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 8 elements: it is thus the sixth member of the 6d series of transition metals. Chemistry experiments have confirmed that hassium behaves as the heavier homologue to osmium in group 8. The chemical properties of hassium are characterized only partly, but they compare well with the chemistry of the other group 8 elements. In bulk quantities, hassium is expected to be a silvery metal that reacts readily with oxygen in the air, forming a volatile tetroxide.\n\nThe synthesis of element 108 was first attempted in 1978 by a Russian research team led by Yuri Oganessian and Vladimir Utyonkov at the Joint Institute for Nuclear Research (JINR) in Dubna, using reactions that would generate the isotopes hassium-270 and hassium-264. The data was uncertain and they carried out new experiments on hassium five years later, where these two isotopes as well as hassium-263 were produced; the hassium-264 experiment was repeated again and confirmed in 1984.\n\nHassium was officially discovered in 1984 by a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt. The team bombarded a target of lead-208 with accelerated nuclei of iron-58 to produce 3 atoms of the isotope hassium-265.\n\nDue to this issue, a controversy arose over who should be recognized as the official discoverer of the element. The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report. They stated that the GSI collaboration was \"more detailed and, of itself, carries conviction\", and that while the combined data from Dubna and Darmstadt confirmed that hassium had been synthesized, the major credit was awarded to the GSI. This statement came in spite of the combined data also supporting the Russian 1983 discovery claim and the TWG also acknowledging that \"very probably element 108 played a role in the Dubna experiment.\"\n\nThe name \"hassium\" was proposed by Peter Armbruster and his colleagues, the officially recognised German discoverers, in 1992, derived from the Latin name (\"Hassia\") for the German state of Hesse where the institute is located. Using Mendeleev's nomenclature for unnamed and undiscovered elements, hassium should be known as \"eka-osmium\". In 1979, during the Transfermium Wars (but before the synthesis of hassium), IUPAC published recommendations according to which the element was to be called \"unniloctium\" (with the corresponding symbol of \"Uno\"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it \"element 108\", with the symbol of \"(108)\" or even simply \"108\", or used the proposed name \"hassium\".\n\nIn 1994 a committee of IUPAC recommended that element 108 be named \"hahnium\" (Hn) after the German physicist Otto Hahn, after an older suggestion of \"ottohahnium\" (Oh) in spite of the long-standing convention to give the discoverer the right to suggest a name, so that elements named after Hahn and Lise Meitner (meitnerium) would be next to each other, honoring their joint discovery of nuclear fission. This was because they felt that Hesse did not merit an element being named after it. After protests from the German discoverers and the American Chemical Society, IUPAC relented and the name \"hassium\" (Hs) was adopted internationally in 1997.\n\nHassium is not known to occur naturally on Earth; the half-lives of all its known isotopes are short enough that no primordial hassium would have survived to the present day. This does not rule out the possibility of unknown longer-lived isotopes or nuclear isomers existing, some of which could still exist in trace quantities today if they are long-lived enough. In the early 1960s, it was predicted that long-lived deformed isomers of hassium might occur naturally on Earth in trace quantities. This was theorized in order to explain the extreme radiation damage in some minerals that could not have been caused by any known natural radioisotopes, but could have been caused by superheavy elements.\n\nIn 1963, Soviet scientist Victor Cherdyntsev, who had previously claimed the existence of primordial curium-247, claimed to have discovered element 108 (specifically, the Hs isotope, which supposedly had a half life of 400 to 500 million years) in natural molybdenite and suggested the name \"sergenium\" (symbol Sg; at the time, this symbol had not yet been taken by seaborgium) for it, after the ancient city of Serik along the Silk Road in Kazakhstan where his molybdenite samples came from. His rationale for claiming that sergenium was the heavier homologue to osmium was that minerals supposedly containing sergenium formed volatile oxides when boiled in nitric acid, similarly to osmium. His findings were criticized by V. M. Kulakov on the grounds that some of the properties Cherdyntsev claimed sergenium had were inconsistent with the then-current nuclear physics.\n\nThe chief questions raised by Kulakov were that the claimed alpha decay energy of sergenium was many orders of magnitude lower than expected and the half-life given was eight orders of magnitude shorter than what would be predicted for a nuclide alpha decaying with the claimed decay energy, but at the same time a corrected half-life in the region of 10 years would be impossible as it would imply that the samples contained about 100 milligrams of sergenium. In 2003 it was suggested that the observed alpha decay with energy 4.5 MeV could be due to a low-energy and strongly enhanced transition between different hyperdeformed states of a hassium isotope around Hs, thus suggesting that the existence of superheavy elements in nature was at least possible, although unlikely.\n\nIn 2004, the Joint Institute for Nuclear Research conducted a search for natural hassium. This was done underground to avoid interference and false positives from cosmic rays, but no results have been released, strongly implying that no natural hassium was found. The possible extent of primordial hassium on Earth is uncertain; it might now only exist in traces, or could even have completely decayed by now after having caused the radiation damage long ago.\n\nIn 2006, it was hypothesized that an isomer of Hs might have a half-life of around (2.5±0.5)×10 y, which would explain the observation of alpha particles with energies of around 4.4 MeV in some samples of molybdenite and osmiridium. This isomer of Hs could be produced from the beta decay of Bh and Sg, which, being homologous to rhenium and molybdenum respectively, should occur in molybdenite along with rhenium and molybdenum if they occurred in nature. Since hassium is homologous to osmium, it should also occur along with osmium in osmiridium if it occurred in nature. The decay chains of Bh and Sg are very hypothetical and the predicted half-life of this hypothetical hassium isomer is not long enough for any sufficient quantity to remain on Earth. It is possible that more Hs may be deposited on the Earth as the Solar System travels through the spiral arms of the Milky Way, which would also explain excesses of plutonium-239 found on the floors of the Pacific Ocean and the Gulf of Finland, but minerals enriched with Hs are predicted to also have excesses of uranium-235 and lead-207, and would have different proportions of elements that are formed during spontaneous fission, such as krypton, zirconium, and xenon. Thus, the occurrence of hassium in nature in minerals such as molybdenite and osmiride is theoretically possible, but highly unlikely.\n\nHassium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes have been reported with atomic masses from 263 to 277 (with the exceptions of 272, 274, and 276), four of which, hassium-265, hassium-267, hassium-269, and hassium-277, have known metastable states (although that of hassium-277 is unconfirmed). Most of these decay predominantly through alpha decay, but some also undergo spontaneous fission.\n\nThe lightest isotopes, which usually have shorter half-lives were synthesized by direct fusion between two lighter nuclei and as decay products. The heaviest isotope produced by direct fusion is Hs; heavier isotopes have only been observed as decay products of elements with larger atomic numbers. In 1999, American scientists at the University of California, Berkeley, announced that they had succeeded in synthesizing three atoms of Og. These parent nuclei were reported to have successively emitted three alpha particles to form hassium-273 nuclei, which were claimed to have undergone an alpha decay, emitting alpha particles with decay energies of 9.78 and 9.47 MeV and half-life 1.2 s, but their claim was retracted in 2001. The isotope was successfully produced in 2010 by the same team. The new data matched the previous (fabricated) data.\n\nAccording to calculations, 108 is a proton magic number for deformed nuclei (nuclei that are far from spherical), and 162 is a neutron magic number for deformed nuclei. This means that such nuclei are permanently deformed in their ground state but have high, narrow fission barriers to further deformation and hence relatively long life-times to spontaneous fission. The spontaneous fission half-lives in this region are typically reduced by a factor of 10 in comparison with those in the vicinity of the spherical doubly magic nucleus Fl, caused by the narrower fission barrier for such deformed nuclei. Hence, the nucleus Hs has promise as a deformed doubly magic nucleus. Experimental data from the decay of the darmstadtium (Z=110) isotopes Ds and Ds provides strong evidence for the magic nature of the N=162 sub-shell. The recent synthesis of Hs, Hs, and Hs also fully support the assignment of N=162 as a magic number. In particular, the low decay energy for Hs is in complete agreement with calculations.\n\nEvidence for the magicity of the Z=108 proton shell can be obtained from two sources: the variation in the partial spontaneous fission half-lives for isotones and the large gap in the alpha Q value for isotonic nuclei of hassium and darmstadtium. For spontaneous fission, it is necessary to measure the half-lives for the isotonic nuclei Sg, Hs and Ds. Since the isotopes Sg and Ds are not currently known, and fission of Hs has not been measured, this method cannot yet be used to confirm the stabilizing nature of the Z=108 shell. Good evidence for the magicity of the Z=108 shell can nevertheless be found from the large differences in the alpha decay energies measured for Hs, Ds and Ds. More conclusive evidence would come from the determination of the decay energy for the unknown nucleus Ds.\n\nVarious calculations show that hassium should be the heaviest known group 8 element, consistent with the periodic law. Its properties should generally match those expected for a heavier homologue of osmium, with a few deviations arising from relativistic effects.\n\nThe previous members of group 8 have relatively high melting points (Fe, 1538 °C; Ru, 2334 °C; Os, 3033 °C). Much like them, hassium is predicted to be a solid at room temperature, although the melting point of hassium has not been precisely calculated. Hassium should crystallize in the hexagonal close-packed structure (/ = 1.59), similarly to its lighter congener osmium. Pure metallic hassium is calculated to have a bulk modulus (resistance to uniform compression) comparable to that of diamond (442 GPa). Hassium is expected to have a bulk density of 40.7 g/cm, the highest of any of the 118 known elements and nearly twice the density of osmium, the most dense measured element, at 22.61 g/cm. This results from hassium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough hassium to measure this quantity would be impractical, and the sample would quickly decay. Osmium is the densest element of the first 6 periods, and its heavier congener hassium is expected to be the densest element of the first 7 periods.\n\nThe atomic radius of hassium is expected to be around 126 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Hs ion is predicted to have an electron configuration of [Rn] 5f 6d 7s, giving up a 6d electron instead of a 7s electron, which is the opposite of the behavior of its lighter homologues. On the other hand, the Hs ion is expected to have an electron configuration of [Rn] 5f 6d 7s, analogous to that calculated for the Os ion.\n\nHassium is the sixth member of the 6d series of transition metals and is expected to be much like the platinum group metals. Calculations on its ionization potentials, atomic radius, as well as radii, orbital energies, and ground levels of its ionized states are similar to that of osmium, implying that hassium's properties would resemble those of the other group 8 elements, iron, ruthenium, and osmium. Some of these properties were confirmed by gas-phase chemistry experiments. The group 8 elements portray a wide variety of oxidation states, but ruthenium and osmium readily portray their group oxidation state of +8 (the second-highest known oxidation state for any element, which is very rare for other elements) and this state becomes more stable as the group is descended. Thus hassium is expected to form a stable +8 state. Analogously to its lighter congeners, hassium is expected to also show other stable lower oxidation states, such as +6, +5, +4, +3, and +2. Indeed, hassium(IV) is expected to be more stable than hassium(VIII) in aqueous solution.\n\nThe group 8 elements show a very distinctive oxide chemistry which allows extrapolations to be made easily for hassium. All the lighter members have known or hypothetical tetroxides, MO. Their oxidising power decreases as one descends the group. FeO is not known due to its extraordinarily large electron affinity (the amount of energy released when an electron is added to a neutral atom or molecule to form a negative ion) which results in the formation of the well-known oxoanion ferrate(VI), . Ruthenium tetroxide, RuO, formed by oxidation of ruthenium(VI) in acid, readily undergoes reduction to ruthenate(VI), . Oxidation of ruthenium metal in air forms the dioxide, RuO. In contrast, osmium burns to form the stable tetroxide, OsO, which complexes with the hydroxide ion to form an osmium(VIII) -\"ate\" complex, [OsO(OH)]. Therefore, eka-osmium properties for hassium should be demonstrated by the formation of a stable, very volatile tetroxide HsO, which undergoes complexation with hydroxide to form a hassate(VIII), [HsO(OH)]. Ruthenium tetroxide and osmium tetroxide are both volatile, due to their symmetrical tetrahedral molecular geometry and their being charge-neutral; hassium tetroxide should similarly be a very volatile solid. The trend of the volatilities of the group 8 tetroxides is known to be RuO < OsO > HsO, which completely confirms the calculated results. In particular, the calculated enthalpies of adsorption (the energy required for the adhesion of atoms, molecules, or ions from a gas, liquid, or dissolved solid to a surface) of HsO, −(45.4 ± 1) kJ·mol on quartz, agrees very well with the experimental value of −(46 ± 2) kJ·mol.\n\nDespite the fact that the selection of a volatile hassium compound (hassium tetroxide) for gas-phase chemical studies was clear from the beginning, the chemical characterization of hassium was considered a difficult task for a long time. Although hassium isotopes were first synthesized in 1984, it was not until 1996 that a hassium isotope long-lived enough to allow chemical studies to be performed was synthesized. Unfortunately, this hassium isotope, Hs, was then synthesized indirectly from the decay of Cn; not only are indirect synthesis methods not favourable for chemical studies, but also the reaction that produced the isotope Cn had a low yield (its cross-section was only 1 pb), and thus did not provide enough hassium atoms for a chemical investigation. The direct synthesis of Hs and Hs in the reaction Cm(Mg,xn)Hs (x = 4 or 5) appeared more promising, as the cross-section for this reaction was somewhat larger, at 7 pb. This yield was still around ten times lower than that for the reaction used for the chemical characterization of bohrium. New techniques for irradiation, separation, and detection had to be introduced before hassium could be successfully characterized chemically as a typical member of group 8 in early 2001.\n\nRuthenium and osmium have very similar chemistry due to the lanthanide contraction, but iron shows some differences from them: for example, although ruthenium and osmium form stable tetroxides in which the metal is in the +8 oxidation state, iron does not. Consequently, in preparation for the chemical characterization of hassium, researches focused on ruthenium and osmium rather than iron, as hassium was expected to also be similar to ruthenium and osmium due to the actinide contraction. Nevertheless, in the planned experiment to study hassocene (Hs(CH)), ferrocene may also be used for comparison along with ruthenocene and osmocene.\nThe first chemistry experiments were performed using gas thermochromatography in 2001, using Os and Os as a reference. During the experiment, 5 hassium atoms were synthesized using the reaction Cm(Mg,5n)Hs. They were then thermalized and oxidized in a mixture of helium and oxygen gas to form the tetroxide.\n\nThe measured deposition temperature indicated that hassium(VIII) oxide is less volatile than osmium tetroxide, OsO, and places hassium firmly in group 8. However, the enthalpy of adsorption for HsO measured, (−46 ± 2) kJ/mol, was significantly lower than what was predicted, (−36.7 ± 1.5) kJ/mol, indicating that OsO was more volatile than HsO, contradicting earlier calculations, which implied that they should have very similar volatilities. For comparison, the value for OsO is (−39 ± 1) kJ/mol. It is possible that hassium tetroxide interacts differently with the different chemicals (silicon nitride and silicon dioxide) used for the detector; further research is required, including more accurate measurements of the nuclear properties of Hs and comparisons with RuO in addition to OsO.\n\nIn 2004 scientists reacted hassium tetroxide and sodium hydroxide to form sodium hassate(VIII), a reaction well-known with osmium. This was the first acid-base reaction with a hassium compound, forming sodium hassate(VIII):\n\nThe team from the University of Mainz are planning to study the electrodeposition of hassium atoms using the new TASCA facility at the GSI. The current aim is to use the reaction Ra(Ca,4n)Hs. In addition, scientists at the GSI are hoping to utilize TASCA to study the synthesis and properties of the hassium(II) compound hassocene, Hs(CH), using the reaction Ra(Ca,xn). This compound is analogous to the lighter ferrocene, ruthenocene, and osmocene, and is expected to have the two cyclopentadienyl rings in an eclipsed conformation like ruthenocene and osmocene and not in a staggered conformation like ferrocene. Hassocene was chosen because it has hassium in the low formal oxidation state of +2 (although the bonding between the metal and the rings is mostly covalent in metallocenes) rather than the high +8 state which had previously been investigated, and relativistic effects were expected to be stronger in the lower oxidation state. Many metals in the periodic table form metallocenes, so that trends could be more easily determined, and the highly symmetric structure of hassocene and its low number of atoms also make relativistic calculations easier. Hassocene should be a stable and highly volatile compound.\n", "id": "13764", "title": "Hassium"}
{"url": "https://en.wikipedia.org/wiki?curid=13765", "text": "Henry Kissinger\n\nHenry Alfred Kissinger (; born Heinz Alfred Kissinger ; May 27, 1923) is an American diplomat and political scientist. He served as National Security Advisor and later concurrently as United States Secretary of State in the administrations of presidents Richard Nixon and Gerald Ford. For his actions negotiating an unsuccessful ceasefire in Vietnam, Kissinger received the 1973 Nobel Peace Prize under controversial circumstances, with two members of the committee resigning in protest. Kissinger later sought, unsuccessfully, to return the prize. After his term, his advice has been sought by world leaders including subsequent U.S. presidents.\n\nA proponent of \"Realpolitik\", Kissinger played a prominent role in United States foreign policy between 1969 and 1977. During this period, he pioneered the policy of \"détente\" with the Soviet Union, orchestrated the opening of relations with the People's Republic of China, and negotiated the Paris Peace Accords, ending American involvement in the Vietnam War. Kissinger's \"Realpolitik\" resulted in controversial policies such as CIA involvement in Chile and U.S. support for Pakistan, despite its genocidal actions during the Bangladesh War. He is the founder and chairman of Kissinger Associates, an international consulting firm. Kissinger has been a prolific author of books on politics and international relations with over one dozen books authored.\n\nGeneral opinion of Henry Kissinger is strongly divided in the Western world. Several scholars have ranked him as the most effective U.S. Secretary of State since 1965, while some journalists, activists, and human rights lawyers have condemned him as a war criminal.\n\nKissinger was born Heinz Alfred Kissinger in Fürth, Bavaria, Germany, in 1923 during the Weimar Republic, to a family of German Jews. His father, Louis Kissinger (1887–1982), was a schoolteacher. His mother, Paula (Stern) Kissinger (1901–1998), from Leutershausen, was a homemaker. Kissinger has a younger brother, Walter Kissinger. The surname Kissinger was adopted in 1817 by his great-great-grandfather Meyer Löb, after the Bavarian spa town of Bad Kissingen. As a youth, Heinz enjoyed playing soccer, and played for the youth wing of his favorite club, SpVgg Fürth, which was one of the nation's best clubs at the time. In 1938, fleeing Nazi persecution, his family moved to London, England, before arriving in New York on September 5.\n\nKissinger spent his high school years in the Washington Heights section of upper Manhattan as part of the German Jewish immigrant community that resided there at the time. Although Kissinger assimilated quickly into American culture, he never lost his pronounced Frankish accent, due to childhood shyness that made him hesitant to speak. Following his first year at George Washington High School, he began attending school at night and worked in a shaving brush factory during the day.\n\nFollowing high school, Kissinger enrolled in the City College of New York, studying accounting. He excelled academically as a part-time student, continuing to work while enrolled. His studies were interrupted in early 1943, when he was drafted into the U.S. Army.\n\nKissinger underwent basic training at Camp Croft in Spartanburg, South Carolina. On June 19, 1943, while stationed in South Carolina, at the age of 20 years, he became a naturalized U.S. citizen. The army sent him to study engineering at Lafayette College, Pennsylvania, but the program was cancelled, and Kissinger was reassigned to the 84th Infantry Division. There, he made the acquaintance of Fritz Kraemer, a fellow immigrant from Germany who noted Kissinger's fluency in German and his intellect, and arranged for him to be assigned to the military intelligence section of the division. Kissinger saw combat with the division, and volunteered for hazardous intelligence duties during the Battle of the Bulge.\n\nDuring the American advance into Germany, Kissinger, only a private, was put in charge of the administration of the city of Krefeld, owing to a lack of German speakers on the division's intelligence staff. Within eight days he had established a civilian administration. Kissinger was then reassigned to the Counter Intelligence Corps, with the rank of sergeant. He was given charge of a team in Hanover assigned to tracking down Gestapo officers and other saboteurs, for which he was awarded the Bronze Star. In June 1945, Kissinger was made commandant of the Bensheim metro CIC detachment, Bergstrasse district of Hesse, with responsibility for de-Nazification of the district. Although he possessed absolute authority and powers of arrest, Kissinger took care to avoid abuses against the local population by his command.\n\nIn 1946, Kissinger was reassigned to teach at the European Command Intelligence School at Camp King, continuing to serve in this role as a civilian employee following his separation from the army.\n\nHenry Kissinger received his AB degree \"summa cum laude,\" Phi Beta Kappa in political science from Harvard College in 1950, where he lived in Adams House and studied under William Yandell Elliott. He received his MA and PhD degrees at Harvard University in 1951 and 1954, respectively. In 1952, while still studying at Harvard, he served as a consultant to the director of the Psychological Strategy Board. His doctoral dissertation was titled \"Peace, Legitimacy, and the Equilibrium (A Study of the Statesmanship of Castlereagh and Metternich)\".\n\nKissinger remained at Harvard as a member of the faculty in the Department of Government and, with Robert R. Bowie, co-founded the Center for International Affairs in 1958. In 1955, he was a consultant to the National Security Council's Operations Coordinating Board. During 1955 and 1956, he was also study director in nuclear weapons and foreign policy at the Council on Foreign Relations. He released his book \"Nuclear Weapons and Foreign Policy\" the following year. From 1956 to 1958 he worked for the Rockefeller Brothers Fund as director of its Special Studies Project. He was director of the Harvard Defense Studies Program between 1958 and 1971. He was also director of the Harvard International Seminar between 1951 and 1971. Outside of academia, he served as a consultant to several government agencies and think tanks, including the Operations Research Office, the Arms Control and Disarmament Agency, Department of State, and the Rand Corporation.\n\nKeen to have a greater influence on U.S. foreign policy, Kissinger became an advisor to Nelson Rockefeller and supported his bid for the Republican nomination for president in 1960, 1964, and 1968. After Richard Nixon won the presidency in 1968, he made Kissinger National Security Advisor.\n\nKissinger served as National Security Advisor and Secretary of State under President Richard Nixon, and continued as Secretary of State under Nixon's successor Gerald Ford.\n\nA proponent of \"Realpolitik\", Kissinger played a dominant role in United States foreign policy between 1969 and 1977. In that period, he extended the policy of \"détente\". This policy led to a significant relaxation in US–Soviet tensions and played a crucial role in 1971 talks with Chinese Premier Zhou Enlai. The talks concluded with a rapprochement between the United States and the People's Republic of China, and the formation of a new strategic anti-Soviet Sino-American alignment. He was jointly awarded the 1973 Nobel Peace Prize with Lê Đức Thọ for helping to establish a ceasefire and U.S. withdrawal from Vietnam. The ceasefire, however, was not durable, Thọ declined to accept the award and Kissinger appeared deeply ambivalent about it (donating his prize money to charity, not attending the award ceremony and later offering to return his prize medal). As National Security Advisor, in 1974 Kissinger directed the much-debated National Security Study Memorandum 200.\n\nAs National Security Advisor under Nixon, Kissinger pioneered the policy of \"détente\" with the Soviet Union, seeking a relaxation in tensions between the two superpowers. As a part of this strategy, he negotiated the Strategic Arms Limitation Talks (culminating in the SALT I treaty) and the Anti-Ballistic Missile Treaty with Leonid Brezhnev, General Secretary of the Soviet Communist Party. Negotiations about strategic disarmament were originally supposed to start under the Johnson Administration but were postponed in protest upon the invasion by Warsaw Pact troops of Czechoslovakia in August 1968.\n\nKissinger sought to place diplomatic pressure on the Soviet Union. He made two trips to the People's Republic of China in July and October 1971 (the first of which was made in secret) to confer with Premier Zhou Enlai, then in charge of Chinese foreign policy. According to Kissinger's book, \"The White House Years\", the first secret China trip was arranged through Pakistan's diplomatic and Presidential involvement, as there were no direct communication channels between the states. His trips paved the way for the groundbreaking 1972 summit between Nixon, Zhou, and Communist Party of China Chairman Mao Zedong, as well as the formalization of relations between the two countries, ending 23 years of diplomatic isolation and mutual hostility. The result was the formation of a tacit strategic anti-Soviet alliance between China and the United States.\n\nWhile Kissinger's diplomacy led to economic and cultural exchanges between the two sides and the establishment of Liaison Offices in the Chinese and American capitals, with serious implications for Indochinese matters, full normalization of relations with the People's Republic of China would not occur until 1979, because the Watergate scandal overshadowed the latter years of the Nixon presidency and because the United States continued to recognize the government of Taiwan.\n\nIn September 1989, the Wall Street Journal's John Fialka disclosed that Kissinger took a direct economic interest in US-China relations in March 1989 with the establishment of China Ventures, Inc., a Delaware limited partnership, of which he was chairman of the board and chief executive officer. A US$75 million investment in a joint venture with the Communist Party government's primary commercial vehicle at the time, China International Trust & Investment Corporation (CITIC), was its purpose. Board members were major clients of Kissinger Associates. Kissinger was criticised for not disclosing his role in the venture when called upon by ABC's Peter Jennings to comment the morning after the June 4, 1989, Tiananmen crackdown. Kissinger's position was generally supportive of Deng Xiaoping's clearance of the square and he opposed economic sanctions.\n\nKissinger's involvement in Indochina started prior to his appointment as National Security Adviser to Nixon. While still at Harvard, he had worked as a consultant on foreign policy to both the White House and State Department. Kissinger says that \"In August 1965 ... <nowiki>[</nowiki>Henry Cabot Lodge, Jr.<nowiki>]</nowiki>, an old friend serving as Ambassador to Saigon, had asked me to visit Vietnam as his consultant. I toured Vietnam first for two weeks in October and November 1965, again for about ten days in July 1966, and a third time for a few days in October 1966 ... Lodge gave me a free hand to look into any subject of my choice\". He became convinced of the meaninglessness of military victories in Vietnam, \"... unless they brought about a political reality that could survive our ultimate withdrawal\". In a 1967 peace initiative, he would mediate between Washington and Hanoi.\n\nNixon had been elected in 1968 on the promise of achieving \"peace with honor\" and ending the Vietnam War. In office, and assisted by Kissinger, Nixon implemented a policy of Vietnamization that aimed to gradually withdraw U.S. troops while expanding the combat role of the South Vietnamese Army so that it would be capable of independently defending its government against the National Front for the Liberation of South Vietnam, a Communist guerrilla organization, and North Vietnamese army (Vietnam People's Army or PAVN). Kissinger played a key role in bombing Cambodia to disrupt PAVN and Viet Cong units launching raids into South Vietnam from within Cambodia's borders and resupplying their forces by using the Ho Chi Minh trail and other routes, as well as the 1970 Cambodian Incursion and subsequent widespread bombing of Khmer Rouge targets in Cambodia. The bombing campaign contributed to the chaos of the Cambodian Civil War, which saw the forces of leader Lon Nol unable to retain foreign support to combat the growing Khmer Rouge insurgency that would overthrow him in 1975. Documents uncovered from the Soviet archives after 1991 reveal that the North Vietnamese invasion of Cambodia in 1970 was launched at the explicit request of the Khmer Rouge and negotiated by Pol Pot's then second in command, Nuon Chea. The American bombing of Cambodia resulted in 40,000-150,000 deaths from 1969 to 1973, including at least 5,000 civilians. Kissinger himself said there were about 50,000 civilian casualties in the bombing. Pol Pot biographer David P. Chandler argues that the bombing \"had the effect the Americans wanted—it broke the Communist encirclement of Phnom Penh.\" However, Ben Kiernan and Taylor Owen suggest that \"the bombs drove ordinary Cambodians into the arms of the Khmer Rouge, a group that seemed initially to have slim prospects of revolutionary success.\"\n\nAlong with North Vietnamese Politburo Member Le Duc Tho, Kissinger was awarded the Nobel Peace Prize on December 10, 1973, for their work in negotiating the ceasefires contained in the Paris Peace Accords on \"Ending the War and Restoring Peace in Vietnam\", signed the previous January. According to Irwin Abrams, this prize was the most controversial to date. For the first time in the history of the Peace Prize, two members left the Nobel Committee in protest. Tho rejected the award, telling Kissinger that peace had not been restored in South Vietnam. Kissinger wrote to the Nobel Committee that he accepted the award \"with humility,\" and \"donated the entire proceeds to the children of American servicemembers killed or missing in action in Indochina.\" After the Fall of Saigon in 1975, Kissinger attempted to return the award.\n\nUnder Kissinger's guidance, the United States government supported Pakistan in the Bangladesh Liberation War in 1971. Kissinger was particularly concerned about the expansion of Soviet influence in South Asia as a result of a treaty of friendship recently signed by India and the USSR, and sought to demonstrate to the People's Republic of China (Pakistan's ally and an enemy of both India and the USSR) the value of a tacit alliance with the United States.\n\nKissinger sneered at people who \"bleed\" for \"the dying Bengalis\" and ignored the first telegram from the United States consul general in East Pakistan, Archer K. Blood, and 20 members of his staff, which informed the US that their allies West Pakistan were undertaking, in Blood's words, \"a selective genocide\". In the second, more famous, Blood Telegram the word genocide was again used to describe the events, and further that with its continuing support for West Pakistan the US government had \"evidenced [...] moral bankruptcy\".\nAs a direct response to the dissent against US policy Kissinger and Nixon ended Archer Blood's tenure as United States consul general in East Pakistan and put him to work in the State Department's Personnel Office.\n\nHenry Kissinger had also come under fire for private comments he made to Nixon during the Bangladesh–Pakistan War in which he described Indian Prime Minister Indira Gandhi as a \"bitch\" and a \"witch\". He also said \"The Indians are bastards\", shortly before the war. Kissinger has since expressed his regret over the comments.\n\nAccording to notes taken by H. R. Haldeman, Nixon \"ordered his aides to exclude all Jewish-Americans from policy-making on Israel\", including Kissinger. One note quotes Nixon as saying \"get K. [Kissinger] out of the play—Haig handle it\".\n\nIn 1973, Kissinger did not feel that pressing the Soviet Union concerning the plight of Jews being persecuted there was in the interest of U.S. foreign policy. In conversation with Nixon shortly after a meeting with Golda Meir on March 1, 1973, Kissinger stated, \"The emigration of Jews from the Soviet Union is not an objective of American foreign policy, and if they put Jews into gas chambers in the Soviet Union, it is not an American concern. Maybe a humanitarian concern.\" Kissinger argued, however:\nDocuments show that Kissinger delayed telling President Richard Nixon about the start of the Yom Kippur War in 1973 to keep him from interfering. On October 6, 1973, the Israelis informed Kissinger about the attack at 6 am; Kissinger waited nearly 3 and a half hours before he informed Nixon.\nAccording to Kissinger, in an interview in November 2013, he was notified at 6:30 a.m. (12:30 pm. Israel time) that war was imminent, and his urgent calls to the Soviets and Egyptians were ineffective. He says Golda Meir's decision not to preempt was wise and reasonable, balancing the risk of Israel looking like the aggressor and Israel's actual ability to strike within such a brief span of time.\n\nThe war began on October 6, 1973, when Egypt and Syria attacked Israel. Kissinger published lengthy telephone transcripts from this period in the 2002 book \"Crisis\". On October 12, under Nixon's direction, and against Kissinger's initial advice, while Kissinger was on his way to Moscow to discuss conditions for a cease-fire, Nixon sent a message to Brezhnev giving Kissinger full negotiating authority.\n\nIsrael regained the territory it lost in the early fighting and gained new territories from Syria and Egypt, including land in Syria east of the previously captured Golan Heights, and additionally on the western bank of the Suez Canal, although they did lose some territory on the eastern side of the Suez Canal that had been in Israeli hands since the end of the Six Day War. Kissinger pressured the Israelis to cede some of the newly captured land back to its Arab neighbors, contributing to the first phases of Israeli-Egyptian non-aggression. The move saw a warming in U.S.–Egyptian relations, bitter since the 1950s, as the country moved away from its former independent stance and into a close partnership with the United States. The peace was finalized in 1978 when U.S. President Jimmy Carter mediated the Camp David Accords, during which Israel returned the Sinai Peninsula in exchange for an Egyptian peace agreement that included the recognition of the state of Israel.\n\nFollowing a period of steady relations between the U.S. Government and the Greek military regime after 1967, Secretary of State Kissinger was faced with the coup by the Greek junta and the Turkish invasion of Cyprus in July and August 1974. In an August 1974 edition of the \"New York Times\", it was revealed that Kissinger and State Department were informed in advance οf the impending coup by the Greek junta in Cyprus. Indeed, according to the journalist, the official version of events as told by the State Department was that it felt it had to warn the Greek military regime not to carry out the coup. The warning had been delivered by July 9, according to repeated assurances from its Athens services, that is, the U.S. embassy and the American ambassador Henry J. Tasca himself.\n\nIoannis Zigdis, then a Greek MP for Centre Union and former minister, stated in an Athenian newspaper that \"the Cyprus crisis will become Kissinger's Watergate\". Zigdis also stressed: “Not only did Kissinger know about the coup for the overthrow of Archbishop Makarios before July 15th, he also encouraged it, if he did not instigate it.”\n\nKissinger was a target of anti-American sentiment which was a significant feature of Greek public opinion at the time—particularly among young people—viewing the U.S. role in Cyprus as negative. In a demonstration by students in Heraklion, Crete, soon after the second phase of the Turkish invasion in August 1974, slogans such as \"Kissinger, murderer\", \"Americans get out\", \"No to Partition\" and \"Cyprus is no Vietnam\" were heard.\n\nSome years later, Kissinger expressed the opinion that the Cyprus issue was resolved in 1974, a position very similar to that held by Turkish prime minister Bulent Ecevit, who had ordered the invasion.\n\nThe United States continued to recognize and maintain relationships with non-left-wing governments, democratic and authoritarian alike. John F. Kennedy's Alliance for Progress was ended in 1973. In 1974, negotiations about a new settlement over the Panama Canal started. They eventually led to the Torrijos-Carter Treaties and the handing over of the Canal to Panamanian control.\n\nKissinger initially supported the normalization of United States-Cuba relations, broken since 1961 (all U.S.–Cuban trade was blocked in February 1962, a few weeks after the exclusion of Cuba from the Organization of American States because of U.S. pressure). However, he quickly changed his mind and followed Kennedy's policy. After the involvement of the Cuban Revolutionary Armed Forces in the independence struggles in Angola and Mozambique, Kissinger said that unless Cuba withdrew its forces relations would not be normalized. Cuba refused.\n\nChilean Socialist Party presidential candidate Salvador Allende was elected by a plurality of 36.2% in 1970, causing serious concern in Washington, D.C. due to his openly socialist and pro-Cuban politics. The Nixon administration, with Kissinger's input, authorized the Central Intelligence Agency (CIA) to encourage a military coup that would prevent Allende's inauguration, but the plan was not successful.\nUnited States-Chile relations remained frosty during Salvador Allende's tenure, following the complete nationalization of the partially U.S.-owned copper mines and the Chilean subsidiary of the U.S.-based ITT Corporation, as well as other Chilean businesses. The U.S. claimed that the Chilean government had greatly undervalued fair compensation for the nationalization by subtracting what it deemed \"excess profits\". Therefore, the U.S. implemented economic sanctions against Chile. The CIA also provided funding for the mass anti-government strikes in 1972 and 1973, and extensive black propaganda in the newspaper \"El Mercurio\".\n\nThe most expeditious way to prevent Allende from assuming office was somehow to convince the Chilean congress to confirm Jorge Alessandri as the winner of the election. Once elected by the congress, Alessandri—a party to the plot through intermediaries—was prepared to resign his presidency within a matter of days so that new elections could be held. This first, nonmilitary, approach to stopping Allende was called the Track I approach. The CIA's second approach, the Track II approach, was designed to encourage a military overthrow.\n\nOn September 11, 1973, Allende died during a military coup launched by Army Commander-in-Chief Augusto Pinochet, who became President.\nA document released by the CIA in 2000 titled \"CIA Activities in Chile\" revealed that the United States, acting through the CIA, actively supported the military junta after the overthrow of Allende and that it made many of Pinochet's officers into paid contacts of the CIA or U.S. military.\n\nIn 1976, Orlando Letelier, a Chilean opponent of the Pinochet regime, was assassinated in Washington, D.C. with a car bomb. Previously, Kissinger had helped secure his release from prison, and had chosen to cancel a letter to Chile warning them against carrying out any political assassinations. The U.S. ambassador to Chile, David H. Popper, said that Pinochet might take as an insult any inference that he was connected with assassination plots. It has been confirmed that Pinochet directly ordered the assassination. This murder was part of Operation \"Condor\", a covert program of political repression and assassination carried out by Southern Cone nations that Kissinger has been accused of being involved in.\n\nOn September 10, 2001, the family of Chilean general René Schneider filed a suit against Kissinger, accusing him of collaborating in arranging Schneider's kidnapping which resulted in his death. According to phone records, Kissinger claimed to have \"turned off\" the operation. However, the CIA claimed that no such \"stand-down\" order was ever received, and he and Nixon later joked that an \"incompetent\" CIA had struggled to kill Schneider. A subsequent Congressional investigation found that the CIA was not directly involved in Schneider's death. The case was later dismissed by a U.S. District Court, citing separation of powers: \"The decision to support a coup of the Chilean government to prevent Dr. Allende from coming to power, and the means by which the United States Government sought to effect that goal, implicate policy makers in the murky realm of foreign affairs and national security best left to the political branches.\" Decades later the CIA admitted its involvement in the kidnapping of General Schneider, but not his murder, and subsequently paid the group responsible for his death $35,000 \"to keep the prior contact secret, maintain the goodwill of the group, and for humanitarian reasons.\"\n\nKissinger took a similar line as he had toward Chile when the Argentinian military, led by Jorge Videla, toppled the elected government of Isabel Perón in 1976 with a process called the National Reorganization Process by the military, with which they consolidated power, launching brutal reprisals and \"disappearances\" against political opponents. During a meeting with Argentinian foreign minister César Augusto Guzzetti, Kissinger assured him that the United States was an ally, but urged him to \"get back to normal procedures\" quickly before the U.S. Congress reconvened and had a chance to consider sanctions. According to declassified state department files, Kissinger also attempted to thwart the Carter Administration's efforts to halt the mass killings by the 1976-83 military dictatorship.\n\nIn September 1976 Kissinger was actively involved in negotiations regarding the Rhodesian Bush War. Kissinger, along with South Africa's Prime Minister John Vorster, pressured Rhodesian Prime Minister Ian Smith to hasten the transition to black majority rule in Rhodesia. With FRELIMO in control of Mozambique and even South Africa withdrawing its support, Rhodesia's isolation was nearly complete. According to Smith's autobiography, Kissinger told Smith of Mrs. Kissinger's admiration for him, but Smith stated that he thought Kissinger was asking him to sign Rhodesia's \"death certificate\". Kissinger, bringing the weight of the United States, and corralling other relevant parties to put pressure on Rhodesia, hastened the end of minority-rule.\n\nThe Portuguese decolonization process brought U.S. attention to the former Portuguese colony of East Timor, which lies within the Indonesian archipelago and declared its independence in 1975. Indonesian president Suharto was a strong U.S. ally in Southeast Asia and began to mobilize the Indonesian army, preparing to annex the nascent state, which had become increasingly dominated by the popular leftist FRETILIN party. In December 1975, Suharto discussed the invasion plans during a meeting with Kissinger and President Ford in the Indonesian capital of Jakarta. Both Ford and Kissinger made clear that U.S. relations with Indonesia would remain strong and that it would not object to the proposed annexation. They only wanted it done \"fast\" and proposed that it be delayed until after they had returned to Washington. Accordingly, Suharto delayed the operation for one day. Finally on December 7 Indonesian forces invaded the former Portuguese colony. U.S. arms sales to Indonesia continued, and Suharto went ahead with the annexation plan. According to Ben Kiernan, the invasion and occupation resulted in the deaths of nearly a quarter of the Timorese population from 1975 to 1981.\n\nIn February 1976 Kissinger considered launching air strikes against ports and military installations in Cuba, as well as deploying Marine battalions based at the US Navy base at Guantanamo Bay, in retaliation for Cuban President Fidel Castro's decision in late 1975 to send troops to Angola to help the newly independent nation fend off attacks from South Africa and right-wing guerrillas.\n\nKissinger left office when Democrat Jimmy Carter defeated Republican Gerald Ford in the 1976 presidential elections. Kissinger continued to participate in policy groups, such as the Trilateral Commission, and to maintain political consulting, speaking, and writing engagements.\n\nShortly after Kissinger left office in 1977, he was offered an endowed chair at Columbia University. There was significant student opposition to the appointment, which eventually became a subject of wide media commentary. Columbia cancelled the appointment as a result.\n\nKissinger was then appointed to Georgetown University's Center for Strategic and International Studies. He taught at Georgetown's Edmund Walsh School of Foreign Service for several years in the late 1970s. In 1982, with the help of a loan from the international banking firm of E.M. Warburg, Pincus and Company, Kissinger founded a consulting firm, Kissinger Associates, and is a partner in affiliate Kissinger McLarty Associates with Mack McLarty, former chief of staff to President Bill Clinton. He also serves on the board of directors of Hollinger International, a Chicago-based newspaper group, and as of March 1999, was a director of Gulfstream Aerospace.\n\nFrom 1995 to 2001, Kissinger served on the board of directors for Freeport-McMoRan, a multinational copper and gold producer with significant mining and milling operations in Papua, Indonesia. In February 2000, then-president of Indonesia Abdurrahman Wahid appointed Kissinger as a political advisor. He also serves as an honorary advisor to the United States-Azerbaijan Chamber of Commerce.\n\nFrom 2000–2006, Kissinger served as chairman of the board of trustees of Eisenhower Fellowships. In 2006, upon his departure from Eisenhower Fellowships, he received the Dwight D. Eisenhower Medal for Leadership and Service.\n\nIn November 2002, he was appointed by President George W. Bush to chair the newly established National Commission on Terrorist Attacks Upon the United States to investigate the September 11 attacks. Kissinger stepped down as chairman on December 13, 2002 rather than reveal his business client list, when queried about potential conflicts of interest.\n\nKissinger—along with William Perry, Sam Nunn, and George Shultz—has called upon governments to embrace the vision of a world free of nuclear weapons, and in three \"Wall Street Journal\" op-eds proposed an ambitious program of urgent steps to that end. The four have created the Nuclear Security Project to advance this agenda. In 2010, the four were featured in a documentary film entitled \"Nuclear Tipping Point\". The film is a visual and historical depiction of the ideas laid forth in the \"Wall Street Journal\" op-eds and reinforces their commitment to a world without nuclear weapons and the steps that can be taken to reach that goal.\n\nOn 17 November 2016, Kissinger met with President Elect Donald Trump during which they discussed \"China, Russia, Iran, the EU and other events and issues around the world\".\n\nIn several articles of his and interviews that he gave during the Yugoslav wars, he criticized the United States' policies in Southeast Europe, among other things for the recognition of Bosnia and Herzegovina as a sovereign state, which he described as a foolish act. Most importantly he dismissed the notion of Serbs, and Croats for that part, being aggressors or separatist, saying that \"they can't be separating from something that has never existed\".\nIn addition, he repeatedly warned the West of inserting itself into a conflict that has its roots at least hundreds of years back in time, and said that the West would do better if it allowed the Serbs and Croats to join their respective countries.\nKissinger shared similarly critical views on Western involvement in Kosovo. In particular, he held a disparaging view of the Rambouillet Agreement:\n\nHowever, as the Serbs did not accept the Rambouillet text and NATO bombings started, he opted for a continuation of the bombing as NATO's credibility was now at stake, but dismissed the use of ground forces, claiming that it was not worth it.\n\nIn 2006, it was reported in the book \"\" by Bob Woodward that Kissinger met regularly with President George W. Bush and Vice President Dick Cheney to offer advice on the Iraq War. Kissinger confirmed in recorded interviews with Woodward that the advice was the same as he had given in an August 12, 2005 column in \"The Washington Post\": \"Victory over the insurgency is the only meaningful exit strategy.\"\n\nIn a November 19, 2006, interview on BBC \"Sunday AM\", Kissinger said, when asked whether there is any hope left for a clear military victory in Iraq, \"If you mean by 'military victory' an Iraqi government that can be established and whose writ runs across the whole country, that gets the civil war under control and sectarian violence under control in a time period that the political processes of the democracies will support, I don't believe that is possible. ... I think we have to redefine the course. But I don't believe that the alternative is between military victory as it had been defined previously, or total withdrawal.\"\n\nIn an April 3, 2008, interview with Peter Robinson of the Hoover Institution, Kissinger reiterated that even though he supported the 2003 invasion of Iraq he thought that the George W. Bush administration rested too much of its case for war on Saddam's supposed weapons of mass destruction. Robinson noted that Kissinger had criticized the administration for invading with too few troops, for disbanding the Iraqi Army, and for mishandling relations with certain allies.\n\nKissinger said in April 2008 that \"India has parallel objectives to the United States,\" and he called it an ally of the U.S.\n\nKissinger was present at the opening ceremony of the 2008 Beijing Summer Olympics.\n\nIn 2011, Kissinger published \"On China\", chronicling the evolution of Sino-American relations and laying out the challenges to a partnership of 'genuine strategic trust' between the U.S. and China.\n\nKissinger's position on this issue of U.S.–Iran talks was reported by the \"Tehran Times\" to be that \"Any direct talks between the U.S. and Iran on issues such as the nuclear dispute would be most likely to succeed if they first involved only diplomatic staff and progressed to the level of secretary of state before the heads of state meet.\"\n\nOn March 5, 2014, \"The Washington Post\" published an op-ed piece by Kissinger, 11 days before the Crimean referendum on whether Autonomous Republic of Crimea should officially rejoin in Ukraine or join neighboring Russia. In it, he attempted to balance the Ukrainian, Russian and Western desires for a functional state. He made four main points:\n\nKissinger also wrote: \"The west speaks Ukrainian; the east speaks mostly Russian. Any attempt by one wing of Ukraine to dominate the other—as has been the pattern—would lead eventually to civil war or break up.\"\n\nFollowing the publication of his new book titled \"World Order\", Kissinger participated in an interview with Charlie Rose and updated his position on Ukraine, which he sees as a possible geographical mediator between Russia and the West. In a question he posed to himself for illustration regarding re-conceiving policy regarding Ukraine, Kissinger stated: \"If Ukraine is considered an outpost, then the situation is that its eastern border is the NATO strategic line, and NATO will be within of Volgograd. That will never be accepted by Russia. On the other hand, if the Russian western line is at the border of Poland, Europe will be permanently disquieted. The Strategic objective should have been to see whether one can build Ukraine as a bridge between East and West, and whether one can do it as a kind of a joint effort.\"\n\nIn December 2016, Kissinger advised President-elect Donald Trump to accept \"Crimea as a part of Russia\" in an attempt to secure a rapprochement between the United States and Russia, whose relations soured as a result of the Crimean crisis.\n\nAt the height of Kissinger's prominence, many commented on his wit. In February 1972, at the Washington Press Club annual congressional dinner, \"Kissinger mocked his reputation as a secret swinger.\" The insight, \"Power is the ultimate aphrodisiac\", is widely attributed to him, although Kissinger was paraphrasing Napoleon Bonaparte. Some scholars have ranked Kissinger as the most effective U.S. Secretary of State in the 50 years to 2015. A number of activists and human rights lawyers, however, have sought his prosecution for alleged war crimes. According to historian and Kissinger biographer Niall Ferguson, however, accusing Kissinger alone of war crimes \"requires a double standard\" because \"nearly all the secretaries of state ... and nearly all the presidents\" have taken similar actions.\n\nKissinger was interviewed in \"\", a documentary examining the underpinnings of the 1979 peace treaty between Israel and Egypt. In the film, Kissinger revealed how close he felt the world came to nuclear war during the 1973 Yom Kippur War launched by Egypt and Syria against Israel.\nAttempts have been made to attach liability to Kissinger for injustices in American foreign policy during his tenure in government. In September 2001, relatives and survivors of General Rene Schneider, the former head of the Chilean general staff, commenced civil proceedings in Federal Court in Washington, DC, and, in April 2002, a petition for Kissinger's arrest was filed in the High Court in London by human rights campaigner Peter Tatchell, citing the destruction of civilian populations and the environment in Indochina during the years 1969-75. Both suits were determined to lack legal foundation and were dismissed before trial. British-American journalist and author Christopher Hitchens authored \"The Trial of Henry Kissinger\", in which Hitchens calls for the prosecution of Kissinger \"for war crimes, for crimes against humanity, and for offenses against common or customary or international law, including conspiracy to commit murder, kidnap, and torture\". Critics on the right, such as Ray Takeyh, have faulted Kissinger for his role in the Nixon administration's opening to China and secret negotiations with North Vietnam. Takeyh writes that while rapprochement with China was a worthy goal, the Nixon administration failed to achieve any meaningful concessions from Chinese officials in return, as China continued to support North Vietnam and various \"revolutionary forces throughout the Third World,\" \"nor does there appear to be even a remote, indirect connection between Nixon and Kissinger's diplomacy and the communist leadership's decision, after Mao's bloody rule, to move away from a communist economy towards state capitalism.\" On Vietnam, Takeyh claims that Kissinger's negotiations with Le Duc Tho were intended only \"to secure a 'decent interval' between America's withdrawal and South Vietnam's collapse.\" Johannes Kadura offers a more positive assessment of Nixon and Kissinger's strategy, arguing that the two men \"simultaneously maintained a Plan A of further supporting Saigon and a Plan B of shielding Washington should their maneuvers prove futile.\" According to Kadura, the \"decent interval\" concept has been \"largely misrepresented,\" in that Nixon and Kissinger \"sought to gain time, make the North turn inward, and create a perpetual equilibrium\" rather than acquiescing in the collapse of South Vietnam, but the strength of the anti-war movement and the sheer unpredictability of events in Indochina compelled them to prepare for the possibility that South Vietnam might collapse despite their best efforts. Kadura concludes: \"Without Nixon, Kissinger, and Ford's clever use of triangular diplomacy ... The Soviets and the Chinese could have been tempted into a far more aggressive stance\" following the \"U.S. defeat in Indochina\" than actually occurred. In 2011, Chimerica Media released an interview-based documentary, titled \"Kissinger\", in which Kissinger \"reflects on some of his most important and controversial decisions\" during his tenure as Secretary of State.\n\nKissinger's record was brought up during the 2016 Democratic Party presidential primaries. Hillary Clinton has cultivated a close relationship with Kissinger, describing him as a \"friend\" and a source of \"counsel.\" During the Democratic Primary Debates, Clinton touted Kissinger's praise for her record as Secretary of State. In response, candidate Bernie Sanders issued a critique of Kissinger's foreign policy, declaring: \"I am proud to say that Henry Kissinger is not my friend. I will not take advice from Henry Kissinger.\"\n\nKissinger married Ann Fleischer, with whom he had two children, Elizabeth and David. They divorced in 1964. Ten years later, he married Nancy Maginnes. They now live in Kent, Connecticut and New York City. His son David Kissinger was an executive with NBCUniversal before becoming head of Conaco, Conan O'Brien's production company.\n\nHe described \"Diplomacy\" as his favorite game in a 1973 interview.\n\nKissinger was described as one of the most influential people in the growth of soccer in the United States.\nKissinger was named chairman of the North American Soccer League board of directors in 1978.\n\nSince his childhood, Kissinger has been a fan of his hometown's soccer club, SpVgg Greuther Fürth. Even during his time in office he was informed about the team's results by the German Embassy every Monday morning. He is an honorary member with lifetime season tickets. In September 2012, Kissinger attended a home game in which SpVgg Greuther Fürth lost, 0–2, against Schalke after promising years ago he would attend a Greuther Fürth home game if they were promoted to the Bundesliga, the top football league in Germany, from the 2. Bundesliga.\nKissinger is an honorary member of the German soccer club FC Bayern München.\n\n\n\n\n\n\n\n", "id": "13765", "title": "Henry Kissinger"}
{"url": "https://en.wikipedia.org/wiki?curid=13767", "text": "Hydra (genus)\n\nHydra is a genus of small, fresh-water animals of the phylum Cnidaria and class Hydrozoa, native to the temperate and tropical regions. Biologists are especially interested in \"Hydra\" because of their regenerative ability – they appear not to age or die of old age.\n\n\"Hydra\" has a tubular, radially symmetric body up to long when extended, secured by a simple adhesive foot called the basal disc. Gland cells in the basal disc secrete a sticky fluid that accounts for its adhesive properties.\n\nAt the free end of the body is a mouth opening surrounded by one to twelve thin, mobile tentacles. Each tentacle, or cnida (plural: cnidae), is clothed with highly specialised stinging cells called cnidocytes. Cnidocytes contain specialized structures called nematocysts, which look like miniature light bulbs with a coiled thread inside. At the narrow outer edge of the cnidocyte is a short trigger hair called a cnidocil. Upon contact with prey, the contents of the nematocyst are explosively discharged, firing a dart-like thread containing neurotoxins into whatever triggered the release which can paralyse the prey, especially if many hundreds of nematocysts are fired.\n\n\"Hydra\" has two main body layers, which makes it \"diploblastic\". The layers are separated by mesoglea, a gel-like substance. The outer layer is the epidermis, and the inner layer is called the gastrodermis, because it lines the stomach. The cells making up these two body layers are relatively simple. Hydramacin is a bactericide recently discovered in \"Hydra\"; it protects the outer layer against infection.\n\nThe nervous system of \"Hydra\" is a nerve net, which is structurally simple compared to mammalian nervous systems. \"Hydra\" does not have a recognizable brain or true muscles. Nerve nets connect sensory photoreceptors and touch-sensitive nerve cells located in the body wall and tentacles.\n\nRespiration and excretion occur by diffusion everywhere through the epidermis.\n\nIf \"Hydra\" are alarmed or attacked, the tentacles can be retracted to small buds, and the body column itself can be retracted to a small gelatinous sphere. \"Hydra\" generally react in the same way regardless of the direction of the stimulus, and this may be due to the simplicity of the nerve nets.\n\n\"Hydra\" are generally or sessile, but do occasionally move quite readily, especially when hunting. They have two distinct methods for moving - 'looping' and 'somersaulting'. They do this by bending over and attaching themselves to the with the mouth and tentacles and then relocate the foot, which provides the usual attachment, this process is called looping. in somersaulting, the body then bends over and makes a new place of attachment with the foot. By this process of \"looping\" or \"somersaulting\", a \"Hydra\" can move several inches (c. 100 mm) in a day. \"Hydra\" may also move by amoeboid motion of their bases or by simply detaching from the substrate and floating away in the current.\n\nWhen food is plentiful, many \"Hydra\" reproduce asexually by producing buds in the body wall, which grow to be miniature adults and simply break away when they are mature. When a hydra is well fed, a new bud can form every two days. When conditions are harsh, often before winter or in poor feeding conditions, sexual reproduction occurs in some \"Hydra\". Swellings in the body wall develop into either a simple ovary or testes. The testes release free-swimming gametes into the water, and these can fertilize the egg in the ovary of another individual. The fertilized eggs secrete a tough outer coating, and, , these resting eggs fall to the bottom of the lake or pond to await better conditions, whereupon they hatch into nymph \"Hydra\". Some, like \"Hydra circumcincta\" and \"Hydra viridissima\", are hermaphrodites and may produce both testes and an ovary at the same time.\n\nMany members of the Hydrozoa go through a body change from a polyp to an adult form called a medusa.\nHowever, all \"Hydra\", despite being hydrozoans, remain as polyps throughout their lives.\n\n\"Hydra\" mainly feed on aquatic invertebrates such as \"Daphnia\" and \"Cyclops\".\n\nWhen feeding, \"Hydra\" extend their body to maximum length and then slowly extend their tentacles. Despite their simple construction, the tentacles of \"Hydra\" are extraordinarily extensible and can be four to five times the length of the body. Once fully extended, the tentacles are slowly manoeuvred around waiting for contact with a suitable prey animal. Upon contact, nematocysts on the tentacle fire into the prey, and the tentacle itself coils around the prey. Within 30 seconds, most of the remaining tentacles will have already joined in the attack to subdue the struggling prey. Within two minutes, the tentacles will have surrounded the prey and moved it into the opened mouth aperture. Within ten minutes, the prey will have been enclosed within the body cavity, and digestion will have started. \"Hydra\" is able to stretch its body wall considerably in order to digest prey more than twice its size. After two or three days, the indigestible remains of the prey will be discharged by contractions through the mouth aperture.\n\nThe feeding behaviour of \"Hydra\" demonstrates the sophistication of what appears to be a simple nervous system.\n\nSome species of \"Hydra\" exist in a mutual relationship with various types of unicellular algae. The algae are protected from predators by \"Hydra\" and, in return, photosynthetic products from the algae are beneficial as a food source to \"Hydra\".\n\nThe feeding response in \"Hydra\" is known to be induced by reduced glutathione released from the injured prey. There are several methods which are conventionally used for quantification of the feeding response. In some of such methods, the duration for which mouth of \"Hydra\" remains open is measured. Whereas, few other methods rely on courting the number of hydra out of a small population showing the feeding response after addition of glutathione. Recently, an assay for measuring the feeding response in hydra has been developed. In this method, the linear two-dimensional distance between the tip of the tentacle and the mouth of hydra was shown to give the direct measure of the extent of feeding response. This method has been validated using a starvation model, as starvation is known to cause enhancement in the feeding response in \"Hydra\".\n\n\"Hydra\" undergoes morphallaxis (tissue regeneration) when injured or severed.\n\nDaniel Martinez claimed in a 1998 article in \"Experimental Gerontology\" that \"Hydra\" are biologically immortal. This publication has been widely cited as evidence that \"Hydra\" do not senesce (do not age), and that they are proof of the existence of non-senescing organisms generally. In 2010 Preston Estep published (also in \"Experimental Gerontology\") a letter to the editor arguing that the Martinez data support rather than refute the hypothesis that \"Hydra\" senesce.\n\nThe controversial unlimited life span of \"Hydra\" has attracted the attention of natural scientists for a long time. Research today appears to confirm Martinez' study. \"Hydra\" stem cells have a capacity for indefinite self-renewal. The transcription factor, \"forkhead box O\" (FoxO) has been identified as a critical driver of the continuous self-renewal of \"Hydra\". A drastically reduced population growth resulted from FoxO down-regulation, so research findings do contribute to both a confirmation and an understanding of \"Hydra\" immortality.\n\nWhile \"Hydra\" immortality is well-supported today, the implications for human aging are still controversial. There is much optimism; however, it appears that researchers still have a long way to go before they are able to understand how the results of their work might apply to the reduction or elimination of human senescence.\n\nA draft of the genome of \"Hydra magnipapillata\" was reported in 2010.\n\n", "id": "13767", "title": "Hydra (genus)"}
{"url": "https://en.wikipedia.org/wiki?curid=13768", "text": "Hydrus\n\nHydrus is a small constellation in the deep southern sky. It was one of twelve constellations created by Petrus Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman and it first appeared on a 35-cm (14 in) diameter celestial globe published in late 1597 (or early 1598) in Amsterdam by Plancius and Jodocus Hondius. The first depiction of this constellation in a celestial atlas was in Johann Bayer's Uranometria of 1603. The French explorer and astronomer Nicolas Louis de Lacaille charted the brighter stars and gave their Bayer designations in 1756. Its name means \"male water snake\", as opposed to Hydra, a much larger constellation that represents a female water snake. It remains below the horizon for most Northern Hemisphere observers.\n\nThe brightest star is the 2.8-magnitude Beta Hydri, also the closest reasonably bright star to the south celestial pole. Pulsating between magnitude 3.26 and 3.33, Gamma Hydri is a variable red giant 60 times the diameter of our Sun. Lying near it is VW Hydri, one of the brightest dwarf novae in the heavens. Four star systems in Hydrus have been found to have exoplanets to date, including HD 10180, which could bear up to nine planetary companions.\n\nHydrus was one of the twelve constellations established by the Dutch astronomer Petrus Plancius from the observations of the southern sky by the Dutch explorers Pieter Dirkszoon Keyser and Frederick de Houtman, who had sailed on the first Dutch trading expedition, known as the \"Eerste Schipvaart\", to the East Indies. It first appeared on a 35-cm (14 in) diameter celestial globe published in 1598 in Amsterdam by Plancius with Jodocus Hondius. The first depiction of this constellation in a celestial atlas was in the German cartographer Johann Bayer's \"Uranometria\" of 1603. De Houtman included it in his southern star catalogue the same year under the Dutch name \"De Waterslang\", \"The Water Snake\", it representing a type of snake encountered on the expedition rather than a mythical creature. The French explorer and astronomer Nicolas Louis de Lacaille called it \"l’Hydre Mâle\" on the 1756 version of his planisphere of the southern skies, distinguishing it from the feminine Hydra. The French name was retained by Jean Fortin in 1776 for his \"Atlas Céleste\", while Lacaille Latinised the name to Hydrus for his revised \"Coelum Australe Stelliferum\" in 1763.\n\nIrregular in shape, Hydrus is bordered by Mensa to the southeast, Eridanus to the east, Horologium and Reticulum to the northeast, Phoenix to the north, Tucana to the northwest and west, and Octans to the south; Lacaille had shortened Hydrus' tail to make space for this last constellation he had drawn up. Covering 243 square degrees and 0.589% of the night sky, it ranks 61st of the 88 constellations in size. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Hyi'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 12 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −57.85° and −82.06°. As one of the deep southern constellations, it remains below the horizon at latitudes north of the 30th parallel in the Northern Hemisphere, and is circumpolar at latitudes south of the 50th parallel in the Southern Hemisphere. Indeed, Herman Melville mentions it and Argo Navis in Moby Dick \"beneath effulgent Antarctic Skies\", highlighting his knowledge of the southern constellations from whaling voyages. A line drawn between the long axis of the Southern Cross to Beta Hydri and then extended 4.5 times will mark a point due south. Hydrus culminates at midnight around 26 October.\n\nKeyzer and de Houtman assigned fifteen stars to the constellation in their Malay and Madagascan vocabulary, with a star that would be later designated as Alpha Hydri marking the head, Gamma the chest and a number of stars that were later allocated to Tucana, Reticulum, Mensa and Horologium marking the body and tail. Lacaille charted and designated 20 stars with the Bayer designations Alpha through to Tau in 1756. Of these, he used the designations Eta, Pi and Tau twice each, for three sets of two stars close together, and omitted Omicron and Xi. He assigned Rho to a star that subsequent astronomers were unable to find.\n\nBeta Hydri, the brightest star in Hydrus, is a yellow star of apparent magnitude 2.8, lying 24 light-years from Earth. It has about 104% of the mass of the Sun and 181% of the Sun's radius, with more than three times the Sun's luminosity. The spectrum of this star matches a stellar classification of G2 IV, with the luminosity class of 'IV' indicating this is a subgiant star. As such, it is a slightly more evolved star than the Sun, with the supply of hydrogen fuel at its core becoming exhausted. It is the nearest subgiant star to the Sun and one of the oldest stars in the solar neighbourhood. Thought to be between 6.4 and 7.1 billion years old, this star bears some resemblance to what the Sun may look like in the far distant future, making it an object of interest to astronomers. It is also the closest bright star to the south celestial pole.\n\nLocated at the northern edge of the constellation and just southwest of Achernar is Alpha Hydri, a white sub-giant star of magnitude 2.9, situated 72 light-years from Earth. Of spectral type F0IV, it is beginning to cool and enlarge as it uses up its supply of hydrogen. It is twice as massive and 3.3 times as wide as our sun and 26 times more luminous. A line drawn between Alpha Hydri and Beta Centauri is bisected by the south celestial pole.\n\nIn the southeastern corner of the constellation is Gamma Hydri, a red giant of spectral type M2III located 214 light-years from Earth. It is a semi-regular variable star, pulsating between magnitudes 3.26 and 3.33. Observations over five years were not able to establish its periodicity. It is around 1.5 to 2 times as massive as our Sun, and has expanded to about 60 times the Sun's diameter. It shines with about 655 times the luminosity of the Sun. Located 3° northeast of Gamma is the VW Hydri, a dwarf nova of the SU Ursae Majoris type. It is a close binary system that consists of a white dwarf and another star, the former drawing off matter from the latter into a bright accretion disk. These systems are characterised by frequent eruptions and less frequent supereruptions. The former are smooth, while the latter exhibit short \"superhumps\" of heightened activity. One of the brightest dwarf novae in the sky, it has a baseline magnitude of 14.4 and can brighten to magnitude 8.4 during peak activity. BL Hydri is another close binary system composed of a low mass star and a strongly magnetic white dwarf. Known as a polar or AM Herculis variable, these produce polarized optical and infrared emissions and intense soft and hard X-ray emissions to the frequency of the white dwarf's rotation period—in this case 113.6 minutes.\nThere are two notable optical double stars in Hydrus. Pi Hydri, composed of Pi Hydri and Pi Hydri, is divisible in binoculars. Around 476 light-years distant, Pi is a red giant of spectral type M1III that varies between magnitudes 5.52 and 5.58. Pi is an orange giant of spectral type K2III and shining with a magnitude of 5.7, around 488 light-years from Earth.\n\nEta Hydri is the other optical double, composed of Eta and Eta. Eta is a blue-white main sequence star of spectral type B9V that was suspected of being variable, and is located just over 700 light-years away. Eta has a magnitude of 4.7 and is a yellow giant star of spectral type G8.5III around 218 light-years distant, which has evolved off the main sequence and is expanding and cooling on its way to becoming a red giant. Calculations of its mass indicate it was most likely a white A-type main sequence star for most of its existence, around twice the mass of our Sun. A planet, Eta2 Hydri b, greater than 6.5 times the mass of Jupiter was discovered in 2005, orbiting around Eta every 711 days at a distance of 1.93 astronomical units (AU).\n\nThree other systems have been found to have planets, most notably the Sun-like star HD 10180, which has seven planets, plus possibly an additional two for a total of nine—as of 2012 more than any other system to date, including the Solar System. Lying around from the Earth, it has an apparent magnitude of 7.33.\n\nGJ 3021 is a solar twin—a star very like our own Sun—around 57 light-years distant with a spectral type G8V and magnitude of 6.7. It has a Jovian planet companion (GJ 3021 b). Orbiting about 0.5 AU from its sun, it has a minimum mass 3.37 times that of Jupiter and a period of around 133 days. The system is a complex one as the faint star GJ 3021B orbits at a distance of 68 AU; it is a red dwarf of spectral type M4V.\n\nHD 20003 is a star of magnitude 8.37. It is a yellow main sequence star of spectral type G8V a little cooler and smaller than our Sun around 143 light-years away. It has two planets that are around 12 and 13.5 times as massive as the Earth with periods of just under 12 and 34 days respectively.\n\nHydrus contains only faint deep-sky objects. IC 1717 was a deep-sky object discovered by the Danish astronomer John Louis Emil Dreyer in the late 19th century. The object at the coordinate Dreyer observed is no longer there, and is now a mystery. It was very likely to have been a faint comet. PGC 6240, known as the White Rose Galaxy, is a giant spiral galaxy surrounded by shells resembling rose petals, located around 345 million light years from the Solar System. Unusually, it has cohorts of globular clusters of three distinct ages suggesting bouts of post-starburst formation following a merger with another galaxy. The constellation also contains a spiral galaxy, NGC 1511, which lies edge on to observers on Earth and is readily viewed in amateur telescopes.\n\nLocated mostly in Dorado, the Large Magellanic Cloud extends into Hydrus. The globular cluster NGC 1466 is an outlying component of the galaxy, and contains many RR Lyrae-type variable stars. It has a magnitude of 11.59 and is thought to be over 12 billion years old. Two stars, HD 24188 of magnitude 6.3 and HD 24115 of magnitude 9.0, lie nearby in its foreground. NGC 602 is composed of an emission nebula and a young, bright open cluster of stars that is an outlying component on the eastern edge of the Small Magellanic Cloud, a satellite galaxy to the Milky Way. Most of the cloud is located in the neighbouring constellation Tucana.\n\n", "id": "13768", "title": "Hydrus"}
{"url": "https://en.wikipedia.org/wiki?curid=13770", "text": "Hercules\n\nHercules is the Roman adaptation of the Greek divine hero Heracles, who was the son of Zeus (Roman equivalent Jupiter) and the mortal Alcmene. In classical mythology, Hercules is famous for his strength and for his numerous far-ranging adventures.\n\nThe Romans adapted the Greek hero's iconography and myths for their literature and art under the name \"Hercules\". In later Western art and literature and in popular culture, \"Hercules\" is more commonly used than \"Heracles\" as the name of the hero. Hercules was a multifaceted figure with contradictory characteristics, which enabled later artists and writers to pick and choose how to represent him. This article provides an introduction to representations of Hercules in the later tradition.\n\nHercules is known for his many adventures, which took him to the far reaches of the Greco-Roman world. One cycle of these adventures became canonical as the \"Twelve Labours,\" but the list has variations. One traditional order of the labours is found in the \"Bibliotheca\" as follows:\n\nHercules had a greater number of \"deeds on the side\" \"(parerga)\" that have been popular subjects for art, including:\nThe Latin name \"Hercules\" was borrowed through Etruscan, where it is represented variously as Heracle, Hercle, and other forms. Hercules was a favorite subject for Etruscan art, and appears often on bronze mirrors. The Etruscan form \"Herceler\" derives from the Greek \"Heracles\" via syncope. A mild oath invoking Hercules (\"Hercule!\" or \"Mehercle!\") was a common interjection in Classical Latin.\nHercules had a number of myths that were distinctly Roman. One of these is Hercules' defeat of Cacus, who was terrorizing the countryside of Rome. The hero was associated with the Aventine Hill through his son Aventinus. Mark Antony considered him a personal patron god, as did the emperor Commodus. Hercules received various forms of religious veneration, including as a deity concerned with children and childbirth, in part because of myths about his precocious infancy, and in part because he fathered countless children. Roman brides wore a special belt tied with the \"knot of Hercules\", which was supposed to be hard to untie. The comic playwright Plautus presents the myth of Hercules' conception as a sex comedy in his play \"Amphitryon\"; Seneca wrote the tragedy \"Hercules Furens\" about his bout with madness. During the Roman Imperial era, Hercules was worshipped locally from Hispania through Gaul.\n\nTacitus records a special affinity of the Germanic peoples for Hercules. In chapter 3 of his \"Germania\", Tacitus states:\nSome have taken this as Tacitus equating the Germanic \"Þunraz\" with Hercules by way of \"interpretatio romana\".\n\nIn the Roman era Hercules' Club amulets appear from the 2nd to 3rd century, distributed over the empire (including Roman Britain, c.f. Cool 1986), mostly made of gold, shaped like wooden clubs. A specimen found in Köln-Nippes bears the inscription [culi]\", confirming the association with Hercules.\n\nIn the 5th to 7th centuries, during the Migration Period, the amulet is theorized to have rapidly spread from the Elbe Germanic area across Europe. These Germanic \"Donar's Clubs\" were made from deer antler, bone or wood, more rarely also from bronze or precious metals.They are found exclusively in female graves, apparently worn either as a belt pendant, or as an ear pendant. The amulet type is replaced by the Viking Age Thor's hammer pendants in the course of the Christianization of Scandinavia from the 8th to 9th century.\n\nAfter the Roman Empire became Christianized, mythological narratives were often reinterpreted as allegory, influenced by the philosophy of late antiquity. In the 4th century, Servius had described Hercules' return from the underworld as representing his ability to overcome earthly desires and vices, or the earth itself as a consumer of bodies. In medieval mythography, Hercules was one of the heroes seen as a strong role model who demonstrated both valor and wisdom, with the monsters he battles as moral obstacles. One glossator noted that when Hercules became a constellation, he showed that strength was necessary to gain entrance to Heaven.\n\nMedieval mythography was written almost entirely in Latin, and original Greek texts were little used as sources for Hercules' myths.\n\nThe Renaissance and the invention of the printing press brought a renewed interest in and publication of Greek literature. Renaissance mythography drew more extensively on the Greek tradition of Heracles, typically under the Romanized name Hercules, or the alternate name Alcides. In a chapter of his book \"Mythologiae\" (1567), the influential mythographer Natale Conti collected and summarized an extensive range of myths concerning the birth, adventures, and death of the hero under his Roman name Hercules. Conti begins his lengthy chapter on Hercules with an overview description that continues the moralizing impulse of the Middle Ages:\n\nHercules, who subdued and destroyed monsters, bandits, and criminals, was justly famous and renowned for his great courage. His great and glorious reputation was worldwide, and so firmly entrenched that he'll always be remembered. In fact the ancients honored him with his own temples, altars, ceremonies, and priests. But it was his wisdom and great soul that earned those honors; noble blood, physical strength, and political power just aren't good enough.\nIn Roman works of art and in Renaissance and post-Renaissance art, Hercules can be identified by his attributes, the lion skin and the gnarled club (his favorite weapon); in mosaic he is shown tanned bronze, a virile aspect.\n\nHercules was among the earliest figures on ancient Roman coinage, and has been the main motif of many collector coins and medals since. One example is the 20 euro Baroque Silver coin issued on September 11, 2002. The obverse side of the coin shows the Grand Staircase in the town palace of Prince Eugene of Savoy in Vienna, currently the Austrian Ministry of Finance. Gods and demi-gods hold its flights, while Hercules stands at the turn of the stairs.\n\nA series of nineteen Italian Hercules movies were made in the late 1950s and early 1960s. The actors who played Hercules in these films were Steve Reeves, Gordon Scott, Kirk Morris, Mickey Hargitay, Mark Forest, Alan Steel, Dan Vadis, Brad Harris, Reg Park, Peter Lupus (billed as Rock Stevens) and Michael Lane. A number of English-dubbed Italian films that featured the name of Hercules in their title were not intended to be movies about Hercules.\n\n\n\n", "id": "13770", "title": "Hercules"}
{"url": "https://en.wikipedia.org/wiki?curid=13772", "text": "History of Poland\n\nThe history of Poland originates in the migrations of Slavs who established permanent settlements in the Polish lands during the Early Middle Ages. The first ruling dynasty, the Piasts, emerged by the 10th century AD. Duke Mieszko I (d. 992) is considered the \"de facto\" creator of the Polish state and is widely recognized for the widespread adoption of Western Christianity that followed his baptism in 966. The duchy of Poland that Mieszko ruled was formally reconstituted as a medieval kingdom in 1025 by his son Bolesław I Chrobry. Perhaps the most successful of the Piast kings was the last one, Casimir the Great, who presided over a brilliant period of economic prosperity and territorial aggrandizement before his death in 1370 without male heirs. The period of the Jagiellonian dynasty in the 14th-16th centuries brought close ties with the Grand Duchy of Lithuania, a cultural Renaissance in Poland and continued territorial expansion that culminated in the establishment of the Polish-Lithuanian Commonwealth in 1569.\n\nIn its early phases, the Commonwealth was able to sustain the levels of prosperity achieved during the Jagiellonian period through its remarkable development of a sophisticated noble democracy. From the mid-17th century, however, the huge state entered a period of decline caused by devastating wars and the deterioration of its political system. Significant internal reforms were introduced during the later part of the 18th century, especially in the Constitution of May 3, 1791, but neighboring powers did not allow the reform process to advance. The independent existence of the Commonwealth ended in 1795 after a series of invasions and partitions of Polish territory carried out by the Russian Empire, the Kingdom of Prussia, and the Austrian Habsburg Monarchy.\n\nFrom 1795 until 1918, no truly independent Polish state existed, although strong Polish resistance movements operated. After the failure of the last military uprising against the Russian Empire, the January Uprising of 1863, the nation preserved its identity through educational initiatives and a program of \"organic work\" intended to modernize the economy and society. The opportunity to regain independence only materialized after World War I, when the three partitioning imperial powers were fatally weakened in the wake of war and revolution.\n\nThe Second Polish Republic, established in 1918, existed as an independent state until 1939, when Nazi Germany and the Soviet Union destroyed it in their invasion of Poland at the beginning of World War II. Millions of Polish citizens perished in the course of the Nazi occupation of Poland between 1939 and 1945 as Germany classified ethnic Poles and other Slavs, Jews and Romani (Gypsies) as subhuman. Nazi authorities targeted the last two groups for extermination in the short term, deferring the extermination and/or enslavement of the Slavs as part of the \"Generalplan Ost\" (\"General Plan for the East\") conceived by the Nazi régime. A Polish government-in-exile nonetheless functioned throughout the war and the Poles contributed to the Allied victory through participation in military campaigns on both the eastern and western fronts. The westward advances of the Soviet Red Army in 1944 and 1945 compelled Nazi Germany's forces to retreat from Poland, which led to the establishment of a communist satellite state of the Soviet Union, known from 1952 as the Polish People's Republic.\n\nAs a result of territorial adjustments mandated by the victorious Allies at the end of World War II in 1945, Poland's geographic centre of gravity shifted towards the west and the re-defined Polish lands largely lost their traditional multi-ethnic character through the extermination, expulsion and migration of various ethnic groups during and after the war.\n\nBy the late 1980s, the Polish reform movement Solidarity became crucial in bringing about a peaceful transition from a communist state to a capitalist economic system and a liberal parliamentary democracy. This process resulted in the creation of the modern Polish state: the Third Polish Republic, founded in 1989.\n\nMembers of the \"Homo\" genus have lived in north Central Europe for thousands of years since the last periods of prehistoric glaciation. In prehistoric and protohistoric times, over a period of at least 500,000 years, the area of present-day Poland went through the Stone Age, Bronze Age and Iron Age stages of development, along with the nearby regions. The Neolithic period ushered in the Linear Pottery culture, whose founders migrated from the Danube River area beginning about 5,500 BC. This culture was distinguished by the establishment of the first settled agricultural communities in modern Polish territory. Later, between about 4,400 and 2,000 BC, the native post-Mesolithic populations would also adopt and further develop the agricultural way of life.\nPoland's Early Bronze Age began around 2300–2400 BC, whereas its Iron Age commenced c. 700–750 BC. One of the many cultures that have been uncovered, the Lusatian culture, spanned the Bronze and Iron Ages and left notable settlement sites. Around 400 BC, Poland was settled by Celts of the La Tène culture. They were soon followed by emerging cultures with a strong Germanic component, influenced first by the Celts and then by the Roman Empire. The Germanic peoples migrated out of the area by about 500 AD during the great Migration Period of the European Dark Ages. Wooded regions to the north and east were settled by Balts.\n\nAccording to mainstream archaeological research, Slavs have resided in modern Polish territories for over 1500 years. Recent genetic studies, however, determined that people who live in the current territory of Poland include the descendants of people who inhabited the area for thousands of years, beginning in the early Neolithic period.\n\nSlavs on the territory of Poland were organized into tribal units, of which the larger ones were later known as the Polish tribes; the names of many tribes are found on the list compiled by the anonymous Bavarian Geographer in the 9th century. In the 9th and 10th centuries, these tribes gave rise to developed regions along the upper Vistula, the coast of the Baltic Sea and in Greater Poland. This latest tribal undertaking resulted in the formation of a lasting political structure in the 10th century that became the state of Poland, one of the West Slavic nations.\n\nPoland was established as a nation state under the Piast dynasty, which ruled the country between the 10th and 14th centuries. Historical records of an official Polish state begin with Duke Mieszko I in the second half of the 10th century. Mieszko, who began his rule sometime before 963 and continued as the Polish monarch until his death in 992, chose to be baptized in the Western Latin Rite, probably on 14 April 966, following his marriage to Princess Dobrawa of Bohemia, a fervent Christian. This event has become known as the baptism of Poland, and its date is often used to mark a symbolic beginning of Polish statehood. Mieszko completed a unification of the West Slavic tribal lands that was fundamental to the new country's existence. Following its emergence, the Polish nation was led by a series of rulers who converted the population to Christianity, created a strong Kingdom of Poland and fostered a distinctive Polish culture that was integrated into broader European culture.\n\nMieszko's son, Duke Bolesław I Chrobry (r. 992–1025), established a Polish Church structure, pursued territorial conquests and was officially crowned the first king of Poland in 1025, near the end of his life. Bolesław also sought to spread Christianity to parts of eastern Europe that remained pagan, but suffered a setback when his greatest missionary, Adalbert of Prague, was killed in Prussia in 997. During the Congress of Gniezno in the year 1000, Holy Roman Emperor Otto III recognized the Archbishopric of Gniezno, an institution crucial for the continuing existence of the sovereign Polish state. During the reign of Otto's successor, Holy Roman Emperor Henry II, Bolesław fought prolonged wars with the Kingdom of Germany between 1002 and 1018.\n\nBolesław's expansive rule overstretched the military resources of the early Polish state, and it was followed by a collapse of the monarchy. Restoration took place under Casimir I (r. 1039–58). Casimir's son Bolesław II the Bold (r. 1058–79) became involved in a conflict with Bishop Stanislaus of Szczepanów that seriously marred his reign. Bolesław had the bishop murdered in 1079 after being excommunicated by the Polish church on charges of adultery. This act sparked a revolt of Polish nobles that led to Bolesław's deposition and expulsion from the country. Around 1116, Gallus Anonymous wrote a seminal chronicle, the \"Gesta principum Polonorum\", intended as a glorification of his patron Bolesław III Wrymouth (r. 1107–38), a ruler who revived the tradition of military prowess of Bolesław I's time. Gallus' work became important as a key source for the early history of Poland.\n\nAfter Bolesław III divided Poland among his sons in his Testament of 1138, internal fragmentation eroded the Piast monarchical structures in the 12th and 13th centuries. In 1180, Casimir II, who sought papal confirmation of his status as a senior duke, granted immunities and additional privileges to the Polish Church at the Congress of Łęczyca. Around 1220, Wincenty Kadłubek wrote his \"Chronica seu originale regum et principum Poloniae\", another major source for early Polish history. In 1226, one of the regional Piast dukes, Konrad I of Masovia, invited the Teutonic Knights to help him fight the Baltic Prussian pagans. Konrad's move caused centuries of warfare between Poland and the Teutonic Knights, and later between Poland and the German Prussian state. The first Mongol invasion of Poland began in 1240; it culminated in the defeat of Polish and allied Christian forces and the death of the Silesian Piast Duke Henry II at the Battle of Legnica in 1241. In 1242, Wrocław became the first Polish municipality to be incorporated, as the period of fragmentation brought economic development and growth of towns. In 1264, Bolesław the Pious granted Jewish liberties in the Statute of Kalisz.\n\nAttempts to reunite the Polish lands gained momentum in the 13th century, and in 1295, Duke Przemysł II of Greater Poland managed to become the first ruler since Bolesław II to be crowned king of Poland. He ruled over a limited territory and was soon killed. In 1300–05 the Czech ruler Václav II also reigned as king of Poland. The Piast Kingdom was effectively restored under Władysław I the Elbow-high (r. 1306–33), who was crowned king in 1320. In 1308, the Teutonic Knights seized Gdańsk and the surrounding region (Pomerelia).\n\nKing Casimir III the Great (r. 1333–70), Władysław's son and the last of the Piast rulers, strengthened and expanded the restored Kingdom of Poland, but the western provinces of Silesia (formally ceded by Casimir in 1339) and most of Pomerania were lost to the Polish state for centuries to come. Progress was made in the recovery of the central province of Mazovia, however, and in 1340, the conquest of Red Ruthenia began, marking Poland's expansion to the east. The Congress of Kraków, a vast convocation of central, eastern, and northern European rulers probably assembled to plan an anti-Turkish crusade, took place in 1364, the same year that the future Jagiellonian University, one of the oldest European universities, was founded. On 9 October 1334, he confirmed the privileges granted to Jews in 1264 by Bolesław the Pious and allowed them to settle in Poland in great numbers.\n\nAfter the Polish royal line and Piast junior branch died out in 1370, Poland came under the rule of Louis I of Hungary of the Angevin dynasty, who presided over a union of Hungary and Poland that lasted until 1382. In 1374, Louis granted the Polish nobility the Privilege of Koszyce to assure the succession of one of his daughters in Poland. His youngest daughter Jadwiga (d. 1399) assumed the Polish throne in 1384.\n\nIn 1386, Grand Duke Jogaila of Lithuania married Queen Jadwiga of Poland. This act enabled him to became a king of Poland himself, and he ruled as Władysław II Jagiełło until his death in 1434. The marriage established a Polish–Lithuanian union ruled by the Jagiellonian dynasty. The first in a series of formal \"unions\" was the Union of Krewo of 1385, whereby arrangements were made for the marriage of Jogaila and Queen Jadwiga. The Polish-Lithuanian partnership brought vast areas of Ruthenia controlled by the Grand Duchy of Lithuania into Poland's sphere of influence and proved beneficial for the nationals of both countries, who coexisted and cooperated in one of the largest political entities in Europe for the next four centuries. When Queen Jadwiga died in 1399, the Kingdom of Poland fell to her husband's sole possession.\nIn the Baltic Sea region, Poland's struggle with the Teutonic Knights continued and culminated in the Battle of Grunwald (1410), a great victory that the Poles and Lithuanians were unable to follow up with a decisive strike against the main seat of the Order at Malbork Castle. The Union of Horodło of 1413 further defined the evolving relationship between the Kingdom of Poland and the Grand Duchy of Lithuania.\n\nThe privileges of the \"szlachta\" (nobility) kept growing and in 1425 the rule of \"Neminem captivabimus\", which protected the noblemen from arbitrary royal arrests, was formulated.\n\nThe reign of the young Władysław III (1434–44), a son of Władysław II who ruled as king of Poland and Hungary, was cut short by his death at the Battle of Varna against the forces of the Ottoman Empire. This disaster led to an interregnum of three years that ended with the accession of Władysław's brother Casimir IV Jagiellon in 1447.\n\nCritical developments of the Jagiellonian period were concentrated during Casimir IV's long reign, which lasted until 1492. In 1454, Royal Prussia was incorporated by Poland and the Thirteen Years' War of 1454–66 with the Teutonic state ensued. In 1466, the milestone Peace of Thorn was concluded. This treaty divided Prussia to create East Prussia, the future Duchy of Prussia, a separate entity that functioned as a fief of Poland under the administration of the Teutonic Knights. Poland also confronted the Ottoman Empire and the Crimean Tatars in the south, and in the east helped Lithuania fight the Grand Duchy of Moscow. The country was developing as a feudal state, with a predominantly agricultural economy and an increasingly dominant landed nobility. Kraków, the royal capital, was turning into a major academic and cultural center, and in 1473 the first printing press began operating there. With the growing importance of the \"szlachta\", the king's council evolved to become by 1493 a bicameral General Sejm (parliament) that no longer represented only the top dignitaries of the realm.\n\nThe \"Nihil novi\" act, adopted in 1505 by the Sejm, transferred most of the legislative power from the monarch to the Sejm. This event marked the beginning of the period known as \"Golden Liberty\", when the state was ruled in principle by the \"free and equal\" Polish nobility. In the 16th century, the massive development of folwark agribusinesses operated by the nobility led to increasingly abusive conditions for the peasant serfs who worked them. The political monopoly of the nobles also stifled the development of cities, some of which were thriving during the late Jagiellonian era, and limited the rights of townspeople, effectively holding back the emergence of a middle class.\n\nIn the 16th century, Protestant Reformation movements made deep inroads into Polish Christianity and the resulting Reformation in Poland involved a number of different denominations. The policies of religious tolerance that developed in Poland were nearly unique in Europe at that time and many who fled regions torn by religious strife found refuge in Poland. The reigns of King Sigismund I (1506-48) and King Sigismund II Augustus (1548-72) witnessed an intense cultivation of culture and science (a Golden Age of the Renaissance in Poland), of which the astronomer Nicolaus Copernicus (died 1543) is the best known representative. In 1525, during the reign of Sigismund I, the Teutonic Order was secularized and Duke Albrecht von Hohenzollern performed an act of homage before the Polish king (the Prussian Homage) for his fief, the Duchy of Prussia. Mazovia was finally fully incorporated into the Polish Crown in 1529.\n\nThe reign of Sigismund II ended the Jagiellonian period, but gave rise to the Union of Lublin (1569), the ultimate fulfillment of the union with Lithuania. This agreement transferred Ukraine from the Grand Duchy of Lithuania to Poland and transformed the Polish-Lithuanian polity into a real union, preserving it beyond the death of the childless Sigismund II, whose active involvement made the completion of this process possible.\n\nLivonia in the far northeast was incorporated by Poland in 1561 and Poland entered the Livonian War against Russia. The executionist movement (an attempt to prevent domination by the magnate families of Poland and Lithuania) peaked at the Sejm in Piotrków in 1562–63. On the religious front, the Polish Brethren split from the Calvinists, and the Protestant Brest Bible was published in 1563. The Jesuits, who arrived in 1564, were destined to make a major impact on Poland's history.\n\nThe Union of Lublin of 1569 established the Polish–Lithuanian Commonwealth, a more closely unified federal state than the earlier political arrangement between Poland and Lithuania. The Union was largely run by the nobility through the system of a central parliament and local assemblies, but was headed by elected kings. The formal rule of the nobility, who were proportionally more numerous than in other European countries, constituted an early democratic system (\"a sophisticated noble democracy\"), in contrast to the absolute monarchies prevalent at that time in the rest of Europe. The beginning of the Commonwealth coincided with a period in Polish history of great political power and advancements in civilization and prosperity. The Polish–Lithuanian Union became an influential participant in European affairs and a vital cultural entity that spread Western culture (with Polish characteristics) eastward. In the second half of the 16th century and the first half of the 17th century, the Commonwealth was one of the largest and most populous states in contemporary Europe, with an area approaching and a population of about ten million. Its economy was dominated by export-focused agriculture. Nationwide religious toleration was guaranteed at the Warsaw Confederation in 1573.\n\nAfter the rule of the Jagiellonian dynasty ended in 1572, Henry of Valois (later King Henry III of France) was the winner of the first \"free election\" by the Polish nobility, held in 1573. He had to agree to the restrictive \"pacta conventa\" obligations, but fled Poland in 1574 when news arrived of the vacancy of the French throne, to which he was the heir presumptive. He remained the nominal ruler of the Commonwealth until 1575. From the start, the royal elections increased foreign influence in the Commonwealth as foreign powers sought to manipulate the Polish nobility to place candidates amicable to their interests. The reign of Stephen Báthory of Hungary followed (1576-86); he was militarily and domestically assertive. The establishment of the legal Crown Tribunal in 1578 meant a transfer of many appellate cases from the royal to noble jurisdiction.\n\nA period of rule under the Swedish House of Vasa began in the Commonwealth in the year 1587. The first two kings from this dynasty, Sigismund III (1587–1632) and Władysław IV (1632–48), constantly attempted to intrigue for accession to the throne of Sweden, a constant source of distraction for the foreign affairs of the Commonwealth. At the same time, the Catholic Church embarked on an ideological counter-offensive and the Counter-Reformation claimed many converts from Polish and Lithuanian Protestant circles. In 1596, the Union of Brest split the Eastern Christians of the Commonwealth to create the Uniate Church of the Eastern Rite, but subject to the authority of the pope. The Zebrzydowski Rebellion against Sigismund III unfolded in 1606–8.\n\nThe Commonwealth fought wars between 1605 and 1618 with Russia for supremacy in Eastern Europe in the wake of Russia's Time of Troubles, a period referred to as the Polish–Muscovite War (or the \"Dymitriads\"). The efforts resulted in expansion of the eastern territories of the Polish–Lithuanian Commonwealth, but the goal of taking over the Russian throne for the Polish ruling dynasty was not achieved. Sweden sought supremacy in the Baltic during the Polish–Swedish wars of 1617–29, and the Ottoman Empire pressed from the south in the Battles at Cecora in 1620 and Khotyn in 1621. The agricultural expansion and serfdom policies in Polish Ukraine resulted in a series of Cossack uprisings. Allied with the Habsburg Monarchy, the Commonwealth did not directly participate in the Thirty Years' War. Władysław's IV reign was mostly peaceful, with a Russian invasion in the form of the Smolensk War of 1632–34 successfully repelled. The Orthodox Church hierarchy, banned in Poland after the Union of Brest, was re-established in 1635.\n\nDuring the reign of John II Casimir Vasa (1648–68), the third and last king of his dynasty, the nobles' democracy fell into decline as a result of foreign invasions and domestic disorder. These calamities multiplied rather suddenly and marked the end of the Polish Golden Age. Their effect was to render the once powerful Commonwealth increasingly vulnerable to foreign intervention.\n\nThe Cossack Khmelnytsky Uprising of 1648–57 engulfed the south-eastern regions of the Polish crown; its long-term effects were disastrous for the Commonwealth. The first \"liberum veto\" (a parliamentary device that allowed any member of the Sejm to dissolve a current session immediately) was exercised by a deputy in 1652. This practice would eventually weaken Poland's central government critically. In the Treaty of Pereyaslav (1654), the Ukrainian rebels declared themselves subjects of the Tsar of Russia. The Second Northern War raged through the core Polish lands in 1655–60, including an invasion of Poland so brutal and devastating that it is referred to as the Swedish Deluge. The war ended in 1660 with the Treaty of Oliva, which resulted in the loss of some of Poland's northern possessions. In 1657 the Treaty of Wehlau-Bromberg established the independence of the Duchy of Prussia. The Commonwealth forces did well in the Russo-Polish War of 1654–67, but the end result was the permanent division of Ukraine between Poland and Russia, as agreed to in the Truce of Andrusovo (1667). Towards the end of the war, the Rokosz of Lubomirski, a major magnate rebellion against the king, destabilized and weakened the country. The large-scale slave raids of the Crimean Tatars also had highly deleterious effects on the Polish economy. \"Merkuriusz Polski\", the first Polish newspaper, was published in 1661.\n\nIn 1668, grief-stricken at the recent death of his wife and frustrated by the disastrous political setbacks of his reign, John II Casimir abdicated the throne and fled to France.\n\nKing Michał Korybut Wiśniowiecki, a native Pole, was elected to replace John II Casimir in 1669. The Second Polish–Ottoman War (1672–76) broke out during his reign, which lasted until 1673, and continued under his successor, John III Sobieski (1674–96). Sobieski intended to pursue Baltic area expansion (and to this end he signed the secret Treaty of Jaworów with France in 1675), but was forced instead to fight protracted wars with the Ottoman Empire. By doing so, Sobieski briefly revived the Commonwealth's military might. He defeated the expanding Muslims at the Battle of Khotyn in 1673 and decisively helped deliver Vienna from a Turkish onslaught at the Battle of Vienna in 1683. Sobieski's reign marked the last high point in the history of the Commonwealth: in the first half of the 18th century, Poland ceased to be an active player in international politics. The Eternal Peace Treaty with Russia of 1686 was the final border settlement between the two countries before the First Partition of Poland in 1772.\n\nThe Commonwealth, subjected to almost constant warfare until 1720, suffered enormous population losses and massive damage to its economy and social structure. The government became ineffective in the wake of large-scale internal conflicts, corrupted legislative processes and manipulation by foreign interests. The nobility fell under the control of a handful of feuding magnate families with established territorial domains. The urban population and infrastructure fell into ruin, together with most peasant farms, whose inhabitants were subjected to increasingly extreme forms of serfdom. The development of science, culture and education came to a halt or regressed.\n\nThe royal election of 1697 brought a ruler of the Saxon House of Wettin to the Polish throne: Augustus II, \"the Strong\" (r. 1697–1733), who was able to assume the throne only by agreeing to convert to Roman Catholicism. He was succeeded by his son Augustus III (r. 1734–63). The reigns of the Saxon kings (who were both simultaneously prince-electors of Saxony) were disrupted by competing candidates for the throne and witnessed further disintegration of the Commonwealth. The Great Northern War of 1700–21, a period seen by the contemporaries as a temporary eclipse, may have been the fatal blow that brought down the Polish political system. Stanisław Leszczyński was installed as king in 1704 under Swedish protection, but lasted only a few years. The Silent Sejm of 1717 marked the beginning of the Commonwealth's existence as a Russian protectorate: the Tsardom would guarantee the reform-impeding Golden Liberty of the nobility from that time on in order to cement the Commonwealth's weak central authority and a state of perpetual political impotence. In a resounding break with traditions of religious tolerance, Protestants were executed during the Tumult of Thorn in 1724. In 1732, Russia, Austria and Prussia, Poland's three increasingly powerful and scheming neighbors, entered into the secret Treaty of the Three Black Eagles with the intention of controlling the future royal succession in the Commonwealth. The War of the Polish Succession was fought in 1733–35 to assist Leszczyński in assuming the throne of Poland for a second time. Amidst considerable foreign involvement, his efforts were unsuccessful. The Kingdom of Prussia became a strong regional power and succeeded in wresting the historically Polish province of Silesia from the Habsburg Monarchy in the Silesian Wars; it thus became an ever-greater threat to Poland's security. The personal union between the Commonwealth and the Electorate of Saxony did give rise to the emergence of a reform movement in the Commonwealth and the beginnings of the Polish Enlightenment culture, the major positive developments of this era. The first Polish public library was the Załuski Library in Warsaw, opened to the public in 1747.\n\nDuring the later part of the 18th century, fundamental internal reforms were attempted in the Polish–Lithuanian Commonwealth as it slid into extinction. The reform activity, initially promoted by the magnate Czartoryski family faction known as the \"Familia\", provoked a hostile reaction and military response from neighboring powers, but it did create conditions that fostered economic improvement. The most populous urban center, the capital city of Warsaw, replaced Danzig (Gdańsk) as the leading trade center, and the importance of the more prosperous urban social classes increased. The last decades of the independent Commonwealth's existence were characterized by aggressive reform movements and far-reaching progress in the areas of education, intellectual life, art and the evolution of the social and political system.\n\nThe royal election of 1764 resulted in the elevation of Stanisław August Poniatowski, a refined and worldly aristocrat connected to the Czartoryski family, but hand-picked and imposed by Empress Catherine the Great of Russia, who expected him to be her obedient follower. Stanisław August ruled the Polish–Lithuanian state until its dissolution in 1795. The king spent his reign torn between his desire to implement reforms necessary to save the failing state and the perceived necessity of remaining in a subordinate relationship to his Russian sponsors.\n\nThe Bar Confederation (1768–72) was a noble rebellion directed against Russia's influence in general and Stanisław August, who was seen as its representative, in particular. It was fought to preserve Poland's independence and the nobility's traditional interests. After several years, it was brought under control by forces loyal to the king and those of the Russian Empire.\n\nFollowing the suppression of the Bar Confederation, parts of the Commonwealth were divided up among Prussia, Austria and Russia in 1772 at the instigation of Frederick the Great of Prussia, an action that became known as the First Partition of Poland: the outer provinces of the Commonwealth were seized by agreement among the country's three powerful neighbors and only a rump state remained. In 1773, the \"Partition Sejm\" ratified the partition under duress as a \"fait accompli\". However, it also established the Commission of National Education, a pioneering in Europe education authority often called the world's first ministry of education.\n\nThe long-lasting Sejm convened by Stanisław August is known as the Great Sejm, or \"Four-Year\" Sejm, which first met in 1788. Its landmark achievement was the passing of the Constitution of May 3, 1791, the first singular pronouncement of a supreme law of the state in modern Europe. A moderately reformist document condemned by detractors as sympathetic to the ideals of the French Revolution, it soon generated strong opposition from the conservative circles of the Commonwealth's upper nobility and the Russian Empress Catherine, who was determined to prevent the rebirth of a strong Commonwealth. The nobility's Targowica Confederation, formed in Russian imperial capital of Saint Petersburg, appealed to Catherine for help, and in May 1792, the Russian army entered the territory of the Commonwealth. The Polish–Russian War of 1792, a defensive war fought by the forces of the Commonwealth against Russian invaders, ended when the Polish king, convinced of the futility of resistance, capitulated by joining the Targowica Confederation. The Confederation took over the government, but Russia and Prussia in 1793 arranged for the Second Partition of Poland, which left the country with a critically reduced territory that rendered it essentially incapable of an independent existence. The Commonwealth's Grodno Sejm of 1793, the last Sejm of its existence, was compelled to confirm the new partition.\n\nRadicalized by recent events, Polish reformers (whether in exile or still resident in the reduced area remaining to the Commonwealth) were soon working on preparations for a national insurrection. Tadeusz Kościuszko, a popular general and a veteran of the American Revolution, was chosen as its leader. He returned from abroad and issued Kościuszko's proclamation in Kraków on March 24, 1794. It called for a national uprising under his supreme command. Kościuszko emancipated many peasants in order to enroll them as \"kosynierzy\" in his army, but the hard-fought insurrection, despite widespread national support, proved incapable of generating the foreign assistance necessary for its success. In the end, it was suppressed by the combined forces of Russia and Prussia, with Warsaw captured in November 1794 at the Battle of Praga. \n\nIn 1795, a Third Partition of Poland was undertaken by Russia, Prussia and Austria as a final division of territory that resulted in the effective dissolution of the Polish–Lithuanian Commonwealth. The Polish king was escorted to Grodno, forced to abdicate, and retired to Saint Petersburg. Kościuszko, initially imprisoned, was allowed to emigrate to the United States in 1796.\n\nThe response of the Polish leadership to the last partition is a matter of historical debate. Literary scholars found that the dominant emotion of the first decade was despair that produced a moral desert ruled by violence and treason. On the other hand, historians have looked for signs of resistance to foreign rule. Apart from those who went into exile, the nobility took oaths of loyalty to their new rulers and served as officers in their armies.\n\nAlthough no sovereign Polish state existed between 1795 and 1918, the idea of Polish independence was kept alive throughout the 19th century. There were a number of uprisings and other military conflicts against the partitioning powers. Military efforts after the partitions were first based on the alliances of Polish émigrés with post-revolutionary France. Jan Henryk Dąbrowski's Polish Legions fought in French campaigns outside of Poland between 1797 and 1802 in hopes that their involvement and contribution would be rewarded with the liberation of their Polish homeland. The Polish national anthem, \"Poland Is Not Yet Lost\", or \"Dąbrowski's Mazurka\", was written in praise of his actions by Józef Wybicki in 1797.\n\nThe Duchy of Warsaw, a small, semi-independent Polish state, was created in 1807 by Napoleon Bonaparte in the wake of his defeat of Prussia and the signing of the Peace of Tilsit with Emperor Alexander I of Russia. The Army of the Duchy of Warsaw, led by Józef Poniatowski, participated in numerous campaigns in alliance with France, including the successful Polish–Austrian War of 1809, which, combined with the outcomes of other theaters of the War of the Fifth Coalition, resulted in an enlargement of the Duchy's territory. The French invasion of Russia in 1812 and the German campaign of 1813 saw the Duchy's last military engagements. The Constitution of the Duchy of Warsaw abolished serfdom as a reflection of the ideals of the French Revolution, but it did not promote land reform.\n\nAfter Napoleon's defeat, a new European order was established at the Congress of Vienna, which met in the years 1814 and 1815. Adam Czartoryski, a former close associate of Alexander I, became the leading advocate for the Polish national cause. The Congress implemented a new partition scheme, which took into account some of the gains realized by the Poles during the Napoleonic period. The Duchy of Warsaw was replaced in 1815 with a new Kingdom of Poland, unofficially known as Congress Poland. The residual Polish kingdom was joined to the Russian Empire in a personal union under the Russian tsar, and it was allowed its own constitution and military. East of the kingdom, large areas of the former Polish–Lithuanian Commonwealth remained directly incorporated into the Russian Empire as the Western Krai. These territories, along with \"Congress Poland,\" are generally considered to form the \"Russian Partition\" as it existed in the 19th century. The Russian, Prussian, and Austrian \"partitions\" were the lands of the former Commonwealth, not actual units of its administrative division in the 19th century. The \"Prussian Partition\" was formed from territories acquired from the Commonwealth and included a portion separated as the Grand Duchy of Posen. Peasants under the Prussian administration were gradually enfranchised under the reforms of 1811 and 1823. The limited legal reforms in the \"Austrian Partition\" were overshadowed by its rural poverty. The Free City of Kraków was a tiny republic newly created by the Congress of Vienna in 1815 under the joint supervision of the three partitioning powers. As bleak as the new political divisions of the former Commonwealth were to Polish patriots, economic progress was made because the period after the Congress of Vienna witnessed a significant development in the building of early industry in the lands taken over by foreign powers.\n\nThe increasingly repressive policies of the partitioning powers led to resistance movements in partitioned Poland, and in 1830 Polish patriots staged the November Uprising. This revolt developed into a full-scale war with Russia, but the leadership was taken over by Polish conservatives who were reluctant to challenge the Russian empire and hostile to broadening the independence movement's social base through measures such as land reform. Despite the significant resources mobilized, a series of military mistakes by several successive chief commanders appointed by the insurgent Polish National Government led to the defeat of its forces by the Russian army in 1831. Congress Poland lost its constitution and military, but formally remained a separate administrative unit within the Russian Empire.\n\nAfter the defeat of the November Uprising, thousands of former Polish combatants and other activists emigrated to Western Europe. This phenomenon, known as the Great Emigration, soon dominated Polish political and intellectual life. Together with the leaders of the independence movement, the Polish community abroad included the greatest Polish literary and artistic minds, including the Romantic poets Adam Mickiewicz (traditionally considered Poland's greatest poet, who died as an émigré in 1855), Juliusz Słowacki, Cyprian Norwid, and the composer Frédéric Chopin. In occupied and repressed Poland, some sought progress through nonviolent activism focused on education and economy, known as organic work; others, in cooperation with emigrant circles, organized conspiracies and prepared for the next armed insurrection.\n\nAs soon as the authorities in the partitions found out about secret preparations, whatever planned national uprisings failed to materialize in the Polish territories for many years. The Greater Poland Uprising ended in a fiasco in early 1846. In the Kraków Uprising of February 1846, patriotic action was combined with revolutionary demands, but the result was the incorporation of the Republic of Kraków into the Austrian Partition. The Austrian officials took advantage of peasant discontent and incited villagers against the noble-dominated insurgent units. This resulted in the Galician slaughter of 1846, a large-scale rebellion of serfs seeking relief from their post-feudal \"folwark\" obligation of unpaid labor. The uprising freed many from bondage and hastened decisions that led to the abolition of Polish serfdom in the Austrian Empire in 1848. A new wave of Polish involvement in revolutionary movements soon took place in the partitions and in other parts of Europe in the context of the Spring of Nations revolutions of 1848 (e.g. Józef Bem's participation in the revolutions in Austria and Hungary). The 1848 revolutions in the German states precipitated the Greater Poland Uprising of 1848, in which peasants in the Prussian Partition, who were by then largely enfranchised, played a prominent role.\n\nDespite the limited liberalization measures allowed in the Congress Poland under the rule of Tsar Alexander II of Russia, a renewal of popular liberation activities took place in 1860–61. The Russian autocracy gave the Polish artisans and gentry reason to rebel in 1863 by assailing national core values of language, religion, culture. During large-scale demonstrations in Warsaw, Russian forces inflicted numerous casualties on the civilian participants. The \"Red\", or left-wing faction, which promoted peasant enfranchisement and cooperated with Russian revolutionaries, became involved in immediate preparations for a national uprising. The \"White\", or right-wing faction, was inclined to cooperate with the Russian authorities and countered with partial reform proposals. In order to cripple the manpower potential of the Reds, Aleksander Wielopolski, the conservative leader of the government of Congress Poland, arranged for a partial selective conscription of young Poles for the Russian army in the years 1862 and 1863. This action hastened the outbreak of hostilities. The January Uprising, joined and led after the initial period by the Whites, was fought by partisan units against an overwhelmingly advantaged enemy. The uprising lasted from January 1863 to the spring of 1864, when Romuald Traugutt, the last supreme commander of the insurgency, was captured by the tsarist police.\n\nOn 2 March 1864, the Russian authority, compelled by the uprising to compete for the loyalty of Polish peasants, officially published an enfranchisement decree in Congress Poland along the lines of an earlier land reform proclamation of the insurgents. The act created the conditions necessary for the development of the capitalist system on central Polish lands. At the time when the futility of armed resistance without external support was realized by most Poles, the various sections of Polish society were undergoing deep and far-reaching social, economic and cultural changes.\n\nThe failure of the January Uprising in Poland caused a major psychological trauma and became a historic watershed; indeed, it sparked the development of modern Polish nationalism. The Poles, subjected within the territories under the Russian and Prussian administrations to still stricter controls and increased persecution, sought to preserve their identity in non-violent ways. After the Uprising, Congress Poland was downgraded in official usage from the \"Kingdom of Poland\" to the \"Vistula Land\" and was more fully integrated into Russia proper, but not entirely obliterated. The Russian and German languages were imposed in all public communication, and the Catholic Church was not spared from severe repression; public education was increasingly subjected to Russification and Germanization measures. Illiteracy was reduced, most effectively in the Prussian partition, but education in Polish was preserved mostly through unofficial efforts. The Prussian government pursued German colonization, including the purchase of Polish-owned land. On the other hand, the region of Galicia in western Ukraine and southern Poland experienced a gradual relaxation of authoritarian policies and even a Polish cultural revival. Economically and socially backward, it was under the milder rule of the Austro-Hungarian Monarchy and from 1867 was allowed increasingly limited autonomy. \"Stańczycy\", a conservative Polish pro-Austrian faction led by great land owners, dominated the Galician government. The Polish Academy of Arts and Sciences was founded in Kraków in 1872. Positivism replaced Romanticism as the leading intellectual, social and literary trend.\n\nSocial activities termed \"organic work\" consisted of self-help organizations that promoted economic advancement and work on improving the competitiveness of Polish-owned businesses, industrial, agricultural or other. New commercial methods of generating higher productivity were discussed and implemented through trade associations and special interest groups, while Polish banking and cooperative financial institutions made the necessary business loans available. The other major area of effort in organic work was the educational and intellectual development of the common people. Many libraries and reading rooms were established in small towns and villages, and numerous printed periodicals reflected the growing interest in popular education. Scientific and educational societies were active in a number of cities. Such activities were most pronounced in the Prussian Partition.\n\nUnder the partitioning powers, large-scale industrialization, economic diversification and progress were introduced in the traditionally agrarian Polish lands, but this development turned out to be very uneven. In the Prussian Partition, advanced agriculture was practiced, except for Upper Silesia, where the coal-mining industry created a large labor force. The densest network of railroads was built in German-ruled western Poland. In Russian Congress Poland, a striking growth of industry, railways and towns was taking place, all against the background of an extensive, but less productive agriculture. Warsaw (a metallurgical center) and Łódź (a textiles center) grew rapidly, as did the total proportion of the urban population, making the region the most advanced in the Russian Empire (industrial production exceeded agricultural production by 1909). The coming of the railways spurred some industrial growth even in the vast Russian Partition territories outside Congress Poland. The Austrian Partition was rural and poor, except for the industrialized Cieszyn Silesia area. Galician economic expansion after 1890 included oil extraction and resulted in the growth of Lemberg (Lwów, Lviv) and Kraków.\n\nEconomic and social changes involving land reform and industrialization, combined with the effects of foreign domination, altered the centuries-old social structure of Polish society. Among the newly emergent strata were wealthy industrialists and financiers, distinct from the traditional, but still critically important landed aristocracy. The intelligentsia, an educated, professional or business middle class, often originated from lower gentry, landless or alienated from their rural possessions, and from urban people. Many smaller agricultural enterprises based on serfdom did not survive the land reforms. The industrial proletariat, a new underprivileged class, was composed mainly of poor peasants or townspeople forced by deteriorating conditions to migrate and search for work in urban centers in their countries of origin or abroad. Millions of residents of the former Commonwealth of various ethnic groups worked or settled in Europe and in North and South America.\n\nSocial and economic changes were partial and gradual, and the degree of (fast-paced in some areas) industrialization generally lagged behind the advanced regions of western Europe. The three partitions developed different economies and were more economically integrated with their mother states than with each other (for example the Prussian Partition's agricultural production depended heavily on the German market, whereas the industrial sector of Congress Poland relied more on the Russian market).\n\nIn the 1870s–90s, large-scale socialist, nationalist, agrarian and other political movements of great ideological fervor became established in partitioned Poland and Lithuania, along with corresponding political parties to promote them. Of the major parties, the socialist First Proletariat was founded in 1882, the Polish League (precursor of National Democracy) in 1887, the Polish Social Democratic Party of Galicia and Silesia in 1890, the Polish Socialist Party in 1892, the Marxist SDKPiL in 1893, the agrarian People's Party of Galicia in 1895 and the Jewish socialist Bund in 1897. Christian democracy regional associations allied with the Catholic Church were also active; they united into the Polish Christian Democratic Party in 1919. The main minority ethnic groups of the former Commonwealth, including Ukrainians, Lithuanians, Belarusians and Jews, were getting involved in their own national movements and plans, which met with disapproval on the part of those Polish independence activists who counted on an eventual rebirth of the Commonwealth or the rise of a Commonwealth-inspired federal structure (a political movement referred to as Prometheism).\n\nAround the start of the 20th century, the Young Poland cultural movement, centered in Galicia, took advantage of a milieu conducive to liberal expression in that region and was the source of Poland's finest artistic and literary productions. In this same era, Marie Skłodowska-Curie, a pioneer radiation scientist, performed her groundbreaking research in Paris.\n\nThe Revolution of 1905–07 in Russian Poland, the result of many years of pent-up political frustrations and stifled national ambitions, was marked by political maneuvering, strikes and rebellion. The revolt was part of much broader disturbances throughout the Russian Empire associated with the general Revolution of 1905. In Poland, the principal revolutionary figures were Roman Dmowski and Józef Piłsudski. Dmowski was associated with the right-wing nationalist movement National Democracy, whereas Piłsudski was associated with the Polish Socialist Party. As the authorities re-established control within the Russian Empire, the revolt in Congress Poland, placed under martial law, withered as well, partially as a result of tsarist concessions in the areas of national and workers' rights, including Polish representation in the newly created Russian Duma. The collapse of the revolt in the Russian Partition coupled with intensified Germanization in the Prussian Partition left Austrian Galicia as the territory most amenable to patriotic action.\n\nIn the Austrian Partition, Polish culture was openly cultivated, and in the Prussian Partition, there were high levels of education and living standards, but the Russian Partition remained of primary importance for the Polish nation and its aspirations. About 15.5 million Polish-speakers lived in what was the central and western territories of the Russian Partition. Much fewer were spread in the east: 1.3 million in Austrian Eastern Galicia and about 2 million along Russia's western districts, with the heaviest concentration in the Vilnius Region.\nPolish paramilitary organizations oriented toward independence, such as the Union of Active Struggle, were being formed in 1908–14, mainly in Galicia. The Poles were divided and their political parties fragmented on the eve of World War I, with Dmowski's National Democracy (pro-Entente) and Piłsudski's faction assuming opposing positions.\n\nThe outbreak of World War I in the Polish lands offered Poles unexpected hopes for achieving independence as a result of the turbulence that engulfed the empires of the partitioning powers. All three of the monarchies that had benefited from the partition of Polish territories (Germany, Austria and Russia) were dissolved by the end of the war, and many of their territories were dispersed into new political units. At the start of the war, the Poles found themselves conscripted into the armies of the partitioning powers in a war that was not theirs. Furthermore, they were frequently forced to fight each other, since the armies of Germany and Austria were allied against Russia. Piłsudski's paramilitary units stationed in Galicia were turned into the Polish Legions in 1914, and as a part of the Austro-Hungarian Army, they fought on the Russian front until 1917, when the formation was disbanded. Piłsudski, who refused demands that his men fight under German command, was arrested and imprisoned by the Germans and became a heroic symbol of Polish nationalism.\n\nDue to a series of German victories on the Eastern Front, the area of Congress Poland became occupied by the Central Powers of Germany and Austria; Warsaw was captured by the Germans on 5 August 1915. In the Act of 5th November 1916, a fresh incarnation of the Kingdom of Poland (\"Królestwo Regencyjne\") was created by Germany and Austria on formerly Russian-controlled territories within the German Mitteleuropa scheme. The sponsor states were never able to agree on a candidate to assume the throne, however; rather, it was governed in turn by German and Austrian Governor-Generals, a Provisional Council of State, and a Regency Council. This increasingly autonomous puppet state existed until November 1918, when it was replaced by the newly established Republic of Poland. The existence of this \"kingdom\" and its planned Polish army had a positive effect on the Polish national efforts on the Allied side. But the Treaty of Brest-Litovsk between Germany and defeated Russia of March 1918 ignored Polish interests.\n\nThe independence of Poland had been campaigned for in Russia and in the West by Dmowski and in the West by Ignacy Paderewski. Tsar Nicholas II of Russia, and then the leaders of the February Revolution and the October Revolution of 1917, installed governments who declared in turn their support for Polish independence. In 1917, France formed the Blue Army (placed under Józef Haller) that comprised about 70,000 Poles by the end of the war, including men captured from German and Austrian units and 20,000 volunteers from the United States. There was also a 30,000-men strong Polish anti-German army in Russia. Dmowski, operating from Paris as head of the Polish National Committee (KNP), became the spokesman for Polish nationalism in the Allied camp. On the initiative of Woodrow Wilson's Fourteen Points, Polish independence was officially endorsed by the Allies in June 1918.\n\nIn all, about two million Poles served in the war, counting both sides, and about 400–450,000 died. Much of the fighting on the Eastern Front took place in Poland, and civilian casualties and devastation were high. Total World War I casualties from 1914 to 1918 within the 1919–39 borders of Poland, military and civilian, were estimated at 1,128,000.\n\nThe final upsurge of the push for independence of Poland took place on the ground in October–November 1918. Near the end of the war, Austro-Hungarian and German units were being disarmed, and the Austrian army's collapse freed Cieszyn and Kraków at the end of October. Lviv was then contested in the Polish–Ukrainian War of 1918–19. Ignacy Daszyński headed the first short-lived independent Polish government in Lublin from November 7, the leftist Provisional People's Government of the Republic of Poland, which was proclaimed as a democracy. Germany, now defeated, was forced by the Allies to stand down its large military forces in Poland. Overtaken by the German Revolution of 1918–19 at home, the Germans released Piłsudski from prison. He arrived in Warsaw on November 10 and was granted extensive authority by the Polish kingdom's Regency Council, which was also recognized by the Lublin government. On November 22 Piłsudski became the temporary head of state. He was held by many in high regard, but was resented by the right-wing National Democrats. The emerging Polish state was internally divided, heavily war-damaged and economically dysfunctional.\n\nAfter more than a century of foreign rule, Poland regained its independence at the end of World War I as one of the outcomes of the negotiations that took place at the Paris Peace Conference of 1919. The Treaty of Versailles that emerged from the conference set up an independent nation with an outlet to the sea, but left some of its boundaries to be decided by plebiscites. The largely German Free City of Danzig was granted a separate status that guaranteed its use as a port by Poland. In the end, the settlement of the German-Polish border turned out to be a prolonged and convoluted process. It helped engender the Greater Poland Uprising of 1918–19, the three Silesian Uprisings of 1919–21, the East Prussian plebiscite of 1920, the Upper Silesia plebiscite of 1921 and the 1922 Silesian Convention in Geneva.\n\nOther boundaries were settled by war and subsequent treaties. A total of six border wars were fought in 1918–21, including the Polish–Czechoslovak border conflicts over Cieszyn Silesia in January 1919.\n\nAs distressing as these border conflicts were, the Polish–Soviet War of 1919–21 was the most important military action of the era. Piłsudski had entertained far-reaching anti-Russian cooperative designs in Eastern Europe, and in 1919 the Polish forces pushed eastward into Lithuania, Belarus and Ukraine by taking advantage of the Russian preoccupation with a civil war, but they were soon confronted with the Soviet westward offensive of 1918–19. Western Ukraine was already a theater of the Polish–Ukrainian War, which eliminated the proclaimed West Ukrainian People's Republic in July 1919. In the autumn of 1919, Piłsudski rejected urgent pleas from the former Entente powers to support Anton Denikin's White movement in its advance on Moscow. The Polish–Soviet War proper began with the Polish Kiev Offensive in April 1920. Allied with the Directorate of Ukraine of the Ukrainian People's Republic, the Polish armies had advanced past Vilnius, Minsk and Kiev by June. At that time, a massive Soviet counter-offensive pushed the Poles out of most of Ukraine. On the northern front, the Soviet army reached the outskirts of Warsaw in early August. A Soviet triumph and the quick end of Poland seemed inevitable. However, the Poles scored a stunning victory at the Battle of Warsaw of 1920. Afterwards, more Polish military successes followed, and the Soviets had to pull back. They left swathes of territory occupied largely by Belarusians or Ukrainians to Polish rule. The new eastern boundary was finalized by the Peace of Riga in 1921.\n\nThe defeat of the Russian armies forced Vladimir Lenin and the Soviet leadership to postpone their strategic objective of linking up with the German and other European revolutionary-minded comrades to spread communist revolution. Lenin's hope of generating support for the Red Army in Poland had already failed to materialize. \n\nPiłsudski's seizure of Vilnius in October 1920 (known as Żeligowski's Mutiny) was a nail in the coffin of the already poor Polish–Lithuanian relations that had been strained by the Polish–Lithuanian War of 1919–20; both states would remain hostile to one another for the remainder of the interwar period. Piłsudski's planned Intermarium (an East European federation of states inspired by the tradition of the multiethnic Polish–Lithuanian Commonwealth that would include a hypothetical multinational successor state to the Grand Duchy of Lithuania) and thus became incompatible with his assumption of Polish domination and encroachment on neighboring peoples' lands and aspirations at the time of rising national movements. It soon ceased to be a feature of Poland's politics. A larger federated structure was also opposed by Dmowski's National Democrats. Their representative at the Peace of Riga talks, Stanisław Grabski, opted for leaving Minsk, Berdychiv, Kamianets-Podilskyi and the surrounding areas on the Soviet side of the border, since the National Democrats did not want to permit population shifts that they considered politically undesirable, especially if the transfers would result in a reduced proportion of citizens who were ethnically Polish.\n\nThe Peace of Riga settled the eastern border by preserving for Poland a substantial portion of the old Commonwealth's eastern territories at the cost of partitioning the lands of the former Grand Duchy of Lithuania (Lithuania and Belarus) and Ukraine. The Ukrainians ended up with no state of their own and felt betrayed by the Riga arrangements; their resentment gave rise to extreme nationalism and anti-Polish hostility. The Kresy (or borderland) territories in the east won by 1921 would form the basis for a swap arranged and carried out by the Soviets in 1943–45, who at that time compensated the re-emerging Polish state for the eastern lands lost to the Soviet Union with conquered areas of eastern Germany.\n\nThe successful outcome of the Polish–Soviet War gave Poland a false sense of its prowess as a self-sufficient military power and encouraged the government to try to resolve international problems through imposed unilateral solutions. The territorial and ethnic policies of the interwar period contributed to bad relations with most of Poland's neighbors and uneasy cooperation with more distant centers of power, especially France and Great Britain.\n\nAmong the chief difficulties faced by the government of the new Polish republic was the lack of an integrated infrastructure among the formerly separate partitions, a deficiency that disrupted industry, transportation, trade and other areas.\n\nThe first Polish legislative election for the re-established Sejm of the Republic of Poland took place in January 1919. A temporary Small Constitution was passed by the body the following month.\n\nThe rapidly growing population of Poland within its new boundaries was ¾ agricultural and ¼ urban; Polish was the primary language of only ⅔ of the inhabitants of the new country. The minorities had very little voice in the government. The permanent March Constitution of Poland was adopted in March 1921. At the insistence of the National Democrats, who were concerned about how aggressively Józef Piłsudski might exercise presidential powers if he were elected to office, the constitution mandated limited prerogatives for the presidency.\n\nThe proclamation of the March Constitution was followed by a short and turbulent period of constitutional order and parliamentary democracy that lasted until 1926. The legislature remained fragmented, without stable majorities, and governments changed frequently. The open-minded Gabriel Narutowicz was elected president constitutionally (without a popular vote) by the National Assembly in 1922. However, members of the nationalist right-wing faction did not regard his elevation as legitimate. They viewed Narutowicz rather as a traitor whose election was pushed through by the votes of alien minorities. Narutowicz and his supporters were subjected to an intense harassment campaign, and the president was assassinated on December 16, 1922, after serving only five days in office.\n\nCorruption was held to be commonplace in the political culture of the early Polish Republic. However, the investigations conducted by the new regime after the 1926 May Coup failed to uncover any major affair or corruption scheme within the state apparatus of its predecessors.\n\nLand reform measures were passed in 1919 and 1925 under pressure from an impoverished peasantry. They were partially implemented, but resulted in the parcellation of only 20% of the great agricultural estates. Poland endured numerous economic calamities and disruptions in the early 1920s, including waves of workers' strikes such as the 1923 Kraków riot. The German–Polish customs war, initiated by Germany in 1925, was one of the most damaging external factors that put a strain on Poland's economy. On the other hand, there were also signs of progress and stabilization, for example a critical reform of finances carried out by the competent government of Władysław Grabski, which lasted almost two years. Certain other achievements of the democratic period having to do with the management of governmental and civic institutions necessary to the functioning of the reunited state and nation were too easily overlooked. Lurking on the sidelines was a disgusted army officer corps unwilling to subject itself to civilian control, but ready to follow the retired Piłsudski, who was highly popular with Poles and just as dissatisfied with the Polish system of government as his former colleagues in the military.\n\nOn May 12, 1926, Piłsudski staged the May Coup, a military overthrow of the civilian government mounted against President Stanisław Wojciechowski and the troops loyal to the legitimate government. Hundreds died in fratricidal fighting. Piłsudski was supported by several leftist factions who ensured the success of his coup by blocking the railway transportation of government forces. He also had the support of the conservative great landowners, a move that left the right-wing National Democrats as the only major social force opposed to the takeover.\n\nFollowing the coup, the new regime initially respected many parliamentary formalities, but gradually tightened its control and abandoned pretenses. Centrolew, a coalition of center-left parties, was formed in 1929, and in 1930 called for the \"abolition of dictatorship\". In 1930, the Sejm was dissolved and a number of opposition deputies were imprisoned at the Brest Fortress. Five thousand political opponents were arrested ahead of the Polish legislative election of 1930, which was rigged to award a majority of seats to the pro-regime Nonpartisan Bloc for Cooperation with the Government (BBWR).\n\nThe authoritarian \"Sanation\" regime (meant to denote a \"healing\" regime) that Piłsudski led until his death in 1935 (and would remain in place until 1939) reflected the dictator's evolution from his center-left past to conservative alliances. Political institutions and parties were allowed to function, but the electoral process was manipulated and those not willing to cooperate submissively were subjected to repression. From 1930, persistent opponents of the regime, many of the leftist persuasion, were imprisoned and subjected to staged legal processes with harsh sentences, such as the Brest trials, or else detained in the Bereza Kartuska prison and similar camps for political prisoners. About three thousand were detained without trial at different times at the Bereza concentration camp between 1934 and 1939. In 1936 for example, 369 activists were taken there, including 342 Polish communists. Rebellious peasants staged riots in 1932, 1933 and the 1937 peasant strike in Poland. Other civil disturbances were caused by striking industrial workers (e.g. events of the \"Bloody Spring\" of 1936), nationalist Ukrainians and the activists of the incipient Belarusian movement. All became targets of ruthless police-military pacification. Besides sponsoring political repression, the regime also fostered a Piłsudski cult of personality that had already existed long before he assumed dictatorial powers.\n\nPiłsudski signed the Soviet–Polish Non-Aggression Pact in 1932 and the German–Polish Non-Aggression Pact in 1934, but in 1933 he insisted that there was no threat from the East or West and said that Poland's politics were focused on becoming fully independent without serving foreign interests. He initiated the policy of maintaining an equal distance and an adjustable middle course regarding the two great neighbors, later continued by Józef Beck. Piłsudski kept personal control of the army, but it was poorly equipped, poorly trained and had poor preparations in place for possible future conflicts. His only war plan was a defensive war against a Soviet invasion. The slow modernization after Piłsudski's death fell far behind the progress made by Poland's neighbors and measures to protect the western border, discontinued by Piłsudski from 1926, were not undertaken until March 1939.\n\nSanation deputies in the Sejm used a parliamentary maneuver to abolish the democratic March Constitution and push through a more authoritarian April Constitution in 1935; it reduced the powers of the Sejm, which Piłsudski despised. The process and the resulting document were seen as illegitimate by the anti-Sanation opposition, but during World War II, the Polish government-in-exile recognized the April Constitution in order to uphold the legal continuity of the Polish state.\n\nWhen Marshal Piłsudski died in 1935, he retained the support of the main sections of Polish society even though he never risked testing his popularity in an honest election. His regime was dictatorial, but at that time only Czechoslovakia remained democratic in all of the regions neighboring Poland. Historians have taken widely divergent views of the meaning and consequences of the coup he perpetrated and his personal rule that followed.\n\nIndependence stimulated the development of Polish culture in the Interbellum and intellectual achievement was high. Warsaw, whose population almost doubled between World War I and World War II, was a restless, burgeoning metropolis. It outpaced Kraków, Lwów and Wilno, the other major population centers of the country.\n\nMainstream Polish society was not affected by the repressions of the Sanation authorities overall; many Poles enjoyed relative stability, and the economy improved markedly between 1926 and 1929, only to become caught up in the global Great Depression. After 1929, the country's industrial production and gross national income slumped by about 50%.\n\nThe Great Depression brought low prices for farmers and unemployment for workers. Social tensions increased, including rising antisemitism. A major economic transformation and multi-year state plan to achieve national industrial development, as embodied in the Central Industrial Region initiative launched in 1936, was led by Minister Eugeniusz Kwiatkowski. Motivated primarily by the need for a native arms industry, the initiative was in progress at the time of the outbreak of World War II. Kwiatkowski was also the main architect of the earlier Gdynia seaport project.\n\nThe prevalent nationalism in political circles was fueled by the large size of Poland's minority populations and their separate agendas. According to the language criterion of the Polish census of 1931, the Poles constituted 69% of the population, Ukrainians 15%, Jews (defined as speakers of the Yiddish language) 8.5%, Belarusians 4.7%, Germans 2.2%, Lithuanians 0.25%, Russians 0.25% and Czechs 0.09%, with some geographical areas dominated by a particular minority. In time, the ethnic conflicts intensified, and the Polish state grew less tolerant of the interests of its national minorities. In interwar Poland, compulsory free general education substantially reduced illiteracy rates, but discrimination was practiced in a way that resulted in a dramatic decrease in the number of Ukrainian language schools and official restrictions on Jewish attendance at selected schools in the late 1930s.\n\nThe population grew steadily, reaching 35 million in 1939. However, the overall economic situation in the interwar period was one of stagnation. There was little money for investment inside Poland, and few foreigners were interested in investing there. Total industrial production barely increased between 1913 and 1939 (within the area delimited by the 1939 borders), but because of population growth (from 26.3 millions in 1919 to 34.8 millions in 1939), the \"per capita\" output actually decreased by 18%.\n\nConditions in the predominant agricultural sector kept deteriorating between 1929 and 1939, which resulted in rural unrest and a progressive radicalization of the Polish peasant movement that became increasingly inclined toward militant anti-state activities. It was firmly repressed by the authorities. According to Norman Davies, the failures of the Sanation regime (combined with the objective economic realities) caused a radicalization of the Polish masses by the end of the 1930s, but he warns against drawing parallels with the incomparably more destructive precedents of Nazi Germany or the Soviet Union under Stalin.\n\nAfter Piłsudski's death in 1935, Poland was governed until the German invasion of 1939 by old allies and subordinates known as \"Piłsudski's colonels\". They had neither the vision nor the resources to cope with the perilous situation facing Poland in the late 1930s. The colonels had gradually assumed greater powers during Piłsudski's life by manipulating the ailing marshal behind the scenes. Eventually they achieved an overt politicization of the army that did nothing to help prepare the country for war.\n\nForeign policy was the responsibility of Józef Beck, under whom Polish diplomacy attempted balanced approaches toward Germany and the Soviet Union, unfortunately without success, on the basis of a flawed understanding of the European geopolitics of his day. Beck had numerous foreign policy schemes and harbored illusions of Poland's status as a great power. He alienated most of Poland's neighbors, but is not blamed by historians for the ultimate failure of relations with Germany. The principal events of his tenure were concentrated in its last two years. In the case of the 1938 Polish ultimatum to Lithuania, the Polish action nearly resulted in a German takeover of southwest Lithuania. Also in 1938, the Polish government opportunistically undertook a hostile action against the Czechoslovak state as weakened by the Munich Agreement and annexed a small piece of territory on its borders. In this case, Beck's understanding of the consequences of the Polish military move turned out to be completely mistaken. In the end, the German occupation of Czechoslovakia ushered in by the Munich Agreement markedly weakened Poland's own position. Furthermore, Beck mistakenly believed that Nazi-Soviet ideological contradictions would preclude their cooperation.\n\nAt home, increasingly alienated minorities threatened unrest and violence and were suppressed. Extreme nationalist circles such as the National Radical Camp grew more outspoken. One of the groups, the Camp of National Unity, combined many nationalists with Sanation supporters and was connected to the new strongman, Marshal Edward Rydz-Śmigły, whose faction of the Sanation ruling movement was increasingly nationalistic.\n\nIn the late 1930s, the exile bloc Front Morges united several major Polish anti-Sanation figures, including Ignacy Paderewski, Władysław Sikorski, Wincenty Witos, Wojciech Korfanty and Józef Haller. It gained little influence inside Poland, but its spirit soon reappeared during World War II, within the Polish government-in-exile.\n\nIn October 1938, Joachim von Ribbentrop first proposed German-Polish territorial adjustments and Poland's participation in the Anti-Comintern Pact against the Soviet Union. The status of the Free City of Danzig was one of the key bones of contention. Approached by Ribbentrop again in March 1939, the Polish government expressed willingness to address issues causing German concern, but effectively rejected Germany's stated demands and thus refused to allow Poland to be turned by Adolf Hitler into a German puppet state. Hitler, incensed by the British and French declarations of support for Poland, abrogated the German–Polish Non-Aggression Pact in late April 1939.\n\nTo protect itself from an increasingly aggressive Nazi Germany, already responsible for the annexations of Austria (in the Anschluss of 1938), Czechoslovakia (in 1939) and a part of Lithuania after the 1939 German ultimatum to Lithuania, Poland entered into a military alliance with Britain and France (the 1939 Anglo-Polish military alliance and the earlier Franco-Polish military alliance of 1921, as updated in 1939). However, the two Western powers were defense-oriented and not in a strong position, either geographically or in terms of resources, to assist Poland. Attempts were therefore made by them to induce Soviet-Polish cooperation, which they viewed as the only militarily viable possibility. \n\nDiplomatic manoeuvers continued in the spring and summer of 1939, but in their final attempts, the Franco-British talks with the Soviets in Moscow on forming an anti-Nazi defensive military alliance failed. Warsaw's refusal to allow the Red Army to operate on Polish territory doomed the Western efforts. The final contentious Allied-Soviet exchanges took place on August 21 and 23, 1939. Stalin's regime was the target of an intense German counter-initiative and was concurrently involved in increasingly effective negotiations with Hitler's agents. On August 23, an outcome contrary to the exertions of the Allies became a reality: in Moscow, Germany and the Soviet Union hurriedly signed the Molotov–Ribbentrop non-aggression pact, which secretly provided for the dismemberment of Poland into Nazi and Soviet-controlled zones.\n\nOn September 1, 1939, Hitler ordered the invasion of Poland, the opening event of World War II. Poland had signed an Anglo-Polish military alliance as recently as August 25, and had long been in alliance with France. The two Western powers soon declared war on Germany, but they remained largely inactive (the period early in the conflict became known as the Phoney War) and extended no aid to the attacked country. The numerically and technically superior \"Wehrmacht\" formations rapidly advanced eastwards and engaged massively in the murder of Polish civilians over the entire occupied territory. On September 17, a Soviet invasion of Poland began. The Soviet Union quickly occupied most of the areas of eastern Poland that contained large populations of Ukrainians and Belarusians. The two invading powers divided up the country as they had agreed in the secret provisions of the Molotov–Ribbentrop Pact. Poland's top government officials and military high command fled the war zone and arrived at the Romanian Bridgehead in mid-September. After the Soviet entry they sought refuge in Romania.\n\nAmong the military operations in which Poles held out the longest (until late September or early October) were the Siege of Warsaw, the Battle of Hel and the resistance of the Independent Operational Group Polesie. Warsaw fell on 27 September after a heavy German bombardment that killed about 40,000 civilians. Poland was ultimately partitioned between Germany and the Soviet Union according to the terms of the German–Soviet Treaty of Friendship, Cooperation and Demarcation signed by the two powers in Moscow on September 29.\n\nGerhard Weinberg has argued that the most significant Polish contribution to World War II was sharing its code-breaking results. This allowed the British to perform the cryptanalysis of the Enigma and decipher the main German military code, which gave the Allies a major advantage in the conflict. As regards actual military campaigns, some Polish historians have argued that simply resisting the initial invasion of Poland was the country's greatest contribution to the victory over Nazi Germany, despite its defeat. The Polish Army of nearly one million men significantly delayed the start of the Battle of France, planned for 1939. When the Nazi offensive in the West did happen, the delay caused it to be less effective, a possibly crucial factor in the victory of the Battle of Britain.\n\nAfter Germany invaded the Soviet Union as part of its Operation Barbarossa in June 1941, the whole of pre-war Poland was overrun and occupied by German troops.\n\nGerman-occupied Poland was divided from 1939 into two regions: Polish areas annexed by Nazi Germany directly into the German Reich and areas ruled under a so-called General Government of occupation. The Poles formed an underground resistance movement and a Polish government-in-exile that operated first in Paris, then, from July 1940, in London. Polish-Soviet diplomatic relations, broken since September 1939, were resumed in July 1941 under the Sikorski–Mayski agreement, which facilitated the formation of a Polish army (the Anders' Army) in the Soviet Union.<ref name=\"Department of State 10/03\">.</ref> In November 1941, Prime Minister Sikorski flew to the Soviet Union to negotiate with Stalin on its role on the Soviet-German front, but the British wanted the Polish soldiers in the Middle East. Stalin agreed, and the army was evacuated there.\n\nThe members of the Polish Underground State that functioned in Poland throughout the war were loyal to and formally under the Polish government-in-exile, acting through its Government Delegation for Poland. During World War II, about 400,000 Poles joined the underground Polish Home Army (\"Armia Krajowa\"), a part of the Polish Armed Forces of the government-in-exile. About 200,000 fought in the Western Front in Polish armed forces loyal to the government-in-exile, and about 300,000 in the Eastern Front. The pro-Soviet resistance movement, led by the Polish Workers' Party, was active from 1941. It was opposed by the gradually forming extreme nationalistic National Armed Forces.\n\nBeginning in late 1939, hundreds of thousands of Poles from the Soviet-occupied areas were deported and taken east. Of the upper-ranking military personnel and others deemed uncooperative or potentially harmful by the Soviets, about 22,000 were secretly executed. \nIn April 1943, the Soviet Union broke off deteriorating relations with the Polish government-in-exile after the German military announced the discovery of mass graves containing Polish army officers murdered by the Soviets at the Katyn massacre. The Soviets claimed that the Poles committed a hostile act by requesting that the Red Cross investigate these reports.\n\nFrom 1941, the implementation of the Final Solution began, and the Holocaust in Poland proceeded with force. As the Jewish ghetto in occupied Warsaw was being liquidated by Nazi SS units, the city was the scene of the Warsaw Ghetto Uprising in April–May 1943. The elimination of Jewish ghettos in German-occupied Poland took place in a number of cities besides Warsaw and other uprisings were waged against impossible odds by desperate Jewish insurgents, whose people were being removed and exterminated.\n\nAt a time of increasing cooperation between the Western Allies and the Soviet Union in the wake of the Nazi invasion of 1941, the influence of the Polish government-in-exile was seriously diminished by the death of Prime Minister Władysław Sikorski, its most capable leader, in a plane crash on July 4, 1943. His successors lacked the ability or willingness to negotiate effectively with the Soviets and proved equally ineffective in pressing for the interests of the Polish people with the Western Allies.\n\nIn July 1944, the Soviet Red Army and Soviet-controlled Polish People's Army entered the territory of future postwar Poland. In protracted fighting in 1944 and 1945, the Soviets and their Polish allies defeated and expelled the German army from Poland at a cost of over 600,000 Soviet and over 60,000 Polish soldiers lost.\n\nThe greatest single action of the Polish resistance movement in World War II and a major political event was the Warsaw Uprising that began on August 1, 1944. The uprising, in which most of the city's population participated, was instigated by the underground Home Army and approved by the Polish government-in-exile in an attempt to establish a non-communist Polish administration ahead of the arrival of the Red Army. The uprising was originally planned as a short-lived armed demonstration in expectation that the Soviet forces approaching Warsaw would assist in any battle to take the city. The Soviets had never agreed to an intervention, however, and they halted their advance at the Vistula River. The Germans used the opportunity to carry out a brutal suppression of the forces of the pro-Western Polish underground.\n\nThe bitterly fought uprising lasted for two months and resulted in the death or expulsion from the city of hundreds of thousands of civilians. After the Poles realised the hopelessness of the situation and surrendered on 2 October, the Germans carried out a planned destruction of Warsaw on Hitler's orders that obliterated the remaining infrastructure of the city. The Polish First Army, fighting alongside the Soviet Red Army, entered a devastated Warsaw on 17 January 1945.\n\nFrom the time of the Tehran Conference in late 1943, there was broad agreement among the United States, Great Britain, and the Soviet Union that the locations of the borders between Germany and Poland and between Poland and the Soviet Union would be fundamentally changed after the conclusion of World War II. Stalin's proposal that Poland should be moved far to the west was readily accepted by the Polish communists, who were at that time in the early stages of forming a post-war government (the State National Council, a quasi-parliamentary body, was created). In July 1944, a communist-controlled Polish Committee of National Liberation was established in Lublin nominally to govern the areas liberated from German control, a move that prompted protests from Prime Minister Stanisław Mikołajczyk and his government-in-exile.\n\nBy the time of the Yalta Conference in February 1945, the communists had already established a Provisional Government of the Republic of Poland. The Soviet position at the conference was strong because of their decisive contribution to the war effort and as a result of their occupation of immense amounts of land in central and eastern Europe. The three Great Powers (the United States, Great Britain, and the Soviet Union) gave assurances that the communist provisional government would be converted into an entity that would include democratic forces from within the country and active abroad, but the London-based government-in-exile was not mentioned. A Provisional Government of National Unity and subsequent democratic elections were the agreed stated goals. The disappointing results of these plans and the failure of the Western powers to ensure the strong participation of non-communists in the immediate post-war Polish government were seen by many Poles as a manifestation of Western betrayal.\n\nA lack of accurate data makes it difficult to document numerically the extent of the human losses suffered by Polish citizens during World War II. Additionally, many assertions made in the past must be considered suspect due to flawed methodology and a desire to promote certain political agendas. The last available enumeration of ethnic Poles and the large ethnic minorities is the Polish census of 1931. Exact population figures for 1939 are therefore not known.\n\nModern research indicates that about 5 million Polish citizens were killed during the war by the Nazis, including 3 million Polish Jews. According to the United States Holocaust Memorial Museum, at least 1.9 to 2 million ethnic Poles and 3 million Polish Jews were killed. Millions of Polish citizens were deported to Germany for forced labor or to German death camps such as Treblinka, Auschwitz and Sobibor. According to another estimate, between 2.35 and 2.9 million Polish Jews and about 2 million ethnic Poles were killed. Nazi Germany intended to exterminate the Jews completely, in actions that have come to be described collectively as the Holocaust. The Poles were to be expelled from areas controlled by Nazi Germany through a process of resettlement that started in 1939 and was expected to be completed within 15 years.\n\nIn an attempt to incapacitate Polish society, the Nazis and the Soviets executed tens of thousands of members of the intelligentsia and community leadership during events such as the German AB-Aktion in Poland, Operation Tannenberg and the Katyn massacre. Over 95% of the Jewish losses and 90% of the ethnic Polish losses were caused directly by Nazi Germany, whereas 5% of the ethnic Polish losses were caused by the Soviets and 5% by Ukrainian nationalists. The large-scale Jewish presence in Poland that had endured for centuries was rather quickly put to an end by the policies of extermination implemented by the Nazis during the war. Waves of displacement and emigration that took place both during and after the war removed from Poland a majority of the Jews who survived. Further significant Jewish emigration followed events such as the Polish October political thaw of 1956 and the 1968 Polish political crisis. The magnitude of the losses of Polish citizens of German, Ukrainian, Belarusian and other nationalities, which were also great, are not known.\n\nIn 1940–41, some 325,000 Polish citizens were deported by the Soviet regime. The number of Polish citizens who died at the hands of the Soviets is estimated at less than 100,000.\n\nIn 1943–44, Ukrainian nationalists associated with the Organization of Ukrainian Nationalists (OUN) and the Ukrainian Insurgent Army perpetrated the Massacres of Poles in Volhynia and Eastern Galicia. The number of Polish civilian victims are estimated at 60 up to 200 thousands of people.\n\nApproximately 90% of Poland's war casualties were the victims of prisons, death camps, raids, executions, the annihilation of ghettos, epidemics, starvation, excessive work and ill treatment. The war left one million children orphaned and 590,000 persons disabled. The country lost 38% of its national assets (whereas Britain lost only 0.8%, and France only 1.5%). Nearly half of pre-war Poland was expropriated by the Soviet Union, including the two great cultural centers of Lwów and Wilno.\n\nBy the terms of the 1945 Potsdam Agreement signed by the United States, the Soviet Union, and Great Britain, the Soviet Union retained most of the territories captured as a result of the Molotov–Ribbentrop Pact of 1939, including western Ukraine and western Belarus, and gained others. Lithuania and the Königsberg area of East Prussia were officially incorporated into the Soviet Union, in the case of the former without the recognition of the Western powers. Poland was compensated with the bulk of Silesia, including Breslau (Wrocław) and Grünberg (Zielona Góra), the bulk of Pomerania, including Stettin (Szczecin), and the greater southern portion of the former East Prussia, along with Danzig (Gdańsk). Collectively referred to as the \"Recovered Territories\", they were included in the reconstituted Polish state. With Germany's defeat, the re-established Polish state was thus shifted west to the area between the Oder–Neisse and Curzon lines. The Poles lost 70% of their pre-war oil capacity to the Soviets, but gained from the Germans a highly developed industrial base and infrastructure that made a diversified industrial economy possible for the first time in Polish history.\n\nThe flight and expulsion of Germans from what was eastern Germany prior to the war began before and during the Soviet conquest of those regions from the Nazis, and the process continued in the years immediately after the war. Of those who remained, many chose to emigrate to post-war Germany. On the other hand, 1.5–2 million Poles moved or were expelled from Polish areas annexed by the Soviet Union. The vast majority were resettled in the former German territories.\n\nMany exiled Poles could not return to the country for which they had fought because they belonged to political groups incompatible with the new communist regimes, or because they originated from areas of pre-war eastern Poland that were incorporated into the Soviet Union (see Polish population transfers of the period 1944-46). Some were deterred from returning simply on the strength of warnings that anyone who had served in Western military units would be endangered under the new communist regimes. Many Poles were pursued, arrested, tortured and imprisoned by the Soviet authorities for belonging to the Home Army or other formations (see Anti-communist resistance in Poland during the period 1944-46), or were persecuted because they had fought on the Western front.\n\nTerritories on both sides of the new Polish-Ukrainian border were also \"ethnically cleansed\". Of the Ukrainians and Lemkos living in Poland within the new borders (about 700,000), close to 95% were forcibly moved to the Soviet Ukraine, or (in 1947) to the new territories in northern and western Poland under Operation Vistula. In Volhynia, 98% of the Polish pre-war population was either killed or expelled; in Eastern Galicia, the Polish population was reduced by 92%. In all, about 70,000 Poles and about 20,000 Ukrainians were killed in the ethnic violence that occurred in the 1940s, both during and after the war.\n\nAccording to an estimate by Polish researchers, 40–60,000 of the 200–250,000 Polish Jews who escaped the Nazis survived without leaving Poland (the remainder perished). More were repatriated from the Soviet Union and elsewhere, and the February 1946 population census showed about 300,000 Jews within the new borders. Of the surviving Jews, many chose to emigrate or felt compelled to because of anti-Jewish violence in Poland.\n\nBecause of changing borders and the mass movements of people of various nationalities, the emerging communist Poland ended up with a mainly homogeneous, ethnically Polish population (97.6% according to the December 1950 census). Minority members were not encouraged by the authorities or their neighbors to emphasize their ethnic identities.\n\nIn response to the February 1945 Yalta Conference directives, a Polish Provisional Government of National Unity was formed in June 1945 under Soviet auspices; it was soon recognized by the United States and many other countries. Communist rule and Soviet domination were apparent from the beginning: sixteen prominent leaders of the Polish anti-Nazi underground were brought to trial in Moscow (\"the Trial of the Sixteen\") already in June 1945. In the immediate post-war years, emerging communist rule was challenged by opposition groups (\"cursed soldiers\"), and many thousands perished in the fight or were pursued by the Ministry of Public Security and executed. Such insurgents often pinned their hopes on expectations of the imminent outbreak of a World War III and the defeat of the Soviet Union. The Polish right-wing insurgency faded after the amnesty of February 1947.\n\nThe Polish people's referendum of June 1946 was arranged by the communist Polish Workers' Party to legitimize its dominance over Polish politics and claim widespread support for the Party's policies. Although the Yalta agreement called for free elections, the Polish legislative election of January 1947 was controlled by the communists. Some democratic and pro-Western elements, led by Stanisław Mikołajczyk, the former prime minister-in-exile, participated in the Provisional Government and the 1947 elections, but were ultimately eliminated through electoral fraud, intimidation and violence. In times of radical political and economic change, members of Mikołajczyk's agrarian movement (the Polish People's Party) attempted to preserve some degree of market economy to protect rights and interests of limited property ownership. After the 1947 elections, the Government of National Unity ceased to exist and the communist-dominated Front of National Unity was officially the only source of governmental authority. The Polish government-in-exile remained in continuous existence until 1990, although its influence declined.\n\nThe Polish People's Republic (\"Polska Rzeczpospolita Ludowa\") was established under the rule of the communist Polish United Workers' Party. The name change from the Polish Republic was not officially adopted, however, until the proclamation of the Constitution of the Polish People's Republic in 1952.\n\nThe ruling party itself was formed by the forced amalgamation in December 1948 of the communist Polish Workers' Party and the historically non-communist Polish Socialist Party. The latter, re-established in 1944 by its left wing, had since been allied with the communists. The ruling communists, who in post-war Poland preferred to use the term \"socialism\" instead of \"communism\" to identify their ideological basis, needed to include the socialist junior partner to broaden their appeal, claim greater legitimacy and eliminate competition on the political Left. The socialists, who were losing their organization, were subjected to political pressure, ideological cleansing and purges in order to become suitable for unification on the terms of the \"Workers' Party\". The leading pro-communist leaders of the socialists were the prime ministers Edward Osóbka-Morawski and Józef Cyrankiewicz.\n\nDuring the most oppressive phase of the Stalinist period (1948–53), terror was justified in Poland as necessary to eliminate reactionary subversion. Many thousands of perceived opponents of the regime were arbitrarily tried, and large numbers were executed. The People's Republic was led by discredited Soviet operatives such as Bolesław Bierut, Jakub Berman and Konstantin Rokossovsky. The independent Catholic Church in Poland was subjected to property confiscations and other curtailments from 1949, and in 1950 was pressured into signing an accord with the government. In 1953 and later, despite a partial thaw after the death of Joseph Stalin that year, the persecution of the Church intensified and its head, Cardinal Stefan Wyszyński, was detained. A key event in the persecution of the Polish church was the Stalinist show trial of the Kraków Curia in January 1953.\n\nIn the Warsaw Pact, formed in 1955, the army of the Polish People's Republic was the second largest, after the Soviet Army.\n\nIn 1944, large agricultural holdings and former German property in Poland started to be redistributed through land reform, and industry started to be nationalized. Communist restructuring and the imposition of work-space rules encountered active worker opposition already in the years 1945–47. The Three-Year Plan of 1947–49 continued with the rebuilding, socialization and socialist restructuring of the economy. It was followed by the Six-Year Plan of 1950–55 for heavy industry. The rejection of the Marshall Plan in 1947 made aspirations for catching up with West European standards of living unrealistic.\n\nThe government's highest economic priority was the development of heavy industry useful to the military. State-run or controlled institutions common in all the socialist countries of eastern Europe were imposed on Poland, including collective farms and worker cooperatives. The latter were dismantled in the late 1940s as not socialist enough, although they were later re-established; even small-scale private enterprises were eradicated. Stalinism introduced heavy political and ideological propaganda and indoctrination in social life, culture and education.\n\nGreat strides were made, however, in the areas of employment (which became nearly full), universal public education (which nearly eradicated adult illiteracy), health care and recreational amenities. Many historic sites, including the central districts of Warsaw and Gdańsk, both devastated during the war, were rebuilt at great cost.\n\nThe communist industrialization program led to increased urbanization and educational and career opportunities for the intended beneficiaries of the social transformation along the lines of the peasants-workers-working intelligentsia paradigm. The most significant improvement was accomplished in the lives of Polish peasants, many of whom were able to leave their impoverished and overcrowded village communities for better conditions in urban centers. Those who stayed behind took advantage of the implementation of the 1944 land reform decree of the Polish Committee of National Liberation, which terminated the antiquated, but widespread parafeudal socioeconomic relations in Poland. Under Stalinism, attempts were made at establishing collective farms; they generally failed. Due to urbanization, the national percentage of the rural population decreased in communist Poland by about 50%. A majority of Poland's residents of cities and towns still live in apartment blocks built during the communist era in part to accommodate migrants from rural areas.\n\nIn March 1956, after the 20th Soviet Party Congress in Moscow ushered in de-Stalinization, Edward Ochab was chosen to replace the deceased Bolesław Bierut as first secretary of the Polish United Workers' Party. As a result, Poland was rapidly overtaken by social restlessness and reformist undertakings; thousands of political prisoners were released and many people previously persecuted were officially rehabilitated. Worker riots in Poznań in June 1956 were violently suppressed, but they gave rise to the formation of a reformist current within the communist party.\n\nAmidst continuing social and national upheaval, a further shakeup took place in the party leadership as part of what is known as the Polish October of 1956. While retaining most traditional communist economic and social aims, the regime led by the new Polish Party's First Secretary Władysław Gomułka liberalized internal life in Poland. The dependence on the Soviet Union was somewhat mollified, and the state's relationships with the Church and Catholic lay activists were put on a new footing. A repatriation agreement with the Soviet Union allowed the repatriation of hundreds of thousands of Poles who were still in Soviet hands, including many former political prisoners. Collectivization efforts were abandoned—agricultural land, unlike in other Comecon countries, mostly remained in the private ownership of farming families. State-mandated provisions of agricultural products at fixed, artificially low prices were reduced and, from 1972, eliminated.\n\nCulture in the Polish People's Republic, to varying degrees linked to the intelligentsia's opposition to the authoritarian system, developed to a sophisticated level under Gomułka and his successors. The creative process was often compromised by state censorship, but significant works were created in fields such as literature, theater, cinema and music, among others. Journalism of veiled understanding and varieties of native and western popular culture were well represented. Uncensored information and works generated by émigré circles were conveyed through a variety of channels. The Paris-based Kultura magazine developed a conceptual framework for dealing with the issues of borders and the neighbors of a future free Poland, but Radio Free Europe was of foremost importance.\n\nThe legislative election of 1957 was followed by several years of political stability that was accompanied by economic stagnation and curtailment of reforms and reformists. One of the last initiatives of the brief reform era was a nuclear weapons–free zone in Central Europe proposed in 1957 by Adam Rapacki, Poland's foreign minister. One of the confirmations of the end of an era of greater tolerance was the expulsion from the communist party of several prominent \"Marxist revisionists\" in the 1960s.\n\nIn 1965, the Conference of Polish Bishops issued the Letter of Reconciliation of the Polish Bishops to the German Bishops, a gesture intended to heal bad mutual feelings left over from World War II. In 1966, the celebrations of the 1,000th anniversary of the Baptism of Poland led by Cardinal Stefan Wyszyński and other bishops turned into a huge demonstration of the power and popularity of the Catholic Church in Poland.\n\nThe post-1956 liberalizing trend, in decline for a number of years, was reversed in March 1968, when student demonstrations were suppressed during the 1968 Polish political crisis. Motivated in part by the Prague Spring movement, the Polish opposition leaders, intellectuals, academics and students used a historical-patriotic Dziady theater spectacle series in Warsaw (and its termination forced by the authorities) as a springboard for protests, which soon spread to other centers of higher education and turned nationwide. The authorities responded with a major crackdown on opposition activity, including the firing of faculty and the dismissal of students at universities and other institutions of learning. At the center of the controversy was also the small number of Catholic deputies in the Sejm (the Znak Association members) who attempted to defend the students.\n\nIn an official speech, Gomułka drew attention to the role of Jewish activists in the events taking place. This provided ammunition to a nationalistic and antisemitic communist party faction headed by Mieczysław Moczar that was opposed to Gomułka's leadership. Using the context of the military victory of Israel in the Six-Day War of 1967, some in the Polish communist leadership waged an antisemitic campaign against the remnants of the Jewish community in Poland. The targets of this campaign were accused of disloyalty and active sympathy with Israeli aggression. Branded \"Zionists\", they were scapegoated and blamed for the unrest in March, which eventually led to the emigration of much of Poland's remaining Jewish population (about 15,000 Polish citizens left the country).\n\nWith the active support of the Gomułka regime, the People's Army of Poland took part in the infamous Warsaw Pact invasion of Czechoslovakia in August 1968 after the informal announcement of the Brezhnev Doctrine.\n\nIn December 1970, the governments of Poland and West Germany signed the Treaty of Warsaw, which normalized their relations and made possible meaningful cooperation in a number of areas of bilateral interest. West Germany recognized the post-war \"de facto\" border between Poland and East Germany.\n\nPrice increases for essential consumer goods triggered the Polish protests of 1970. In December, there were disturbances and strikes in the port cities of Gdańsk, Gdynia, and Szczecin that reflected deep dissatisfaction with living and working conditions in the country. The activity was centered in the industrial shipyard areas of the three coastal cities. Dozens of protesting workers and bystanders were killed in police and military actions, generally under the authority of Gomułka and Minister of Defense Wojciech Jaruzelski. In the aftermath, Edward Gierek replaced Gomułka as first secretary of the communist party. The new regime was seen as more modern, friendly and pragmatic, and at first it enjoyed a degree of popular and foreign support.\n\nGierek's regime introduced wide-ranging (but ultimately unsuccessful) government reforms to revitalize the economy between 1970 and 1980. Another attempt to raise food prices resulted in the June 1976 protests. Jacek Kuroń was among the activists who defended accused rioters from Radom and other towns. The Workers' Defence Committee (KOR), established in response to the crackdown, consisted of dissident intellectuals willing to support industrial workers, farmers and students who were struggling with and persecuted by the authorities throughout the late 1970s. During this period, the opposition circles were emboldened by the Helsinki Conference processes.\n\nIn October 1978, the Archbishop of Kraków, Cardinal Karol Józef Wojtyła, became Pope John Paul II, head of the Roman Catholic Church. Catholics and others rejoiced at the elevation of a Pole to the papacy and greeted his June 1979 visit to Poland with an outpouring of emotion.\n\nFueled by large infusions of Western credit, Poland's economic growth rate was one of the world's highest during the first half of the 1970s, but much of the borrowed capital was misspent, and the centrally planned economy was unable to use the new resources effectively. The 1973 oil crisis caused recession and high interest rates in the West, to which the Polish government had to respond with sharp domestic consumer price increases. The growing debt burden became insupportable in the late 1970s, and negative economic growth set in by 1979.\n\nAround July 1, 1980, with the Polish foreign debt standing at more than $20 billion, the government made another attempt to increase meat prices. Workers responded with escalating work stoppages that culminated in the 1980 general strikes in Lublin. In mid-August, labor protests at the Gdańsk Shipyard gave rise to a chain reaction of strikes that virtually paralyzed the Baltic coast by the end of the month and, for the first time, closed most coal mines in Silesia. The Inter-Enterprise Strike Committee coordinated the strike action across hundreds of workplaces and formulated the 21 demands as the basis for negotiations with the authorities. The Strike Committee was sovereign in its decision-making, but was aided by a team of \"expert\" advisers that included Bronisław Geremek and Tadeusz Mazowiecki, well-known intellectuals and dissidents.\n\nOn August 31, 1980, representatives of workers at the Gdańsk Shipyard, led by an electrician and activist Lech Wałęsa, signed the Gdańsk Agreement with the government that ended their strike. Similar agreements were concluded in Szczecin (the Szczecin Agreement) and in Silesia. The key provision of these agreements was the guarantee of the workers' right to form independent trade unions and the right to strike. Following the successful resolution of the largest labor confrontation in communist Poland's history, nationwide union organizing movements swept the country.\n\nEdward Gierek was blamed by the Soviets for not following their \"fraternal\" advice, not shoring up the communist party and the official trade unions and allowing \"anti-socialist\" forces to emerge. On September 5, 1980, Gierek was replaced by Stanisław Kania as first secretary.\n\nDelegates of the emergent worker committees from all over Poland gathered in Gdańsk on September 17 and decided to form a single national union organization named \"Solidarity\" (the name was adopted following a suggestion by Karol Modzelewski).\n\nWhile party–controlled courts took up the contentious issues of Solidarity's legal registration as a trade union (finalized by November 10), planning had already begun for the imposition of martial law. A parallel farmers' union was organized and strongly opposed by the regime, but Rural Solidarity was finally registered on May 12, 1981. In the meantime, a rapid deterioration of the authority of the communist party, the disintegration of state power and an escalation of demands and threats by the various Solidarity–affiliated groups were occurring. According to Kuroń, a \"tremendous social democratization movement in all spheres\" was taking place and could not be contained. Wałęsa had meetings with Kania, which brought no resolution to the impasse. Following the Warsaw Pact summit in Moscow, the Soviet Union proceeded with a massive military build-up along Poland's border in December 1980, but during the summit, Kania forcefully argued with Leonid Brezhnev and other allied communists leaders against the feasibility of an external military intervention, and no action was taken. The United States, under presidents Jimmy Carter and Ronald Reagan, repeatedly warned the Soviets about the consequences of a direct intervention, while discouraging an open insurrection in Poland and signaling to the Polish opposition that there would be no rescue by the NATO forces.\n\nIn February 1981, Defense Minister General Wojciech Jaruzelski assumed the position of prime minister. A World War II veteran with a generally positive image, Jaruzelski engaged in preparations for calming the Polish unrest by the use of force, utilizing ZOMO troops and other security forces backed up by the Polish and Soviet bloc military. The 1980–81 Solidarity social revolt had thus far been free of any major use of force, but in March 1981 in Bydgoszcz, three activists were beaten up by the secret police. A nationwide \"warning strike\" took place, in which the 9.5-million-strong Solidarity union was supported by the population at large. A general strike was called off by Wałęsa after the March 30 settlement with the government. Both Solidarity and the communist party were badly split and the Soviets were losing patience. Kania was re-elected at the Party Congress in July, but the collapse of the economy continued and so did the general disorder.\n\nAt the first Solidarity National Congress in September–October 1981 in Gdańsk, Lech Wałęsa was elected national chairman of the Union with 55% of the vote. An appeal was issued to the workers of the other East European countries, urging them to follow in the footsteps of Solidarity. To the Soviets, the gathering was an \"anti-socialist and anti-Soviet orgy\" and the Polish communist leaders, increasingly led by Jaruzelski and General Czesław Kiszczak, were ready to apply force.\n\nIn October 1981, Jaruzelski was named the first secretary of the communist party, an unusual advancement for a military figure in the communist world. The Plenum's vote was 180 to 4, and he kept his government posts. Jaruzelski asked parliament to ban strikes and allow him to exercise extraordinary powers, but when neither request was granted, he decided to proceed with his plans anyway.\n\nOn December 12–13, 1981, the regime declared martial law in Poland, under which the army and ZOMO riot police were used to crush Solidarity. In the Soviet reaction to the Polish crisis of 1980–81, the Soviet leaders insisted that Jaruzelski pacify the opposition with the forces at his disposal, without direct Soviet involvement or backup. Virtually all Solidarity leaders and many affiliated intellectuals were arrested or detained. Nine workers were killed in the Pacification of Wujek. The United States and other Western countries responded by imposing economic sanctions against Poland and the Soviet Union. Unrest in the country was subdued, but continued.\n\nDuring martial law, Poland was ruled by the so-called Military Council of National Salvation. The open or semi-open opposition communications, as recently practiced, were replaced by underground publishing (known in the eastern bloc as Samizdat), and Solidarity was reduced to a few thousand underground activists.\n\nHaving achieved some semblance of stability, the Polish regime relaxed and then rescinded martial law over several stages. By December 1982, martial law was suspended, and a small number of political prisoners, including Wałęsa, were released. Although martial law formally ended in July 1983 and a partial amnesty was enacted, several hundred political prisoners remained in jail. Jerzy Popiełuszko, a popular pro-Solidarity priest, was abducted and murdered by security functionaries in October 1984.\n\nFurther developments in Poland occurred concurrently with and were influenced by the reformist leadership of Mikhail Gorbachev in the Soviet Union (processes known as Glasnost and Perestroika). In September 1986, a general amnesty was declared, and the government released nearly all political prisoners, but the authorities continued to harass dissidents and Solidarity activists. The regime's efforts to organize society from the top down had failed, while the opposition's attempts at creating an \"alternate society\" were also unsuccessful. With the economic crisis unresolved and societal institutions dysfunctional, both the ruling establishment and the opposition led by Solidarity leaders began looking for ways out of the stalemate. Facilitated by the indispensable mediation of the Catholic Church, exploratory contacts were established.\n\nStudent protests resumed in February 1988. The government's inability to forestall Poland's economic decline led to the 1988 Polish strikes across the country in April, May and August. The Soviet Union was becoming increasingly destabilized and unwilling to apply military or other pressure to prop up allied regimes in trouble. The Polish government felt compelled to negotiate with the opposition, and in September 1988, preliminary talks with Solidarity leaders ensued in Magdalenka. Numerous meetings took place involving Wałęsa and General Kiszczak, among others, and the regime made a major public relations mistake by allowing a televised debate in November between Wałęsa and Alfred Miodowicz, chief of the All-Poland Alliance of Trade Unions, the official trade union organization. The fitful bargaining and intra-party squabbling led to the official Round Table Negotiations in the following year, followed by the Polish legislative election of 1989, a watershed event marking the fall of communism in Poland.\n\nThe Polish Round Table Agreement of April 1989 called for local self-government, policies of job guarantees, legalization of independent trade unions and many wide-ranging reforms. The current Sejm promptly implemented the deal and agreed to National Assembly elections that were set for June 4 and June 18. Only 35% of the seats in the Sejm (the national legislature's lower house) and all of the Senate seats were freely contested; the remaining Sejm seats (65%) were guaranteed for the communists and their allies.\n\nThe failure of the communists at the polls (almost all of the contested seats were won by the opposition) resulted in a political crisis. The new April constitutional agreement called for the re-establishment of the Polish presidency and on July 19 the National Assembly elected the communist leader General Wojciech Jaruzelski to that office. His election, seen at the time as politically necessary, was barely accomplished with tacit support from some Solidarity deputies, and the new president's position was not strong. Moreover, the unexpected definitiveness of the parliamentary election results created new dynamics and attempts by the communists to form a government failed.\n\nOn August 19, President Jaruzelski asked journalist and Solidarity activist Tadeusz Mazowiecki to form a government; on September 12, the Sejm voted approval of Prime Minister Mazowiecki and his cabinet. Mazowiecki decided to leave the economic reform entirely in the hands of economic liberals led by the new Deputy Prime Minister Leszek Balcerowicz, who proceeded with the design and implementation of his \"shock therapy\" policy. For the first time in post-war history, Poland had a government led by non-communists, setting a precedent soon to be followed by other communist-ruled nations in a phenomenon known as the Revolutions of 1989. Mazowiecki's acceptance of the \"thick line\" formula meant that there would be no \"witch-hunt\", i.e., an absence of revenge seeking or exclusion from politics in regard to former communist officials.\n\nIn part because of the attempted indexation of wages, inflation reached 900% by the end of 1989, but was soon dealt with by means of radical methods. In December 1989, the Sejm approved the Balcerowicz Plan to transform the Polish economy rapidly from a centrally planned one to a free market economy. The Constitution of the Polish People's Republic was amended to eliminate references to the \"leading role\" of the communist party and the country was renamed the \"Republic of Poland\". The communist Polish United Workers' Party dissolved itself in January 1990. In its place, a new party, Social Democracy of the Republic of Poland, was created. \"Territorial self-government\", abolished in 1950, was legislated back in March 1990, to be led by locally elected officials; its fundamental unit was the administratively independent gmina.\n\nIn October 1990, the constitution was amended to curtail the term of President Jaruzelski. In November 1990, the German–Polish Border Treaty was signed.\n\nIn November 1990, Lech Wałęsa was elected president for a five-year term; in December, he became the first popularly elected president of Poland. Poland's first free parliamentary election was held in October 1991. 18 parties entered the new Sejm, but the largest representation received only 12% of the total vote.\n\nSeveral post-Solidarity governments were in existence between the 1989 election and the 1993 election, after which the \"post-communist\" left-wing parties took over. In 1993, the formerly Soviet Northern Group of Forces, a vestige of past domination, left Poland.\n\nIn 1995, Aleksander Kwaśniewski of the social democratic party was elected president and remained in that capacity for the next ten years (two terms).\n\nIn 1997, the new Constitution of Poland was finalized and approved in a referendum; it replaced the Small Constitution of 1992, an amended version of the communist constitution.\n\nPoland joined NATO in 1999. Elements of the Polish Armed Forces have since participated in the Iraq War and the Afghanistan War. Poland joined the European Union as part of its enlargement in 2004. The two memberships were indicative of the Third Polish Republic's integration with the West. Poland has not adopted the euro currency, however.\n\n\"a.\"Piłsudski's family roots in the Polonized gentry of the Grand Duchy of Lithuania and the resulting perspective of seeing himself and people like him as legitimate Lithuanians put him in conflict with modern Lithuanian nationalists (who in Piłsudski's lifetime redefined the scope of the meaning of \"Lithuanian\"), and by extension with other nationalists and also with the Polish modern nationalist movement.\n\n\"b.\"In 1938 Poland and Romania refused to agree to a Franco-British proposal that in the event of war with Germany Soviet forces would be allowed to cross their territories to aid Czechoslovakia. The Polish ruling elites considered the Soviets in some ways more threatening than the Nazis.\n\nThe Soviet Union repeatedly declared its intention to fulfill its obligations under the 1935 treaty with Czechoslovakia and defend Czechoslovakia militarily. A transfer of land and air forces through Poland and/or Romania was required and the Soviets approached the French about it, who also had a treaty with Czechoslovakia (and with Poland and with the Soviet Union). Edward Rydz-Śmigły rebuked the French suggestion on that matter in 1936, and in 1938 Józef Beck pressured Romania not to allow even Soviet warplanes to fly over its territory. Like Hungary, Poland was looking into using the German-Czechoslovak conflict to settle its own territorial grievances, namely disputes over parts of Zaolzie, Spiš and Orava.\n\n\"c.\" In October 1939, the British Foreign Office notified the Soviets that the United Kingdom would be satisfied with a post-war creation of small ethnic Poland, patterned after the Duchy of Warsaw. An establishment of Poland restricted to \"minimal size\", according to ethnographic boundaries (such as the lands common to both the prewar Poland and postwar Poland), was planned by the Soviet People's Commissariat for Foreign Affairs in 1943–44. Such territorial reduction was recommended by Ivan Maisky to Vyacheslav Molotov in early 1944, because of what Maisky saw as Poland's historically unfriendly disposition toward Russia and the Soviet Union, likely in some way to continue. Joseph Stalin opted for a larger version, allowing a \"swap\" (territorial compensation for Poland), which involved the eastern lands gained by Poland at the Peace of Riga of 1921 and now lost, and eastern Germany conquered from the Nazis in 1944–45. In regard to the several major disputed areas: Lower Silesia west of the Oder and the Nysa Kłodzka rivers (the British wanted it to remain a part of the future German state), Stettin (in 1945 the German communists already established their administration there), \"Zakerzonia\" (western Red Ruthenia demanded by the Ukrainians), and the Białystok region (Białystok was claimed by the communists of the Byelorussian SSR), the Soviet leader made decisions that favored Poland.\n\nOther territorial and ethnic scenarios were also possible, generally with outcomes less advantageous to Poland than its present form.\n\n\"d.\"Timothy Snyder spoke of about 100,000 Jews killed by Poles during the Nazi occupation, the majority probably by members of the collaborationist Blue Police. This number would have likely been many times higher had Poland entered into an alliance with Germany in 1939, as advocated by some Polish historians and others.\n\n\"e.\"Some may have falsely claimed Jewish identity hoping for permission to emigrate. The communist authorities, pursuing the concept of a Poland of single ethnicity (in accordance with the recent border changes and expulsions), were allowing the Jews to leave the country. For a discussion of early communist Poland's ethnic politics, see Timothy Snyder, \"The Reconstruction of Nations\", chapters on modern \"Ukrainian Borderland\".\n\n\"f.\"A Communist Party of Poland had existed in the past, but was eliminated in Stalin's purges in 1938.\n\n\"g.\"The Soviet leadership, which had previously ordered the crushing of the Uprising in East Germany, the Hungarian Revolution and the Prague Spring, now became worried about the demoralization of the Polish army, a crucial Warsaw Pact component, because of its deployment against Polish workers. The Soviets withdrew their support for Gomułka, who insisted on the use of force; he and his close associates were subsequently ousted from the Polish Politburo by the Polish Central Committee.\n\n\"h.\"East of the Molotov-Ribbentrop line, the population was 43% Polish, 33% Ukrainian, 8% Belarusian and 8% Jewish. The Soviet Union did not want to appear as an aggressor, and moved its troops to Eastern Poland under the pretext of offering protection to \"the kindred Ukrainian and Belorussian people\".\n\n\"i.\"Joseph Stalin at the 1943 Tehran Conference discussed with Winston Churchill and Franklin Roosevelt new post-war borders in central-eastern Europe, including the shape of a future Poland. He endorsed the Piast Concept, which justified a massive shift of Poland's frontiers to the west. Stalin resolved to secure and stabilize the western reaches of the Soviet Union and disable the future military potential of Germany by constructing a compact and ethnically defined Poland (along with the Soviet ethnic Ukraine, Belarus and Lithuania) and by radically altering the region's system of national borders. After 1945, the Polish communist regime wholeheartedly adopted and promoted the Piast Concept, making it the centerpiece of their claim to be the true inheritors of Polish nationalism. After all the killings and population transfers during and after the war the country was 99% \"Polish\".\n\n\"j.\"\"All the currently available documents of Nazi administration show that, together with the Jews, the stratum of the Polish intelligentsia was marked for total extermination. In fact, Nazi Germany achieved this goal almost by half, since Poland lost 50 percent of her citizens with university diplomas and 35 percent of those with a gimnazium diploma.\"\n\n\"k.\"Decisive political events took place in Poland shortly before the Soviet intervention in Hungary. Władysław Gomułka, a reformist leader at that time, was reinstated to the Polish Politburo and the Eighth Plenum of the Party's Central Committee was announced to convene on October 19, 1956, all without seeking a Soviet approval. The Soviet Union responded with military moves and intimidation and its \"military-political delegation\", led by Nikita Khrushchev, quickly arrived in Warsaw. Gomułka tried to convince them of his loyalty but insisted on the reforms that he considered essential, including a replacement of Poland's Soviet-trusted minister of defense, Konstantin Rokossovsky. The disconcerted Soviets returned to Moscow, the Polish Plenum elected Gomułka first secretary and removed Rokossovsky from the Politburo. On October 21, the Soviet Presidium followed Khrushchev's lead and decided unanimously to \"refrain from military intervention\" in Poland, a decision likely influenced also by the ongoing preparations for the invasion of Hungary. The Soviet gamble paid off because Gomułka in the coming years turned out to be a very dependable Soviet ally and an orthodox communist.\n\nUnlike the other Warsaw Pact countries, Poland did not endorse the Soviet armed intervention in Hungary. The Hungarian Uprising was intensely supported by the Polish public.\n\n\"l.\"The delayed reinforcements were coming and the government military commanders General Tadeusz Rozwadowski and Władysław Anders wanted to keep on fighting the coup perpetrators, but President Stanisław Wojciechowski and the government decided to surrender to prevent the imminent spread of civil war. The coup brought to power the \"Sanation\" regime under Józef Piłsudski and Edward Rydz-Śmigły after Piłsudski's death. The Sanation regime persecuted the opposition within the military and in general. Rozwadowski died after abusive imprisonment, according to some accounts murdered. According to Aleksandra Piłsudska, the marshal's wife, following the coup and for the rest of his life Piłsudski lost his composure and appeared over-burdened.\n\nAt the time of Rydz-Śmigły's command, the Sanation camp embraced the ideology of Roman Dmowski, Piłsudski's nemesis. Rydz-Śmigły did not allow General Władysław Sikorski, an anti-Sanation enemy, to participate as a soldier in the defense of the country in September 1939. During World War II in France and Britain the Polish government in exile became dominated by anti-Sanation politicians. The perceived Sanation followers were in turn persecuted (in exile) under prime ministers Sikorski and Stanisław Mikołajczyk.\n\n\"m.\"General Zygmunt Berling of the Soviet-allied First Polish Army attempted in mid-September a crossing of the Vistula and landing at Czerniaków to aid the insurgents, but the operation was defeated by the Germans and the Poles suffered heavy losses.\n\n\"n.\"The decision to launch the Warsaw Uprising resulted in the destruction of the city, its population and its elites and has been a source of lasting controversy. According to the historians Czesław Brzoza and Andrzej Leon Sowa, orders of further military offensives, issued at the end of August 1944 as a part of Operation Tempest, show the loss of a sense of responsibility for the country's fate on the part of the Polish leadership.\n\n\"o.\"One of the party leaders Mieczysław Rakowski, who abandoned his mentor Gomułka following the 1970 crisis, saw the demands of the demonstrating workers as \"exclusively socialist\" in character, because of the way they were phrased. Most people in communist Poland, including opposition activists, did not question the supremacy of socialism or the socialist idea; misconduct by party officials, such as not following the provisions of the constitution, was blamed. This assumed standard of political correctness was increasingly challenged in the decades that followed, when pluralism became a frequently used concept.\n\n\"p.\"The Polish Sanation authorities were provoked by the independence-seeking Organization of Ukrainian Nationalists (OUN). OUN engaged in political assassinations, terror and sabotage, to which the Polish state responded with a repressive campaign in the 1930s, as Józef Piłsudski and his successors imposed collective responsibility on the villagers in the affected areas. After the disturbances of 1933 and 1934, a prison camp in Bereza Kartuska was established, which became notorious for its brutal regime. The government brought Polish settlers and administrators to Volhynian areas with a centuries-old tradition of Ukrainian peasant rising against Polish land owners (and to Eastern Galicia). In the late 1930s, after Piłsudski's death, military persecution intensified and a policy of \"national assimilation\" was aggressively pursued. Military raids, public beatings, property confiscations and the closing and destruction of Orthodox churches aroused lasting enmity in Galicia and antagonized Ukrainian society in Volhynia at, according to Timothy Snyder, the worst possible moment. However, he also notes that \"Ukrainian terrorism and Polish reprisals touched only part of the population, leaving vast regions unaffected\" and \"the OUN's nationalist prescription, a Ukrainian state for ethnic Ukrainians alone was far from popular\". Halik Kochanski wrote of the legacy of bitterness between the Ukrainians and Poles that soon exploded in the context of the World War II. See also: History of the Ukrainian minority in Poland.\n\n\"q.\"In Poland, officials of central government (\"wojewoda\" provincial office) can overrule elected local territorial and municipal governments.\n\n\"r.\"Foreign policy was one of the few governmental areas in which Piłsudski took an active interest. He saw Poland's role and opportunity as lying in Eastern Europe and advocated passive relations with the West. He felt that a German attack should not be feared because, even if this unlikely event were to take place, the Western powers would be bound to restrain Germany and come to Poland's rescue.\n\n\"s.\"According to the researcher Jan Sowa, the Commonwealth failed as a state because it was not able to conform to the emerging new European order established at the Peace of Westphalia of 1648. Poland's elective kings, restricted by the self-serving but short-sighted nobility, could not impose a strong and efficient central government, with its characteristic post-Westphalian internal and external sovereignty. The inability of Polish kings to levy and collect taxes (and therefore sustain a standing army) and conduct independent foreign policy were among the chief obstacles to Poland competing effectively on the changed European scene, where absolutist power was a prerequisite for survival and became the foundation for the abolition of serfdom and gradual formation of parliamentarism.\n\n\"t.\"Besides the Home Army there were other major underground fighting formations: Bataliony Chłopskie, National Armed Forces (NSZ) and Gwardia Ludowa (later Armia Ludowa). From 1943, the leaders of the nationalistic NSZ collaborated with Nazi Germany in a case unique in occupied Poland. The NSZ conducted an anti-communist civil war. Before the arrival of the Soviets, the NSZ's Holy Cross Mountains Brigade left Poland under the protection of the German army. According to the historians Czesław Brzoza and Andrzej Leon Sowa, participation figures given for the underground resistance are often inflated. In the spring of 1944, the time of the most extensive involvement of the underground organizations, there were most likely considerably fewer than 500,000 military and civilian personnel participating, over the entire spectrum, from the right wing to the communists.\n\n\"u.\"According to Jerzy Eisler, about 1.1 million people may have been imprisoned or detained in 1944–56 and about 50,000 may have died because of the struggle and persecution, including about 7,000 soldiers of the right-wing underground killed in the 1940s. According to Adam Leszczyński, up to 30,000 people were killed by the communist regime during the first several years after the war.\n\n\"v.\"According to Andrzej Stelmachowski, one of the key participants of the Polish systemic transformation, Minister Leszek Balcerowicz pursued extremely liberal economic policies, often unusually painful for society. The December 1989 Sejm statute of credit relations reform introduced an \"incredible\" system of privileges for banks. Banks were allowed to alter unilaterally interest rates on already existing contracts. The exceedingly high rates they instantly introduced ruined many previously profitable enterprises and caused a complete breakdown of the apartment block construction industry, which had long-term deleterious effects on the state budget as well. Balcerowicz's policies also caused permanent damage to Polish agriculture, which Balcerowicz \"did not understand\", and to the often successful and useful Polish cooperative movement.\n\n\"w.\"Led by Władysław Anders, the Polish II Corps fought at the famous Battle of Monte Cassino in 1944, as part of the Allied Italian Campaign.\n\n\"x.\"The concept which had become known as the Piast Idea, the chief proponent of which was Jan Ludwik Popławski, was based on the statement that the Piast homeland was inhabited by so-called \"native\" aboriginal Slavs and Slavonic Poles since time immemorial and only later was \"infiltrated\" by \"alien\" Celts, Germans and others. After 1945, the so-called \"autochthonous\" or \"aboriginal\" school of Polish prehistory received official backing in Poland and a considerable degree of popular support. According to this view, the Lusatian Culture which archaeologists have identified between the Oder and the Vistula in the early Iron Age, was said to be Slavonic; all non-Slavonic tribes and peoples recorded in the area at various points in ancient times were dismissed as \"migrants\" and \"visitors\". In contrast, the critics of this theory, such as Marija Gimbutas, regarded it as an unproved hypotheses and for them the date and origin of the westward migration of the Slavs were largely uncharted; the Slavonic connections of the Lusatian Culture were entirely imaginary; and the presence of an ethnically mixed and constantly changing collection of peoples on the North European Plain was taken for granted.\n\n\"y.\"According to the count presented by Prime Minister and Internal Affairs Minister Felicjan Sławoj Składkowski before the Sejm committee in January 1938, 818 people were killed in police suppression of labor protests (industrial and agricultural) during the 1932–37 period.\n\nMore recent general history of Poland books in English\n\nPublished in Poland\n\n\n\n\n", "id": "13772", "title": "History of Poland"}
{"url": "https://en.wikipedia.org/wiki?curid=13773", "text": "Hradčany\n\nHradčany (common ; ), the Castle District, is the district of the city of Prague, Czech Republic surrounding the Prague Castle.\n\nThe castle is said to be the biggest castle in the world at about 570 meters in length and an average of about 130 meters wide. Its history stretches back to the 9th century. St Vitus Cathedral is located in the castle area.\n\nMost of the district consists of noble historical . There are many other attractions for visitors: romantic nooks, peaceful places and beautiful lookouts.\n\nHradčany was an independent borough until 1784, when the four independent boroughs that had formerly constituted Prague were proclaimed a single city. The other three were Malá Strana (, ), (, ) and (, ).\n\n", "id": "13773", "title": "Hradčany"}
{"url": "https://en.wikipedia.org/wiki?curid=13774", "text": "Houston\n\nHouston ( ) is the most populous city in Texas and the fourth-most populous city in the United States, located in Southeast Texas near the Gulf of Mexico. With a census-estimated 2014 population of 2.239 million within an area of , it also is the largest city in the Southern United States, as well as the seat of Harris County. It is the principal city of Houston–The Woodlands–Sugar Land, which is the fifth-most populated metropolitan area in the United States of America.\n\nHouston was founded on August 28, 1836, near the banks of Buffalo Bayou (now known as Allen's Landing) and incorporated as a city on June 5, 1837. The city was named after former General Sam Houston, who was president of the Republic of Texas and had commanded and won at the Battle of San Jacinto east of where the city was established. The burgeoning port and railroad industry, combined with oil discovery in 1901, has induced continual surges in the city's population. In the mid-20th century, Houston became the home of the Texas Medical Center—the world's largest concentration of healthcare and research institutions—and NASA's Johnson Space Center, where the Mission Control Center is located.\n\nHouston's economy has a broad industrial base in energy, manufacturing, aeronautics, and transportation. It is also leading in health care sectors and building oilfield equipment; only New York City is home to more Fortune 500 headquarters within its city limits. The Port of Houston ranks first in the United States in international waterborne tonnage handled and second in total cargo tonnage handled. Nicknamed the \"Space City\", Houston is a global city, with strengths in business, international trade, entertainment, culture, media, fashion, science, sports, technology, education, medicine, and research. The city has a population from various ethnic and religious backgrounds and a large and growing international community. Houston is the most diverse city in Texas and has been described as the most diverse in the United States. It is home to many cultural institutions and exhibits, which attract more than 7 million visitors a year to the Museum District. Houston has an active visual and performing arts scene in the Theater District and offers year-round resident companies in all major performing arts.\n\nIn August 1836, two real estate entrepreneurs from New York—Augustus Chapman Allen and John Kirby Allen—purchased of land along Buffalo Bayou with the intent of founding a city. The Allen brothers decided to name the city after Sam Houston, the popular general at the Battle of San Jacinto, who was elected President of Texas in September 1836. The great majority of slaves in Texas came with their owners from the older slave states. Sizable numbers, however, came through the domestic slave trade. New Orleans was the center of this trade in the Deep South, but slave dealers were in Houston. Thousands of enslaved African Americans lived near the city before the Civil War. Many of them near the city worked on sugar and cotton plantations, while most of those in the city limits had domestic and artisan jobs.\nHouston was granted incorporation on June 5, 1837, with James S. Holman becoming its first mayor. In the same year, Houston became the county seat of Harrisburg County (now Harris County) and the temporary capital of the Republic of Texas. In 1840, the community established a chamber of commerce in part to promote shipping and waterborne business at the newly created port on Buffalo Bayou.\n\nBy 1860, Houston had emerged as a commercial and railroad hub for the export of cotton. Railroad spurs from the Texas inland converged in Houston, where they met rail lines to the ports of Galveston and Beaumont. During the American Civil War, Houston served as a headquarters for General John Bankhead Magruder, who used the city as an organization point for the Battle of Galveston. After the Civil War, Houston businessmen initiated efforts to widen the city's extensive system of bayous so the city could accept more commerce between downtown and the nearby port of Galveston. By 1890, Houston was the railroad center of Texas.\n\nIn 1900, after Galveston was struck by a devastating hurricane, efforts to make Houston into a viable deep-water port were accelerated. The following year, oil discovered at the Spindletop oil field near Beaumont prompted the development of the Texas petroleum industry. In 1902, President Theodore Roosevelt approved a $1 million improvement project for the Houston Ship Channel. By 1910, the city's population had reached 78,800, almost doubling from a decade before. African Americans formed a large part of the city's population, numbering 23,929 people, or nearly one-third of the residents.\n\nPresident Woodrow Wilson opened the deep-water Port of Houston in 1914, seven years after digging began. By 1930, Houston had become Texas' most populous city and Harris County the most populous county. In 1940, the Census Bureau reported Houston's population as 77.5% white and 22.4% black.\n\nWhen World War II started, tonnage levels at the port decreased and shipping activities were suspended; however, the war did provide economic benefits for the city. Petrochemical refineries and manufacturing plants were constructed along the ship channel because of the demand for petroleum and synthetic rubber products by the defense industry during the war. Ellington Field, initially built during World War I, was revitalized as an advanced training center for bombardiers and navigators. The Brown Shipbuilding Company was founded in 1942 to build ships for the U.S. Navy during World War II. Due to the boom in defense jobs, thousands of new workers migrated to the city, both blacks and whites competing for the higher-paying jobs. President Roosevelt had established a policy of nondiscrimination for defense contractors, and blacks gained some opportunities, especially in shipbuilding, although not without resistance from whites and increasing social tensions that erupted into occasional violence. Economic gains of blacks who entered defense industries continued in the postwar years.\n\nIn 1945, the M.D. Anderson Foundation formed the Texas Medical Center. After the war, Houston's economy reverted to being primarily port-driven. In 1948, the city annexed several unincorporated areas, more than doubling its size. Houston proper began to spread across the region.\n\nIn 1950, the availability of air conditioning provided impetus for many companies to relocate to Houston, where wages were lower than the North; this resulted in an economic boom and produced a key shift in the city's economy toward the energy sector. \nThe increased production of the expanded shipbuilding industry during World War II spurred Houston's growth, as did the establishment in 1961 of NASA's \"Manned Spacecraft Center\" (renamed the Lyndon B. Johnson Space Center in 1973). This was the stimulus for the development of the city's aerospace industry. The Astrodome, nicknamed the \"Eighth Wonder of the World\", opened in 1965 as the world's first indoor domed sports stadium.\n\nDuring the late 1970s, Houston had a population boom as people from the Rust Belt states moved to Texas in large numbers. The new residents came for numerous employment opportunities in the petroleum industry, created as a result of the Arab oil embargo. With the increase in professional jobs, Houston has become a destination for many college-educated persons, including African Americans in a reverse Great Migration from northern areas.\n\nOne wave of the population boom ended abruptly in the mid-1980s, as oil prices fell precipitously. The space industry also suffered in 1986 after the Space Shuttle \"Challenger\" disintegrated shortly after launch. A cutback in some activities existed for a period. In the late 1980s, the city's economy suffered from the nationwide recession. After the early 1990s recession, Houston made efforts to diversify its economy by focusing on aerospace and health care/biotechnology, and reduced its dependence on the petroleum industry. Since the increase of oil prices in the 2000s, the petroleum industry has again increased its share of the local economy.\n\nIn 1997, Houstonians elected Lee P. Brown as the city's first African American mayor.\n\nIn June 2001, Tropical Storm Allison dumped up to of rain on parts of Houston, causing the worst flooding in the city's history. The storm cost billions of dollars in damage and killed 20 people in Texas. By December of that same year, Houston-based energy company Enron collapsed into the third-largest ever U.S. bankruptcy during an investigation surrounding fabricated partnerships that were allegedly used to hide debt and inflate profits.\n\nIn August 2005, Houston became a shelter to more than 150,000 people from New Orleans, who evacuated from Hurricane Katrina. One month later, about 2.5 million Houston-area residents evacuated when Hurricane Rita approached the Gulf Coast, leaving little damage to the Houston area. This was the largest urban evacuation in the history of the United States. In September 2008, Houston was hit by Hurricane Ike. As many as 40% refused to leave Galveston Island because they feared the traffic problems that happened after Hurricane Rita.\n\nDuring the floods in 2015 and 2016, parts of the city were covered in several inches of water.\n\nHouston is located east of Austin, west of the Louisiana border, and south of Dallas. According to the United States Census Bureau, the city has a total area of ; this comprises of land and covered by water. The Piney Woods are north of Houston. Most of Houston is located on the gulf coastal plain, and its vegetation is classified as temperate grassland and forest. Much of the city was built on forested land, marshes, swamp, or prairie which resembles the Deep South, and are all still visible in surrounding areas. The flatness of the local terrain, when combined with urban sprawl, has made flooding a recurring problem for the city. Downtown stands about above sea level, and the highest point in far northwest Houston is about in elevation. The city once relied on groundwater for its needs, but land subsidence forced the city to turn to ground-level water sources such as Lake Houston, Lake Conroe, and Lake Livingston. The city owns surface water rights for 1.20 billion gallons of water a day in addition to 150 million gallons a day of groundwater.\n\nHouston has four major bayous passing through the city. Buffalo Bayou runs through downtown and the Houston Ship Channel, and has three tributaries: White Oak Bayou, which runs through the Houston Heights community northwest of Downtown and then towards Downtown; Brays Bayou, which runs along the Texas Medical Center; and Sims Bayou, which runs through the south of Houston and downtown Houston. The ship channel continues past Galveston and then into the Gulf of Mexico.\n\nUnderpinning Houston's land surface are unconsolidated clays, clay shales, and poorly cemented sands up to several miles deep. The region's geology developed from river deposits formed from the erosion of the Rocky Mountains. These sediments consist of a series of sands and clays deposited on decaying organic marine matter, that over time, transformed into oil and natural gas. Beneath the layers of sediment is a water-deposited layer of halite, a rock salt. The porous layers were compressed over time and forced upward. As it pushed upward, the salt dragged surrounding sediments into salt dome formations, often trapping oil and gas that seeped from the surrounding porous sands. The thick, rich, sometimes black, surface soil is suitable for rice farming in suburban outskirts where the city continues to grow.\n\nThe Houston area has over 150 active faults (estimated to be 300 active faults) with an aggregate length of up to , including the Long Point–Eureka Heights fault system which runs through the center of the city. No significant historically recorded earthquakes have occurred in Houston, but researchers do not discount the possibility of such quakes having occurred in the deeper past, nor occurring in the future. Land in some areas southeast of Houston is sinking because water has been pumped out of the ground for many years. It may be associated with slip along the faults; however, the slippage is slow and not considered an earthquake, where stationary faults must slip suddenly enough to create seismic waves. These faults also tend to move at a smooth rate in what is termed \"fault creep\", which further reduces the risk of an earthquake.\n\nHouston's climate is classified as humid subtropical (\"Cfa\" in the Köppen climate classification system), typical of the lower South. While not located in \"Tornado Alley\", like much of the rest of Texas, spring supercell thunderstorms sometimes bring tornadoes to the area. Prevailing winds are from the south and southeast during most of the year, which bring heat and moisture from the nearby Gulf of Mexico.\n\nDuring the summer, temperatures commonly reach over , with an average of 106.5 days per year, including a majority from June to September, with a high of or above and 4.6 days at or over . However, humidity usually yields a higher heat index. Summer mornings average over 90% relative humidity. Although sea breezes are present, they don't offer substantial relief, except in the southeastern areas of the city closer to the Gulf. To cope with the strong humidity and heat, people use air conditioning in nearly every vehicle and building. In 1980, Houston was described as the \"most air-conditioned place on earth\". Officially, the hottest temperature ever recorded in Houston is , which was reached both on September 4, 2000, and August 28, 2011.\n\nHouston has mild winters in contrast to most areas of the United States. In January, the normal mean temperature at Intercontinental Airport is , while that station has an average of 13 days with a low at or below freezing. Snowfall is rare. Recent snow events in Houston include a storm on December 24, 2004 when 1.0 in (2.5 cm) of snow accumulated in parts of the metro area. Falls of at least on both December 10, 2008, and December 4, 2009, marked the first time measurable snowfall had occurred in two consecutive years in the city's recorded history. The coldest temperature officially recorded in Houston was on January 18, 1930. Houston has historically received an ample amount of rainfall, averaging about annually per 1981–2010 normals. Localized flooding often occurs, owing to the extremely flat topography and widespread typical clay-silt prairie soils, which do not drain quickly.\n\nHouston has excessive ozone levels and is routinely ranked among the most ozone-polluted cities in the United States. Ground-level ozone, or smog, is Houston's predominant air pollution problem, with the American Lung Association rating the metropolitan area's ozone level sixth on the \"Top 10 Most Ozone-Polluted Cities\" in 2014. The industries located along the ship channel are a major cause of the city's air pollution. In 2006, Houston's air quality was comparable to that of Los Angeles.\n\nHouston was incorporated in 1837 under the ward system of representation. The ward designation is the progenitor of the 11 current-day geographically oriented Houston City Council districts. Locations in Houston are generally classified as either being inside or outside the Interstate 610 Loop. The inside encompasses the central business district and many residential neighborhoods that antedate World War II. More recently, high-density residential areas have been developed within the loop. The city's outlying areas, suburbs, and enclaves are located outside of the loop. Beltway 8 encircles the city another farther out.\n\nThough Houston is the largest city in the United States without formal zoning regulations, it has developed similarly to other Sun Belt cities because the city's land use regulations and legal covenants have played a similar role. Regulations include mandatory lot size for single-family houses and requirements that parking be available to tenants and customers. Such restrictions have had mixed results. Though some have blamed the city's low density, urban sprawl, and lack of pedestrian-friendliness on these policies, the city's land use has also been credited with having significant affordable housing, sparing Houston the worst effects of the 2008 real estate crisis. The city issued 42,697 building permits in 2008 and was ranked first in the list of healthiest housing markets for 2009.\n\nVoters rejected efforts to have separate residential and commercial land-use districts in 1948, 1962, and 1993. Consequently, rather than a single central business district as the center of the city's employment, multiple districts have grown throughout the city in addition to downtown which include Uptown, Texas Medical Center, Midtown, Greenway Plaza, Memorial City, Energy Corridor, Westchase, and Greenspoint.\n\nHouston has the fourth-tallest skyline in North America (after New York City, Chicago, and Toronto) and 12th-tallest in the world, . A seven-mile (11 km) system of tunnels and skywalks links downtown buildings containing shops and restaurants, enabling pedestrians to avoid summer heat and rain while walking between buildings.\n\nIn the 1960s, Downtown Houston consisted of a collection of midrise office structures. Downtown was on the threshold of an energy industryled boom in 1970. A succession of skyscrapers was built throughout the 1970s—many by real estate developer Gerald D. Hines—culminating with Houston's tallest skyscraper, the 75-floor, -tall JPMorgan Chase Tower (formerly the Texas Commerce Tower), completed in 1982. It is the tallest structure in Texas, 15th tallest building in the United States, and the 85th-tallest skyscraper in the world, based on highest architectural feature. In 1983, the 71-floor, -tall Wells Fargo Plaza (formerly Allied Bank Plaza) was completed, becoming the second-tallest building in Houston and Texas. Based on highest architectural feature, it is the 17th-tallest in the United States and the 95th-tallest in the world. In 2007, downtown Houston had over 43 million square feet (4,000,000 m²) of office space.\n\nCentered on Post Oak Boulevard and Westheimer Road, the Uptown District boomed during the 1970s and early 1980s when a collection of midrise office buildings, hotels, and retail developments appeared along Interstate 610 West. Uptown became one of the most prominent instances of an edge city. The tallest building in Uptown is the 64-floor, -tall, Philip Johnson and John Burgee designed landmark Williams Tower (known as the Transco Tower until 1999). At the time of construction, it was believed to be the world's tallest skyscraper outside of a central business district. The new 20-story Skanska building and BBVA Compass Plaza are the newest office buildings built in Uptown after 30 years. The Uptown District is also home to buildings designed by noted architects I. M. Pei, César Pelli, and Philip Johnson. In the late 1990s and early 2000s, a mini-boom of midrise and highrise residential tower construction occurred, with several over 30 stories tall. Since 2000 more than 30 high-rise buildings have gone up in Houston; all told, 72 high-rises tower over the city, which adds up to about 8,300 units. In 2002, Uptown had more than 23 million square feet (2,100,000 m²) of office space with 16 million square feet (1,500,000 m²) of class A office space.\n\nHouston is multicultural, in part because of its many academic institutions and strong industries, as well as being a major port city. Over 90 languages are spoken in the city. It has among the youngest populations in the nation, partly due to an influx of immigrants into Texas. An estimated 400,000 illegal aliens reside in the Houston area.\n\nAccording to the 2010 U.S. Census, whites made up 51% of Houston's population; 26% of the total population was non-Hispanic Whites. Blacks or African Americans made up 25% of Houston's population. American Indians made up 0.7% of the population. Asians made up 6% (1.7% Vietnamese, 1.3% Chinese, 1.3% Indian, 0.9% Pakistani, 0.4% Filipino, 0.3% Korean, 0.1% Japanese), while Pacific Islanders made up 0.1%. Individuals from some other race made up 15.2% of the city's population, of which 0.2% were non-Hispanic. Individuals from two or more races made up 3.3% of the city. \nAt the 2000 Census, 1,953,631 people inhabited the city, and the population density was 3,371.7 people per square mile (1,301.8/km²). The racial makeup of the city was 49.3% White, 25.3% African American, 6.3% Asian, 0.7% American Indian, 0.1% Pacific Islander, 16.5% from some other race, and 3.1% from two or more races. In addition, Hispanics made up 37.4% of Houston's population, while non-Hispanic Whites made up 30.8%, down from 62.4% in 1970.\n\nThe median income for a household in the city was $37,000, and for a family was $40,000. Males had a median income of $32,000 versus $27,000 for females. The per capita income was $20,000. About 19% of the population and 16% of families were below the poverty line. Of the total population, 26% of those under the age of 18 and 14% of those 65 and older were living below the poverty line.\n\nAccording to a 2014 study by the Pew Research Center, 73% of the population of the city identified themselves as Christians, with 50% professing attendance at a variety of churches that could be considered Protestant, and 19% professing Roman Catholic beliefs. while 20% claim no religious affiliation. The same study says that other religions (including Judaism, Buddhism, Islam, and Hinduism) collectively make up about 7% of the population\n\nHouston is recognized worldwide for its energy industry—particularly for oil and natural gas—as well as for biomedical research and aeronautics. Renewable energy sources—wind and solar—are also growing economic bases in the city. The Houston Ship Channel is also a large part of Houston's economic base. Because of these strengths, Houston is designated as a global city by the Globalization and World Cities Study Group and Network and global management consulting firm A.T. Kearney. The Houston area is the top U.S. market for exports, surpassing New York City in 2013, according to data released by the U.S. Department of Commerce's International Trade Administration. In 2012, the Houston–The Woodlands–Sugar Land area recorded $110.3 billion in merchandise exports. Petroleum products, chemicals, and oil and gas extraction equipment accounted for roughly two-thirds of the metropolitan area's exports last year. The top three destinations for exports were Mexico, Canada, and Brazil.\n\nThe Houston area is a leading center for building oilfield equipment. Much of its success as a petrochemical complex is due to its busy ship channel, the Port of Houston. In the United States, the port ranks first in international commerce and 10th among the largest ports in the world. Unlike most places, high oil and gasoline prices are beneficial for Houston's economy, as many of its residents are employed in the energy industry. Houston is the beginning or end point of numerous oil, gas, and products pipelines:\n\nThe Houston–The Woodlands–Sugar Land MSA's gross domestic product (GDP) in 2012 was $489 billion, making it the fourth-largest of any metropolitan area in the United States and larger than Austria's, Venezuela's, or South Africa's GDP. Only 26 countries other than the United States have a gross domestic product exceeding Houston's regional gross area product (GAP). In 2010, mining (which consists almost entirely of exploration and production of oil and gas in Houston) accounted for 26.3% of Houston's GAP up sharply in response to high energy prices and a decreased worldwide surplus of oil production capacity, followed by engineering services, health services, and manufacturing.\n\nThe University of Houston System's annual impact on the Houston area's economy equates to that of a major corporation: $1.1 billion in new funds attracted annually to the Houston area, $3.13 billion in total economic benefit, and 24,000 local jobs generated. This is in addition to the 12,500 new graduates the U.H. System produces every year who enter the workforce in Houston and throughout the state of Texas. These degree-holders tend to stay in Houston. After five years, 80.5% of graduates are still living and working in the region.\n\nIn 2006, the Houston metropolitan area ranked first in Texas and third in the U.S. within the category of \"Best Places for Business and Careers\" by \"Forbes\" magazine. Foreign governments have established 92 consular offices in Houston's metropolitan area, the third-highest in the nation. Forty foreign governments maintain trade and commercial offices here with 23 active foreign chambers of commerce and trade associations. Twenty-five foreign banks representing 13 nations operate in Houston, providing financial assistance to the international community.\n\nIn 2008, Houston received top ranking on \"Kiplinger's Personal Finance\" Best Cities of 2008 list, which ranks cities on their local economy, employment opportunities, reasonable living costs, and quality of life. The city ranked fourth for highest increase in the local technological innovation over the preceding 15 years, according to \"Forbes\" magazine. In the same year, the city ranked second on the annual \"Fortune\" 500 list of company headquarters, first for \"Forbes\" magazine's Best Cities for College Graduates, and first on their list of Best Cities to Buy a Home. In 2010, the city was rated the best city for shopping, according to \"Forbes\".\n\nIn 2012, the city was ranked number one for paycheck worth by \"Forbes\" and in late May 2013, Houston was identified as America's top city for employment creation.\n\nIn 2013, Houston was identified as the number one U.S. city for job creation by the U.S. Bureau of Statistics after it was not only the first major city to regain all the jobs lost in the preceding economic downturn, but also after the crash, more than two jobs were added for every one lost. Economist and vice president of research at the Greater Houston Partnership Patrick Jankowski attributed Houston's success to the ability of the region's real estate and energy industries to learn from historical mistakes. Furthermore, Jankowski stated that \"more than 100 foreign-owned companies relocated, expanded or started new businesses in Houston\" between 2008 and 2010, and this openness to external business boosted job creation during a period when domestic demand was problematically low. Also in 2013, Houston again appeared on \"Forbes\"' list of Best Places for Business and Careers.\n\nLocated in the American South, Houston is a diverse city with a large and growing international community. The metropolitan area is home to an estimated 1.1 million (21.4 percent) residents who were born outside the United States, with nearly two-thirds of the area's foreign-born population from south of the United States–Mexico border. Additionally, more than one in five foreign-born residents are from Asia. The city is home to the nation's third-largest concentration of consular offices, representing 86 countries.\n\nMany annual events celebrate the diverse cultures of Houston. The largest and longest-running is the annual Houston Livestock Show and Rodeo, held over 20 days from early to late March, and is the largest annual livestock show and rodeo in the world. Another large celebration is the annual night-time Houston Pride Parade, held at the end of June. Other annual events include the Houston Greek Festival, Art Car Parade, the Houston Auto Show, the Houston International Festival, and the Bayou City Art Festival, which is considered to be one of the top five art festivals in the United States.\n\nHouston received the official nickname of \"Space City\" in 1967 because it is the location of NASA's Lyndon B. Johnson Space Center. Other nicknames often used by locals include \"Bayou City\", \"Clutch City\", \"Magnolia City\", and \"H-Town\".\n\nThe Houston Theater District, located downtown, is home to nine major performing arts organizations and six performance halls. It is the second-largest concentration of theater seats in a downtown area in the United States. Houston is one of few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Houston Grand Opera), ballet (Houston Ballet), music (Houston Symphony Orchestra), and theater (The Alley Theatre, Theatre Under the Stars). Houston is also home to folk artists, art groups and various small progressive arts organizations. Houston attracts many touring Broadway acts, concerts, shows, and exhibitions for a variety of interests. Facilities in the Theater District include the Jones Hall—home of the Houston Symphony Orchestra and Society for the Performing Arts—and the Hobby Center for the Performing Arts.\n\nThe Museum District's cultural institutions and exhibits attract more than 7 million visitors a year. Notable facilities include The Museum of Fine Arts, Houston Museum of Natural Science, the Contemporary Arts Museum Houston, the Station Museum of Contemporary Art, Holocaust Museum Houston, and the Houston Zoo. Located near the Museum District are The Menil Collection, Rothko Chapel, and the Byzantine Fresco Chapel Museum.\n\nBayou Bend is a facility of the Museum of Fine Arts that houses one of America's best collections of decorative art, paintings, and furniture. Bayou Bend is the former home of Houston philanthropist Ima Hogg.\n\nThe National Museum of Funeral History is located in Houston near the George Bush Intercontinental Airport. The museum houses the original Popemobile used by Pope John Paul II in the 1980s along with numerous hearses, embalming displays, and information on famous funerals.\n\nVenues across Houston regularly host local and touring rock, blues, country, dubstep, and Tejano musical acts. While Houston has never been widely known for its music scene, Houston hip-hop has become a significant, independent music scene that is influential nationwide.\n\nThe Theater District is a 17-block area in the center of downtown Houston that is home to the Bayou Place entertainment complex, restaurants, movies, plazas, and parks. Bayou Place is a large multilevel building containing full-service restaurants, bars, live music, billiards, and Sundance Cinema. The Bayou Music Center stages live concerts, stage plays, and stand-up comedy.\nSpace Center Houston is the official visitors' center of NASA's Lyndon B. Johnson Space Center. The Space Center has many interactive exhibits including moon rocks, a shuttle simulator, and presentations about the history of NASA's manned space flight program. Other tourist attractions include the Galleria (Texas's largest shopping mall, located in the Uptown District), Old Market Square, the Downtown Aquarium, and Sam Houston Race Park.\nOf worthy mention are Houston's current Chinatown and the Mahatma Gandhi District. Both areas offer a picturesque view of Houston's multicultural makeup. Restaurants, bakeries, traditional-clothing boutiques, and specialty shops can be found in both areas.\n\nHouston is home to 337 parks, including Hermann Park, Terry Hershey Park, Lake Houston Park, Memorial Park, Tranquility Park, Sesquicentennial Park, Discovery Green, and Sam Houston Park. Within Hermann Park are the Houston Zoo and the Houston Museum of Natural Science. Sam Houston Park contains restored and reconstructed homes which were originally built between 1823 and 1905. A proposal has been made to open the city's first botanic garden at Herman Brown Park.\n\nOf the 10 most populous U.S. cities, Houston has the most total area of parks and green space, . The city also has over 200 additional green spaces—totaling over that are managed by the city—including the Houston Arboretum and Nature Center. The Lee and Joe Jamail Skatepark is a public skatepark owned and operated by the city of Houston, and is one of the largest skateparks in Texas consisting of a 30,000-ft (2,800 m)in-ground facility. The Gerald D. Hines Waterwall Park—located in the Uptown District of the city—serves as a popular tourist attraction and for weddings and various celebrations. A 2011 study by Walk Score ranked Houston the 23rd most walkable of the 50 largest cities in the United States. Wet'n'Wild SplashTown is a water park located north of Houston.\n\nThe Bayport Cruise Terminal on the Houston Ship Channel is port of call for both Princess Cruises and Norwegian Cruise Line.\n\nHouston has sports teams for every major professional league except the National Hockey League. The Houston Astros are a Major League Baseball expansion team formed in 1962 (known as the \"Colt .45s\" until 1965) that made one World Series appearance in 2005. The Houston Rockets are a National Basketball Association franchise based in the city since 1971. They have won two NBA Championships: in 1994 and 1995 under star players Hakeem Olajuwon, Otis Thorpe, Clyde Drexler, Vernon Maxwell, and Kenny Smith. The Houston Texans are a National Football League expansion team formed in 2002. The Houston Dynamo is a Major League Soccer franchise that has been based in Houston since 2006, winning two MLS Cup titles in 2006 and 2007. The Houston Dash team plays in the National Women's Soccer League. The Scrap Yard Dawgs, a women's professional softball team, are expected to play in the National Pro Fastpitch from 2016.\n\nMinute Maid Park (home of the Astros) and Toyota Center (home of the Rockets), are located in downtown Houston. Houston has the NFL's first retractable-roof stadium with natural grass, NRG Stadium (home of the Texans). Minute Maid Park is also a retractable-roof stadium. Toyota Center also has the largest screen for an indoor arena in the United States built to coincide with the arena's hosting of the 2013 NBA All-Star Game. BBVA Compass Stadium is a soccer-specific stadium for the Houston Dynamo, the Texas Southern Tigers football team, and Houston Dash, located in East Downtown. In addition, NRG Astrodome was the first indoor stadium in the world, built in 1965. Other sports facilities include Hofheinz Pavilion (Houston Cougars basketball), Rice Stadium (Rice Owls football), and Reliant Arena. TDECU Stadium is where the University of Houston Houston Cougars football team plays.\nHouston has hosted several major sports events: the 1968, 1986 and 2004 Major League Baseball All-Star Games; the 1989, 2006 and 2013 NBA All-Star Games; Super Bowl VIII and Super Bowl XXXVIII, as well as hosting the 2005 World Series and 1981, 1986, 1994 and 1995 NBA Finals, winning the latter two. Super Bowl LI is currently slated to be hosted in NRG Stadium in 2017.\n\nThe city has hosted several major professional and college sporting events, including the annual Houston Open golf tournament. Houston hosts the annual NCAA College Baseball Classic every February and NCAA football's Texas Bowl in December.\n\nThe Grand Prix of Houston, an annual auto race on the IndyCar Series circuit is held on a 1.7-mile temporary street circuit in Reliant Park. The October 2013 event was held using a tweaked version of the 2006–2007 course. The event has a 5-year race contract through 2017 with IndyCar. In motorcycling, the Astrodome hosted an AMA Supercross Championship round from 1974 to 2003 and the NRG Stadium since 2003.\n\nThe city of Houston has a strong mayoral form of municipal government. Houston is a home rule city and all municipal elections in the state of Texas are nonpartisan. The city's elected officials are the mayor, city controller and 16 members of the Houston City Council. The current mayor of Houston is Sylvester Turner, a Democrat elected on a nonpartisan ballot. Houston's mayor serves as the city's chief administrator, executive officer, and official representative, and is responsible for the general management of the city and for seeing that all laws and ordinances are enforced.\n\nThe original city council line-up of 14 members (nine district-based and five at-large positions) was based on a U.S. Justice Department mandate which took effect in 1979. At-large council members represent the entire city. Under the city charter, once the population in the city limits exceeded 2.1 million residents, two additional districts were to be added. The city of Houston's official 2010 census count was 600 shy of the required number; however, as the city was expected to grow beyond 2.1 million shortly thereafter, the two additional districts were added for, and the positions filled during, the August 2011 elections.\n\nThe city controller is elected independently of the mayor and council. The controller's duties are to certify available funds prior to committing such funds and processing disbursements. The city's fiscal year begins on July 1 and ends on June 30. Ronald Green is the city controller, serving his first term as of January 2010.\n\nAs the result of a 2015 referendum in Houston, a mayor is elected for a four-year term, and can be elected to as many as two consecutive terms. The term limits were spearheaded by conservative political activist Clymer Wright. The city controller and city council members are also subject to the same two-year, three-term limitations.\n\nHouston is considered to be a politically divided city whose balance of power often sways between Republicans and Democrats. Much of the city's wealthier areas vote Republican while the city's working class and minority areas vote Democratic. According to the 2005 Houston Area Survey, 68 percent of non-Hispanic whites in Harris County are declared or favor Republicans while 89 percent of non-Hispanic blacks in the area are declared or favor Democrats. About 62 percent Hispanics (of any race) in the area are declared or favor Democrats. The city has often been known to be the most politically diverse city in Texas, a state known for being generally conservative. As a result, the city is often a contested area in statewide elections. In 2009, Houston became the first US city with a population over 1 million citizens to elect a gay mayor, by electing Annise Parker.\n\nHouston's murder rate ranked 46th of U.S. cities with a population over 250,000 in 2005 (per capita rate of 16.3 murders per 100,000 population). In 2010, the city's murder rate (per capita rate of 11.8 murders per 100,000 population) was ranked sixth among U.S. cities with a population of over 750,000 (behind New York City, Chicago, Detroit, Dallas, and Philadelphia) according to the FBI.\n\nMurders fell by 37 percent from January to June 2011, compared with the same period in 2010. Houston's total crime rate including violent and nonviolent crimes decreased by 11 percent.\n\nHouston is a significant hub for trafficking of cocaine, cannabis, heroin, MDMA, and methamphetamine due to its size and proximity to major illegal drug exporting nations. Houston is one of the country's largest hubs for human trafficking.\n\nIn the early 1970s, Houston, Pasadena and several coastal towns were the site of the Houston Mass Murders, which at the time were the deadliest case of serial killing in American history.\n\nSeventeen school districts exist within the city of Houston. The Houston Independent School District (HISD) is the seventh-largest school district in the United States. HISD has 112 campuses that serve as magnet or vanguard schools—specializing in such disciplines as health professions, visual and performing arts, and the sciences. There are also many charter schools that are run separately from school districts. In addition, some public school districts also have their own charter schools.\n\nThe Houston area encompasses more than 300 private schools, many of which are accredited by Texas Private School Accreditation Commission recognized agencies. The Houston Area Independent Schools offer education from a variety of different religious as well as secular viewpoints. The Houston area Catholic schools are operated by the Archdiocese of Galveston-Houston.\n\n Four separate and distinct state universities are located in Houston. The University of Houston is a nationally recognized Tier One research university, and is the flagship institution of the University of Houston System. The university in Texas, the University of Houston has nearly 40,000 students on its 667-acre campus in southeast Houston. The University of Houston–Clear Lake and the University of Houston–Downtown are universities; they are not branch campuses of the University of Houston. Located in the historic community of Third Ward is Texas Southern University, one of the largest historically black colleges and universities in the United States.\nSeveral private institutions of higher learning—ranging from liberal arts colleges, such as The University of St. Thomas, Houston's only Catholic university, to Rice University, the nationally recognized research university—are located within the city. Rice, with a total enrollment of slightly more than 6,000 students, has a number of distinguished graduate programs and research institutes, such as the James A. Baker Institute for Public Policy. Houston Baptist University, affiliated with the Baptist General Convention of Texas, offers bachelor's and graduate degrees. It was founded in 1960 and is located in the Sharpstown area in Southwest Houston.\n\nThree community college districts exist with campuses in and around Houston. The Houston Community College System serves most of Houston. The northwestern through northeastern parts of the city are served by various campuses of the Lone Star College System, while the southeastern portion of Houston is served by San Jacinto College, and a northeastern portion is served by Lee College. The Houston Community College and Lone Star College systems are within the 10 largest institutions of higher learning in the United States.\nThe primary network-affiliated television stations are KPRC-TV (NBC), KHOU-TV (CBS), KTRK-TV (ABC), KRIV (Fox), KIAH (The CW), and KTXH (MyNetworkTV). KTRK-TV, KRIV and KTXH operate as owned-and-operated stations of their networks.\n\nThe Houston–The Woodlands–Sugar Land metropolitan area is served by one public television station and one public radio station. KUHT (\"HoustonPBS\") is a PBS member station and is the first public television station in the United States. Houston Public Radio is listener-funded and comprises one NPR member station, KUHF (\"KUHF News\"). The University of Houston System owns and holds broadcasting licenses to KUHT and KUHF. The stations broadcast from the Melcher Center for Public Broadcasting, located on the campus of the University of Houston.\n\nHouston is served by the \"Houston Chronicle\", its only major daily newspaper with wide distribution. The Hearst Corporation, which owns and operates the \"Houston Chronicle\", bought the assets of the \"Houston Post\"—its long-time rival and main competition—when \"Houston Post\" ceased operations in 1995. The \"Houston Post\" was owned by the family of former Lieutenant Governor Bill Hobby of Houston. The only other major publication to serve the city is the \"Houston Press\"—a free alternative weekly with a weekly readership of more than 300,000.\n\nHouston is the seat of the internationally renowned Texas Medical Center, which contains the world's largest concentration of research and healthcare institutions. All 49 member institutions of the Texas Medical Center are non-profit organizations. They provide patient and preventive care, research, education, and local, national, and international community well-being.\nEmploying more than 73,600 people, institutions at the medical center include 13 hospitals and two specialty institutions, two medical schools, four nursing schools, and schools of dentistry, public health, pharmacy, and virtually all health-related careers. It is where one of the first—and still the largest—air emergency service, Life Flight, was created, and a very successful inter-institutional transplant program was developed. More heart surgeries are performed at the Texas Medical Center than anywhere else in the world.\n\nSome of the academic and research health institutions at the center include MD Anderson Cancer Center, Baylor College of Medicine, UT Health Science Center, Memorial Hermann Hospital, The Methodist Hospital, Texas Children's Hospital, and University of Houston College of Pharmacy.\n\nThe Baylor College of Medicine has annually been considered within the top ten medical schools in the nation; likewise, the MD Anderson Cancer Center has consistently ranked as one of the top two U.S. hospitals specializing in cancer care by \"U.S. News & World Report\" since 1990. The Menninger Clinic, a renowned psychiatric treatment center, is affiliated with Baylor College of Medicine and The Methodist Hospital System. With hospital locations nationwide and headquarters in Houston, the Triumph Healthcare hospital system is the third largest long term acute care provider nationally.\n\n71.7 percent of residents drive alone to work. Houston's freeway system comprises of freeways and expressways in a ten-county metropolitan area. However, the Texas Transportation Institute's annual Urban Mobility Report found that Houston had the fourth-worst congestion in the country with commuters spending an average of 58 hours in traffic in 2009.\n\nHouston's highway system has a hub-and-spoke freeway structure serviced by multiple loops. The innermost loop is Interstate 610, which encircles downtown, the medical center, and many core neighborhoods with around a diameter. Beltway 8 and its freeway core, the Sam Houston Tollway, form the middle loop at a diameter of roughly . A proposed highway project, State Highway 99 (Grand Parkway), will form a third loop outside of Houston, totaling 180 miles in length and making an almost-complete circumference, with the exception of crossing the ship channel. As of June 2014, two of eleven segments of State Highway 99 have been completed to the west of Houston, and three northern segments totaling 38 miles. In addition to the Sam Houston Tollway loop mentioned above, the Harris County Toll Road Authority currently operates four spoke tollways: The Katy Managed Lanes of Interstate 10, the Hardy Toll Road, the Westpark Tollway, and the Fort Bend Parkway Extension. Other spoke roads either planned or under construction include Crosby Freeway, and the future Alvin Freeway.\n\nHouston's freeway system is monitored by Houston TranStar—a partnership of four government agencies that are responsible for providing transportation and emergency management services to the region.\n\nThe Metropolitan Transit Authority of Harris County (METRO) provides public transportation in the form of buses, light rail, and lift vans.\n\nMETRO began light rail service on January 1, 2004, with the inaugural track (\"Red Line\") running about from the University of (UHD), which traverses through the Texas Medical Center and terminates at NRG Park. METRO is currently in the design phase of a 10-year expansion plan that will add five more lines. and expand the current Red Line. Amtrak, the national passenger rail system, provides service three times a week to Houston via the (Los Angeles–New Orleans), which stops at a train station on the north side of the downtown area. The station saw 14,891 boardings and alightings in fiscal year 2008. In 2012, there was a 25 percent increase in ridership to 20,327 passengers embarking from the Houston Amtrak station.\n\nHouston has the largest number of bike commuters in Texas with over 160 miles of dedicated bikeways. The city is currently in the process of expanding its on and off street bikeway network. A bicycle sharing system known as Houston B-Cycle currently operates 29 different stations in downtown and neighboring areas.\n\nHouston is served by three airports, two of which are commercial that served 52 million passengers in 2007 and managed by the Houston Airport System. The Federal Aviation Administration and the state of Texas selected the \"Houston Airport System as Airport of the Year\" for 2005, largely because of its multi-year, $3.1 billion airport improvement program for both major airports in Houston.\n\nThe primary city airport is George Bush Intercontinental Airport (IAH), the tenth-busiest in the United States for total passengers, and twenty eighth-busiest worldwide. Bush Intercontinental currently ranks fourth in the United States for non-stop domestic and international service with 182 destinations. In 2006, the United States Department of Transportation named IAH the fastest-growing of the top ten airports in the United States. The Houston Air Route Traffic Control Center stands on the George Bush Intercontinental Airport grounds.\n\nHouston was the headquarters of Continental Airlines until its 2010 merger with United Airlines with headquarters in Chicago; regulatory approval for the merger was granted in October of that year. Bush Intercontinental became United Airlines' largest airline hub. The airline retained a significant operational presence in Houston while offering more than 700 daily departures from the city. In early 2007, Bush Intercontinental Airport was named a model \"port of entry\" for international travelers by U.S. Customs and Border Protection.\n\nThe second-largest commercial airport is William P. Hobby Airport (named Houston International Airport until 1967) which operates primarily short- to medium-haul domestic flights. However, in 2015 Southwest Airlines launched service from a new international terminal at Hobby airport to several destinations in Mexico, Central America, and the Caribbean. These were the first international flights flown from Hobby since 1969. Houston's aviation history is showcased in the 1940 Air Terminal Museum located in the old terminal building on the west side of the airport. Hobby Airport has been recognized with two awards for being one of the top five performing airports in the world and for customer service by Airports Council International.\n\nHouston's third municipal airport is Ellington Airport (a former U.S. Air Force base) used by military, government, NASA, and general aviation sectors.\nThe Houston Office of Protocol and International Affairs is the city's liaison to Houston's sister cities and to the national governing organization, Sister Cities International. Through their official city-to-city relationships, these volunteer associations promote people-to-people diplomacy and encourage citizens to develop mutual trust and understanding through commercial, cultural, educational, and humanitarian exchanges.\n\n", "id": "13774", "title": "Houston"}
{"url": "https://en.wikipedia.org/wiki?curid=13776", "text": "Head (disambiguation)\n\nThe head is the part of an animal that usually comprises the brain, eyes, ears, nose, and mouth.\n\nHead may also refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "13776", "title": "Head (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=13777", "text": "Hard disk drive\n\nA hard disk drive (HDD), hard disk, hard drive or fixed disk is a data storage device that uses magnetic storage to store and retrieve digital information using one or more rigid rapidly rotating disks (platters) coated with magnetic material. The platters are paired with magnetic heads, usually arranged on a moving actuator arm, which read and write data to the platter surfaces. Data is accessed in a random-access manner, meaning that individual blocks of data can be stored or retrieved in any order and not only sequentially. HDDs are a type of non-volatile memory, retaining stored data even when powered off.\n\nIntroduced by IBM in 1956, HDDs became the dominant secondary storage device for general-purpose computers by the early 1960s. Continuously improved, HDDs have maintained this position into the modern era of servers and personal computers. More than 200 companies have produced HDDs historically, though after extensive industry consolidation most current units are manufactured by Seagate, Toshiba, and Western Digital. , HDD production (in bytes per year) is growing, although unit shipments and sales revenues are declining. The primary competing technology for secondary storage is flash memory in the form of solid-state drives (SSDs), which have higher data-transfer rates, higher areal storage density, better reliability, and much lower latency and access times. While SSDs have higher cost per bit, SSDs are replacing HDDs where speed, power consumption, small size, and durability are important.\n\nThe primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of : a 1-terabyte (TB) drive has a capacity of gigabytes (GB; where 1 gigabyte = bytes). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Performance is specified by the time required to move the heads to a track or cylinder (average access time) plus the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).\n\nThe two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as PATA (Parallel ATA), SATA (Serial ATA), USB or SAS (Serial attached SCSI) cables.\n\nHard disk drives were introduced in 1956, as data storage for an IBM real-time transaction processing computer and were developed for use with general-purpose mainframe and minicomputers. The first IBM drive, the 350 RAMAC in 1956, was approximately the size of two medium-sized refrigerators and stored five million six-bit characters (3.75 megabytes) on a stack of 50 disks.\n\nIn 1962 the IBM 350 RAMAC disk storage unit was superseded by the IBM 1301 disk storage unit, which consisted of 50 platters, each about 1/8-inch thick and 24 inches in diameter. Whereas the IBM 350 used only two read/write heads which were pneumatically actuated and moved in two dimensions, the 1301 was one of the first disk storage units to use an array of heads, one per platter, moving as a single unit. Cylinder-mode read/write operations were supported, and the heads flew about 250 micro-inches (about 6 µm) above the platter surface. Motion of the head array depended upon a binary adder system of hydraulic actuators which assured repeatable positioning. The 1301 cabinet was about the size of three home refrigerators placed side by side, storing the equivalent of about 21 million eight-bit bytes. Access time was about a quarter of a second.\n\nAlso in 1962, IBM introduced the model 1311 disk drive, which was about the size of a washing machine and stored two million characters on a removable disk pack. Users could buy additional packs and interchange them as needed, much like reels of magnetic tape. Later models of removable pack drives, from IBM and others, became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s. Non-removable HDDs were called \"fixed disk\" drives.\n\nSome high-performance HDDs were manufactured with one head per track (e.g. IBM 2305 in 1970) so that no time was lost physically moving the heads to a track. Known as fixed-head or head-per-track disk drives they were very expensive and are no longer in production.\n\nIn 1973, IBM introduced a new type of HDD code-named \"Winchester\". Its primary distinguishing feature was that the disk heads were not withdrawn completely from the stack of disk platters when the drive was powered down. Instead, the heads were allowed to \"land\" on a special area of the disk surface upon spin-down, \"taking off\" again when the disk was later powered on. This greatly reduced the cost of the head actuator mechanism, but precluded removing just the disks from the drive as was done with the disk packs of the day. Instead, the first models of \"Winchester technology\" drives featured a removable disk module, which included both the disk pack and the head assembly, leaving the actuator motor in the drive upon removal. Later \"Winchester\" drives abandoned the removable media concept and returned to non-removable platters.\n\nLike the first removable pack drive, the first \"Winchester\" drives used platters in diameter. A few years later, designers were exploring the possibility that physically smaller platters might offer advantages. Drives with non-removable eight-inch platters appeared, and then drives that used a form factor (a mounting width equivalent to that used by contemporary floppy disk drives). The latter were primarily intended for the then-fledgling personal computer (PC) market.\n\nAs the 1980s began, HDDs were a rare and very expensive additional feature in PCs, but by the late 1980s their cost had been reduced to the point where they were standard on all but the cheapest computers.\n\nMost HDDs in the early 1980s were sold to PC end users as an external, add-on subsystem. The subsystem was not sold under the drive manufacturer's name but under the subsystem manufacturer's name such as Corvus Systems and Tallgrass Technologies, or under the PC system manufacturer's name such as the Apple ProFile. The IBM PC/XT in 1983 included an internal 10 MB HDD, and soon thereafter internal HDDs proliferated on personal computers.\n\nExternal HDDs remained popular for much longer on the Apple Macintosh. Many Macintosh computers made between 1986 and 1998 featured a SCSI port on the back, making external expansion simple. Older compact Macintosh computers did not have user-accessible hard drive bays (indeed, the Macintosh 128K, Macintosh 512K, and Macintosh Plus did not feature a hard drive bay at all), so on those models external SCSI disks were the only reasonable option for expanding upon any internal storage.\n\nThe 2011 Thailand floods damaged the manufacturing plants and impacted hard disk drive cost adversely between 2011 and 2013.\n\nDriven by ever increasing areal density since their invention, HDDs have continuously improved their characteristics; a few highlights are listed in the table above. At the same time, market application expanded from mainframe computers of the late 1950s to most mass storage applications including computers and consumer applications such as storage of entertainment content.\n\nA modern HDD records data by magnetizing a thin film of ferromagnetic material on a disk. Sequential changes in the direction of magnetization represent binary data bits. The data is read from the disk by detecting the transitions in magnetization. User data is encoded using an encoding scheme, such as run-length limited encoding, which determines how the data is represented by the magnetic transitions.\n\nA typical HDD design consists of a \"\" that holds flat circular disks, also called platters, which hold the recorded data. The platters are made from a non-magnetic material, usually aluminum alloy, glass, or ceramic, and are coated with a shallow layer of magnetic material typically 10–20 nm in depth, with an outer layer of carbon for protection. For reference, a standard piece of copy paper is .\n\nThe platters in contemporary HDDs are spun at speeds varying from 4,200 rpm in energy-efficient portable devices, to 15,000 rpm for high-performance servers. The first HDDs spun at 1,200 rpm and, for many years, 3,600 rpm was the norm. As of December 2013, the platters in most consumer-grade HDDs spin at either 5,400 rpm or 7,200 rpm.\n\nInformation is written to and read from a platter as it rotates past devices called read-and-write heads that are positioned to operate very close to the magnetic surface, with their flying height often in the range of tens of nanometers. The read-and-write head is used to detect and modify the magnetization of the material passing immediately under it.\n\nIn modern drives, there is one head for each magnetic platter surface on the spindle, mounted on a common arm. An actuator arm (or access arm) moves the heads on an arc (roughly radially) across the platters as they spin, allowing each head to access almost the entire surface of the platter as it spins. The arm is moved using a voice coil actuator or in some older designs a stepper motor. Early hard disk drives wrote data at some constant bits per second, resulting in all tracks having the same amount of data per track but modern drives (since the 1990s) use zone bit recording—increasing the write speed from inner to outer zone and thereby storing more data per track in the outer zones.\n\nIn modern drives, the small size of the magnetic regions creates the danger that their magnetic state might be lost because of thermal effects, thermally induced magnetic instability which is commonly known as the \"superparamagnetic limit\". To counter this, the platters are coated with two parallel magnetic layers, separated by a 3-atom layer of the non-magnetic element ruthenium, and the two layers are magnetized in opposite orientation, thus reinforcing each other. Another technology used to overcome thermal effects to allow greater recording densities is perpendicular recording, first shipped in 2005, and as of 2007 the technology was used in many HDDs.\n\nIn 2004, a new concept was introduced to allow further increase of the data density in magnetic recording, using recording media consisting of coupled soft and hard magnetic layers. That so-called \"exchange spring media\", also known as \"exchange coupled composite media\", allows good writability due to the write-assist nature of the soft layer. However, the thermal stability is determined only by the hardest layer and not influenced by the soft layer.\n\nA typical HDD has two electric motors; a spindle motor that spins the disks and an actuator (motor) that positions the read/write head assembly across the spinning disks. The disk motor has an external rotor attached to the disks; the stator windings are fixed in place. Opposite the actuator at the end of the head support arm is the read-write head; thin printed-circuit cables connect the read-write heads to amplifier electronics mounted at the pivot of the actuator. The head support arm is very light, but also stiff; in modern drives, acceleration at the head reaches 550 \"g\".\n\nThe \"\" is a permanent magnet and moving coil motor that swings the heads to the desired position. A metal plate supports a squat neodymium-iron-boron (NIB) high-flux magnet. Beneath this plate is the moving coil, often referred to as the \"voice coil\" by analogy to the coil in loudspeakers, which is attached to the actuator hub, and beneath that is a second NIB magnet, mounted on the bottom plate of the motor (some drives have only one magnet).\n\nThe voice coil itself is shaped rather like an arrowhead, and made of doubly coated copper magnet wire. The inner layer is insulation, and the outer is thermoplastic, which bonds the coil together after it is wound on a form, making it self-supporting. The portions of the coil along the two sides of the arrowhead (which point to the actuator bearing center) then interact with the magnetic field of the fixed magnet. Current flowing radially outward along one side of the arrowhead and radially inward on the other produces the tangential force. If the magnetic field were uniform, each side would generate opposing forces that would cancel each other out. Therefore, the surface of the magnet is half north pole and half south pole, with the radial dividing line in the middle, causing the two sides of the coil to see opposite magnetic fields and produce forces that add instead of canceling. Currents along the top and bottom of the coil produce radial forces that do not rotate the head.\n\nThe HDD's electronics control the movement of the actuator and the rotation of the disk, and perform reads and writes on demand from the disk controller. Feedback of the drive electronics is accomplished by means of special segments of the disk dedicated to servo feedback. These are either complete concentric circles (in the case of dedicated servo technology), or segments interspersed with real data (in the case of embedded servo technology). The servo feedback optimizes the signal to noise ratio of the GMR sensors by adjusting the voice-coil of the actuated arm. The spinning of the disk also uses a servo motor. Modern disk firmware is capable of scheduling reads and writes efficiently on the platter surfaces and remapping sectors of the media which have failed.\n\nModern drives make extensive use of error correction codes (ECCs), particularly Reed–Solomon error correction. These techniques store extra bits, determined by mathematical formulas, for each block of data; the extra bits allow many errors to be corrected invisibly. The extra bits themselves take up space on the HDD, but allow higher recording densities to be employed without causing uncorrectable errors, resulting in much larger storage capacity. For example, a typical 1 TB hard disk with 512-byte sectors provides additional capacity of about 93 GB for the ECC data.\n\nIn the newest drives, as of 2009, low-density parity-check codes (LDPC) were supplanting Reed-Solomon; LDPC codes enable performance close to the Shannon Limit and thus provide the highest storage density available.\n\nTypical hard disk drives attempt to \"remap\" the data in a physical sector that is failing to a spare physical sector provided by the drive's \"spare sector pool\" (also called \"reserve pool\"), while relying on the ECC to recover stored data while the amount of errors in a bad sector is still low enough. The S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology) feature counts the total number of errors in the entire HDD fixed by ECC (although not on all hard drives as the related S.M.A.R.T attributes \"Hardware ECC Recovered\" and \"Soft ECC Correction\" are not consistently supported), and the total number of performed sector remappings, as the occurrence of many such errors may predict an HDD failure.\n\nThe \"No-ID Format\", developed by IBM in the mid-1990s, contains information about which sectors are bad and where remapped sectors have been located.\n\nOnly a tiny fraction of the detected errors ends up as not correctable. For example, specification for an enterprise SAS disk (a model from 2013) estimates this fraction to be one uncorrected error in every 10 bits, and another SAS enterprise disk from 2013 specifies similar error rates. Another modern (as of 2013) enterprise SATA disk specifies an error rate of less than 10 non-recoverable read errors in every 10 bits. An enterprise disk with a Fibre Channel interface, which uses 520 byte sectors to support the Data Integrity Field standard to combat data corruption, specifies similar error rates in 2005.\n\nThe worst type of errors are those that go unnoticed, and are not even detected by the disk firmware or the host operating system. These errors are known as silent data corruption, some of which may be caused by hard disk drive malfunctions.\n\nThe rate of areal density advancement was similar to Moore's law (doubling every two years) through 2010: 60% per year during 1988–1996, 100% during 1996–2003 and 30% during 2003–2010. Gordon Moore (1997) called the increase \"flabbergasting,\" while observing later that growth cannot continue forever. Areal density advancement slowed to 10% per year during 2011–2014, due to difficulty in migrating from perpendicular recording to newer technologies.\n\nAreal density is the inverse of bit cell size, so an increase in areal density corresponds to a decrease in bit cell size. In 2013, a production desktop 3 TB HDD (with four platters) would have had an areal density of about 500 Gbit/in which would have amounted to a bit cell comprising about 18 magnetic grains (11 by 1.6 grains). Since the mid-2000s areal density progress has increasingly been challenged by a superparamagnetic trilemma involving grain size, grain magnetic strength and ability of the head to write. In order to maintain acceptable signal to noise smaller grains are required; smaller grains may self-reverse (electrothermal instability) unless their magnetic strength is increased, but known write head materials are unable to generate a magnetic field sufficient to write the medium. Several new magnetic storage technologies are being developed to overcome or at least abate this trilemma and thereby maintain the competitiveness of HDDs with respect to products such as flash memory-based solid-state drives (SSDs).\n\nIn 2013, Seagate introduced one such technology, shingled magnetic recording (SMR). Additionally, SMR comes with design complexities that may cause reduced write performance. Other new recording technologies that, , still remain under development include heat-assisted magnetic recording (HAMR), microwave-assisted magnetic recording (MAMR), two-dimensional magnetic recording (TDMR), bit-patterned recording (BPR), and \"current perpendicular to plane\" giant magnetoresistance (CPP/GMR) heads.\n\nThe rate of areal density growth has dropped below the historical Moore's law rate of 40% per year, and the deceleration is expected to persist through at least 2020. Depending upon assumptions on feasibility and timing of these technologies, the median forecast by industry observers and analysts for 2020 and beyond for areal density growth is 20% per year with a range of 10–30%. The achievable limit for the HAMR technology in combination with BPR and SMR may be 10 Tbit/in, which would be 20 times higher than the 500 Gbit/in represented by 2013 production desktop HDDs. As of 2015, HAMR HDDs have been delayed several years, and are expected in 2018. They require a different architecture, with redesigned media and read/write heads, new lasers, and new near-field optical transducers.\n\nThe capacity of a hard disk drive, as reported by an operating system to the end user, is smaller than the amount stated by the manufacturer, which has several reasons: the operating system using some space, use of some space for data redundancy, and space use for file system structures. The difference in capacity reported in true SI-based units vs. binary prefixes can lead to a false impression of missing capacity.\n\nModern hard disk drives appear to their host controller as a contiguous set of logical blocks, and the gross drive capacity is calculated by multiplying the number of blocks by the block size. This information is available from the manufacturer's product specification, and from the drive itself through use of operating system functions that invoke low-level drive commands.\n\nThe gross capacity of older HDDs is calculated as the product of the number of cylinders per recording zone, the number of bytes per sector (most commonly 512), and the count of zones of the drive. Some modern SATA drives also report cylinder-head-sector (CHS) capacities, but these are not physical parameters because the reported values are constrained by historic operating system interfaces. The C/H/S scheme has been replaced by logical block addressing (LBA), a simple linear addressing scheme that locates blocks by an integer index, which starts at LBA 0 for the first block and increments thereafter. When using the C/H/S method to describe modern large drives, the number of heads is often set to 64, although a typical hard disk drive, , has between one and four platters.\n\nIn modern HDDs, spare capacity for defect management is not included in the published capacity; however, in many early HDDs a certain number of sectors were reserved as spares, thereby reducing the capacity available to the operating system.\n\nFor RAID subsystems, data integrity and fault-tolerance requirements also reduce the realized capacity. For example, a RAID 1 array has about half the total capacity as a result of data mirroring, while a RAID 5 array with drives loses of capacity (which equals to the capacity of a single drive) due to storing parity information. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provide fault tolerance. Most RAID vendors use checksums to improve data integrity at the block level. Some vendors design systems using HDDs with sectors of 520 bytes to contain 512 bytes of user data and eight checksum bytes, or by using separate 512-byte sectors for the checksum data.\n\nSome systems may use hidden partitions for system recovery, reducing the capacity available to the end user.\n\nThe presentation of a hard disk drive to its host is determined by the disk controller. The actual presentation may differ substantially from the drive's native interface, particularly in mainframes or servers. Modern HDDs, such as SAS and SATA drives, appear at their interfaces as a contiguous set of logical blocks that are typically 512 bytes long, though the industry is in the process of changing to the 4,096-byte logical blocks layout, known as the Advanced Format (AF).\n\nThe process of initializing these logical blocks on the physical disk platters is called \"low-level formatting\", which is usually performed at the factory and is not normally changed in the field. As a next step in preparing an HDD for use, \"high-level formatting\" writes partition and file system structures into selected logical blocks to make the remaining logical blocks available to the host's operating system and its applications. The file system uses some of the disk space to structure the HDD and organize files, recording their file names and the sequence of disk areas that represent the file. Examples of data structures stored on disk to retrieve files include the File Allocation Table (FAT) in the DOS file system and inodes in many UNIX file systems, as well as other operating system data structures (also known as metadata). As a consequence, not all the space on an HDD is available for user files, but this system overhead is usually negligible.\n\nThe total capacity of HDDs is given by manufacturers in SI-based units such as gigabytes (1 GB = 1,000,000,000 bytes) and terabytes (1 TB = 1,000,000,000,000 bytes). The practice of using SI-based prefixes (denoting powers of 1,000) in the hard disk drive and computer industries dates back to the early days of computing; by the 1970s, \"million\", \"mega\" and \"M\" were consistently used in the decimal sense for drive capacity. However, capacities of memory (RAM, ROM) and CDs are traditionally quoted using a binary interpretation of the prefixes, i.e. using powers of 1024 instead of 1000.\n\nInternally, computers do not represent either hard disk drive or memory capacity in powers of 1,024, but reporting it in this manner is a convention. The Microsoft Windows family of operating systems uses the binary convention when reporting storage capacity, so an HDD offered by its manufacturer as a 1 TB drive is reported by these operating systems as a 931 GB HDD. Mac OS X 10.6 (\"Snow Leopard\") uses decimal convention when reporting HDD capacity. The default behavior of the command-line utility on Linux is to report the HDD capacity as a number of 1024-byte units.\n\nThe difference between the decimal and binary prefix interpretation caused some consumer confusion and led to class action suits against HDD manufacturers. The plaintiffs argued that the use of decimal prefixes effectively misled consumers while the defendants denied any wrongdoing or liability, asserting that their marketing and advertising complied in all respects with the law and that no class member sustained any damages or injuries.\n\nHDD price per byte improved at the rate of −40% per year during 1988–1996, −51% per year during 1996–2003, and −34% per year during 2003–2010. The price improvement decelerated to −13% per year during 2011–2014, as areal density increase slowed and the 2011 Thailand floods damaged manufacturing facilities.\n\nIBM's first hard drive, the IBM 350, used a stack of fifty 24-inch platters and was of a size comparable to two large refrigerators. In 1962, IBM introduced its model 1311 disk, which used six 14-inch (nominal size) platters in a removable pack and was roughly the size of a washing machine. This became a standard platter size and drive form-factor for many years, used also by other manufacturers. The IBM 2314 used platters of the same size in an eleven-high pack and introduced the \"drive in a drawer\" layout, although the \"drawer\" was not the complete drive.\n\nLater drives were designed to fit entirely into a chassis that would mount in a 19-inch rack. Digital's RK05 and RL01 were early examples using single 14-inch platters in removable packs, the entire drive fitting in a 10.5-inch-high rack space (six rack units). In the mid-to-late 1980s the similarly sized Fujitsu Eagle, which used (coincidentally) 10.5-inch platters, was a popular product.\n\nSuch large platters were never used with microprocessor-based systems. With increasing sales of microcomputers having built in floppy-disk drives (FDDs), HDDs that would fit to the FDD mountings became desirable. Thus HDD \"Form factors\", initially followed those of 8-inch, 5.25-inch, and 3.5-inch floppy disk drives. Because there were no smaller floppy disk drives, smaller HDD form factors developed from product offerings or industry standards.\n\n\n, 2.5-inch and 3.5-inch hard disks were the most popular sizes.\n\nBy 2009, all manufacturers had discontinued the development of new products for the 1.3-inch, 1-inch and 0.85-inch form factors due to falling prices of flash memory, which has no moving parts.\n\nWhile these sizes are customarily described by an approximately correct figure in inches, actual sizes have long been specified in millimeters.\n\nThe factors that limit the time to access the data on an HDD are mostly related to the mechanical nature of the rotating disks and moving heads. Seek time is a measure of how long it takes the head assembly to travel to the track of the disk that contains data. Rotational latency is incurred because the desired disk sector may not be directly under the head when data transfer is requested. These two delays are on the order of milliseconds each. The bit rate or data transfer rate (once the head is in the right position) creates delay which is a function of the number of blocks transferred; typically relatively small, but can be quite long with the transfer of large contiguous files. Delay may also occur if the drive disks are stopped to save energy.\n\nAn HDD's \"Average Access Time\" is its average seek time which technically is the time to do all possible seeks divided by the number of all possible seeks, but in practice is determined by statistical methods or simply approximated as the time of a seek over one-third of the number of tracks.\n\nDefragmentation is a procedure used to minimize delay in retrieving data by moving related items to physically proximate areas on the disk. Some computer operating systems perform defragmentation automatically. Although automatic defragmentation is intended to reduce access delays, performance will be temporarily reduced while the procedure is in progress.\n\nTime to access data can be improved by increasing rotational speed (thus reducing latency) or by reducing the time spent seeking. Increasing areal density increases throughput by increasing data rate and by increasing the amount of data under a set of heads, thereby potentially reducing seek activity for a given amount of data. The time to access data has not kept up with throughput increases, which themselves have not kept up with growth in bit density and storage capacity.\n\nAverage seek time ranges from under 4 ms for high-end server drives to 15 ms for mobile drives, with the most common mobile drives at about 12 ms and the most common desktop type typically being around 9 ms. The first HDD had an average seek time of about 600 ms; by the middle of 1970s, HDDs were available with seek times of about 25 ms. Some early PC drives used a stepper motor to move the heads, and as a result had seek times as slow as 80–120 ms, but this was quickly improved by voice coil type actuation in the 1980s, reducing seek times to around 20 ms. Seek time has continued to improve slowly over time.\n\nSome desktop and laptop computer systems allow the user to make a tradeoff between seek performance and drive noise. Faster seek rates typically require more energy usage to quickly move the heads across the platter, causing louder noises from the pivot bearing and greater device vibrations as the heads are rapidly accelerated during the start of the seek motion and decelerated at the end of the seek motion. Quiet operation reduces movement speed and acceleration rates, but at a cost of reduced seek performance.\n\nLatency is the delay for the rotation of the disk to bring the required disk sector under the read-write mechanism. It depends on rotational speed of a disk, measured in revolutions per minute (rpm). Average rotational latency is shown in the table on the right, based on the statistical relation that the average latency in milliseconds for such a drive is one-half the rotational period. Average latency (in milliseconds) is computed as 30,000 divided by rotational speed (in rpm).\n\n, a typical 7,200-rpm desktop HDD has a sustained \"disk-to-buffer\" data transfer rate up to 1,030 Mbit/s. This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the \"buffer-to-computer\" interface is 3.0 Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by file system fragmentation and the layout of the files.\n\nHDD data transfer rate depends upon the rotational speed of the platters and the data recording density. Because heat and vibration limit rotational speed, advancing density becomes the main method to improve sequential transfer rates. Higher speeds require a more powerful spindle motor, which creates more heat. While areal density advances by increasing both the number of tracks across the disk and the number of sectors per track, only the latter increases the data transfer rate for a given rpm. Since data transfer rate performance tracks only one of the two components of areal density, its performance improves at a lower rate.\n\nOther performance considerations include quality-adjusted price, power consumption, audible noise, and both operating and non-operating shock resistance.\n\nThe Federal Reserve Board has a quality-adjusted price index for large-scale enterprise storage systems including three or more enterprise HDDs and associated controllers, racks and cables. Prices for these large-scale storage systems improved at the rate of ‒30% per year during 2004–2009 and ‒22% per year during 2009–2014.\n\nHDDs are accessed over one of a number of bus types, including parallel ATA (PATA, also called IDE or EIDE; described before the introduction of SATA as ATA), Serial ATA (SATA), SCSI, Serial Attached SCSI (SAS), and Fibre Channel. Bridge circuitry is sometimes used to connect HDDs to buses with which they cannot communicate natively, such as IEEE 1394, USB and SCSI.\n\nModern HDDs present a consistent interface to the rest of the computer, no matter what data encoding scheme is used internally. Typically a DSP in the electronics inside the HDD takes the raw analog voltages from the read head and uses PRML and Reed–Solomon error correction to decode the sector boundaries and sector data, then sends that data out the standard interface. That DSP also watches the error rate detected by error detection and correction, and performs bad sector remapping, data collection for Self-Monitoring, Analysis, and Reporting Technology, and other internal tasks.\n\nModern interfaces connect an HDD to a host bus interface adapter (today typically integrated into the \"south bridge\") with one data/control cable. Each drive also has an additional power cable, usually direct to the power supply unit.\n\n\nDue to the extremely close spacing between the heads and the disk surface, HDDs are vulnerable to being damaged by a head crash—a failure of the disk in which the head scrapes across the platter surface, often grinding away the thin magnetic film and causing data loss. Head crashes can be caused by electronic failure, a sudden power failure, physical shock, contamination of the drive's internal enclosure, wear and tear, corrosion, or poorly manufactured platters and heads.\n\nThe HDD's spindle system relies on air density inside the disk enclosure to support the heads at their proper flying height while the disk rotates. HDDs require a certain range of air densities in order to operate properly. The connection to the external environment and density occurs through a small hole in the enclosure (about 0.5 mm in breadth), usually with a filter on the inside (the \"breather filter\"). If the air density is too low, then there is not enough lift for the flying head, so the head gets too close to the disk, and there is a risk of head crashes and data loss. Specially manufactured sealed and pressurized disks are needed for reliable high-altitude operation, above about . Modern disks include temperature sensors and adjust their operation to the operating environment. Breather holes can be seen on all disk drives—they usually have a sticker next to them, warning the user not to cover the holes. The air inside the operating drive is constantly moving too, being swept in motion by friction with the spinning platters. This air passes through an internal recirculation (or \"recirc\") filter to remove any leftover contaminants from manufacture, any particles or chemicals that may have somehow entered the enclosure, and any particles or outgassing generated internally in normal operation. Very high humidity present for extended periods of time can corrode the heads and platters.\n\nFor giant magnetoresistive (GMR) heads in particular, a minor head crash from contamination (that does not remove the magnetic surface of the disk) still results in the head temporarily overheating, due to friction with the disk surface, and can render the data unreadable for a short period until the head temperature stabilizes (so called \"thermal asperity\", a problem which can partially be dealt with by proper electronic filtering of the read signal).\n\nWhen the logic board of a hard disk fails, the drive can often be restored to functioning order and the data recovered by replacing the circuit board with one of an identical hard disk. In the case of read-write head faults, they can be replaced using specialized tools in a dust-free environment. If the disk platters are undamaged, they can be transferred into an identical enclosure and the data can be copied or cloned onto a new drive. In the event of disk-platter failures, disassembly and imaging of the disk platters may be required. For logical damage to file systems, a variety of tools, including fsck on UNIX-like systems and CHKDSK on Windows, can be used for data recovery. Recovery from logical damage can require file carving.\n\nA common expectation is that hard disk drives designed and marketed for server use will fail less frequently than consumer-grade drives usually used in desktop computers. However, two independent studies by Carnegie Mellon University and Google found that the \"grade\" of a drive does not relate to the drive's failure rate.\n\nA 2011 summary of research, into SSD and magnetic disk failure patterns by Tom's Hardware summarized research findings as follows:\n\n\n\n\n\n\nMore than 200 companies have manufactured HDDs over time. But consolidations have concentrated production into just three manufacturers today: Western Digital, Seagate, and Toshiba.\n\nWorldwide revenues for disk storage were $28 billion in 2015, down from $32 billion in 2013. Worldwide shipments were 469 million units in 2015, down 17% from 564 million in 2014 and 551 million in 2013. Market shares are 40–45% each for Seagate and Western Digital and 13–17% for Toshiba. The two largest manufacturers had an average sales price of USD $60 per HDD unit in 2015.\n\nThe maximum areal storage density for flash memory used in SSDs is 2.8 Tbit/in in laboratory demonstrations as of 2016, and the maximum for HDDs is 1.5 Tbit/in. The areal density of flash memory is doubling every two years, similar to Moore's law (40% per year) and faster than the 10–20% per year for HDDs. As of 2016, maximum capacity was 10 terabytes for an HDD, and 15 terabytes for an SSD. HDDs were used in 70% of the desktop and notebook computers produced in 2016, and SSDs were used in 30%. The usage share of HDDs is declining and could drop below 50% in 2018–2019 according to one forecast, because SSDs are replacing smaller-capacity (less than one-terabyte) HDDs in desktop and notebook computers and MP3 players.\n\nExternal hard disk drives typically connect via USB; variants using USB 2.0 interface generally have slower data transfer rates when compared to internally mounted hard drives connected through SATA. Plug and play drive functionality offers system compatibility and features large storage options and portable design. , available capacities for external hard disk drives ranged from 500 GB to 8 TB.\n\nExternal hard disk drives are usually available as pre-assembled integrated products, but may be also assembled by combining an external enclosure (with USB or other interface) with a separately purchased drive. They are available in 2.5-inch and 3.5-inch sizes; 2.5-inch variants are typically called \"portable external drives\", while 3.5-inch variants are referred to as \"desktop external drives\". \"Portable\" drives are packaged in smaller and lighter enclosures than the \"desktop\" drives; additionally, \"portable\" drives use power provided by the USB connection, while \"desktop\" drives require external power bricks.\n\nFeatures such as biometric security or multiple interfaces (for example, Firewire) are available at a higher cost. There are pre-assembled external hard disk drives that, when taken out from their enclosures, cannot be used internally in a laptop or desktop computer due to embedded USB interface on their printed circuit boards, and lack of SATA (or Parallel ATA) interfaces.\n\nHard disk drives are traditionally symbolized as a stylized stack of platters or as a cylinder, and are as such found in various diagrams; sometimes, they are depicted with small lights to indicate data access. In most modern graphical user environments (GUIs), hard disk drives are represented by an illustration or photograph of the drive enclosure.\n\n\n", "id": "13777", "title": "Hard disk drive"}
{"url": "https://en.wikipedia.org/wiki?curid=13782", "text": "Hebrew calendar\n\nThe Hebrew or Jewish calendar (, \"Ha-Luah ha-Ivri\") is a lunisolar calendar used today predominantly for Jewish religious observances. It determines the dates for Jewish holidays and the appropriate public reading of Torah portions, \"yahrzeits\" (dates to commemorate the death of a relative), and daily Psalm readings, among many ceremonial uses. In Israel, it is used for religious purposes, provides a time frame for agriculture and is an official calendar for civil purposes, although the latter usage has been steadily declining in favor of the Gregorian calendar.\n\nThe present Hebrew calendar is the product of evolution, including a Babylonian influence. Until the Tannaitic period (approximately 10–220 CE), the calendar employed a new crescent moon, with an additional month normally added every two or three years to correct for the difference between twelve lunar months and the solar year. When to add it was based on observation of natural agriculture-related events in Israel. Through the Amoraic period (200–500 CE) and into the Geonic period, this system was gradually displaced by the mathematical rules used today. The principles and rules were fully codified by Maimonides in the \"Mishneh Torah\" in the 12th century. Maimonides' work also replaced counting \"years since the destruction of the Temple\" with the modern creation-era \"Anno Mundi.\"\n\nThe Hebrew lunar year is about eleven days shorter than the solar year and uses the 19-year Metonic cycle to bring it into line with the solar year, with the addition of an intercalary month every two or three years, for a total of seven times per 19 years. Even with this intercalation, the average Hebrew calendar year is longer by about 6 minutes and 40 seconds than the current mean tropical year, so that every 216 years the Hebrew calendar will fall a day behind the current mean tropical year; and about every 231 years it will fall a day behind the mean Gregorian calendar year.\n\nThe era used since the Middle Ages is the \"Anno Mundi\" epoch (Latin for \"in the year of the world\"; Hebrew: , \"from the creation of the world\"). As with \"Anno Domini\" (\"A.D.\" or \"AD\"), the words or abbreviation for \"Anno Mundi\" (\"A.M.\" or \"AM\") for the era should properly \"precede\" the date rather than follow it.\n\nAM began at sunset on and will end at sunset on .\n\nThe Jewish day is of no fixed length. The Jewish day is modeled on the reference to \"...there was evening and there was morning...\" in the creation account in the first chapter of Genesis. Based on the classic rabbinic interpretation of this text, a day in the rabbinic Hebrew calendar runs from sunset (start of \"the evening\") to the next sunset. (In most populated parts of the world this is always approximately 24 standard hours; but, depending on the season of the year, it can be slightly less or slightly more.) The time between sunset and the time when three stars are visible (known as 'tzait ha'kochavim') is known as 'bein hashmashot' and for some uses it is debated as what day it is.\n\nThere is no clock in the Jewish scheme, so that a civil clock is used. Though the civil clock, including the one in use in Israel, incorporates local adoptions of various conventions such as time zones, standard times and daylight saving, these have no place in the Jewish scheme. The civil clock is used only as a reference point – in expressions such as: \"Shabbat starts at ...\". The steady progression of sunset around the world and seasonal changes results in gradual civil time changes from one day to the next based on observable astronomical phenomena (the sunset) and not on man-made laws and conventions.\n\nIn Judaism, an hour is defined as 1/12 of the time from sunrise to sunset, so, during the winter, an hour can be much less than 60 minutes, and during the summer, it can be much more than 60 minutes. This proportional hour is known as a 'sha'ah z'manit' (lit. a timely hour).\n\nInstead of the international date line convention, there are varying opinions as to where the day changes. One opinion uses the antimeridian of Jerusalem. (Jerusalem is 35°13’ east of the prime meridian, so the antimeridian is at 144°47' W, passing through eastern Alaska.) Other opinions exist as well.\n\nEvery hour is divided into 1080 \"halakim\" (singular: \"helek\") or parts. A part is 3⅓ seconds or / minute. The ultimate ancestor of the helek was a small Babylonian time period called a \"barleycorn\", itself equal to / of a Babylonian \"time degree\" (1° of celestial rotation).\n\nThe weekdays start with Sunday (day 1, or \"Yom Rishon\") and proceed to Saturday (day 7), Shabbat. Since some calculations use division, a remainder of 0 signifies Saturday.\n\nWhile calculations of days, months and years are based on fixed hours equal to / of a day, the beginning of each \"halachic\" day is based on the local time of sunset. The end of the Shabbat and other Jewish holidays is based on nightfall (\"Tzeth haKochabim\") which occurs some amount of time, typically 42 to 72 minutes, after sunset. According to Maimonides, nightfall occurs when three medium-sized stars become visible after sunset. By the 17th century, this had become three second-magnitude stars. The modern definition is when the center of the sun is 7° below the geometric (airless) horizon, somewhat later than civil twilight at 6°. The beginning of the daytime portion of each day is determined both by dawn and sunrise. Most \"halachic\" times are based on some combination of these four times and vary from day to day throughout the year and also vary significantly depending on location. The daytime hours are often divided into \"Sha`oth Zemaniyoth\" or \"Halachic hours\" by taking the time between sunrise and sunset or between dawn and nightfall and dividing it into 12 equal hours. The nighttime hours are similarly divided into 12 equal portions, albeit a different amount of time than the \"hours\" of the daytime. The earliest and latest times for Jewish services, the latest time to eat chametz on the day before Passover and many other rules are based on \"Sha`oth Zemaniyoth\". For convenience, the modern day using \"Sha`oth Zemaniyoth\" is often discussed as if sunset were at 6:00pm, sunrise at 6:00am and each hour were equal to a fixed hour. For example, \"halachic\" noon may be after 1:00pm in some areas during daylight saving time. Within the Mishnah, however, the numbering of the hours starts with the \"first\" hour after the start of the day.\n\nShavua [שבוע] is a weekly cycle of seven days, mirroring the seven-day period of the Book of Genesis in which the world is created. The names for the days of the week, like those in the creation account, are simply the day number within the week, with Shabbat being the seventh day. Each day of the week runs from sunset to the following sunset and is figured locally.\n\nThe Hebrew calendar follows a seven-day weekly cycle, which runs concurrently with but independently of the monthly and annual cycles. The names for the days of the week are simply the day number within the week. In Hebrew, these names may be abbreviated using the numerical value of the Hebrew letters, for example (\"Day 1\", or Yom Rishon ()):\n\n\nThe names of the days of the week are modeled on the seven days mentioned in the creation story. For example, \"... And there was evening and there was morning, one day\". \"One day\" () in Genesis 1:15 is translated in JPS as \"first day\", and in some other contexts (including KJV) as \"day one\". In subsequent verses, the Hebrew refers to the days using ordinal numbers, e.g., 'second day', 'third day', and so forth, but with the sixth and seventh days the Hebrew includes the definite article (\"the\").\n\nThe Jewish Shabbat has a special role in the Jewish weekly cycle. There are many special rules that relate to the Shabbat, discussed more fully in the Talmudic tractate Shabbat.\n\nIn (Talmudic) Hebrew, the word \"Shabbat\" () can also mean \"week\", so that in ritual liturgy a phrase like \"Yom Reviʻi bəShabbat\" means \"the fourth day in the week\".\n\nThe period from 1 Adar (or Adar II, in leap years) to 29 Marcheshvan contains all of the festivals specified in the Bible – Purim (14 Adar), Pesach (15 Nisan), Shavuot (6 Sivan), Rosh Hashanah (1 Tishrei), Yom Kippur (10 Tishrei), Sukkot (15 Tishrei), and Shemini Atzeret (22 Tishrei). This period is fixed, during which no adjustments are made.\nThere are additional rules in the Hebrew calendar to prevent certain holidays from falling on certain days of the week. (See Rosh Hashanah postponement rules, below.) These rules are implemented by adding an extra day to Marcheshvan (making it 30 days long) or by removing one day from Kislev (making it 29 days long). Accordingly, a common Hebrew calendar year can have a length of 353, 354 or 355 days, while a leap Hebrew calendar year can have a length of 383, 384 or 385 days.\n\nThe Hebrew calendar is a lunisolar calendar, meaning that months are based on lunar months, but years are based on solar years. The calendar year features twelve lunar months of twenty-nine or thirty days, with an intercalary lunar month added periodically to synchronize the twelve lunar cycles with the longer solar year. (These extra months are added seven times every nineteen years. See Leap months, below.) The beginning of each Jewish lunar month is based on the appearance of the new moon. Although originally the new lunar crescent had to be observed and certified by witnesses, the moment of the true new moon is now approximated arithmetically as the molad, which is the mean new moon to a precision of one part.\n\nThe mean period of the lunar month (precisely, the synodic month) is very close to 29.5 days. Accordingly, the basic Hebrew calendar year is one of twelve lunar months alternating between 29 and 30 days:\n\nIn leap years (such as 5774) an additional month, Adar I (30 days) is added after Shevat, while the regular Adar is referred to as \"Adar II.\"\n\nThe insertion of the leap month mentioned above is based on the requirement that Passover—the festival celebrating the Exodus from Egypt, which took place in the spring—always occurs in the [northern hemisphere's] spring season. Since the adoption of a fixed calendar, intercalations in the Hebrew calendar have been assigned to fixed points in a 19-year cycle. Prior to this, the intercalation was determined empirically:\n\nThe year may be intercalated on three grounds: 'aviv [i.e.the ripeness of barley], fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone.\n\nFrom very early times, the Mesopotamian lunisolar calendar was in wide use by the countries of the western Asia region. The structure, which was also used by the Israelites, was based on lunar months with the intercalation of an additional month to bring the cycle closer to the solar cycle, although there is no evidence of a thirteenth month mentioned anywhere in the Hebrew Bible.\n\nAccording to the \"Mishnah\" and Tosefta, in the Maccabean, Herodian, and Mishnaic periods, new months were determined by the sighting of a new crescent, with two eyewitnesses required to testify to the Sanhedrin to having seen the new lunar crescent at sunset. The practice in the time of Gamaliel II (c. 100 CE) was for witnesses to select the appearance of the moon from a collection of drawings that depicted the crescent in a variety of orientations, only a few of which could be valid in any given month. These observations were compared against calculations.\n\nAt first the beginning of each Jewish month was signaled to the communities of Israel and beyond by fires lit on mountaintops, but after the Samaritans began to light false fires, messengers were sent. The inability of the messengers to reach communities outside Israel before mid-month High Holy Days (Succot and Passover) led outlying communities to celebrate scriptural festivals for two days rather than one, observing the second feast-day of the Jewish diaspora because of uncertainty of whether the previous month ended after 29 or 30 days.\n\nIn his work \"Mishneh Torah\" (1178), Maimonides included a chapter \"Sanctification of the New Moon\", in which he discusses the calendrical rules and their scriptural basis. He notes, \"By how much does the solar year exceed the lunar year? By approximately 11 days. Therefore, whenever this excess accumulates to about 30 days, or a little more or less, one month is added and the particular year is made to consist of 13 months, and this is the so-called embolismic (intercalated) year. For the year could not consist of twelve months plus so-and-so many days, since it is said: throughout the months of the year (), which implies that we should count the year by months and not by days.\"\n\nBoth the Syrian calendar, currently used in the Arabic-speaking countries of the Fertile crescent, and the modern Assyrian calendar share many of the names for months with the Hebrew calendar, such as Nisan, Iyyar, Tammuz, Ab, Elul, Tishri and Adar, indicating a common origin. The origin is thought to be the Babylonian calendar. The modern Turkish calendar includes the names Şubat (February), Nisan (April), Temmuz (July) and Eylul (September). The former name for October was Tesrin.\n\nBiblical references to the pre-exilic calendar include ten months identified by number rather than by name. In parts of the Torah portion \"Noach\" (\"Noah\") (specifically, , , ) it is implied that the months are thirty days long. There is also an indication that there were twelve months in the annual cycle (, ). Prior to the Babylonian exile, the names of only four months are referred to in the Tanakh:\n\nAll of these are believed to be Canaanite names. These names are only mentioned in connection with the building of the First Temple. Håkan Ulfgard suggests that the use of what are rarely used Canaanite (or in the case of Ethanim perhaps Northwest-semitic) names indicates that \"the author is consciously utilizing an archaizing terminology, thus giving the impression of an ancient story...\".\n\nIn a regular (\"kesidran\") year, Marcheshvan has 29 days and Kislev has 30 days. However, because of the Rosh Hashanah postponement rules (see below) Kislev may lose a day to have 29 days, and the year is called a short (\"chaser\") year, or Marcheshvan may acquire an additional day to have 30 days, and the year is called a full (\"maleh\") year. The calendar rules have been designed to ensure that Rosh Hashanah does not fall on a Sunday, Wednesday or Friday. This is to ensure that Yom Kippur does not directly precede or follow Shabbat, which would create practical difficulties, and that Hoshana Rabbah is not on a Shabbat, in which case certain ceremonies would be lost for a year. Hebrew names and romanized transliteration may somewhat differ, as they do for Marcheshvan/Cheshvan () or Kislev (): the Hebrew words shown here are those commonly indicated, for example, in newspapers.\n\nThe solar year is about eleven days longer than twelve lunar months. The Bible does not directly mention the addition of \"embolismic\" or intercalary months. However, without the insertion of embolismic months, Jewish festivals would gradually shift outside of the seasons required by the Torah. This has been ruled as implying a requirement for the insertion of embolismic months to reconcile the lunar cycles to the seasons, which are integral to solar yearly cycles.\n\nWhen the observational form of the calendar was in use, whether or not an embolismic month was announced after the \"last month\" (Adar) depended on 'aviv [i.e., the ripeness of barley], fruits of trees, and the equinox. On two of these grounds it should be intercalated, but not on one of them alone. It may be noted that in the Bible the name of the first month, \"Aviv\", literally means \"spring\". Thus, if Adar was over and spring had not yet arrived, an additional month was observed.\n\nTraditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle forms the basis of the Christian ecclesiastical calendar and the Hebrew calendar and is used for the computation of the date of Easter each year\n\nDuring leap years Adar I (or Adar Aleph — \"first Adar\") is added before the regular Adar. Adar I is actually considered to be the extra month, and has 30 days. Adar II (or Adar Bet — \"second Adar\") is the \"real\" Adar, and has the usual 29 days. For this reason, holidays such as Purim are observed in Adar II, not Adar I.\n\nChronology was a chief consideration in the study of astronomy among the Jews; sacred time was based upon the cycles of the Sun and the Moon. The Talmud identified the twelve constellations of the zodiac with the twelve months of the Hebrew calendar. The correspondence of the constellations with their names in Hebrew and the months is as follows:\n\n\nSome scholars identified the 12 signs of the zodiac with the 12 sons of Jacob/twelve tribes of Israel.\nIt should be noted that the 12 lunar months of the Hebrew calendar are the normal months from new moon to new: the year normally contains twelve months averaging 29.52 days each. The discrepancy compared to the mean synodic month of 29.53 days is due to Adar I in a leap year always having thirty days. This means that the calendar year normally contains 354 days.\n\nThe Hebrew calendar year conventionally begins on Rosh Hashanah. However, other dates serve as the beginning of the year for different religious purposes.\n\nThere are three qualities that distinguish one year from another: whether it is a leap year or a common year, on which of four permissible days of the week the year begins, and whether it is a deficient, regular, or complete year. Mathematically, there are 24 (2×4×3) possible combinations, but only 14 of them are valid. Each of these patterns is called a \"keviyah\" (Hebrew קביעה for \"a setting\" or \"an established thing\"), and is encoded as a series of two or three Hebrew letters. See Four gates.\n\nIn Hebrew there are two common ways of writing the year number: with the thousands, called (\"major era\"), and without the thousands, called (\"minor era\").\n\nIn 1178 CE, Maimonides wrote in the \"Mishneh Torah\", \"Sanctification of the Moon\" (11.16), that he had chosen the epoch from which calculations of all dates should be as \"the third day of Nisan in this present year ... which is the year 4938 of the creation of the world\" (March 22, 1178 CE). He included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, and beginning formal usage of the \"anno mundi\" era. From the eleventh century, \"anno mundi\" dating became dominant throughout most of the world's Jewish communities. Today, the rules detailed in Maimonides' calendrical code are those generally used by Jewish communities throughout the world.\n\nSince the codification by Maimonides in 1178 CE, the Jewish calendar has used the Anno Mundi epoch (Latin for “in the year of the world,” abbreviated \"AM\" or \"A.M.;\" Hebrew ), sometimes referred to as the “Hebrew era”, to distinguish it from other systems based on some computation of creation, such as the Byzantine calendar.\n\nThere is also reference in the Talmud to years since the creation based on the calculation in the \"Seder Olam Rabbah\" of Rabbi Jose ben Halafta in about 160 CE. By his calculation, based on the Masoretic Text, Adam was created in 3760 BCE, later confirmed by the Muslim chronologist al-Biruni as 3448 years before the Seleucid era. An example is the c. 8th century Baraita of Samuel.\n\nAccording to rabbinic reckoning, the beginning of \"year 1\" is \"not\" Creation, but about one year before Creation, with the new moon of its first month (Tishrei) to be called \"molad tohu\" (the mean new moon of chaos or nothing). The Jewish calendar's epoch (reference date), 1 Tishrei AM 1, is equivalent to Monday, 7 October 3761 BC/BCE in the proleptic Julian calendar, the equivalent tabular date (same daylight period) and is about one year \"before\" the traditional Jewish date of Creation on 25 Elul AM 1, based upon the \"Seder Olam Rabbah\". Thus, adding 3760 before Rosh Hashanah or 3761 after to a Julian year number starting from 1 CE (AD 1) will yield the Hebrew year. For earlier years there may be a discrepancy (see: Missing years (Jewish calendar)).\n\nThe \"Seder Olam Rabbah\" also recognized the importance of the Jubilee and Sabbatical cycles as a long-term calendrical system, and attempted at various places to fit the Sabbatical and Jubilee years into its chronological scheme.\n\nBefore the adoption of the current AM year numbering system, other systems were in use. In early times, the years were counted from some significant historic event. (e.g., ) During the period of the monarchy, it was the widespread practice in western Asia to use era year numbers according to the accession year of the monarch of the country involved. This practice was also followed by the united kingdom of Israel (e.g., ), kingdom of Judah (e.g., ), kingdom of Israel (e.g., ), Persia (e.g., ) and others. Besides, the author of Kings coordinated dates in the two kingdoms by giving the accession year of a monarch in terms of the year of the monarch of the other kingdom, (e.g., ) though some commentators note that these dates do not always synchronise. Other era dating systems have been used at other times. For example, Jewish communities in the Babylonian diaspora counted the years from the first deportation from Israel, that of Jehoiachin in 597 BCE, (e.g., ). The era year was then called \"year of the captivity of Jehoiachin\". (e.g., )\n\nDuring the Hellenistic Maccabean period, Seleucid era counting was used, at least in the Greek-influenced area of Israel. The Books of the Maccabees used Seleucid era dating exclusively (e.g., , , , , ). Josephus writing in the Roman period also used Seleucid era dating exclusively. During the Talmudic era, from the 1st to the 10th century, the center of world Judaism was in the Middle East, primarily in the Talmudic Academies of Iraq and Palestine. Jews in these regions used Seleucid era dating (also known as the \"Era of Contracts\"). The Avodah Zarah states:\n\nRav Aha b. Jacob then put this question: How do we know that our Era [of Documents] is connected with the Kingdom of Greece at all? Why not say that it is reckoned from the Exodus from Egypt, omitting the first thousand years and giving the years of the next thousand? In that case, the document is really post-dated!<br> Said Rav Nahman: In the Diaspora the Greek Era alone is used. He [the questioner] thought that Rav Nahman wanted to dispose of him anyhow, but when he went and studied it thoroughly he found that it is indeed taught [in a Baraita]: In the Diaspora the Greek Era alone is used.\n\nThe use of the era of documents (i.e., Seleucid era) continued till the 16th century in the East, and was employed even in the 19th century among the Jews of Yemen.\n\nOccasionally in Talmudic writings, reference was made to other starting points for eras, such as destruction era dating, being the number of years since the 70 CE destruction of the Second Temple. In the 8th and 9th centuries, as the center of Jewish life moved from Babylonia to Europe, counting using the Seleucid era \"became meaningless\". There is indication that Jews of the Rhineland in the early Middle Ages used the \"years after the destruction of the Temple\" (e.g., ).\n\n and set Aviv (now Nisan) as \"the first of months\":\n\nNisan 1 is referred to as the \"ecclesiastical new year\".\n\nIn ancient Israel, the start of the ecclesiastical new year for the counting of months and festivals (i.e., Nisan) was determined by reference to Passover. Passover is on 15 Nisan, () which corresponds to the full moon of Nisan. As Passover is a spring festival, it should fall on a full moon day around, and normally just after, the vernal (northward) equinox. If the twelfth full moon after the previous Passover is too early compared to the equinox, a leap month is inserted near the end of the previous year before the new year is set to begin. According to normative Judaism, the verses in require that the months be determined by a proper court with the necessary authority to sanctify the months. Hence the court, not the astronomy, has the final decision.\n\nAccording to some Christian and Karaite sources, the tradition in ancient Israel was that 1 Nisan would not start until the barley is ripe, being the test for the onset of spring. If the barley was not ripe an intercalary month would be added before Nisan.\n\nThe day most commonly referred to as the \"New Year\" is 1 Tishrei, which actually begins in the seventh month of the ecclesiastical year. On that day the formal New Year for the counting of years (such as Shmita and Yovel), Rosh Hashanah (\"head of the year\") is observed. (see , which uses the phrase \"beginning of the year\".) This is the civil new year, and the date on which the year number advances. Certain agricultural practices are also marked from this date.\n\nIn the 1st century, Josephus stated that while –\n\nMoses...appointed Nisan...as the first month for the festivals...the commencement of the year for everything relating to divine worship, but for selling and buying and other ordinary affairs he preserved the ancient order [i. e. the year beginning with Tishrei].\"\n\nEdwin Thiele has concluded that the ancient northern Kingdom of Israel counted years using the ecclesiastical new year starting on 1 Aviv (Nisan), while the southern Kingdom of Judah counted years using the civil new year starting on 1 Tishrei. The practice of the Kingdom of Israel was also that of Babylon, as well as other countries of the region. The practice of Judah is still followed.\n\nIn fact the Jewish calendar has a multiplicity of new years for different purposes. The use of these dates has been in use for a long time. The use of multiple starting dates for a year is comparable to different starting dates for civil \"calendar years\", \"tax or fiscal years\", \"academic years\", \"religious cycles\", etc. By the time of the redaction of the \"Mishnah\", (c. 200 CE), jurists had identified four new-year dates:\n\nThe 1st of Nisan is the new year for kings and feasts; the 1st of Elul is the new year for the tithe of cattle... the 1st of Tishri is the new year for years, of the years of release and jubilee years, for the planting and for vegetables; and the 1st of Shevat is the new year for trees-so the school of Shammai; and the school of Hillel say: On the 15th thereof.\n\nThe month of Elul is the new year for counting animal tithes (\"ma'aser behemah\"). \"Tu Bishvat\" (\"the 15th of Shevat\") marks the new year for trees (and agricultural tithes).\n\nFor the dates of the Jewish New Year see Jewish and Israeli holidays 2000–2050 or calculate using the section \"Conversion between Jewish and civil calendars\".\n\nThe Jewish calendar is based on the Metonic cycle of 19 years, of which 12 are common (non-leap) years of 12 months and 7 are leap years of 13 months. To determine whether a Jewish year is a leap year, one must find its position in the 19-year Metonic cycle. This position is calculated by dividing the Jewish year number by 19 and finding the remainder. For example, the Jewish year divided by 19 results in a remainder of % 19, indicating that it is year of the Metonic cycle. Since there is no year 0, a remainder of 0 indicates that the year is year 19 of the cycle.\n\nYears 3, 6, 8, 11, 14, 17, and 19 of the Metonic cycle are leap years. To assist in remembering this sequence, some people use the mnemonic Hebrew word <nowiki>GUCHADZaT</nowiki> , where the Hebrew letters \"gimel-vav-het aleph-dalet-zayin-tet\" are used as Hebrew numerals equivalent to 3, 6, 8, 1, 4, 7, 9. The \"keviyah\" records whether the year is leap or common: פ for \"peshuta\" (פשוטה), meaning simple and indicating a common year, and מ indicating a leap year (me'uberet, מעוברת).\n\nAnother memory aid notes that intervals of the major scale follow the same pattern as do Jewish leap years, with \"do\" corresponding to year 19 (or 0): a whole step in the scale corresponds to two common years between consecutive leap years, and a half step to one common year between two leap years. This connection with the major scale is more plain in the context of 19 equal temperament: counting the tonic as 0, the notes of the major scale in 19 equal temperament are numbers 0 (or 19), 3, 6, 8, 11, 14, 17, the same numbers as the leap years in the Hebrew calendar.\n\nA simple rule for determining whether a year is a leap year has been given above. However, there is another rule which not only tells whether the year is leap but also gives the fraction of a month by which the calendar is behind the seasons, useful for agricultural purposes. To determine whether year \"n\" of the calendar is a leap year, find the remainder on dividing [(7 × \"n\") + 1] by 19. If the remainder is 6 or less it is a leap year; if it is 7 or more it is not. For example, the The This works because as there are seven leap years in nineteen years the difference between the solar and lunar years increases by 7/19 month per year. When the difference goes above 18/19 month this signifies a leap year, and the difference is reduced by one month.\n\nTo calculate the day on which Rosh Hashanah of a given year will fall, it is necessary first to calculate the expected molad (moment of lunar conjunction or new moon) of Tishrei in that year, and then to apply a set of rules to determine whether the first day of the year must be postponed. The molad can be calculated by multiplying the number of months that will have elapsed since some (preceding) molad whose weekday is known by the mean length of a (synodic) lunar month, which is 29 days, 12 hours, and 793 parts (there are 1080 \"parts\" in an hour, so that one part is equal to 3 seconds). The very first molad, the molad tohu, fell on Sunday evening at 11.11, or in Jewish terms Day 2, 5 hours, and 204 parts.\n\nIn calculating the number of months that will have passed since the known molad that one uses as the starting point, one must remember to include any leap month(s) that falls within the elapsed interval, according to the cycle of leap years. A 19-year cycle of 235 synodic months has 991 weeks 2 days 16 hours 595 parts, a common year of 12 synodic months has 50 weeks 4 days 8 hours 876 parts, while a leap year of 13 synodic months has 54 weeks 5 days 21 hours 589 parts.\n\nThe two months whose numbers of days may be adjusted, Marcheshvan and Kislev, are the eighth and ninth months of the Hebrew year, whereas Tishrei is the seventh month (in the traditional counting of the months, even though it is the first month of a new calendar year). Any adjustments needed to postpone Rosh Hashanah must be made to the adjustable months in the year that precedes the year of which the Rosh Hashanah will be the first day.\n\nJust four potential conditions are considered to determine whether the date of Rosh Hashanah must be postponed. These are called the Rosh Hashanah postponement rules, or \"deḥiyyot\":\n\n\nThe first of these rules (deḥiyyah \"molad zaken\") is referred to in the Talmud. Nowadays, molad zaken is used as a device to prevent the molad falling on the second day of the month. The second rule, (deḥiyyah \"lo ADU\"), is applied for religious reasons.\n\nAnother two rules are applied much less frequently and serve to prevent impermissible year lengths. Their names are Hebrew acronyms that refer to the ways they are calculated:\n\n\nAt the innovation of the sages, the calendar was arranged to ensure that Yom Kippur would not fall on a Friday or Sunday, and Hoshana Rabbah would not fall on Shabbat. These rules have been instituted because Shabbat restrictions also apply to Yom Kippur, so that if Yom Kippur were to fall on Friday, it would not be possible to make necessary preparations for Shabbat (such as candle lighting). Similarly, if Yom Kippur fell on a Sunday, it would not be possible to make preparations for Yom Kippur because the preceding day is Shabbat. Additionally, the laws of Shabbat override those of Hoshana Rabbah, so that if Hoshana Rabbah were to fall on Shabbat certain rituals that are a part of the Hoshana Rabbah service (such as carrying willows, which is a form of work) could not be performed.\n\nTo prevent Yom Kippur (10 Tishrei) from falling on a Friday or Sunday, Rosh Hashanah (1 Tishrei) cannot fall on Wednesday or Friday. Likewise, to prevent Hoshana Rabbah (21 Tishrei) from falling on a Saturday, Rosh Hashanah cannot fall on a Sunday. This leaves only four days on which Rosh Hashanah can fall: Monday, Tuesday, Thursday, and Saturday, which are referred to as the \"four gates\". Each day is associated with a number (its order in the week, beginning with Sunday as day 1). Numbers in Hebrew have been traditionally denominated by Hebrew letters. Thus the \"keviyah\" uses the letters ה ,ג ,ב and ז (representing 2, 3, 5, and 7, for Monday, Tuesday, Thursday, and Saturday) to denote the starting day of the year.\n\nThe postponement of the year is compensated for by adding a day to the second month or removing one from the third month. A Jewish common year can only have 353, 354, or 355 days. A leap year is always 30 days longer, and so can have 383, 384, or 385 days.\n\n\nWhether a year is deficient, regular, or complete is determined by the time between two adjacent Rosh Hashanah observances and the leap year. While the \"keviyah\" is sufficient to describe a year, a variant specifies the day of the week for the first day of Pesach (Passover) in lieu of the year length.\n\nA Metonic cycle equates to 235 lunar months in each 19-year cycle. This gives an average of 6939 days, 16 hours, and 595 parts for each cycle. But due to the Rosh Hashanah postponement rules (preceding section) a cycle of 19 Jewish years can be either 6939, 6940, 6941, or 6942 days in duration. Since none of these values is evenly divisible by seven, the Jewish calendar repeats exactly only following 36,288 Metonic cycles, or 689,472 Jewish years. There is a near-repetition every 247 years, except for an excess of about 50 minutes (905 parts).\n\nThe annual calendar of a numbered Hebrew year, displayed as 12 or 13 months partitioned into weeks, can be determined by consulting the table of Four gates, whose inputs are the year's position in the 19-year cycle and its molad Tishrei. The resulting \"keviyah\" of the desired year in the body of the table is a triple consisting of two numbers and a letter (written left-to-right in English). The left number of each triple is the day of the week of , Rosh Hashanah ; the letter indicates whether that year is deficient (D), regular (R), or complete (C), the number of days in Chesvan and Kislev; while the right number of each triple is the day of the week of , the first day of Passover or Pesach , within the same Hebrew year (next Julian/Gregorian year). The \"keviyah\" in Hebrew letters are written right-to-left, so their days of the week are reversed, the right number for and the left for . The year within the 19-year cycle alone determines whether that year has one or two Adars.\n\nThis table numbers the days of the week and hours for the limits of molad Tishrei in the Hebrew manner for calendrical calculations, that is, both begin at , thus is noon Saturday. The years of a 19-year cycle are organized into four groups: common years after a leap year but before a common year ; common years between two leap years ; common years after a common year but before a leap year ; and leap years , all between common years. The oldest surviving table of Four gates was written by Saadia Gaon (892–942). It is so named because it identifies the four allowable days of the week on which can occur.\n\nComparing the days of the week of molad Tishrei with those in the \"keviyah\" shows that during 39% of years is not postponed beyond the day of the week of its molad Tishrei, 47% are postponed one day, and 14% are postponed two days. This table also identifies the seven types of common years and seven types of leap years. Most are represented in any 19-year cycle, except one or two may be in neighboring cycles. The most likely type of year is 5R7 in 18.1% of years, whereas the least likely is 5C1 in 3.3% of years. The day of the week of is later than that of by one, two or three days for common years and three, four or five days for leap years in deficient, regular or complete years, respectively.\n\nSee Jewish and Israeli holidays 2000–2050\n\nThe Tanakh contains several commandments related to the keeping of the calendar and the lunar cycle, and records changes that have taken place to the Hebrew calendar.\n\nIt has been noted that the procedures described in the Mishnah and Tosefta are all plausible procedures for regulating an empirical lunar calendar. Fire-signals, for example, or smoke-signals, are known from the pre-exilic Lachish ostraca. Furthermore, the Mishnah contains laws that reflect the uncertainties of an empirical calendar. Mishnah Sanhedrin, for example, holds that when one witness holds that an event took place on a certain day of the month, and another that the same event took place on the following day, their testimony can be held to agree, since the length of the preceding month was uncertain. Another Mishnah takes it for granted that it cannot be known in advance whether a year's lease is for twelve or thirteen months. Hence it is a reasonable conclusion that the Mishnaic calendar was actually used in the Mishnaic period.\n\nThe accuracy of the Mishnah's claim that the Mishnaic calendar was also used in the late Second Temple period is less certain. One scholar has noted that there are no laws from Second Temple period sources that indicate any doubts about the length of a month or of a year. This led him to propose that the priests must have had some form of computed calendar or calendrical rules that allowed them to know in advance whether a month would have 30 or 29 days, and whether a year would have 12 or 13 months.\n\nBetween 70 and 1178 CE, the observation-based calendar was gradually replaced by a mathematically calculated one. Except for the epoch year number, the calendar rules reached their current form by the beginning of the 9th century, as described by the Persian Muslim astronomer al-Khwarizmi (c. 780–850 CE) in 823.\n\nOne notable difference between the calendar of that era and the modern form was the date of the epoch (the fixed reference point at the beginning of year 1), which at that time was one year later than the epoch of the modern calendar.\n\nMost of the present rules of the calendar were in place by 823, according to a treatise by al-Khwarizmi. Al-Khwarizmi's study of the Jewish calendar, \"Risāla fi istikhrāj taʾrīkh al-yahūd\" \"Extraction of the Jewish Era\" describes the 19-year intercalation cycle, the rules for determining on what day of the week the first day of the month Tishrī shall fall, the interval between the Jewish era (creation of Adam) and the Seleucid era, and the rules for determining the mean longitude of the sun and the moon using the Jewish calendar. Not all the rules were in place by 835.\n\nIn 921, Aaron ben Meïr proposed changes to the calendar. Though the proposals were rejected, they indicate that all of the rules of the modern calendar (except for the epoch) were in place before that date. In 1000, the Muslim chronologist al-Biruni described all of the modern rules of the Hebrew calendar, except that he specified three different epochs used by various Jewish communities being one, two, or three years later than the modern epoch.\n\nThere is a tradition, first mentioned by Hai Gaon (died 1038 CE), that Hillel b. R. Yehuda \"in the year 670 of the Seleucid era\" (i.e., 358–359 CE) was responsible for the new calculated calendar with a fixed intercalation cycle. Later writers, such as Nachmanides, explained Hai Gaon's words to mean that the entire computed calendar was due to Hillel b. Yehuda in response to persecution of Jews. Maimonides, in the 12th century, stated that the Mishnaic calendar was used \"until the days of Abaye and Rava\", who flourished c. 320–350 CE, and that the change came when \"the land of Israel was destroyed, and no permanent court was left.\" Taken together, these two traditions suggest that Hillel b. Yehuda (whom they identify with the mid-4th-century Jewish patriarch Ioulos, attested in a letter of the Emperor Julian, and the Jewish patriarch Ellel, mentioned by Epiphanius) instituted the computed Hebrew calendar because of persecution. H. Graetz linked the introduction of the computed calendar to a sharp repression following a failed Jewish insurrection that occurred during the rule of the Christian emperor Constantius and Gallus. A later writer, S. Lieberman, argued instead that the introduction of the fixed calendar was due to measures taken by Christian Roman authorities to prevent the Jewish patriarch from sending calendrical messengers.\n\nBoth the tradition that Hillel b. Yehuda instituted the complete computed calendar, and the theory that the computed calendar was introduced due to repression or persecution, have been questioned. Furthermore, two Jewish dates during post-Talmudic times (specifically in 506 and 776) are impossible under the rules of the modern calendar, indicating that its arithmetic rules were developed in Babylonia during the times of the Geonim (7th to 8th centuries). The Babylonian rules required the delay of the first day of Tishrei when the new moon occurred after noon.\n\nThe Talmuds do, however, indicate at least the beginnings of a transition from a purely empirical to a computed calendar. According to a statement attributed to Yose, an Amora who lived during the second half of the 3rd century, the feast of Purim, 14 Adar, could not fall on a Sabbath nor a Monday, lest 10 Tishrei (Yom Kippur) fall on a Friday or a Sunday. This indicates that, by the time of the redaction of the Jerusalem Talmud (c. 400 CE), there were a fixed number of days in all months from Adar to Elul, also implying that the extra month was already a second Adar added before the regular Adar. In another passage, a sage is reported to have counseled \"those who make the computations\" not to set the first day of Tishrei or the Day of the Willow on the sabbath. This indicates that there was a group who \"made computations\" and were in a position to control, to some extent, the day of the week on which Rosh Hashanah would fall.\n\nEarly Zionist pioneers were impressed by the fact that the calendar preserved by Jews over many centuries in far-flung diasporas, as a matter of religious ritual, was geared to the climate of their original country: the Jewish New Year marks the transition from the dry season to the rainy one, and major Jewish holidays such as Sukkot, Passover, and Shavuot correspond to major points of the country's agricultural year such as planting and harvest.\n\nAccordingly, in the early 20th century the Hebrew calendar was re-interpreted as an agricultural rather than religious calendar. The Kibbutz movement was especially inventive in creating new rituals fitting this interpretation.\n\nAfter the creation of the State of Israel, the Hebrew calendar became one of the official calendars of Israel, along with the Gregorian calendar. Holidays and commemorations not derived from previous Jewish tradition were to be fixed according to the Hebrew calendar date. For example, the Israeli Independence Day falls on 5 Iyar, Jerusalem Reunification Day on 28 Iyar, and the Holocaust Commemoration Day on 27 Nisan.\n\nNevertheless, since the 1950s usage of the Hebrew calendar has steadily declined, in favor of the Gregorian calendar. At present, Israelis—except for the religiously observant—conduct their private and public life according to the Gregorian calendar, although the Hebrew calendar is still widely acknowledged, appearing in public venues such as banks (where it is legal for use on cheques and other documents, though only rarely do people make use of this option) and on the mastheads of newspapers.\n\nThe Jewish New Year (Rosh Hashanah) is a two-day public holiday in Israel. However, since the 1980s an increasing number of secular Israelis celebrate the Gregorian New Year (usually known as \"Silvester Night\"—\"ליל סילבסטר\") on the night between 31 December and 1 January. Prominent rabbis have on several occasions sharply denounced this practice, but with no noticeable effect on the secularist celebrants.\n\nWall calendars commonly used in Israel are hybrids. Most are organised according to Gregorian rather than Jewish months, but begin in September, when the Jewish New Year usually falls, and provide the Jewish date in small characters.\n\nOutside of Rabbinic Judaism, evidence shows a diversity of practice.\n\nKaraites use the lunar month and the solar year, but the Karaite calendar differs from the current Rabbinic calendar in a number of ways. The Karaite calendar is identical to the Rabbinic calendar used before the Sanhedrin changed the Rabbinic calendar from the lunar, observation based calendar, to the current mathematically based calendar used in Rabbinic Judaism today.\n\nIn the lunar Karaite calendar, the beginning of each month, the Rosh Chodesh, can be calculated, but is confirmed by the observation in Israel of the first sightings of the new moon. This may result in an occasional variation of a maximum of one day, depending on the inability to observe the new moon. The day is usually \"picked up\" in the next month.\n\nThe addition of the leap month (Adar II) is determined by observing in Israel the ripening of barley at a specific stage (defined by Karaite tradition) (called aviv), rather than using the calculated and fixed calendar of rabbinic Judaism. Occasionally this results in Karaites being one month ahead of other Jews using the calculated rabbinic calendar. The \"lost\" month would be \"picked up\" in the next cycle when Karaites would observe a leap month while other Jews would not.\n\nFurthermore, the seasonal drift of the rabbinic calendar is avoided, resulting in the years affected by the drift starting one month earlier in the Karaite calendar.\n\nAlso, the four rules of postponement of the rabbinic calendar are not applied, since they are not mentioned in the Tanakh. This can affect the dates observed for all the Jewish holidays in a particular year by one day.\n\nIn the Middle Ages many Karaite Jews outside Israel followed the calculated rabbinic calendar, because it was not possible to retrieve accurate aviv barley data from the land of Israel. However, since the establishment of the State of Israel, and especially since the Six Day War, the Karaite Jews that have made \"aliyah\" can now again use the observational calendar.\n\nMany of the Dead Sea (Qumran) Scrolls have references to a unique calendar, used by the people there, who are often assumed to be Essenes.\n\nThe year of this calendar used the ideal Mesopotamian calendar of twelve 30-day months, to which were added 4 days at the equinoxes and solstices (cardinal points), making a total of 364 days.\n\nThere was some ambiguity as to whether the cardinal days were at the beginning of the months or at the end, but the clearest calendar attestations give a year of four seasons, each having three months of 30, 30, and 31 days with the cardinal day the extra day at the end, for a total of 91 days, or exactly 13 weeks. Each season started on the 4th day of the week (Wednesday), every year. (Ben-Dov, \"Head of All Years\", pp. 16–17)\n\nWith only 364 days, it is clear that the calendar would after a few years be very noticeably different from the actual seasons, but there is nothing to indicate what was done about this problem. Various suggestions have been made by scholars. One is that nothing was done and the calendar was allowed to change with respect to the seasons. Another suggestion is that changes were made irregularly, only when the seasonal anomaly was too great to be ignored any longer. (Ben-Dov, \"Head of All Years\", pp. 19–20)\n\nThe writings often discuss the moon, but the calendar was not based on the movement of the moon any more than indications of the phases of the moon on a modern western calendar indicate that that is a lunar calendar.\n\nCalendrical evidence for the postexilic Persian period is found in papyri from the Jewish colony at Elephantine, in Egypt. These documents show that the Jewish community of Elephantine used the Egyptian and Babylonian calendars.\n\nThe Sardica paschal table shows that the Jewish community of some eastern city, possibly Antioch, used a calendrical scheme that kept Nisan 14 within the limits of the Julian month of March. Some of the dates in the document are clearly corrupt, but they can be emended to make the sixteen years in the table consistent with a regular intercalation scheme. Peter, the bishop of Alexandria (early 4th century CE), mentions that the Jews of his city \"hold their Passover according to the course of the moon in the month of Phamenoth, or according to the intercalary month every third year in the month of Pharmuthi\", suggesting a fairly consistent intercalation scheme that kept Nisan 14 approximately between Phamenoth 10 (March 6 in the 4th century CE) and Pharmuthi 10 (April 5). Jewish funerary inscriptions from Zoar, south of the Dead Sea, dated from the 3rd to the 5th century, indicate that when years were intercalated, the intercalary month was at least sometimes a repeated month of Adar. The inscriptions, however, reveal no clear pattern of regular intercalations, nor do they indicate any consistent rule for determining the start of the lunar month.\n\nIn 1178, Maimonides included all the rules for the calculated calendar and their scriptural basis, including the modern epochal year in his work, \"Mishneh Torah\". Today, the rules detailed in Maimonides' code are those generally used by Jewish communities throughout the world.\n\nA \"new moon\" (astronomically called a lunar conjunction and, in Hebrew, a molad) is the moment at which the sun and moon are aligned horizontally with respect to a north-south line (technically, they have the same ecliptical longitude). The period between two new moons is a synodic month. The actual length of a synodic month varies from about 29 days 6 hours and 30 minutes (29.27 days) to about 29 days and 20 hours (29.83 days), a variation range of about 13 hours and 30 minutes. Accordingly, for convenience, a long-term average length, identical to the mean synodic month of ancient times (also called the molad interval) is used. The molad interval is formula_1 days, or 29 days, 12 hours, and 793 \"parts\" (1 \"part\" = / minute; 3 \"parts\" = 10 seconds) (i.e., 29.530594 days), and is the same value determined by the Babylonians in their System B about 300 BCE and was adopted by the Greek astronomer Hipparchus in the 2nd century BCE and by the Alexandrian astronomer Ptolemy in the \"Almagest\" four centuries later (who cited Hipparchus as his source). Its remarkable accuracy (less than one second from the true value) is thought to have been achieved using records of lunar eclipses from the 8th to 5th centuries BCE.\n\nThis value is as close to the correct value of 29.530589 days as it is possible for a value to come that is rounded off to whole \"parts\". The discrepancy makes the molad interval about 0.6 seconds too long. Put another way, if the molad is taken as the time of mean conjunction at some reference meridian, then this reference meridian is drifting slowly eastward. If this drift of the reference meridian is traced back to the mid-4th century, the traditional date of the introduction of the fixed calendar, then it is found to correspond to a longitude midway between the Nile and the end of the Euphrates. The modern molad moments match the mean solar times of the lunar conjunction moments near the meridian of Kandahar, Afghanistan, more than 30° east of Jerusalem.\n\nFurthermore, the discrepancy between the molad interval and the mean synodic month is accumulating at an accelerating rate, since the mean synodic month is progressively shortening due to gravitational tidal effects. Measured on a strictly uniform time scale, such as that provided by an atomic clock, the mean synodic month is becoming gradually longer, but since the tides slow Earth's rotation rate even more, the mean synodic month is becoming gradually shorter in terms of mean solar time.\n\nThe mean year of the current mathematically based Hebrew calendar is 365 days 5 hours 55 minutes and 25+/ seconds (365.2468 days) – computed as the molad/monthly interval of 29.530594 days × 235 months in a 19-year metonic cycle ÷ 19 years per cycle. In relation to the Gregorian calendar, the mean Gregorian calendar year is 365 days 5 hours 49 minutes and 12 seconds (365.2425 days), and the drift of the Hebrew calendar in relation to it is about a day every 231 years.\n\nAlthough the molad of Tishrei is the only molad moment that is not ritually announced, it is actually the only one that is relevant to the Hebrew calendar, for it determines the provisional date of Rosh Hashanah, subject to the Rosh Hashanah postponement rules. The other monthly molad moments are announced for mystical reasons. With the moladot on average almost 100 minutes late, this means that the molad of Tishrei lands one day later than it ought to in (100 minutes) ÷ (1440 minutes per day) = 5 of 72 years or nearly 7% of years.\n\nTherefore, the seemingly small drift of the moladot is already significant enough to affect the date of Rosh Hashanah, which then cascades to many other dates in the calendar year and sometimes, due to the Rosh Hashanah postponement rules, also interacts with the dates of the prior or next year. The molad drift could be corrected by using a progressively shorter molad interval that corresponds to the actual mean lunar conjunction interval at the original molad reference meridian. Furthermore, the molad interval determines the calendar mean year, so using a progressively shorter molad interval would help correct the excessive length of the Hebrew calendar mean year, as well as helping it to \"hold onto\" the northward equinox for the maximum duration.\n\nWhen the 19-year intercalary cycle was finalised in the 4th century, the earliest Passover (in year 16 of the cycle) coincided with the northward equinox, which means that Passover fell near the \"first\" full moon after the northward equinox, or that the northward equinox landed within one lunation before 16 days after the \"molad\" of \"Nisan\". This is still the case in about 80% of years; but, in about 20% of years, Passover is a month late by these criteria (as it was in AM 5765 and 5768, the 8th and 11th years of the 19-year cycle = Gregorian 2005 and 2008 CE). Presently, this occurs after the \"premature\" insertion of a leap month in years 8, 11, and 19 of each 19-year cycle, which causes the northward equinox to land on exceptionally early Hebrew dates in such years. This problem will get worse over time, and so beginning in AM 5817 (2057 CE), year 3 of each 19-year cycle will also be a month late. If the calendar is not amended, then Passover will start to land on or after the summer solstice around AM 16652 (12892 CE). (The exact year when this will begin to occur depends on uncertainties in the future tidal slowing of the Earth rotation rate, and on the accuracy of predictions of precession and Earth axial tilt.)\n\nThe seriousness of the spring equinox drift is widely discounted on the grounds that Passover will remain in the spring season for many millennia, and the text of the Torah is generally not interpreted as having specified tight calendrical limits. Of course, the Hebrew calendar also drifts with respect to the autumn equinox, and at least part of the harvest festival of Sukkot is already more than a month after the equinox in years 1, 9, and 12 of each 19-year cycle; beginning in AM 5818 (2057 CE), this will also be the case in year 4. (These are the same year numbers as were mentioned for the spring season in the previous paragraph, except that they get incremented at Rosh Hashanah.) This progressively increases the probability that Sukkot will be cold and wet, making it uncomfortable or impractical to dwell in the traditional \"succah\" during Sukkot. The first winter seasonal prayer for rain is not recited until \"Shemini Atzeret\", after the end of Sukkot, yet it is becoming increasingly likely that the rainy season in Israel will start before the end of Sukkot.\n\nNo equinox or solstice will ever be more than a day or so away from its mean date according to the solar calendar, while nineteen Jewish years average 6939d 16h 33m 03s compared to the 6939d 14h 26m 15s of nineteen mean tropical years. This discrepancy has mounted up to six days, which is why the earliest Passover currently falls on 26 March (as in AM 5773 / 2013 CE).\n\nGiven the length of the year, the length of each month is fixed as described above, so the real problem in determining the calendar for a year is determining the number of days in the year. In the modern calendar, this is determined in the following manner.\n\nThe day of Rosh Hashanah and the length of the year are determined by the time and the day of the week of the Tishrei \"molad\", that is, the moment of the average conjunction. Given the Tishrei \"molad\" of a certain year, the length of the year is determined as follows:\n\nFirst, one must determine whether each year is an ordinary or leap year by its position in the 19-year Metonic cycle. Years 3, 6, 8, 11, 14, 17, and 19 are leap years.\n\nSecondly, one must determine the number of days between the starting Tishrei \"molad\" (TM1) and the Tishrei \"molad\" of the next year (TM2). For calendar descriptions in general the day begins at 6 p.m., but for the purpose of determining Rosh Hashanah, a \"molad\" occurring on or after noon is treated as belonging to the next day (the first \"deḥiyyah\"). All months are calculated as 29d, 12h, 44m, 3s long (MonLen). Therefore, in an ordinary year TM2 occurs 12 × MonLen days after TM1. This is usually 354 calendar days after TM1, but if TM1 is on or after 3:11:20 a.m. and before noon, it will be 355 days. Similarly, in a leap year, TM2 occurs 13 × MonLen days after TM1. This is usually 384 days after TM1, but if TM1 is on or after noon and before 2:27:16 p.m., TM2 will be only 383 days after TM1. In the same way, from TM2 one calculates TM3. Thus the four natural year lengths are 354, 355, 383, and 384 days.\n\nHowever, because of the holiday rules, Rosh Hashanah cannot fall on a Sunday, Wednesday, or Friday, so if TM2 is one of those days, Rosh Hashanah in year 2 is postponed by adding one day to year 1 (the second \"deḥiyyah\"). To compensate, one day is subtracted from year 2. It is to allow for these adjustments that the system allows 385-day years (long leap) and 353-day years (short ordinary) besides the four natural year lengths.\n\nBut how can year 1 be lengthened if it is already a long ordinary year of 355 days or year 2 be shortened if it is a short leap year of 383 days? That is why the third and fourth \"deḥiyyah\"s are needed.\n\nIf year 1 is already a long ordinary year of 355 days, there will be a problem if TM1 is on a Tuesday, as that means TM2 falls on a Sunday and will have to be postponed, creating a 356-day year. In this case, Rosh Hashanah in year 1 is postponed from Tuesday (the third \"deḥiyyah\"). As it cannot be postponed to Wednesday, it is postponed to Thursday, and year 1 ends up with 354 days.\n\nOn the other hand, if year 2 is already a short year of 383 days, there will be a problem if TM2 is on a Wednesday. because Rosh Hashanah in year 2 will have to be postponed from Wednesday to Thursday and this will cause year 2 to be only 382 days long. In this case, year 2 is extended by one day by postponing Rosh Hashanah in year 3 from Monday to Tuesday (the fourth \"deḥiyyah\"), and year 2 will have 383 days.\n\nThe attribution of the fixed arithmetic Hebrew calendar solely to Hillel II has, however, been questioned by a few authors, such as Sasha Stern, who claim that the calendar rules developed gradually over several centuries.\n\nGiven the importance in Jewish ritual of establishing the accurate timing of monthly and annual times, some futurist writers and researchers have considered whether a \"corrected\" system of establishing the Hebrew date is required. The mean year of the current mathematically based Hebrew calendar has \"drifted\" an average of 7–8 days late relative to the equinox relationship that it originally had. It is not possible, however, for any individual Hebrew date to be a week or more \"late\", because Hebrew months always begin within a day or two of the \"molad\" moment. What happens instead is that the traditional Hebrew calendar \"prematurely\" inserts a leap month one year before it \"should have been\" inserted, where \"prematurely\" means that the insertion causes the spring equinox to land more than 30 days before the latest acceptable moment, thus causing the calendar to run \"one month late\" until the time when the leap month \"should have been\" inserted prior to the following spring. This presently happens in 4 years out of every 19-year cycle (years 3, 8, 11, and 19), implying that the Hebrew calendar currently runs \"one month late\" more than 21% of the time.\n\nDr. Irv Bromberg has proposed a 353-year cycle of 4366 months, which would include 130 leap months, along with use of a progressively shorter \"molad\" interval, which would keep an amended fixed arithmetic Hebrew calendar from drifting for more than seven millennia. It takes about 3 centuries for the spring equinox to drift an average of th of a \"molad\" interval earlier in the Hebrew calendar. That is a very important time unit, because it can be cancelled by simply truncating a 19-year cycle to 11 years, omitting 8 years including three leap years from the sequence. That is the essential feature of the 353-year leap cycle ().\n\nReligious questions abound about how such a system might be implemented and administered throughout the diverse aspects of the world Jewish community.\n\nThe list below gives a time which can be used to determine the day the Jewish ecclesiastical (spring) year starts over a period of nineteen years:\n\nEvery nineteen years this time is 2 days, 16 hours, 33 1/18 minutes later in the week. That is either the same or the previous day in the civil calendar, depending on whether the difference in the day of the week is three or two days. If 29 February is included fewer than five times in the nineteen - year period the date will be later by the number of days which corresponds to the difference between the actual number of insertions and five. If the year is due to start on Sunday, it actually begins on the following Tuesday if the following year is due to start on Friday morning. If due to start on Monday, Wednesday or Friday it actually begins on the following day. If due to start on Saturday, it actually begins on the following day if the previous year was due to begin on Monday morning.\n\nThe table below lists, for a Jewish year commencing on 23 March, the civil date of the first day of each month. If the year does not begin on 23 March, each month's first day will differ from the date shown by the number of days that the start of the year differs from 23 March. The correct column is the one which shows the correct starting date for the following year in the last row. If 29 February falls within a Jewish month the first day of later months will be a day earlier than shown.\n\nFor long period calculations, dates should be reduced to the Julian calendar and converted back to the civil calendar at the end of the calculation. The civil calendar used here (Exigian) is correct to one day in 44,000 years and omits the leap day in centennial years which do not give remainder 200 or 700 when divided by 900. It is identical to the Gregorian calendar between 15 October 1582 CE and 28 February 2400 CE (both dates inclusive).\n\nTo find how many days the civil calendar is ahead of the Julian in any year from 301 BCE (the calendar is proleptic [assumed] up to 1582 CE) add 300 to the year, multiply the hundreds by 7, divide by 9 and subtract 4. Ignore any fraction of a day. When the difference between the calendars changes the calculated value applies on and from March 1 (civil date) for conversions to Julian. For earlier dates reduce the calculated value by one. For conversions to the civil date the calculated value applies on and from February 29 (Julian date). Again, for earlier dates reduce the calculated value by one. The difference is applied to the calendar one is converting into. A negative value indicates that the Julian date is ahead of the civil date. In this case it is important to remember that when calculating the civil equivalent of February 29 (Julian), February 29 is discounted. Thus if the calculated value is -4 the civil equivalent of this date is February 24. Before 1 CE use astronomical years rather than years BCE. The astronomical year is (year BCE) - 1.\n\nUp to the 4th century CE, these tables give the day of the Jewish month to within a day or so and the number of the month to within a month or so. From the 4th century, the number of the month is given exactly and from the 9th century the day of the month is given exactly as well.\n\nIn the Julian calendar, every 76 years the Jewish year is due to start 5h 47 14/18m earlier, and 3d 18h 12 4/18m later in the week.\n\n\nOn what civil date does the eighth month begin in CE 20874-5?\n\n20874=2026+(248x76). In (248x76) Julian years the Jewish year is due to start (248x3d 18h 12 4/18m) later in the week, which is 932d 2h 31 2/18m or 1d 2h 31 2/18m later after removing complete weeks. Allowing for the current difference of thirteen days between the civil and Julian calendars, the Julian date is 13+(248x0d 5h 47 4/18m) earlier, which is 72d 21h 28 16/18m earlier. Convert back to the civil calendar by applying the formula.\n\nSo, in 20874 CE, the Jewish year is due to begin 87d 2h 31 2/18m later than in 2026 CE and 1d 2h 31 2/18m later in the week. In 20874 CE, therefore, the Jewish year is due to begin at 11.30 3/18 A.M. on Friday, 14 June. Because of the displacements, it actually begins on Saturday, 15 June. Odd months have 30 days and even months 29, so the starting dates are 2, 15 July; 3, 13 August; 4, 12 September; 5, 11 October; 6, 10 November; 7, 9 December, and 8, 8 January.\n\nThe rules are based on the theory that Maimonides explains in his book \"Rabbinical Astronomy\" - no allowance is made for the secular (centennial) decrease of ½ second in the length of the mean tropical year and the increase of about four yards in the distance between the earth and the moon resulting from tidal friction because astronomy was not sufficiently developed in the 12th century (when Maimonides wrote his book) to detect this.\n\n\n723–730.\n\n\n", "id": "13782", "title": "Hebrew calendar"}
{"url": "https://en.wikipedia.org/wiki?curid=13786", "text": "The Holocaust Industry\n\nThe Holocaust Industry: Reflections on the Exploitation of Jewish Suffering is a 2000 book by Norman G. Finkelstein, in which Finkelstein argues that the American Jewish establishment exploits the memory of the Nazi Holocaust for political and financial gain, as well as to further the interests of Israel. According to Finkelstein, this \"Holocaust industry\" has corrupted Jewish culture and the authentic memory of the Holocaust.\n\nFinkelstein states that his consciousness of \"the Nazi holocaust\" is rooted in his parents' experiences in the Warsaw Ghetto; with the exception of his parents themselves, \"every family member on both sides was exterminated by the Nazis\". Nonetheless, during his childhood, no one ever asked any questions about what his mother and father had suffered. He suggests, \"This was not a respectful silence. It was indifference.\" It was only after the establishment of \"the Holocaust industry\", he suggests, that outpourings of anguish over the plight of the Jews in World War II began. This ideology in turn served to endow Israel with a status as \"'victim' state\" despite its \"horrendous\" human rights record.\n\nAccording to Finkelstein, his book is \"an anatomy and an indictment of the Holocaust industry\". He argues that \"'The Holocaust' is an ideological representation of the Nazi holocaust\".\n\nIn the foreword to the first paperback edition, Finkelstein notes that the first hardback edition had been a considerable hit in several European countries and many languages, but had been largely ignored in the United States. He sees \"The New York Times\" as the main promotional vehicle of the \"Holocaust industry\", and notes that the 1999 Index listed 273 entries for the Holocaust and just 32 entries for the entire continent of Africa.\n\nThe second (2003) edition contained 100 pages of new material, primarily in chapter 3 on the World Jewish Congress lawsuit against Swiss banks. Finkelstein set out to provide a guide to the relevant sections of the case. He feels that the presiding judge elected not to docket crucial documents, and that the Claims Resolution Tribunal could no longer be trusted. Finkelstein claims the CRT was on course to vindicate the Swiss banks before it changed tack in order to \"protect the blackmailers' reputation\".\n\nThe critical response has been varied. In addition to prominent supporters, such as Noam Chomsky and Alexander Cockburn, the Holocaust historian Raul Hilberg is on record as praising Finkelstein's book: \n\nOn the other hand, many have argued that \"The Holocaust Industry\" is an unscholarly work that promotes antisemitic stereotypes. For example, according to Israeli journalist Yair Sheleg, in August 2000, German historian Hans Mommsen called it \"a most trivial book, which appeals to easily aroused anti-Semitic prejudices.\" Wolfgang Benz stated to \"Le Monde\": \"It is impossible to learn anything from Finkelstein's book. At best, it is interesting for a psychotherapist.\" The reviewer of this daily added that Norman Finkelstein \"hardly cares about nuance\" and Rony Brauman wrote in the preface to the French edition (\"L'Industrie de l'Holocauste\", Paris, La Fabrique, 2001) that some assertions of N. Finkelstein (especially on the impact of the Six-days war) are wrong, others being pieces of \"propaganda\".\n\nUniversity of Chicago Professor Peter Novick, whose work Finkelstein described as providing the \"initial stimulus\" for \"The Holocaust Industry\", asserted in the July 28, 2000 \"Jewish Chronicle\" (London) that the book is replete with \"false accusations\", \"egregious misrepresentations\", \"absurd claims\" and \"repeated mis-statements\" (\"A charge into darkness that sheds no light\"). Finkelstein replied to the allegations by Novick on his homepage.\n\nHasia Diner has accused Peter Novick and Finkelstein of being \"harsh critics of American Jewry from the left,\" and challenges the notion reflected in their books that American Jews did not begin to commemorate the Holocaust until post 1967.\n\nAndrew Ross, reviewing the book for Salon magazine, wrote:\n\nFinkelstein responded to his critics in the foreword to the second edition:\n\nFinkelstein describes two known frauds, that of \"The Painted Bird\" by Polish writer Jerzy Kosinski and \"Fragments\" by Binjamin Wilkomirski, and how they were defended by people even after they had been exposed. He identifies some of these people as members of the \"Holocaust Industry\", and notes that they also support each other. Elie Wiesel supported Kosinski; Israel Gutman and Daniel Goldhagen (see below) supported Wilkomirski; Wiesel and Gutman support Goldhagen.\n\nFinkelstein scathingly compared the media treatment of the Holocaust and the media treatment of other genocides such as the Holodomor and the Armenian Genocide, particularly by members of what he calls \"The Holocaust Industry\". 1 to 1.5 million Armenians died in the years between 1915 and 1917/1923 - denial includes the claim that they were the result of a civil war within World War I, or refusal to accept there were deaths. In 2001, Israeli Foreign Minister Shimon Peres went so far as to dismiss it as \"allegations\". However, by this time historical consensus was changing, and he was \"angrily compared ... to a holocaust denier\" by Israel Charny, executive director of the Institute on the Holocaust and Genocide in Jerusalem.\n\nAccording to Finkelstein, Elie Wiesel characterized any suggestion that he has profited from the \"Holocaust Industry\", or even any criticism at all, as Holocaust denial. Questioning a survivor's testimony, denouncing the role of Jewish collaborators, suggesting that Germans suffered during the bombing of Dresden or that any state except Germany committed crimes in World War II are all evidence of Holocaust denial – according to Deborah Lipstadt – and the most \"insidious\" forms of Holocaust denial are \"immoral equivalencies\", denying the uniqueness of The Holocaust. Finkelstein examines the implications of applying this standard to another member of the \"Holocaust Industry\", Daniel Goldhagen, who argued that Serbian actions in Kosovo \"are, in their essence, different from those of Nazi Germany only in scale\".\n\nAccording to Finkelstein, Deborah Lipstadt claims there is widespread Holocaust denial - yet in \"Denying the Holocaust\" (1993) her prime example is Arthur Butz, author of \"The Hoax of the Twentieth Century\". The chapter on him is entitled \"Entering the Mainstream\" - but Finkelstein considers that, were it not for the likes of Lipstadt, no one would ever have heard of Arthur Butz. Holocaust deniers have as much influence in the US as the Flat Earth Society (p. 69). Finkelstein believes there to be only one \"truly mainstream\" holocaust denier—Bernard Lewis, who was convicted in France of denying the Armenian genocide. Since Lewis is pro-Israel, \"this instance ... raises no hackles in the United States.\"\n\nPublishing history of \"The Holocaust Industry\":\n\n\n", "id": "13786", "title": "The Holocaust Industry"}
{"url": "https://en.wikipedia.org/wiki?curid=13787", "text": "Hermetic Order of the Golden Dawn\n\nThe Hermetic Order of the Golden Dawn (; or, more commonly, The Golden Dawn (\"Aurora Aurea\")) was an organization devoted to the study and practice of the occult, metaphysics, and paranormal activities during the late 19th and early 20th centuries. Known as a magical order, the Hermetic Order of the Golden Dawn was active in Great Britain and focused its practices on theurgy and spiritual development. Many present-day concepts of ritual and magic that are at the centre of contemporary traditions, such as Wicca and Thelema, were inspired by the Golden Dawn, which became one of the largest single influences on 20th-century Western occultism.\n\nThe three founders, William Robert Woodman, William Wynn Westcott, and Samuel Liddell MacGregor Mathers, were Freemasons and members of Societas Rosicruciana in Anglia (S.R.I.A.). Westcott appears to have been the initial driving force behind the establishment of the Golden Dawn.\n\nThe Golden Dawn system was based on hierarchy and initiation like the Masonic Lodges; however women were admitted on an equal basis with men. The \"Golden Dawn\" was the first of three Orders, although all three are often collectively referred to as the \"Golden Dawn\". The First Order taught esoteric philosophy based on the Hermetic Qabalah and personal development through study and awareness of the four Classical Elements as well as the basics of astrology, tarot divination, and geomancy. The Second or \"Inner\" Order, the \"Rosae Rubeae et Aureae Crucis\" (the Ruby Rose and Cross of Gold), taught magic, including scrying, astral travel, and alchemy. The Third Order was that of the \"Secret Chiefs\", who were said to be highly skilled; they supposedly directed the activities of the lower two orders by spirit communication with the Chiefs of the Second Order.\n\nThe foundational documents of the original Order of the Golden Dawn, known as the Cipher Manuscripts, are written in English using the Trithemius cipher. The manuscripts give the specific outlines of the Grade Rituals of the Order and prescribe a curriculum of graduated teachings that encompass the Hermetic Qabalah, astrology, occult tarot, geomancy, and alchemy.\n\nAccording to the records of the Order, the manuscripts passed from Kenneth R. H. Mackenzie, a Masonic scholar, to the Rev. A. F. A. Woodford, whom British occult writer Francis King describes as the fourth founder (although Woodford died shortly after the Order was founded). The documents did not excite Woodford, and in February 1886 he passed them on to Freemason William Wynn Westcott, who managed to decode them in 1887. Westcott, pleased with his discovery, called on fellow Freemason Samuel Liddell MacGregor Mathers for a second opinion. Westcott asked for Mathers' help to turn the manuscripts into a coherent system for lodge work. Mathers in turn asked fellow Freemason William Robert Woodman to assist the two, and he accepted. Mathers and Westcott have been credited with developing the ritual outlines in the Cipher Manuscripts into a workable format. Mathers, however, is generally credited with the design of the curriculum and rituals of the Second Order, which he called the \"Rosae Rubae et Aureae Crucis\" (\"Ruby Rose and Golden Cross\" or the \"RR et AC\").\n\nIn October 1887, Westcott claimed to have written to a German countess and prominent Rosicrucian named Anna Sprengel, whose address was said to have been found in the decoded Cipher Manuscripts. According to Westcott, Sprengel claimed the ability to contact certain supernatural entities, known as the Secret Chiefs, that were considered the authorities over any magical order or esoteric organization. Westcott purportedly received a reply from Sprengel granting permission to establish a Golden Dawn temple and conferring honorary grades of Adeptus Exemptus on Westcott, Mathers, and Woodman. The temple was to consist of the five grades outlined in the manuscripts.\n\nIn 1888, the Isis-Urania Temple was founded in London. In contrast to the S.R.I.A. and Masonry, women were allowed and welcome to participate in the Order in \"perfect equality\" with men. The Order was more of a philosophical and metaphysical teaching order in its early years. Other than certain rituals and meditations found in the Cipher manuscripts and developed further, \"magical practices\" were generally not taught at the first temple.\n\nFor the first four years, the Golden Dawn was one cohesive group later known as \"the Outer Order\" or \"First Order.\" An \"Inner Order\" was established and became active in 1892. The Inner Order consisted of members known as \"adepts,\" who had completed the entire course of study for the Outer Order. This group of adepts eventually became known as the Second Order.\n\nEventually, the Osiris temple in Weston-super-Mare, the Horus temple in Bradford (both in 1888), and the Amen-Ra temple in Edinburgh (1893) were founded. In 1893 Mathers founded the Ahathoor temple in Paris.\n\nIn 1891, Westcott's alleged correspondence with Anna Sprengel suddenly ceased. He claimed to have received word from Germany that she was either dead or that her companions did not approve of the founding of the Order and no further contact was to be made. If the founders were to contact the Secret Chiefs, apparently, it had to be done on their own. In 1892, Mathers professed that a link to the Secret Chiefs had been established. Subsequently, he supplied rituals for the Second Order, calling them the Red Rose and Cross of Gold. The rituals were based on the tradition of the tomb of Christian Rosenkreuz, and a \"Vault of Adepts\" became the controlling force behind the Outer Order. Later in 1916, Westcott claimed that Mathers also constructed these rituals from materials he received from Frater Lux ex Tenebris, a purported \"Continental Adept\".\n\nSome followers of the Golden Dawn tradition believe that the Secret Chiefs were not human or supernatural beings but, rather, symbolic representations of actual or legendary sources of spiritual esotericism. The term came to stand for a great leader or teacher of a spiritual path or practice that found its way into the teachings of the Order.\n\nBy the mid-1890s, the Golden Dawn was well established in Great Britain, with over one hundred members from every class of Victorian society. Many celebrities belonged to the Golden Dawn, such as the actress Florence Farr, the Irish revolutionary Maud Gonne, the Irish poet William Butler Yeats, the Welsh author Arthur Machen, and the English authors Evelyn Underhill and Aleister Crowley.\n\nIn 1896 or 1897, Westcott broke all ties to the Golden Dawn, leaving Mathers in control. It has been speculated that his departure was due to his having lost a number of occult-related papers in a hansom cab. Apparently, when the papers were found, Westcott's connection to the Golden Dawn was discovered and brought to the attention of his employers. He may have been told to either resign from the Order or to give up his occupation as coroner. After Westcott's departure, Mathers appointed Florence Farr to be Chief Adept in Anglia. Dr. Henry B. Pullen Burry succeeded Westcott as Cancellarius—one of the three Chiefs of the Order.\n\nMathers was the only active founding member after Westcott's departure. Due to personality clashes with other members and frequent absences from the center of Lodge activity in Great Britain, however, challenges to Mathers's authority as leader developed among the members of the Second Order.\n\nToward the end of 1899, the Adepts of the Isis-Urania and Amen-Ra temples had become dissatisfied with Mathers' leadership, as well as his growing friendship with Aleister Crowley. They had also become anxious to make contact with the Secret Chiefs themselves, instead of relying on Mathers as an intermediary. Within the Isis-Urania temple, disputes were arising between Farr's \"The Sphere\", a secret society within the Isis-Urania, and the rest of the Adepti Minores.\n\nCrowley was refused initiation into the Adeptus Minor grade by the London officials. Mathers overrode their decision and quickly initiated him at the Ahathoor temple in Paris on January 16, 1900. Upon his return to the London temple, Crowley requested from Miss Cracknell, the acting secretary, the papers acknowledging his grade, to which he was now entitled. To the London Adepts, this was the final straw. Farr, already of the opinion that the London temple should be closed, wrote to Mathers expressing her wish to resign as his representative, although she was willing to carry on until a successor was found. Mathers believed Westcott was behind this turn of events and replied on February 16. On March 3, a committee of seven Adepts was elected in London, and requested a full investigation of the matter. Mathers sent an immediate reply, declining to provide proof, refusing to acknowledge the London temple, and dismissing Farr as his representative on March 23. In response, a general meeting was called on March 29 in London to remove Mathers as chief and expel him from the Order.\n\nIn 1901, W. B. Yeats privately published a pamphlet titled \"Is the Order of R. R. & A. C. to Remain a Magical Order?\"\nAfter the Isis-Urania temple claimed its independence, there were even more disputes, leading to Yeats resigning. A committee of three was to temporarily govern, which included P.W. Bullock, M.W. Blackden and J. W. Brodie-Innes. After a short time, Bullock resigned, and Dr. Robert Felkin took his place.\n\nIn 1903, A. E. Waite and Blackden joined forces to retain the name Isis-Urania, while Felkin and other London members formed the Stella Matutina. Yeats remained in the Stella Matutina until 1921, while Brodie-Innes continued his Amen-Ra membership in Edinburgh.\n\nOnce Mathers realised that reconciliation was impossible, he made efforts to reestablish himself in London. The Bradford and Weston-super-Mare temples remained loyal to him, but their numbers were few. He then appointed Edward Berridge as his representative. According to Francis King, historical evidence shows that there were \"twenty three members of a flourishing Second Order under Berridge-Mathers in 1913.\"\n\nJ.W. Brodie-Innes continued leading the Amen-Ra temple, deciding that the revolt was unjustified. By 1908, Mathers and Brodie-Innes were in complete accord. According to sources that differ regarding the actual date, sometime between 1901 and 1913 Mathers renamed the branch of the Golden Dawn remaining loyal to his leadership to Alpha et Omega. Brodie-Innes assumed command of the English and Scottish temples, while Mathers concentrated on building up his Ahathoor temple and extending his American connections. According to occultist Israel Regardie, the Golden Dawn had spread to the United States of America before 1900 and a Thoth-Hermes temple had been founded in Chicago. By the beginning of the First World War in 1914, Mathers had established two to three American temples.\n\nMost temples of the Alpha et Omega and Stella Matutina closed or went into abeyance by the end of the 1930s, with the exceptions of two Stella Matutina temples: Hermes Temple in Bristol, which operated sporadically until 1970, and the Smaragdum Thallasses Temple (commonly referred to as Whare Ra) in Havelock North, New Zealand, which operated regularly until its closure in 1978.\n\nMuch of the hierarchical structure for the Golden dawn came from the Societas Rosicruciana in Anglia, which was itself derived from the Order of the Golden and Rosy Cross.\n\n\n\n\nThe paired numbers attached to the Grades relate to positions on the Tree of Life. The Neophyte Grade of \"0=0\" indicates no position on the Tree. In the other pairs, the first numeral is the number of steps up from the bottom (Malkuth), and the second numeral is the number of steps down from the top (Kether).\n\nThe First Order Grades were related to the four elements of Earth, Air, Water, and Fire, respectively. The Aspirant to a Grade received instruction on the metaphysical meaning of each of these Elements and had to pass a written examination and demonstrate certain skills to receive admission to that Grade.\n\nThe Portal Grade was an \"Invisible\" or in-between grade separating the First Order from the Second Order. The Circle of existing Adepts from the Second Order had to consent to allow an Aspirant to be initiated as an Adept and join the Second Order.\n\nThe Second Order was not, properly, part of the \"Golden Dawn\", but a separate Order in its own right, known as the R.R. et A.C. The Second Order directed the teachings of the First Order and was the governing force behind the First Order.\n\nAfter passing the Portal, the Aspirant was instructed in the techniques of practical magic. When another examination was passed, and the other Adepts consented, the Aspirant attained the Grade of Adeptus Minor (5=6). There were also four sub-Grades of instruction for the Adeptus Minor, again relating to the four Outer Order grades.\n\nA member of the Second Order had the power and authority to initiate aspirants to the First Order, though usually not without the permission of the Chiefs of his or her Lodge.\n\n\"The Golden Dawn\", by Israel Regardie; was published in 1937. The book is divided into several basic sections. First are the knowledge lectures, which describe the basic teaching of the Kabalah, symbolism, meditation, geomancy, etc. This is followed by the rituals of the Outer Order, consisting of five initiation rituals into the degrees of the Golden Dawn. The next section covers the rituals of the Inner Order including two initiation rituals and equinox ceremonies.\n\n\nWhile no temples in the original chartered lineage of the Golden Dawn survived past the 1970s, several organizations have since revived its teachings and rituals. Among these, the following are notable:\n\n\n\n", "id": "13787", "title": "Hermetic Order of the Golden Dawn"}
{"url": "https://en.wikipedia.org/wiki?curid=13790", "text": "Hash function\n\nA hash function is any function that can be used to map data of arbitrary size to data of fixed size. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. \nOne use is a data structure called a hash table, widely used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file. An example is finding similar stretches in DNA sequences. They are also useful in cryptography. A cryptographic hash function allows one to easily verify that some input data maps to a given hash value, but if the input data is unknown, it is deliberately difficult to reconstruct it (or equivalent alternatives) by knowing the stored hash value. This is used for assuring integrity of transmitted data, and is the building block for HMACs, which provide message authentication.\n\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalogue of file fingerprints than of hash values.\n\nHash functions are primarily used in hash tables, to quickly locate a data record (e.g., a dictionary definition) given its search key (the headword). Specifically, the hash function is used to map the search key to an index; the index gives the place in the hash table where the corresponding record should be stored. Hash tables, in turn, are used to implement associative arrays and dynamic sets.\n\nTypically, the domain of a hash function (the set of possible keys) is larger than its range (the number of different table indices), and so it will map several different keys to the same index. Therefore, each slot of a hash table is associated with (implicitly or explicitly) a set of records, rather than a single record. For this reason, each slot of a hash table is often called a \"bucket\", and hash values are also called \"bucket indices\".\n\nThus, the hash function only hints at the record's location — it tells where one should start looking for it. Still, in a half-full table, a good hash function will typically narrow the search down to only one or two entries.\n\nHash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table, since any collision can be resolved by discarding or writing back the older of the two colliding items. This is also used in file comparison.\n\nHash functions are an essential ingredient of the Bloom filter, a space-efficient probabilistic data structure that is used to test whether an element is a member of a set.\n\nWhen storing records in a large unsorted file, one may use a hash function to map each record to an index into a table \"T\", and to collect in each bucket \"T\"[\"i\"] a list of the numbers of all records with the same hash value \"i\". Once the table is complete, any two duplicate records will end up in the same bucket. The duplicates can then be found by scanning every bucket \"T\"[\"i\"] which contains two or more members, fetching those records, and comparing them. With a table of appropriate size, this method is likely to be much faster than any alternative approach (such as sorting the file and comparing all consecutive pairs).\n\nA hash value can be used to uniquely identify secret information. This requires that the hash function is collision-resistant, which means that it is very hard to find data that will generate the same hash value. These functions are categorized into cryptographic hash functions and provably secure hash functions. Functions in the second category are the most secure but also too slow for most practical purposes. Collision resistance is accomplished in part by generating very large hash values. For example, SHA-1, one of the most widely used cryptographic hash functions, generates 160 bit values.\n\nHash functions can also be used to locate table records whose key is similar, but not identical, to a given key; or pairs of records in a large file which have similar keys. For that purpose, one needs a hash function that maps similar keys to hash values that differ by at most \"m\", where \"m\" is a small integer (say, 1 or 2). If one builds a table \"T\" of all record numbers, using such a hash function, then similar records will end up in the same bucket, or in nearby buckets. Then one need only check the records in each bucket \"T\"[\"i\"] against those in buckets \"T\"[\"i\"+\"k\"] where \"k\" ranges between −\"m\" and \"m\".\n\nThis class includes the so-called acoustic fingerprint algorithms, that are used to locate similar-sounding entries in large collection of audio files. For this application, the hash function must be as insensitive as possible to data capture or transmission errors, and to trivial changes such as timing and volume changes, compression, etc.\n\nThe same techniques can be used to find equal or similar stretches in a large collection of strings, such as a document repository or a genomic database. In this case, the input strings are broken into many small pieces, and a hash function is used to detect potentially equal pieces, as above.\n\nThe Rabin–Karp algorithm is a relatively fast string searching algorithm that works in O(\"n\") time on average. It is based on the use of hashing to compare strings.\n\nThis principle is widely used in computer graphics, computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space, such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database, and so on. In these applications, the set of all inputs is some sort of metric space, and the hashing function can be interpreted as a partition of that space into a grid of \"cells\". The table is often an array with two or more indices (called a \"grid file\", \"grid index\", \"bucket grid\", and similar names), and the hash function returns an index tuple. This special case of hashing is known as geometric hashing or \"the grid method\". Geometric hashing is also used in telecommunications (usually under the name vector quantization) to encode and compress multi-dimensional signals.\n\nSome standard applications that employ hash functions include authentication, message integrity (using an HMAC (Hashed MAC)), message fingerprinting, data corruption detection, and digital signature efficiency.\n\nGood hash functions, in the original sense of the term, are usually required to satisfy certain properties listed below. The exact requirements are dependent on the application, for example a hash function well suited to indexing data will probably be a poor choice for a cryptographic hash function.\n\nA hash procedure must be deterministic—meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection), although sometimes rehashing of the item is possible.\n\nThe determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash is still a valid hash function when used in within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ.\n\nA good hash function should map the expected inputs as evenly as possible over its output range. That is, every hash value in the output range should be generated with roughly the same probability. The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of \"collisions\"—pairs of inputs that are mapped to the same hash value—increases. If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries.\n\nNote that this criterion only requires the value to be \"uniformly distributed\", not \"random\" in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function, but the converse need not be true.\n\nHash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries.\n\nIn other words, if a typical set of \"m\" records is hashed to \"n\" table slots, the probability of a bucket receiving many more than \"m\"/\"n\" records should be vanishingly small. In particular, if \"m\" is less than \"n\", very few buckets should have more than one or two records. (In an ideal \"perfect hash function\", no bucket should have more than one record; but a small number of collisions is virtually inevitable, even if \"n\" is much larger than \"m\" – see the birthday paradox).\n\nWhen testing a hash function, the uniformity of the distribution of hash values can be evaluated by the chi-squared test.\n\nIt is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. On the other hand, cryptographic hash functions produce much larger hash values, in order to ensure the computational complexity of brute-force inversion. For example, SHA-1, one of the most widely used cryptographic hash functions, produces a 160-bit value.\n\nProducing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression which iteratively processes chunks of the input (such as the characters in a string) to produce the hash value. In cryptographic hash functions, these chunks are processed by a one-way compression function, with the last chunk being padded if necessary. In this case, their size, which is called \"block size\", is much bigger than the size of the hash value. For example, in SHA-1, the hash value is 160 bits and the block size 512 bits.\n\nIn many applications, the range of hash values may be different for each run of the program, or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters—the input data \"z\", and the number \"n\" of allowed hash values.\n\nA common solution is to compute a fixed hash function with a very large range (say, 0 to 2 − 1), divide the result by \"n\", and use the division's remainder. If \"n\" is itself a power of 2, this can be done by bit masking and bit shifting. When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and \"n\" − 1, for any value of \"n\" that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of \"n\", e.g. odd or prime numbers.\n\nWe can allow the table size \"n\" to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let \"n\" be significantly less than 2. Consider a pseudorandom number generator (PRNG) function \"P\"(key) that is uniform on the interval [0, 2 − 1]. A hash function uniform on the interval [0, n-1] is \"n\" \"P\"(key)/2. We can replace the division by a (possibly faster) right bit shift: \"nP\"(key) » \"b\".\n\nWhen the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table.\n\nA hash function that will relocate the minimum number of records when the table is – where \"z\" is the key being hashed and \"n\" is the number of allowed hash values – such that \"H\"(\"z\",\"n\" + 1) = \"H\"(\"z\",\"n\") with probability close to \"n\"/(\"n\" + 1).\n\nLinear hashing and spiral storage are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property.\n\nExtendible hashing uses a dynamic hash function that requires space proportional to \"n\" to compute the hash function, and it becomes a function of the previous keys that have been inserted.\n\nSeveral algorithms that preserve the uniformity property but require time proportional to \"n\" to compute the value of \"H\"(\"z\",\"n\") have been invented.\n\nIn some applications, the input data may contain features that are irrelevant for comparison purposes. For example, when looking up a personal name, it may be desirable to ignore the distinction between upper and lower case letters. For such data, one must use a hash function that is compatible with the data equivalence criterion being used: that is, any two inputs that are considered equivalent must yield the same hash value. This can be accomplished by normalizing the input before hashing it, as by upper-casing all letters.\n\n\"A hash function that is used to search for similar (as opposed to equivalent) data must be as continuous as possible; two inputs that differ by a little should be mapped to equal or nearly equal hash values.\"\n\nNote that continuity is usually considered a fatal flaw for checksums, cryptographic hash functions, and other related concepts. Continuity is desirable for hash functions only in some applications, such as hash tables used in Nearest neighbor search.\n\nIn cryptographic applications, hash functions are typically expected to be practically non-invertible, meaning that it is not realistic to reconstruct the input datum from its hash value () alone without spending great amounts of computing time (see also One-way function).\n\nFor most types of hashing functions, the choice of the function depends strongly on the nature of the input data, and their probability distribution in the intended application.\n\nIf the data to be hashed is small enough, one can use the data itself (reinterpreted as an integer) as the hashed value. The cost of computing this \"trivial\" (identity) hash function is effectively zero. This hash function is perfect, as it maps each input to a distinct hash value.\n\nThe meaning of \"small enough\" depends on the size of the type that is used as the hashed value. For example, in Java, the hash code is a 32-bit integer. Thus the 32-bit integer codice_1 and 32-bit floating-point codice_2 objects can simply use the value directly; whereas the 64-bit integer codice_3 and 64-bit floating-point codice_4 cannot use this method.\n\nOther types of data can also use this perfect hashing scheme. For example, when mapping character strings between upper and lower case, one can use the binary encoding of each character, interpreted as an integer, to index a table that gives the alternative form of that character (\"A\" for \"a\", \"8\" for \"8\", etc.). If each character is stored in 8 bits (as in extended ASCII or ISO Latin 1), the table has only 2 = 256 entries; in the case of Unicode characters, the table would have 17×2 = entries.\n\nThe same technique can be used to map two-letter country codes like \"us\" or \"za\" to country names (26 = 676 table entries), 5-digit zip codes like 13083 to city names ( entries), etc. Invalid data values (such as the country code \"xx\" or the zip code 00000) may be left undefined in the table or mapped to some appropriate \"null\" value.\n\nA hash function that is injective—that is, maps each valid input to a different hash value—is said to be perfect. With such a function one can directly locate the desired entry in a hash table, without any additional searching.\n\nA perfect hash function for \"n\" keys is said to be minimal if its range consists of \"n\" \"consecutive\" integers, usually from 0 to \"n\"−1. Besides providing single-step lookup, a minimal perfect hash function also yields a compact hash table, without any vacant slots. Minimal perfect hash functions are much harder to find than perfect ones with a wider range.\n\nIf the inputs are bounded-length strings and each input may independently occur with uniform probability (such as telephone numbers, car license plates, invoice numbers, etc.), then a hash function needs to map roughly the same number of inputs to each hash value. For instance, suppose that each input is an integer \"z\" in the range 0 to \"N\"−1, and the output must be an integer \"h\" in the range 0 to \"n\"−1, where \"N\" is much larger than \"n\". Then the hash function could be \"h\" = \"z\" mod \"n\" (the remainder of \"z\" divided by \"n\"), or \"h\" = (\"z\" × \"n\") ÷ \"N\" (the value \"z\" scaled down by \"n\"/\"N\" and truncated to an integer), or many other formulas.\n\nThese simple formulas will not do if the input values are not equally likely, or are not independent. For instance, most patrons of a supermarket will live in the same geographic area, so their telephone numbers are likely to begin with the same 3 to 4 digits. In that case, if \"m\" is 10000 or so, the division formula (\"z\" × \"m\") ÷ \"M\", which depends mainly on the leading digits, will generate a lot of collisions; whereas the remainder formula \"z\" mod \"m\", which is quite sensitive to the trailing digits, may still yield a fairly even distribution.\n\nWhen the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, very characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.\n\nIn cryptographic hash functions, a Merkle–Damgård construction is usually used. In general, the scheme for hashing such data is to break the input into a sequence of small units (bits, bytes, words, etc.) and combine all the units \"b\"[1], \"b\"[2], …, \"b\"[\"m\"] sequentially, as follows\n\nThis schema is also used in many text checksum and fingerprint algorithms. The state variable \"S\" may be a 32- or 64-bit unsigned integer; in that case, \"S0\" can be 0, and \"G\"(\"S\",\"n\") can be just \"S\" mod \"n\". The best choice of \"F\" is a complex issue and depends on the nature of the data. If the units \"b\"[\"k\"] are single bits, then \"F\"(\"S\",\"b\") could be, for instance\n\nHere \"highbit\"(\"S\") denotes the most significant bit of \"S\"; the '*' operator denotes unsigned integer multiplication with lost overflow; '^' is the bitwise exclusive or operation applied to words; and \"P\" is a suitable fixed word.\n\nIn many cases, one can design a special-purpose (heuristic) hash function that yields many fewer collisions than a good general-purpose hash function. For example, suppose that the input data are file names such as FILE0000.CHK, FILE0001.CHK, FILE0002.CHK, etc., with mostly sequential numbers. For such data, a function that extracts the numeric part \"k\" of the file name and returns \"k\" mod \"n\" would be nearly optimal. Needless to say, a function that is exceptionally good for a specific kind of data may have dismal performance on data with different distribution.\n\nIn some applications, such as substring search, one must compute a hash function \"h\" for every \"k\"-character substring of a given \"n\"-character string \"t\"; where \"k\" is a fixed integer, and \"n\" is \"k\". The straightforward solution, which is to extract every such substring \"s\" of \"t\" and compute \"h\"(\"s\") separately, requires a number of operations proportional to \"k\"·\"n\". However, with the proper choice of \"h\", one can use the technique of rolling hash to compute all those hashes with an effort proportional to \"k\" + \"n\".\n\nA universal hashing scheme is a randomized algorithm that selects a hashing function \"h\" among a family of such functions, in such a way that the probability of a collision of any two distinct keys is 1/\"n\", where \"n\" is the number of distinct hash values desired—independently of the two keys. Universal hashing ensures (in a probabilistic sense) that the hash function application will behave as well as if it were using a random function, for any distribution of the input data. It will, however, have more collisions than perfect hashing and may require more operations than a special-purpose hash function. See also unique permutation hashing.\n\nOne can adapt certain checksum or fingerprinting algorithms for use as hash functions. Some of those algorithms will map arbitrary long string data \"z\", with any typical real-world distribution—no matter how non-uniform and dependent—to a 32-bit or 64-bit string, from which one can extract a hash value in 0 through \"n\" − 1.\n\nThis method may produce a sufficiently uniform distribution of hash values, as long as the hash range size \"n\" is small compared to the range of the checksum or fingerprint function. However, some checksums fare poorly in the avalanche test, which may be a concern in some applications. In particular, the popular CRC32 checksum provides only 16 bits (the higher half of the result) that are usable for hashing. Moreover, each bit of the input has a deterministic effect on each bit of the CRC32, that is one can tell without looking at the rest of the input, which bits of the output will flip if the input bit is flipped; so care must be taken to use all 32 bits when computing the hash from the checksum.\n\nMultiplicative hashing is a simple type of hash function often used by teachers introducing students to hash tables. Multiplicative hash functions are simple and fast, but have higher collision rates in hash tables than more sophisticated hash functions.\n\nIn many applications, such as hash tables, collisions make the system a little slower but are otherwise harmless.\nIn such systems, it is often better to use hash functions based on multiplication—such as MurmurHash and the SBoxHash—or even simpler hash functions such as CRC32—and tolerate more collisions; rather than use a more complex hash function that avoids many of those collisions but takes longer to compute. Multiplicative hashing is susceptible to a \"common mistake\" that leads to poor diffusion—higher-value input bits do not affect lower-value output bits.\n\nSome cryptographic hash functions, such as SHA-1, have even stronger uniformity guarantees than checksums or fingerprints, and thus can provide very good general-purpose hashing functions.\n\nIn ordinary applications, this advantage may be too small to offset their much higher cost. However, this method can provide uniformly distributed hashes even when the keys are chosen by a malicious agent. This feature may help to protect services against denial of service attacks.\n\nTables of random numbers (such as 256 random 32-bit integers) can provide high-quality nonlinear functions to be used as hash functions or for other purposes such as cryptography. The key to be hashed is split into 8-bit (one-byte) parts, and each part is used as an index for the nonlinear table. The table values are then added by arithmetic or XOR addition to the hash output value. Because the table is just 1024 bytes in size, it fits into the cache of modern microprocessors and allow very fast execution of the hashing algorithm. As the table value is on average much longer than 8 bits, one bit of input affects nearly all output bits.\n\nThis algorithm has proven to be very fast and of high quality for hashing purposes (especially hashing of integer-number keys).\n\nModern microprocessors will allow for much faster processing, if 8-bit character strings are not hashed by processing one character at a time, but by interpreting the string as an array of 32 bit or 64 bit integers and hashing/accumulating these \"wide word\" integer values by means of arithmetic operations (e.g. multiplication by constant and bit-shifting). The remaining characters of the string which are smaller than the word length of the CPU must be handled differently (e.g. being processed one character at a time).\n\nThis approach has proven to speed up hash code generation by a factor of five or more on modern microprocessors of\na word size of 64 bit.\n\nAnother approach is to convert strings to a 32 or 64 bit numeric value and then apply a hash function. One method that avoids the problem of strings having great similarity (\"Aaaaaaaaaa\" and \"Aaaaaaaaab\") is to use a Cyclic redundancy check (CRC) of the string to compute a 32- or 64-bit value. While it is possible that two different strings will have the same CRC, the likelihood is very small and only requires that one check the actual string found to determine whether one has an exact match. CRCs will be different for strings such as \"Aaaaaaaaaa\" and \"Aaaaaaaaab\". Although, CRC codes can be used as hash values they are not cryptographically secure since they are not collision-resistant.\n\nLocality-sensitive hashing (LSH) is a method of performing probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items). This is different from the conventional hash functions, such as those used in cryptography, as in this case the goal is to maximize the probability of \"collision\" of similar items rather than to avoid collisions.\n\nOne example of LSH is MinHash algorithm used for finding similar documents (such as web-pages):\n\nLet \"h\" be a hash function that maps the members of and to distinct integers, and for any set \"S\" define to be the member of with the minimum value of . Then exactly when the minimum hash value of the union lies in the intersection .\nTherefore,\nIn other words, if is a random variable that is one when and zero otherwise, then is an unbiased estimator of , although it has too high a variance to be useful on its own. The idea of the MinHash scheme is to reduce the variance by averaging together several variables constructed in the same way.\n\nThe term \"hash\" offers a natural analogy with its non-technical meaning (to \"chop\" or \"make a mess\" out of something), given how hash functions scramble their input data to derive their output. In his research for the precise origin of the term, Donald Knuth notes that, while Hans Peter Luhn of IBM appears to have been the first to use the concept of a hash function in a memo dated January 1953, the term itself would only appear in published literature in the late 1960s, on Herbert Hellerman's \"Digital Computer System Principles\", even though it was already widespread jargon by then.\n\n", "id": "13790", "title": "Hash function"}
{"url": "https://en.wikipedia.org/wiki?curid=13791", "text": "High jump\n\nThe high jump is a track and field event in which competitors must jump unaided over a horizontal bar placed at measured heights without dislodging it. In its modern most practised format, a bar is placed between two standards with a crash mat for landing. In the modern era, athletes run towards the bar and use the Fosbury Flop method of jumping, leaping head first with their back to the bar. Performed since ancient times, competitors have introduced increasingly more effective techniques to arrive at the current form.\n\nThe discipline is, alongside the pole vault, one of two vertical clearance events to feature on the Olympic athletics programme. It is contested at the World Championships in Athletics and IAAF World Indoor Championships, and is a common occurrence at track and field meetings. The high jump was among the first events deemed acceptable for women, having been held at the 1928 Olympic Games.\n\nJavier Sotomayor (Cuba) is the current men's record holder with a jump of set in 1993 – the longest standing record in the history of the men's high jump. Stefka Kostadinova (Bulgaria) has held the women's world record at since 1987, also the longest-held record in the event.\nThe rules for the high jump are set internationally by the International Association of Athletics Federations (IAAF). Jumpers must take off on one foot. A jump is considered a failure if the bar is dislodged by the action of the jumper whilst jumping or the jumper touches the ground or breaks the plane of the near edge of the bar before clearance. The technique one uses for the jump must be almost flawless in order to have a chance of clearing a high bar.\n\nCompetitors may begin jumping at any height announced by the chief judge, or may pass, at their own discretion. Three consecutive missed jumps, at any height or combination of heights, will eliminate the jumper from competition.\n\nThe victory goes to the jumper who clears the greatest height during the final. If two or more jumpers tie for first place, the tie-breakers are: 1) The fewest misses at the height at which the tie occurred; and 2) The fewest misses throughout the competition.\n\nIf the event remains tied for first place (or a limited advancement position to a subsequent meet), the jumpers have a jump-off, beginning at the next greater height. Each jumper has one attempt. The bar is then alternately lowered and raised until only one jumper succeeds at a given height.\n\nThe first recorded high jump event took place in Scotland in the 19th century. Early jumpers used either an elaborate straight-on approach or a scissors technique. In the later years, the bar was approached diagonally, and the jumper threw first the inside leg and then the other over the bar in a scissoring motion. Around the turn of the 20th century, techniques began to modernise, starting with the Irish-American Michael Sweeney's \"Eastern cut-off\". By taking off like the scissors, but extending his back and flattening out over the bar, Sweeney achieved a more economic clearance and raised the world record to in 1895.\n\nAnother American, George Horine, developed an even more efficient technique, the \"Western roll\". In this style, the bar again is approached on a diagonal, but the inner leg is used for the take-off, while the outer leg is thrust up to lead the body sideways over the bar. Horine increased the world standard to in 1912. His technique was predominant through the Berlin Olympics of 1936, in which the event was won by Cornelius Johnson at .\n\nAmerican and Soviet jumpers held the playing field for the next four decades, and they pioneered the evolution of the straddle technique. Straddle jumpers took off as in the Western roll, but rotated their (belly-down) torso around the bar, obtaining the most economical clearance up to that time. Straddle-jumper Charles Dumas was the first to clear 7 feet (2.13 m), in 1956, and American John Thomas pushed the world mark to in 1960. Valeriy Brumel took over the event for the next four years. The elegant Soviet jumper radically sped up his approach run, took the record up to , and won the Olympic gold medal in 1964, before a motorcycle accident ended his career.\n\nAmerican coaches, including two-time NCAA champion Frank Costello of the University of Maryland, flocked to Russia to learn from Brumel and his coaches. However, it would be a solitary innovator at Oregon State University, Dick Fosbury, who would bring the high jump into the next century. Taking advantage of the raised, softer landing areas by then in use, Fosbury added a new twist to the outmoded Eastern Cut-off. He directed himself over the bar head and shoulders first, sliding over on his back and landing in a fashion which would likely have broken his neck in the old, sawdust landing pits. After he used this Fosbury flop to win the 1968 Olympic gold medal, the technique began to spread around the world, and soon \"floppers\" were dominating international high jump competitions. The last straddler to set a world record was Vladimir Yashchenko, who cleared in 1977 and then indoors in 1978.\n\nAmong renowned high jumpers following Fosbury's lead were Americans Dwight Stones and his rival, tall Franklin Jacobs of Paterson, NJ, who cleared , over his head (a feat equaled 27 years later by Sweden's Stefan Holm); Chinese record-setters Ni-chi Chin and Zhu Jianhua; Germans Gerd Wessig and Dietmar Mögenburg; Swedish Olympic medalist and former world record holder Patrik Sjöberg; and female jumpers Iolanda Balaş of Romania, Ulrike Meyfarth of Germany and Italy's Sara Simeoni.\n\nThe approach of the high jump may actually be more important than the take-off. If a high jumper runs with bad timing or without enough aggression, clearing a high bar becomes more of a challenge. The approach requires a certain shape or curve, the right amount of speed, and the correct number of strides. The approach angle is also critical for optimal height.\n\nMost great straddle jumpers have a run at angles of about 30 to 40 degrees. The length of the run is determined by the speed of the person's approach. A slower run requires about 8 strides. However, a faster high jumper might need about 13 strides. A greater run speed allows a greater part of the body's forward momentum to be converted upward .\n\nThe J type approach, favored by Fosbury floppers, allows for horizontal speed, the ability to turn in the air (centripetal force), and good take-off position. The approach should be a hard controlled stride so that a person does not fall from creating an angle with speed. Athletes should run tall and lean on the curve, from the ankles and not the hips.\n\nUnlike the classic straddle technique, where the take-off foot is \"planted\" in the same spot at every height, flop-style jumpers must adjust their take-off as the bar is raised. Their J approach run must be adjusted slightly so that their take-off spot is slightly further out from the bar in order to allow their hips to clear the bar while still maintaining enough momentum to carry their legs across the bar. Jumpers attempting to reach record heights commonly fail when most of their energy is directed into the vertical effort, and they brush the bar off the standards with the backs of their legs as they stall out in mid-air.\n\nAn effective approach shape can be derived from physics. For example, the rate of backward spin required as the jumper crosses the bar to facilitate shoulder clearance on the way up and foot clearance on the way down can be determined by computer simulation. This rotation rate can be back-calculated to determine the required angle of lean away from the bar at plant, based on how long the jumper is on the take-off foot. This information, together with the jumper's speed in the curve, can be used to calculate the radius of the curved part of the approach. This is a lot of work and requires measurements of running speed and time of take-off foot on the ground. However, one can work in the opposite direction by assuming an approach radius and watching the resulting backward rotation. This only works if some basic rules are followed in how one executes the approach and take-off. \nDrills can be practiced to solidify the approach. One drill is to run in a straight line (the linear part of the approach) and then run two to three circles spiraling into one another. Another is to run or skip a circle of any size, two to three times in a row. It is important to train to leap upwards without first leaning into the bar, allowing the momentum of the J approach to carry the body across the bar.\n\nIn competition the winner is the person who cleared the highest height. In case of a tie, fewer failed attempts at that height are better: \"i.e.\", the jumper who makes a height on his or her first attempt is placed ahead of someone who clears the same height on the second or third attempt. If there still is a tie, all the failed attempts at lower heights are added up, and the one with the fewest total misses is declared the winner. If still tied, a playoff is held. Starting height is the next higher height after the overjumped one. If all the competitors clear the height, the bar is raised , and if they fail, the bar is lowered 2 cm. That continues until only one competitor succeeds in overjumping that height, and he or she is declared the winner.\n\n\n\n\nAthletes who have won multiple titles at the two most important competitions, the Olympic Games and the World Championships:\n\n\nKostadinova and Sotomayor are the only high jumpers to have been Olympic Champion, World Champion and broken the world record.\n\nAs of June 5, 2015\n\n\nAll time lists of athletes with the highest recorded jumps above their own height.\n\n, 65 different female athletes had ever been able to jump . The following table shows the only ten countries from which more than one athlete has cleared that mark. Blanka Vlasic of Croatia is the single athlete from that country who has cleared 2.08, in Zagreb placing her equal with Sweden's Kajsa Bergqvist in second place.\n\n\n\n", "id": "13791", "title": "High jump"}
{"url": "https://en.wikipedia.org/wiki?curid=13792", "text": "Heraclitus\n\nHeraclitus of Ephesus (; , ; c. 535 – c. 475 BC) was a pre-Socratic Greek philosopher, and a native of the city of Ephesus, then part of the Persian Empire. He was of distinguished parentage. Little is known about his early life and education, but he regarded himself as self-taught and a pioneer of wisdom. From the lonely life he led, and still more from the apparently riddled and allegedly paradoxical nature of his philosophy and his stress upon the needless unconsciousness of humankind, he was called \"The Obscure\" and the \"Weeping Philosopher\".\n\nHeraclitus was famous for his insistence on ever-present change as being the fundamental essence of the universe, as stated in the famous saying, \"No man ever steps in the same river twice\" (see panta rhei, below). This position was complemented by his stark commitment to a unity of opposites in the world, stating that \"the path up and down are one and the same\". Through these doctrines Heraclitus characterized all existing entities by pairs of contrary properties, whereby no entity may ever occupy a single state at a single time. This, along with his cryptic utterance that \"all entities come to be in accordance with this \"Logos\"\" (literally, \"word\", \"reason\", or \"account\") has been the subject of numerous interpretations.\n\nThe main source for the life of Heraclitus is Diogenes Laërtius, although some have questioned the validity of his account as \"\"a tissue of Hellenistic anecdotes, most of them obviously fabricated on the basis of statements in the preserved fragments\".\" Diogenes said that Heraclitus flourished in the 69th Olympiad, 504–501 BC. All the rest of the evidence — the people Heraclitus is said to have known, or the people who were familiar with his work — confirms the \"floruit\". His dates of birth and death are based on a life span of 60 years, the age at which Diogenes says he died, with the floruit in the middle.\n\nHeraclitus was born to an aristocratic family in Ephesus, in the Persian Empire, in what is now called present-day Efes, Turkey. His father was named either Blosôn or Herakôn. Diogenes says that he abdicated the kingship (\"basileia\") in favor of his brother and Strabo confirms that there was a ruling family in Ephesus descended from the Ionian founder, Androclus, which still kept the title and could sit in the chief seat at the games, as well as a few other privileges. How much power the king had is another question. Ephesus had been part of the Persian Empire since 547 and was ruled by a satrap, a more distant figure, as the Great King allowed the Ionians considerable autonomy. Diogenes says that Heraclitus used to play knucklebones with the youths in the temple of Artemis and when asked to start making laws he refused saying that the constitution (\"politeia\") was \"ponêra\", which can mean either that it was fundamentally wrong or that he considered it toilsome. Two extant letters between Heraclitus and Darius I, quoted by Diogenes, are undoubtedly later forgeries.\n\nWith regard to education, Diogenes says that Heraclitus was \"wondrous\" (\"thaumasios\", which, as Socrates explains in Plato's \"Theaetetus\" and \"Gorgias\", is the beginning of philosophy) from childhood. Diogenes relates that Sotion said he was a \"hearer\" of Xenophanes, which contradicts Heraclitus' statement (so says Diogenes) that he had taught himself by questioning himself. Burnet states in any case that \"... Xenophanes left Ionia before Herakleitos was born.\" Diogenes relates that as a boy Heraclitus had said he \"knew nothing\" but later claimed to \"know everything.\" His statement that he \"heard no one\" but \"questioned himself,\" can be placed alongside his statement that \"the things that can be seen, heard and learned are what I prize the most.\"\n\nDiogenes relates that Heraclitus had a poor opinion of human affairs. He believed that Hesiod and Pythagoras lacked understanding though learned and that Homer and Archilochus deserved to be beaten. Laws needed to be defended as though they were city walls. Timon is said to have called him a \"mob-reviler.\" Heraclitus hated the Athenians and his fellow Ephesians, wishing the latter wealth in punishment for their wicked ways. Says Diogenes: \"Finally, he became a hater of his kind (\"misanthrope\") and wandered the mountains ... making his diet of grass and herbs.\"\n\nHeraclitus' life as a philosopher was interrupted by dropsy. The physicians he consulted were unable to prescribe a cure. Diogenes lists various stories about Heraclitus' death: In two versions, Heraclitus was cured of the dropsy and died of another disease. In one account, however, the philosopher \"buried himself in a cowshed, expecting that the noxious damp humour would be drawn out of him by the warmth of the manure\", while another says he treated himself with a liniment of cow manure and, after a day prone in the sun, died and was interred in the marketplace. According to Neathes of Cyzicus, after smearing himself with dung, Heraclitus was devoured by dogs.\n\nDiogenes states that Heraclitus' work was \"a continuous treatise \"On Nature\", but was divided into three discourses, one on the universe, another on politics, and a third on theology.\" Theophrastus says (in Diogenes) \"...some parts of his work [are] half-finished, while other parts [made] a strange medley.\"\n\nDiogenes also tells us that Heraclitus deposited his book as a dedication in the great temple of Artemis, the Artemisium, one of the largest temples of the 6th century BC and one of the Seven Wonders of the Ancient World. Ancient temples were regularly used for storing treasures, and were open to private individuals under exceptional circumstances; furthermore, many subsequent philosophers in this period refer to the work. Says Kahn: \"Down to the time of Plutarch and Clement, if not later, the little book of Heraclitus was available in its original form to any reader who chose to seek it out.\" Diogenes says: \"the book acquired such fame that it produced partisans of his philosophy who were called Heracliteans.\"\n\nAs with other pre-Socratics, his writings survive now only in quoted by other authors.\n\nAt some time in antiquity he acquired this epithet denoting that his major sayings were difficult to understand. According to Diogenes Laërtius, Timon of Phlius called him \"the riddler\" ( \"ainiktēs\"), and explained that Heraclitus wrote his book \"rather unclearly\" (\"asaphesteron\") so that only the \"capable\" should attempt it. By the time of Cicero he had become \"the dark\" ( — ) because he had spoken \"nimis obscurē\", \"too obscurely\", concerning nature and had done so deliberately in order to be misunderstood. The customary English translation of follows the Latin, \"the Obscure.\"\n\nDiogenes Laërtius ascribes the theory that Heraclitus did not complete some of his works because of melancholia to Theophrastus. Later he was referred to as the \"weeping philosopher,\" as opposed to Democritus, who is known as the \"laughing philosopher.\" If Stobaeus writes correctly, Sotion in the early 1st century CE was already combining the two in the imaginative duo of weeping and laughing philosophers: \"Among the wise, instead of anger, Heraclitus was overtaken by tears, Democritus by laughter.\" The view is expressed by the satirist Juvenal:\n\nThe motif was also adopted by Lucian of Samosata in his \"Sale of Creeds,\" in which the duo is sold together as a complementary product in the satirical auction of philosophers. Subsequently, they were considered an indispensable feature of philosophic landscapes. Montaigne proposed two archetypical views of human affairs based on them, selecting Democritus' for himself. The weeping philosopher may have been mentioned in William Shakespeare's \"The Merchant of Venice\". Donato Bramante painted a fresco, \"Democritus and Heraclitus,\" in Casa Panigarola in Milan.\n\n\"The idea that all things come to pass in accordance with this \"Logos\"\" and \"the \"Logos\" is common,\" is expressed in two famous but obscure fragments:\nThis \"Logos\" holds always but humans always prove unable to understand it, both before hearing it and when they have first heard it. For though all things come to be in accordance with this \"Logos\", humans are like the inexperienced when they experience such words and deeds as I set out, distinguishing each in accordance with its nature and saying how it is. But other people fail to notice what they do when awake, just as they forget what they do while asleep. (DK 22B1)\nFor this reason it is necessary to follow what is common. But although the \"Logos\" is common, most people live as if they had their own private understanding. (DK 22B2)\nThe meaning of \"Logos\" also is subject to interpretation: \"word\", \"account\", \"principle\", \"plan\", \"formula\", \"measure\", \"proportion\", \"reckoning.\" Though Heraclitus \"quite deliberately plays on the various meanings of \"logos\"\", there is no compelling reason to suppose that he used it in a special technical sense, significantly different from the way it was used in ordinary Greek of his time.\n\nThe later Stoics understood it as \"the account which governs everything,\" and Hippolytus, in the 3rd century CE, identified it as meaning the Christian \"Word of God\".\n\nThe phrase (\"panta rhei\") \"everything flows\" either was spoken by Heraclitus or survived as a quotation of his. This famous aphorism used to characterize Heraclitus' thought comes from Simplicius, a neoplatonist, and from Plato's \"Cratylus\". The word \"rhei\" (cf. rheology) is the Greek word for \"to stream\", and is etymologically related to Rhea according to Plato's \"Cratylus\".\n\nThe philosophy of Heraclitus is summed up in his cryptic utterance: \n<br>\"Potamoisi toisin autoisin embainousin, hetera kai hetera hudata epirrei\"<br>\"Ever-newer waters flow on those who step into the same rivers.\" \nThe quote from Heraclitus appears in Plato's \"Cratylus\" twice; in 401d as:\n<br>\"Ta onta ienai te panta kai menein ouden\"<br>\"All entities move and nothing remains still\"and in 402a\n\n<br>\"Panta chōrei kai ouden menei kai dis es ton auton potamon ouk an embaies\"\n\"Everything changes and nothing remains still ... and ... you cannot step twice into the same stream\"\nInstead of \"flow\" Plato uses \"chōrei\", \"to change place\" (χῶρος \"chōros\").\n\nThe assertions of flow are coupled in many fragments with the enigmatic river image:\n\n<br>\"We both step and do not step in the same rivers. We are and are not.\"\n\nCompare with the Latin adages \"Omnia mutantur\" and \"Tempora mutantur\" () and the Japanese tale \"Hōjōki,\" () which contains the same image of the changing river, and the central Buddhist doctrine of impermanence.\n\nHowever, the German classicist and philosopher interprets this fragment as an indication by Heraclitus, for the world as a steady constant: \"You will not find anything, in which the river remains constant. ... Just the fact, that there is a particular river bed, that there is a source and a estuary etc. is something, that stays identical. And this is ... the concept of a river\"\n\nIn the structure \"anō katō\" is more accurately translated as a hyphenated word: \"the upward-downward path.\" They go on simultaneously and instantaneously and result in \"hidden harmony\". A way is a series of transformations: the , \"turnings of fire,\" first into sea, then half of sea to earth and half to rarefied air.\n\nThe transformation is a replacement of one element by another: \"The death of fire is the birth of air, and the death of air is the birth of water.\"\nThis world, which is the same for all, no one of gods or men has made. But it always was and will be: an ever-living fire, with measures of it kindling, and measures going out. This latter phraseology is further elucidated:All things are an interchange for fire, and fire for all things, just like goods for gold and gold for goods.\n\nHeraclitus considered fire as the most fundamental element. He believed fire gave rise to the other elements and thus to all things. He regarded the soul as being a mixture of fire and water, with fire being the noble part of the soul, and water the ignoble part. A soul should therefore aim toward becoming more full of fire and less full of water: a \"dry\" soul was best. According to Heraclitus, worldly pleasures made the soul \"moist\", and he considered mastering one's worldly desires to be a noble pursuit which purified the soul's fire. Norman Melchert interpreted Heraclitus as using \"fire\" metaphorically, in lieu of \"Logos\", as the origin of all things.\n\nIf objects are new from moment to moment so that one can never touch the same object twice, then each object must dissolve and be generated continually momentarily and an object is a harmony between a building up and a tearing down. Heraclitus calls the oppositional processes ἔρις \"eris\", \"strife\", and hypothesizes that the apparently stable state, δίκη \"dikê\", or \"justice,\" is a harmony of it:We must know that war (πόλεμος \"polemos\") is common to all and strife is justice, and that all things come into being through strife necessarily.\nAs Diogenes explains:All things come into being by conflict of opposites, and the sum of things (τὰ ὅλα \"ta hola\", \"the whole\") flows like a stream.\nIn the bow metaphor Heraclitus compares the resultant to a strung bow held in shape by an equilibrium of the string tension and spring action of the bow:There is a harmony in the bending back (παλίντροπος \"palintropos\") as in the case of the bow and the lyre.\n\nPeople must \"follow the common\" ( \"hepesthai tō koinō\") and not live having \"their own judgement (\"phronēsis\")\". He distinguishes between human laws and divine law (τοῦ θείου \"tou theiou\" \"of God\"). By \"God\" Heraclitus does not mean the Judeo-Christian version of a single God as primum mobile of all things, God as Creator, but the divine as opposed the human, the immortal (which we tend to confuse with the \"eternal\") as opposed to the mortal, the cyclical as opposed to the transient. It is more accurate to speak of \"the god\" and not of \"God\".\n\nHe removes the human sense of justice from his concept of God; i.e., humanity is not the image of God: \"To God all things are fair and good and just, but people hold some things wrong and some right.\" God's custom has wisdom but human custom does not, and yet both humans and God are childish (inexperienced): \"human opinions are children's toys\" and \"Eternity is a child moving counters in a game; the kingly power is a child's.\"\n\nWisdom is \"to know the thought by which all things are steered through all things\", which must not imply that people are or can be wise. Only Zeus is wise. To some degree then Heraclitus seems to be in the mystic's position of urging people to follow God's plan without much of an idea what that may be. In fact there is a note of despair: \"The fairest universe ( \"kallistos kosmos\") is but a heap of rubbish ( \"sarma\", sweepings) piled up (κεχυμένον \"kechumenon\", i.e. \"poured out\") at random ( \"eikê\", \"aimlessly\").\"\n\nThis influential quote by Heraclitus \"ἦθος ἀνθρώπῳ δαίμων\" (DK 22B119) has led to numerous interpretations. Whether in this context, \"daimon\" can indeed be translated to mean \"fate\" is disputed; however, it lends much sense to Heraclitus' observations and conclusions about human nature in general. While the translation with \"fate\" is generally accepted as in Kahn's \"a man's character is his divinity\", in some cases, it may also stand for the soul of the departed.\n\nIn Heraclitus a perceived object is a harmony between two fundamental units of change, a waxing and a waning. He typically uses the ordinary word \"to become\" (\"gignesthai\" or \"ginesthai\", present tense or aorist tense of the verb, with the root sense of \"being born\"), which led to his being characterized as the philosopher of becoming rather than of being. He recognizes the fundamental changing of objects with the flow of time.\n\nPlato argues against Heraclitus as follows:How can that be a real thing which is never in the same state? ... for at the moment that the observer approaches, then they become other ... so that you cannot get any further in knowing their nature or state ... but if that which knows and that which is known exist ever ... then I do not think they can resemble a process or flux ...\n\nIn Plato one experienced unit is a state, or object existing, which can be observed. The time parameter is set at \"ever\"; that is, the state is to be presumed present between observations. Change is to be deduced by comparing observations and is thus presumed a function that \"happens to\" objects already in being, rather than something ontologically essential to them (such that something that does not change cannot exist) as in Heraclitus. In Plato, no matter how many of those experienced units you are able to tally, you cannot get through the mysterious gap between them to account for the change that must be occurring there. This limitation is considered a fundamental limitation of reality by Plato and in part underpins his differentiation between imperfect experience from more perfect Forms. The fact that this is no limitation for Heraclitus motivates Plato's condemnation.\n\nStoicism was a philosophical school which flourished between the 3rd century BCE and about the 3rd century CE. It began among the Greeks and became the major philosophy of the Roman Empire before declining with the rise of Christianity in the 3rd century.\n\nThroughout their long tenure the Stoics believed that the major tenets of their philosophy derived from the thought of Heraclitus. According to Long, \"the importance of Heraclitus to later Stoics is evident most plainly in Marcus Aurelius.\" Explicit connections of the earliest Stoics to Heraclitus showing how they arrived at their interpretation are missing but they can be inferred from the Stoic fragments, which Long concludes are \"modifications of Heraclitus.\"\n\nThe Stoics were interested in Heraclitus' treatment of fire. In addition to seeing it as the most fundamental of the four elements and the one that is quantified and determines the quantity (\"logos\") of the other three, he presents fire as the cosmos, which was not made by any of the gods or men, but \"was and is and ever shall be ever-living fire.\" Fire is both a substance and a motivator of change, it is active in altering other things quantitatively and performing an activity Heraclitus describes as \"the judging and convicting of all things.\" It is \"the thunderbolt that steers the course of all things.\" There is no reason to interpret the judgement, which is actually \"to separate\" (κρίνειν \"krinein\"), as outside of the context of \"strife is justice\" (see subsection above).\n\nThe earliest surviving Stoic work, the \"Hymn to Zeus\" of Cleanthes, though not explicitly referencing Heraclitus, adopts what appears to be the Heraclitean logos modified. Zeus rules the universe with law (\"nomos\") wielding on its behalf the \"forked servant\", the \"fire\" of the \"ever-living lightning.\" So far nothing has been said that differs from the Zeus of Homer. But then, says Cleanthes, Zeus uses the fire to \"straighten out the common logos\" that travels about (\"phoitan\", \"to frequent\") mixing with the greater and lesser lights (heavenly bodies). This is Heraclitus' logos, but now it is confused with the \"common \"nomos\"\", which Zeus uses to \"make the wrong (\"perissa\", left or odd) right (\"artia\", right or even)\" and \"order (\"kosmein\") the disordered (\"akosma\").\"\n\nThe Stoic modification of Heraclitus' idea of the Logos was also influential on Jewish philosophers such as Philo of Alexandria, who connected it to \"Wisdom personified\" as God's creative principle. Philo uses the term Logos throughout his treatises on Hebrew Scripture in a manner clearly influenced by the Stoics.\n\nThe church fathers were the leaders of the early Christian Church during its first five centuries of existence, roughly contemporaneous to Stoicism under the Roman Empire. The works of dozens of writers in hundreds of pages have survived.\n\nAll of them had something to say about the Christian form of the Logos. The Catholic Church found it necessary to distinguish between the Christian logos and that of Heraclitus as part of its ideological distancing from paganism. The necessity to convert by defeating paganism was of paramount importance. Hippolytus of Rome therefore identifies Heraclitus along with the other Pre-Socratics (and Academics) as sources of heresy. Church use of the methods and conclusions of ancient philosophy as such was as yet far in the future, even though many were converted philosophers.\n\nIn \"Refutation of All Heresies\" Hippolytus says: \"What the blasphemous folly is of Noetus, and that he devoted himself to the tenets of Heraclitus the Obscure, not to those of Christ.\" Hippolytus then goes on to present the inscrutable DK B67: \"God (\"theos\") is day and night, winter and summer, ... but he takes various shapes, just as fire, when it is mingled with spices, is named according to the savor of each.\" The fragment seems to support pantheism if taken literally. German physicist and philosopher Max Bernard Weinstein classed these views with pandeism.\n\nHippolytus condemns the obscurity of it. He cannot accuse Heraclitus of being a heretic so he says instead: \"Did not (Heraclitus) the Obscure anticipate Noetus in framing a system ...?\" The apparent pantheist deity of Heraclitus (if that is what DK B67 means) must be equal to the union of opposites and therefore must be corporeal and incorporeal, divine and not-divine, dead and alive, etc., and the Trinity can only be reached by some sort of illusory shape-shifting.\n\nThe Apologist Justin Martyr, however, took a much more positive view of him. In his First Apology, he said both Socrates and Heraclitus were Christians before Christ: \"those who lived reasonably are Christians, even though they have been thought atheists; as, among the Greeks, Socrates and Heraclitus, and men like them.\" \n\nThe following articles on other topics contain non-trivial information that relates to Heraclitus in some way.\n\n\n\n", "id": "13792", "title": "Heraclitus"}
{"url": "https://en.wikipedia.org/wiki?curid=13793", "text": "Harrison Schmitt\n\nHarrison Hagan \"Jack\" Schmitt (born July 3, 1935) is an American geologist, retired NASA astronaut, university professor, former U.S. senator from New Mexico, and the most recent living person to have walked on the Moon. He is also the last living crew member of Apollo 17. \n\nIn December 1972, as one of the crew on board Apollo 17, Schmitt became the first member of NASA's first scientist-astronaut group to fly in space. As Apollo 17 was the last of the Apollo missions, he also became the twelfth and youngest person to set foot on the Moon, and the second-to-last person to step off of the Moon (he boarded the Lunar Module shortly before commander Eugene Cernan). Schmitt also remains the first and only professional scientist to have flown beyond low Earth orbit and to have visited the Moon. He was influential within the community of geologists supporting the Apollo program and, before starting his own preparations for an Apollo mission, had been one of the scientists training those Apollo astronauts chosen to visit the lunar surface.\n\nSchmitt resigned from NASA in August 1975 in order to run for election to the United States Senate as a member from New Mexico. As the Republican candidate in the 1976 election, he defeated the two-term Democratic incumbent Joseph Montoya, but, running for re-election in 1982, was defeated by Democrat Jeff Bingaman.\n\nBorn in Santa Rita, New Mexico, Schmitt grew up in nearby Silver City, and he is a graduate of the Western High School (class of 1953). He received a B.S. degree in geology from the California Institute of Technology in 1957 and then spent a year studying geology at the University of Oslo in Norway. He received a Ph.D. in geology from Harvard University in 1964, based on his geological field studies in Norway.\n\nBefore joining NASA as a member of the first group of scientist-astronauts in June 1965, he worked at the U.S. Geological Survey's Astrogeology Center at Flagstaff, Arizona, developing geological field techniques that would be used by the Apollo crews. Following his selection, Schmitt spent his first year at Air Force UPT learning to become a jet pilot. Upon his return to the astronaut corps in Houston, he played a key role in training Apollo crews to be geologic observers when they were in lunar orbit and competent geologic field workers when they were on the lunar surface. After each of the landing missions, he participated in the examination and evaluation of the returned lunar samples and helped the crews with the scientific aspects of their mission reports.\n\nSchmitt spent considerable time becoming proficient in the CSM and LM systems. In March 1970 he became the first of the scientist-astronauts to be assigned to space flight, joining Richard F. Gordon, Jr. (Commander) and Vance Brand (Command Module Pilot) on the Apollo 15 backup crew. The flight rotation put these three in line to fly as prime crew on the third following mission, Apollo 18. When Apollo flights 18 and 19 were cancelled in September 1970, the community of lunar geologists supporting Apollo felt so strongly about the need to land a professional geologist on the Moon, that they pressured NASA to reassign Schmitt to a remaining flight. As a result, Schmitt was assigned in August 1971 to fly on the last mission, Apollo 17, replacing Joe Engle as Lunar Module Pilot. Schmitt landed on the Moon with commander Gene Cernan in December 1972.\n\nSchmitt claims to have taken the photograph of the Earth known as \"The Blue Marble\", one of the most widely distributed photographic images in existence. (NASA officially credits the image to the entire Apollo 17 crew.)\n\nWhile on the Moon's surface, Schmitt — the only geologist in the astronaut corps — collected the rock sample designated Troctolite 76535, which has been called \"without doubt the most interesting sample returned from the Moon\". Among other distinctions, it is the central piece of evidence suggesting that the Moon once possessed an active magnetic field.\n\nAs he returned to the Lunar Module before Cernan, Schmitt is the next-to-last person to have walked on the Moon's surface.\n\nAfter the completion of Apollo 17, Schmitt played an active role in documenting the Apollo geologic results and also took on the task of organizing NASA's Energy Program Office.\n\nIn August 1975, Schmitt resigned from NASA to seek election as a Republican to the United States Senate representing New Mexico in the 1976 election. Schmitt faced two-term Democratic incumbent Joseph Montoya, whom he defeated, 57% to 42%. He served one term and, notably, was the ranking Republican member of the Science, Technology, and Space Subcommittee. He sought a second term in 1982, facing state Attorney General Jeff Bingaman. Bingaman attacked Schmitt for not paying enough attention to local matters; his campaign slogan asked, \"What on Earth has he done for you lately?\" This, combined with the deep recession, proved too much for Schmitt to overcome; he was defeated, 54% to 46%.\n\nDuring his term in the Senate, Schmitt sat at the chamber's candy desk.\n\nFollowing his Senate term, Schmitt has been a consultant in business, geology, space, and public policy.\n\nSchmitt is an adjunct professor of engineering physics at the University of Wisconsin–Madison, and has long been a proponent of lunar resource utilization. In 1997 he proposed the Interlune InterMars Initiative, listing among its goals the advancement of private-sector acquisition and use of lunar resources, particularly lunar helium-3 as a fuel for notional nuclear fusion reactors.\n\nSchmitt was chair of the NASA Advisory Council, whose mandate is to provide technical advice to the NASA Administrator, from November 2005 until his abrupt resignation on October 16, 2008. In November 2008, he quit the Planetary Society over policy advocacy differences, citing the organization's statements on \"focusing on Mars as the driving goal of human spaceflight\" (Schmitt said that going back to the Moon would speed progress toward a manned Mars mission), on \"accelerating research into global climate change through more comprehensive Earth observations\" (Schmitt voiced objections to the notion of a present \"scientific consensus\" on climate change as any policy guide), and on international cooperation (which he felt would retard rather than accelerate progress), among other points of divergence.\n\nIn January 2011, he was appointed as secretary of the New Mexico Energy, Minerals and Natural Resources Department in the cabinet of Governor Susana Martinez, but was forced to give up the appointment the following month after refusing to submit to a required background investigation. \"El Paso Times\" called him the \"most celebrated\" candidate for New Mexico energy secretary.\n\nSchmitt wrote a book entitled \"Return to the Moon: Exploration, Enterprise, and Energy in the Human Settlement of Space\" in 2006.\n\nHe lives in Silver City, New Mexico, and spends some of his summer at his northern Minnesota lake cabin.\n\nSchmitt is also involved in several civic projects, including the improvement of the Senator Harrison H. Schmitt Big Sky Hang Glider Park in Albuquerque, New Mexico.\n\nSchmitt's view on climate change diverges from the frequently reported scientific consensus, as he emphasizes natural over human factors as driving climate. Schmitt has expressed the view that the risks posed by climate change are overrated, and suggests instead that climate change is a tool for people who are trying to increase the size of government. He resigned his membership in the Planetary Society because of its stance on the subject, writing in his resignation letter that the \"global warming scare is being used as a political tool to increase government control over American lives, incomes and decision-making.\" He spoke at the March 2009 International Conference on Climate Change sponsored by the Heartland Institute. He appeared in December that year on the Fox Business Network, saying \"[t]he CO scare is a red herring\".\n\nIn a 2009 interview with libertarian talk-radio host Alex Jones, Schmitt asserted a link between Soviet Communism and the American environmental movement: \"I think the whole trend really began with the fall of the Soviet Union. Because the great champion of the opponents of liberty, namely communism, had to find some other place to go and they basically went into the environmental movement.\" At the Heartland Institute's sixth International Conference on Climate Change Schmitt said that climate change was a stalking horse for National Socialism.\n\nSchmitt co-authored a May 8, 2013 \"Wall Street Journal\" opinion column with William Happer, contending that increasing levels of carbon dioxide in the atmosphere are not significantly correlated with global warming, attributing the \"single-minded demonization of this natural and essential atmospheric gas\" to advocates of government control of energy production. Noting a positive relationship between crop resistance to drought and increasing carbon dioxide levels, the authors argued, \"Contrary to what some would have us believe, increased carbon dioxide in the atmosphere will benefit the increasing population on the planet by increasing agricultural productivity.\"\n\n\n\nSchmitt is one of the astronauts featured in the documentary \"In the Shadow of the Moon\". He also contributed to the book \"NASA's Scientist-Astronauts\" by David Shayler and Colin Burgess.\n\n", "id": "13793", "title": "Harrison Schmitt"}
{"url": "https://en.wikipedia.org/wiki?curid=13795", "text": "Hilaire Rouelle\n\nHilaire Marin Rouelle (15 February 1718 – 7 April 1779) was an 18th-century French chemist. Commonly cited as the 1773 discoverer of urea, he was not the first to do so. Dutch scientist Herman Boerhaave had discovered this chemical as early as 1727. Rouelle is known as \"le cadet\" (the younger) to distinguish him from his older brother, Guillaume-François Rouelle, who was also a chemist.\n", "id": "13795", "title": "Hilaire Rouelle"}
{"url": "https://en.wikipedia.org/wiki?curid=13798", "text": "Halon\n\nHalon may refer to:\n\n", "id": "13798", "title": "Halon"}
{"url": "https://en.wikipedia.org/wiki?curid=13802", "text": "Hammer\n\nA hammer is a tool or device that delivers a blow (a sudden impact) to an object. Most hammers are hand tools used to drive nails, fit parts, forge metal, and break apart objects. Hammers vary in shape, size, and structure, depending on their purposes.\n\nHammers are basic tools in many trades. The usual features are a head (most often made of steel) and a handle (also called a helve or haft). Although most hammers are hand tools, powered versions exist; they are known as powered hammers. Types of power hammer include steam hammers and trip hammers, often for heavier uses, such as forging.\n\nSome hammers have other names, such as \"sledgehammer\", \"mallet\" and \"gavel\". The term \"hammer\" also applies to big von devices that deliver blows, such as the hammer of a firearm or the hammer of a piano or the hammer ice scraper.\n\nThe use of simple hammers dates to about 2,600,000 BCE when various shaped stones were used to strike wood, bone, or other stones to break them apart and shape them. Stones attached to sticks with strips of leather or animal sinew were being used as hammers with handles by about 30,000 BCE during the middle of the Paleolithic Stone Age.\n\nThe hammer's archeological record shows that it may be the oldest tool for which definite evidence exists of its early existence.\n\nA traditional hand-held hammer consists of a separate head and a handle, fastened together by means of a special wedge made for the purpose, or by glue, or both. This two-piece design is often used, to combine a dense metallic striking head with a non-metallic mechanical-shock-absorbing handle (to reduce user fatigue from repeated strikes). If wood is used for the handle, it is often hickory or ash, which are tough and long-lasting materials that can dissipate shock waves from the hammer head. Rigid fiberglass resin may be used for the handle; this material does not absorb water or decay, but does not dissipate shock as well as wood.\n\nA loose hammer head is hazardous because it can literally \"fly off the handle\" when in use, becoming a dangerous uncontrolled missile. Wooden handles can often be replaced when worn or damaged; specialized kits are available covering a range of handle sizes and designs, plus special wedges for attachment.\n\nSome hammers are one-piece designs made primarily of a single material. A one-piece metallic hammer may optionally have its handle coated or wrapped in a resilient material such as rubber, for improved grip and reduced user fatigue.\n\nThe hammer head may be surfaced with a variety of materials, including brass, bronze, wood, plastic, rubber, or leather. Some hammers have interchangeable striking surfaces, which can be selected as needed or replaced when worn out.\n\nA large hammer-like tool is a \"maul\" (sometimes called a \"beetle\"), a wood- or rubber-headed hammer is a \"mallet\", and a hammer-like tool with a cutting blade is usually called a \"hatchet\". The essential part of a hammer is the head, a compact solid mass that is able to deliver a blow to the intended target without itself deforming. The impacting surface of the tool is usually flat or slightly rounded; the opposite end of the impacting mass may have a ball shape, as in the ball-peen hammer. Some upholstery hammers have a magnetized face, to pick up tacks. In the hatchet, the flat hammer head may be secondary to the cutting edge of the tool.\n\nThe impact between steel hammer heads and the objects being hit can create sparks, which may ignite flammable or explosive gases. These are a hazard in some industries such as underground coal mining (due to the presence of methane gas), or in other hazardous environments such as petroleum refineries and chemical plants. In these environments, a variety of non-sparking metal tools are used, primarily made of aluminium or beryllium copper. In recent years, the handles have been made of durable plastic or rubber, though wood is still widely used because of its shock-absorbing qualities and repairability.\n\n\nMechanically-powered hammers often look quite different from the hand tools, but nevertheless most of them work on the same principle. They include:\nIn professional framing carpentry, the manual hammer has almost been completely replaced by the nail gun. In professional upholstery, its chief competitor is the staple gun.\n\n\nA hammer is basically a force amplifier that works by converting mechanical work into kinetic energy and back.\n\nIn the swing that precedes each blow, the hammer head stores a certain amount of kinetic energy—equal to the length \"D\" of the swing times the force \"f\" produced by the muscles of the arm and by gravity. When the hammer strikes, the head is stopped by an opposite force coming from the target, equal and opposite to the force applied by the head to the target. If the target is a hard and heavy object, or if it is resting on some sort of anvil, the head can travel only a very short distance \"d\" before stopping. Since the stopping force \"F\" times that distance must be equal to the head's kinetic energy, it follows that \"F\" is much greater than the original driving force \"f\"—roughly, by a factor \"D\"/\"d\". In this way, great strength is not needed to produce a force strong enough to bend steel, or crack the hardest stone.\n\nThe amount of energy delivered to the target by the hammer-blow is equivalent to one half the mass of the head times the square of the head's speed at the time of impact (formula_1). While the energy delivered to the target increases linearly with mass, it increases quadratically with the speed (see the effect of the handle, below). High tech titanium heads are lighter and allow for longer handles, thus increasing velocity and delivering the same energy with less arm fatigue than that of a heavier steel head hammer. A titanium head has about 3% recoil energy and can result in greater efficiency and less fatigue when compared to a steel head with up to 30% recoil. Dead blow hammers use special rubber or steel shot to absorb recoil energy, rather than bouncing the hammer head after impact.\n\nThe handle of the hammer helps in several ways. It keeps the user's hands away from the point of impact. It provides a broad area that is better-suited for gripping by the hand. Most importantly, it allows the user to maximize the speed of the head on each blow. The primary constraint on additional handle length is the lack of space to swing the hammer. This is why sledge hammers, largely used in open spaces, can have handles that are much longer than a standard carpenter's hammer. The second most important constraint is more subtle. Even without considering the effects of fatigue, the longer the handle, the harder it is to guide the head of the hammer to its target at full speed.\n\nMost designs are a compromise between practicality and energy efficiency. With too long a handle, the hammer is inefficient because it delivers force to the wrong place, off-target. With too short a handle, the hammer is inefficient because it doesn't deliver enough force, requiring more blows to complete a given task. Modifications have also been made with respect to the effect of the hammer on the user. Handles made of shock-absorbing materials or varying angles attempt to make it easier for the user to continue to wield this age-old device, even as nail guns and other powered drivers encroach on its traditional field of use.\n\nAs hammers must be used in many circumstances, where the position of the person using them cannot be taken for granted, trade-offs are made for the sake of practicality. In areas where one has plenty of room, a long handle with a heavy head (like a sledge hammer) can deliver the maximum amount of energy to the target. It is not practical to use such a large hammer for all tasks, however, and thus the overall design has been modified repeatedly to achieve the optimum utility in a wide variety of situations.\n\nGravity exerts a force on the hammer head. If hammering downwards, gravity increases the acceleration during the hammer stroke and increases the energy delivered with each blow. If hammering upwards, gravity reduces the acceleration during the hammer stroke and therefore reduces the energy delivered with each blow. Some hammering methods, such as traditional mechanical pile drivers, rely entirely on gravity for acceleration on the down stroke.\n\nBoth manual and powered hammers can cause peripheral neuropathy or a variety of other ailments when used improperly. Awkward handles can cause repetitive stress injury (RSI) to hand and arm joints, and uncontrolled shock waves from repeated impacts can injure nerves and the skeleton.\n\nA war hammer is a late medieval weapon of war intended for close combat action.\n\nThe hammer, being one of the most used tools by \"Homo sapiens\", has been used very much in symbols such as flags and heraldry. In the Middle Ages, it was used often in blacksmith guild logos, as well as in many family symbols. The hammer and pick is used as a symbol of mining.\n\nA well known symbol with a hammer in it is the Hammer and Sickle, which was the symbol of the former Soviet Union and is very interlinked with communism and early socialism. The hammer in this symbol represents the industrial working class (and the sickle represents the agricultural working class). The hammer is used in some coat of arms in (former) socialist countries like East Germany. Similarly, the Hammer and Sword symbolizes Strasserism, a strand of National Socialism seeking to appeal to the working class.\n\nThe gavel, a small wooden mallet, is used to symbolize a mandate to preside over a meeting or judicial proceeding, and a graphic image of one is used as a symbol of legislative or judicial decision-making authority.\n\nIn Norse mythology, Thor, the god of thunder and lightning, wields a hammer named Mjölnir. Many artifacts of decorative hammers have been found, leading modern practitioners of this religion to often wear reproductions as a sign of their faith.\n\nJudah Maccabee was nicknamed \"The Hammer\", possibly in recognition of his ferocity in battle. The name \"Maccabee\" may derive from the Aramaic \"maqqaba\". (see ).\n\nIn American folkore, the hammer of John Henry represents the strength and endurance of a man.\n\nThe hammer in the song \"If I Had a Hammer\" represents a relentless message of justice broadcast across the land. The song became a symbol of the American Civil Rights movement.\n\n\n", "id": "13802", "title": "Hammer"}
{"url": "https://en.wikipedia.org/wiki?curid=13804", "text": "Hiragana\n\nHiragana and katakana are both kana systems. With one or two minor exceptions, each sound in the Japanese language (strictly, each mora) is represented by one character (or one digraph) in each system. This may be either a vowel such as \"\"a\"\" (hiragana あ); a consonant followed by a vowel such as \"\"ka\"\" (か); or \"\"n\"\" (ん), a nasal sonorant which, depending on the context, sounds either like English \"m\", \"n\", or \"ng\" (), or like the nasal vowels of French. Because the characters of the kana do not represent single consonants (except in the case of ん \"n\"), the kana are referred to as syllabaries and not alphabets.\n\nHiragana is used to write \"okurigana\" (kana suffixes following a kanji root, for example to inflect verbs and adjectives), various grammatical and function words including particles, as well as miscellaneous other native words for which there are no kanji or whose kanji form is obscure or too formal for the writing purpose. Words that do have common kanji renditions may also sometimes be written instead in hiragana, according to an individual author's preference, for example to impart an informal feel. Hiragana is also used to write \"furigana\", a reading aid that shows the pronunciation of kanji characters.\n\nThere are two main systems of ordering hiragana: the old-fashioned iroha ordering and the more prevalent gojūon ordering.\n\nThe modern hiragana syllabary consists of 46 base characters:\n\nThese are conceived as a 5×10 grid (\"gojūon\", , \"Fifty Sounds\"), as illustrated in the adjacent table, read and so forth, with the singular consonant appended to the end. Of the 50 theoretically possible combinations, \"yi\" and \"wu\" do not exist in the language, and \"ye\", \"wi\" and \"we\" are obsolete (or virtually obsolete) in modern Japanese. \"wo\" is usually pronounced as a vowel (\"o\") in modern Japanese, and is preserved in only one use, as a particle.\n\nRomanization of the kana does not always strictly follow the consonant-vowel scheme laid out in the table. For example, ち, nominally \"ti\", is very often romanised as \"chi\" in an attempt to better represent the actual sound in Japanese.\n\nThese basic characters can be modified in various ways. By adding a \"dakuten\" marker ( ゛), a voiceless consonant is turned into a voiced consonant: \"k\"→\"g\", \"ts/s\"→\"z\", \"t\"→\"d\", \"h\"→\"b\" and \"ch\"/\"sh\"→\"j\". For example, か (\"ka\") becomes が (\"ga\"). Hiragana beginning with an \"h\" can also add a \"handakuten\" marker ( ゜) changing the \"h\" to a \"p\". For example, は (\"ha\") becomes ぱ (\"pa\").\n\nA small version of the hiragana for \"ya\", \"yu\" or \"yo\" (ゃ, ゅ or ょ respectively) may be added to hiragana ending in \"i\". This changes the \"i\" vowel sound to a glide (palatalization) to \"a\", \"u\" or \"o\". For example, き (\"ki\") plus ゃ (small \"ya\") becomes (\"kya\"). Addition of the small \"y\" kana is called \"yōon\". \n\nA small \"tsu\" っ, called a \"sokuon\", indicates that the following consonant is geminated (doubled). In Japanese this is an important distinction in pronunciation; for example, compare \"saka\" \"hill\" with \"sakka\" \"author\". The \"sokuon\" also sometimes appears at the end of utterances, where it denotes a glottal stop, as in ( Ouch!). However, it cannot be used to double the \"na\", \"ni\", \"nu\", \"ne\", \"no\" syllables' consonants – to double these, the singular \"n\" (ん) is added in front of the syllable, as in みんな (\"minna\", \"all\").\n\nHiragana usually spells long vowels with the addition of a second vowel kana; for example, おかあさん (\"o-ka-a-sa-n\", \"mother\"). The \"chōonpu\" (long vowel mark) (ー) used in katakana is rarely used with hiragana, for example in the word , \"rāmen\", but this usage is considered non-standard. In informal writing, small versions of the five vowel kana are sometimes used to represent trailing off sounds ( \"haa\", \"nee\"). Standard and voiced iteration marks are written in hiragana as ゝ and ゞ respectively.\n\nThe following table shows the complete hiragana together with the Hepburn romanization and IPA transcription in the \"gojūon\" order. Hiragana with \"dakuten\" or \"handakuten\" follow the \"gojūon\" kana without them, with the \"yōon\" kana following. Obsolete and normally unused kana are shown in gray. For all syllables besides ん, the pronunciation indicated is for word-initial syllables, for mid-word pronunciations see below.\n\nIn the middle of words, the \"g\" sound (normally ) often turns into a velar nasal and less often (although increasing recently) into the voiced velar fricative . An exception to this is numerals; 15 \"juugo\" is considered to be one word, but is pronounced as if it was \"jū\" and \"go\" stacked end to end: .\n\nAdditionally, the \"j\" sound (normally ) can be pronounced in the middle of words. For example, \"sūji\" 'number'.\n\nIn archaic forms of Japanese, there existed the \"kwa\" ( ) and \"gwa\" ( ) digraphs. In modern Japanese, these phonemes have been phased out of usage and only exist in the extended katakana digraphs for approximating foreign language words.\n\nThe singular \"n\" is pronounced before \"t\", \"ch\", \"ts\", \"n\", \"r\", \"z\", \"j\" and \"d\", before \"m\", \"b\" and \"p\", before \"k\" and \"g\", at the end of utterances, before vowels, palatal approximants (\"y\"), consonants \"s\", \"sh\", \"h\", \"f\" and \"w\", and finally after the vowel \"i\" if another vowel, palatal approximant or consonant \"s\", \"sh\", \"h\", \"f\" or \"w\" follows.\n\nIn kanji readings, the diphthongs \"ou\" and \"ei\" are today usually pronounced (long o) and (long e) respectively. For example, (lit. \"toukyou\") is pronounced 'Tokyo', and \"sensei\" is 'teacher'. However, \"tou\" is pronounced 'to inquire', because the \"o\" and \"u\" are considered distinct, \"u\" being the infinitive verb ending. Similarly, \"shite iru\" is pronounced 'is doing'.\n\nFor a more thorough discussion on the sounds of Japanese, please refer to Japanese phonology.\n\nWith a few exceptions for sentence particles は, を, and へ (normally \"ha\", \"wo\", and \"he\", but instead pronounced as \"wa\", \"o\", and \"e\", respectively), and a few other arbitrary rules, Japanese, when written in kana, is phonemically orthographic, i.e. there is a one-to-one correspondence between kana characters and sounds, leaving only words' pitch accent unrepresented. This has not always been the case: a previous system of spelling, now referred to as historical kana usage, differed substantially from pronunciation; the three above-mentioned exceptions in modern usage are the legacy of that system. The old spelling is referred to as .\n\nThere are two hiragana pronounced \"ji\" (じ and ぢ) and two hiragana pronounced \"zu\" (ず and づ), but to distinguish them, sometimes \"ぢ\" is written as \"di\" and \"づ\" is written as \"dzu\". These pairs are not interchangeable. Usually, \"ji\" is written as じ and \"zu\" is written as ず. There are some exceptions. If the first two syllables of a word consist of one syllable without a \"dakuten\" and the same syllable with a \"dakuten\", the same hiragana is used to write the sounds. For example, \"chijimeru\" ('to boil down' or 'to shrink') is spelled ちぢめる and \"tsudzuku\" ('to continue') is . For compound words where the dakuten reflects \"rendaku\" voicing, the original hiragana is used. For example, \"chi\" ( 'blood') is spelled ち in plain hiragana. When \"hana\" ('nose') and \"chi\" ('blood') combine to make \"hanaji\" ( 'nose bleed'), the sound of changes from \"chi\" to \"dji\". So \"hanadji\" is spelled according to ち: the basic hiragana used to transcribe . Similarly, \"tsukau\" (; 'to use') is spelled in hiragana, so \"kanazukai\" (; 'kana use', or 'kana orthography') is spelled in hiragana.\n\nHowever, this does not apply when kanji are used phonetically to write words that do not relate directly to the meaning of the kanji (see also ateji). The Japanese word for 'lightning', for example, is \"inazuma\" (). The component means 'rice plant', is written in hiragana and is pronounced: \"ina\". The component means 'wife' and is pronounced \"tsuma\" (つま) when written in isolation—or frequently as \"zuma\" when it features after another syllable. Neither of these components have anything to do with 'lightning', but together they do when they compose the word for 'lightning'. In this case, the default spelling in hiragana rather than is used.\n\nOfficially, ぢ and づ do not occur word-initially pursuant to modern spelling rules. There were words such as \"jiban\" 'ground' in the historical kana usage, but they were unified under じ in the modern kana usage in 1946, so today it is spelled exclusively . However, \"zura\" 'wig' (from \"katsura\") and \"zuke\" (a sushi term for lean tuna soaked in soy sauce) are examples of word-initial づ today. Some people write the word for hemorrhoids as ぢ (normally じ) for emphasis.\n\nNo standard Japanese words begin with the kana ん (\"n\"). This is the basis of the word game shiritori. ん \"n\" is normally treated as its own syllable and is separate from the other \"n\"-based kana (\"na\", \"ni\" etc.). A notable exception to this is the colloquial negative verb conjugation; for example \"wakaranai\" meaning \"[I] don't understand\" is rendered as \"wakaran\". It is however not a contraction of the former, but instead comes from the classic negative verb conjugation ぬ \"nu\" ( \"wakaranu\").\n\nん is sometimes directly followed by a vowel (\"a\", \"i\", \"u\", \"e\" or \"o\") or a palatal approximant (\"ya\", \"yu\" or \"yo\"). These are clearly distinct from the \"na\", \"ni\" etc. syllables, and there are minimal pairs such as \"kin'en\" 'smoking forbidden', \"kinen\" 'commemoration', \"kinnen\" 'recent years'. In Hepburn romanization, they are distinguished with an apostrophe, but not all romanization methods make the distinction. For example, past prime minister Junichiro Koizumi's first name is actually \"Jun'ichirō\" pronounced \n\nThere are a few hiragana that are rarely used. ゐ \"wi\" and ゑ \"we\" are obsolete outside of Okinawan orthography. 𛀁 \"e\" was an alternate version of え \"e\" before spelling reform, and was briefly reused for \"ye\" during initial spelling reforms, but is now completely obsolete. ゔ \"vu\" is a modern addition used to represent the /v/ sound in foreign languages such as English, but since Japanese from a phonological standpoint does not have a /v/ sound, it is pronounced as /b/ and mostly serves as a more accurate indicator of a word's pronunciation in its original language. However, it is rarely seen because loanwords and transliterated words are usually written in katakana, where the corresponding character would be written as ヴ. , , for \"ja\"/\"ju\"/\"jo\" are theoretically possible in rendaku, but are practically never used. For example, 'throughout Japan' could be written , but is practically always \n\nThe \"myu\" kana is extremely rare in originally Japanese words; linguist Haruhiko Kindaichi raises the example of the Japanese family name Omamyūda and claims it is the only occurrence amongst pure Japanese words. Its katakana counterpart is used in many loanwords, however.\n\nHiragana developed from \"man'yōgana\", Chinese characters used for their pronunciations, a practice that started in the 5th century. The oldest example of Man'yōgana is the Inariyama Sword, an iron sword excavated at the Inariyama Kofun in 1968. This sword is thought to be made in year of (which is A.D. 471 in commonly accepted theory).\nThe forms of the hiragana originate from the cursive script style of Chinese calligraphy. The figure below shows the derivation of hiragana from manyōgana via cursive script. The upper part shows the character in the regular script form, the center character in red shows the cursive script form of the character, and the bottom shows the equivalent hiragana. Note also that the cursive script forms are not strictly confined to those in the illustration.\n\nMale authors came to write literature using hiragana. Hiragana was used for unofficial writing such as personal letters, while katakana and Chinese were used for official documents. In modern times, the usage of hiragana has become mixed with katakana writing. Katakana is now relegated to special uses such as recently borrowed words (i.e., since the 19th century), names in transliteration, the names of animals, in telegrams, and for emphasis.\n\nOriginally, for all syllables there was more than one possible hiragana. In 1900, the system was simplified so each syllable had only one hiragana. The deprecated hiragana are now known as .\n\nThe pangram poem \"Iroha-uta\" (\"ABC song/poem\"), which dates to the 10th century, uses every hiragana once (except \"n\" ん, which was just a variant of む before the Muromachi era).\n\nThe following table shows the method for writing each hiragana character. It is arranged in the traditional way, beginning top right and reading columns down. The numbers and arrows indicate the stroke order and direction respectively. <br>\n\nHiragana was added to the Unicode Standard in October, 1991 with the release of version 1.0.\n\nThe Unicode block for Hiragana is U+3040–U+309F:\n\nThe Unicode hiragana block contains precomposed characters for all hiragana in the modern set, including small vowels and yōon kana for compound syllables, plus the archaic ゐ \"wi\" and ゑ \"we\" and the rare ゔ \"vu\"; the archaic 𛀁 \"ye\" is included in plane 1 at U+1B001 (see below). All combinations of hiragana with \"dakuten\" and \"handakuten\" used in modern Japanese are available as precomposed characters, and can also be produced by using a base hiragana followed by the combining dakuten and handakuten characters (U+3099 and U+309A, respectively). This method is used to add the diacritics to kana that are not normally used with them, for example applying the dakuten to a pure vowel or the handakuten to a kana not in the h-group.\n\nCharacters U+3095 and U+3096 are small か (\"ka\") and small け (\"ke\"), respectively. U+309F is a ligature of より (\"yori\") occasionally used in vertical text. U+309B and U+309C are spacing (non-combining) equivalents to the combining dakuten and handakuten characters, respectively.\n\nHistoric and variant forms of Japanese kana characters were added to the Unicode Standard in October, 2010 with the release of version 6.0.\n\nThe Unicode block for Kana Supplement is U+1B000–U+1B0FF:\n\nIn the following character sequences a kana from the /k/ row is modified by a \"handakuten\" combining mark to indicate that a syllable starts with an initial nasal, known as \"bidakuon\". As of Unicode 9.0, these character combinations are explicitly called out as Named Sequences.\n\n\n", "id": "13804", "title": "Hiragana"}
{"url": "https://en.wikipedia.org/wiki?curid=13805", "text": "Hohenstaufen\n\nThe Hohenstaufen, also called the Staufer or Staufen, were a dynasty of German kings (1138–1254) during the Middle Ages. Besides Germany, they also ruled the Kingdom of Sicily (1194–1268). In Italian historiography, they are known as the \"Svevi\" (Swabians), since they were (successive) dukes of Swabia from 1079. Three members of the dynasty—Frederick I, Henry VI and Frederick II—were crowned Holy Roman Emperor.\n\nThe name \"Hohenstaufen\", meaning \"high Staufen\", originates in the 14th century, when it was first used to distinguish the conical hill named Staufen in the Swabian Jura, in the district of Göppingen, from the village of the same name in the valley below. The name \"Staufen\" derives from \"Stauf\" (formerly \"stouf\"), meaning \"chalice\", and was commonly applied to conical hills in Swabia in the Middle Ages. The family derives its name from the castle which the first Swabian duke of the lineage built there in the latter half of the 11th century. Staufen castle was only finally called Hohenstaufen by historians in the 19th century, to distinguish it from other castles of the same name. The name of the dynasty followed, but in recent decades the trend in German historiography has been to prefer the name Staufer.\n\nThe noble family first appeared in the late 10th century in the Swabian \"Riesgau\" region around the former Carolingian court of Nördlingen. A local count Frederick (d. about 1075) is mentioned as progenitor in a pedigree drawn up by Abbot Wibald of Stavelot at the behest of Emperor Frederick Barbarossa in 1153. He held the office of a Swabian count palatine; his son Frederick of Buren (c.1020–1053) married Hildegard of Egisheim-Dagsburg (d. 1094/95), a niece of Pope Leo IX. Their son Frederick I was appointed Duke of Swabia at Hohenstaufen Castle by the Salian king Henry IV of Germany in 1079. \n\nAt the same time, Duke Frederick I was engaged to the king's approximately seventeen-year-old daughter, Agnes. Nothing is known about Frederick's life before this event, but he proved to be an imperial ally throughout Henry's struggles against other Swabian lords, namely Rudolf of Rheinfelden, Frederick's predecessor, and the Zähringen and Welf lords. Frederick's brother Otto was elevated to the Strasbourg bishopric in 1082.\n\nUpon Frederick's death, he was succeeded by his son, Duke Frederick II, in 1105. Frederick II remained a close ally of the Salians, he and his younger brother Conrad were named the king's representatives in Germany when the king was in Italy. Around 1120, Frederick II married Judith of Bavaria from the rival House of Welf.\n\nWhen the last male member of the Salian dynasty, Emperor Henry V, died without heirs in 1125, a controversy arose about the succession. Duke Frederick II and Conrad, the two current male Staufers, by their mother Agnes were grandsons of late Emperor Henry IV and nephews of Henry V. Frederick attempted to succeed to the throne of the Holy Roman Emperor (formally known as the King of the Romans) through a customary election, but lost to the Saxon duke Lothair of Supplinburg. A civil war between Frederick's dynasty and Lothair's ended with Frederick's submission in 1134. After Lothair's death in 1137, Frederick's brother Conrad was elected King as Conrad III.\n\nBecause the Welf duke Henry the Proud, son-in-law and heir of Lothair and the most powerful prince in Germany, who had been passed over in the election, refused to acknowledge the new king, Conrad III deprived him of all his territories, giving the Duchy of Saxony to Albert the Bear and that of Bavaria to Leopold IV, Margrave of Austria. In 1147, Conrad heard Bernard of Clairvaux preach the Second Crusade at Speyer, and he agreed to join King Louis VII of France in a great expedition to the Holy Land which failed.\n\nConrad's brother Duke Frederick II died in 1147, and was succeeded in Swabia by his son, Duke Frederick III. When King Conrad III died without adult heir in 1152, Frederick also succeeded him, taking both German royal and Imperial titles.\n\nFrederick I, known as Frederick Barbarossa because of his red beard, struggled throughout his reign to restore the power and prestige of the German monarchy against the dukes, whose power had grown both before and after the Investiture Controversy under his Salian predecessors. As royal access to the resources of the church in Germany was much reduced, Frederick was forced to go to Italy to find the finances needed to restore the king's power in Germany. He was soon crowned emperor in Italy, but decades of warfare on the peninsula yielded scant results. The Papacy and the prosperous city-states of the Lombard League in northern Italy were traditional enemies, but the fear of Imperial domination caused them to join ranks to fight Frederick. Under the skilled leadership of Pope Alexander III, the alliance suffered many defeats but ultimately was able to deny the emperor a complete victory in Italy. Frederick returned to Germany. He had vanquished one notable opponent, his Welf cousin, Duke Henry the Lion of Saxony and Bavaria in 1180, but his hopes of restoring the power and prestige of the monarchy seemed unlikely to be met by the end of his life.\n\nDuring Frederick's long stays in Italy, the German princes became stronger and began a successful colonization of Slavic lands. Offers of reduced taxes and manorial duties enticed many Germans to settle in the east in the course of the \"Ostsiedlung\". In 1163 Frederick waged a successful campaign against the Kingdom of Poland in order to re-install the Silesian dukes of the Piast dynasty. With the German colonization, the Empire increased in size and came to include the Duchy of Pomerania. A quickening economic life in Germany increased the number of towns and Imperial cities, and gave them greater importance. It was also during this period that castles and courts replaced monasteries as centers of culture. Growing out of this courtly culture, Middle High German literature reached its peak in lyrical love poetry, the Minnesang, and in narrative epic poems such as \"Tristan\", \"Parzival\", and the \"Nibelungenlied\".\nFrederick died in 1190 while on the Third Crusade and was succeeded by his son, Henry VI. Elected king even before his father's death, Henry went to Rome to be crowned emperor. He married Queen Constance of Sicily, and deaths in his wife's family gave him claim of succession and possession of the Kingdom of Sicily in 1189 and 1194 respectively, a source of vast wealth. Henry failed to make royal and Imperial succession hereditary, but in 1196 he succeeded in gaining a pledge that his infant son Frederick would receive the German crown. Faced with difficulties in Italy and confident that he would realize his wishes in Germany at a later date, Henry returned to the south, where it appeared he might unify the peninsula under the Hohenstaufen name. After a series of military victories, however, he fell ill and died of natural causes in Sicily in 1197. His underage son Frederick could only succeed him in Sicily and Malta, while in the Empire the struggle between the House of Staufen and the House of Welf erupted once again.\n\nBecause the election of a three-year-old boy to be German king appeared likely to make orderly rule difficult, the boy's uncle, Duke Philip of Swabia, brother of late Henry VI, was designated to serve in his place. Other factions however favoured a Welf candidate. In 1198, two rival kings were chosen: the Hohenstaufen Philip of Swabia and the son of the deprived Duke Henry the Lion, the Welf Otto IV. A long civil war began; Philip was about to win when he was murdered by the Bavarian count palatine Otto VIII of Wittelsbach in 1208. Pope Innocent III initially had supported the Welfs, but when Otto, now sole elected monarch, moved to appropriate Sicily, Innocent changed sides and accepted young Frederick II and his ally, King Philip II of France, who defeated Otto at the 1214 Battle of Bouvines. Frederick had returned to Germany in 1212 from Sicily, where he had grown up, and was elected king in 1215. When Otto died in 1218, Fredrick became the undisputed ruler, and in 1220 was crowned Holy Roman Emperor.\n\nPhilip changed the coat of arms from a black lion on a gold shield to three leopards, probably derived from the arms of his Welf rival Otto IV.\n\nThe conflict between the Staufer dynasty and the Welf had irrevocably weakened the Imperial authority and the Norman kingdom of Sicily became the base for Staufer rule.\n\nEmperor Frederick II spent little time in Germany as his main concerns lay in Southern Italy. He founded the University of Naples in 1224 to train future state officials and reigned over Germany primarily through the allocation of royal prerogatives, leaving the sovereign authority and imperial estates to the ecclesiastical and secular princes. He made significant concessions to the German nobles, such as those put forth in an imperial statute of 1232, which made princes virtually independent rulers within their territories. These measures favoured the further fragmentation of the Empire.\n\nBy the 1226 Golden Bull of Rimini, Frederick had assigned the military order of the Teutonic Knights to complete the conquest and conversion of the Prussian lands. A reconciliation with the Welfs took place in 1235, whereby Otto the Child, grandson of the late Saxon duke Henry the Lion, was named Duke of Brunswick and Lüneburg. The power struggle with the popes continued and resulted in Fredrick's excommunication in 1227. In 1239, Pope Gregory IX excommunicated Fredrick again, and in 1245 he was condemned as a heretic by a church council. Although Frederick was one of the most energetic, imaginative, and capable rulers of the time, he was not concerned with drawing the disparate forces in Germany together. His legacy was thus that local rulers had more authority after his reign than before it. The clergy also had become more powerful.\n\nBy the time of Frederick's death in 1250, little centralized power remained in Germany. The Great Interregnum, a period in which there were several elected rival kings, none of whom was able to achieve any position of authority, followed the death of Frederick's son King Conrad IV of Germany in 1254. The German princes vied for individual advantage and managed to strip many powers away from the diminished monarchy. Rather than establish sovereign states however, many nobles tended to look after their families. Their many male heirs created more and smaller estates, and from a largely free class of officials previously formed, many of these assumed or acquired hereditary rights to administrative and legal offices. These trends compounded political fragmentation within Germany. The period was ended in 1273 with the election of Rudolph of Habsburg, a godson of Frederick.\n\nConrad IV was succeeded as duke of Swabia by his only son, two-year-old Conradin. By this time, the office of duke of Swabia had been fully subsumed into the office of the king, and without royal authority had become meaningless. In 1261, attempts to elect young Conradin king were unsuccessful. He also had to defend Sicily against an invasion, sponsored by Pope Urban IV (Jacques Pantaléon) and Pope Clement IV (Guy Folques), by Charles of Anjou, a brother of the French king. Charles had been promised by the popes the Kingdom of Sicily, where he would replace the relatives of Frederick II. Charles had defeated Conradin's uncle Manfred, King of Sicily in the Battle of Benevento on 26 February 1266. The king himself, refusing to flee, rushed into the midst of his enemies and was killed. Conradin's campaign to retake control ended with his defeat in 1268 at the Battle of Tagliacozzo, after which he was handed over to Charles, who had him publicly executed at Naples. With Conradin, the direct line of the Dukes of Swabia finally ceased to exist, though most of the later emperors were descended from the Staufer dynasty indirectly.\n\nDuring the political decentralization of the late Staufer period, the population had grown from an estimated 8 million in 1200 to about 14 million in 1300, and the number of towns increased tenfold. The most heavily urbanized areas of Germany were located in the south and the west. Towns often developed a degree of independence, but many were subordinate to local rulers if not immediate to the emperor. Colonization of the east also continued in the thirteenth century, most notably through the efforts of the Teutonic Knights. German merchants also began trading extensively on the Baltic.\n\nThe first ruling Hohenstaufen, Conrad III, like the last one, Conrad IV, was never crowned emperor. After a 20-year period (Great interregnum 1254–1273), the first Habsburg was elected king.\n\n\"Note: The following kings are already listed above as German Kings\"\n\n\"Note: Some of the following kings are already listed above as German Kings\"\n\n\"Note: Some of the following dukes are already listed above as German Kings\"\n\nModern history\n", "id": "13805", "title": "Hohenstaufen"}
{"url": "https://en.wikipedia.org/wiki?curid=13806", "text": "History of Malaysia\n\nMalaysia is a Southeast Asian country located on strategic sea-lane that exposes it to global trade and foreign culture. Hinduism from India and Buddhism from China dominated early regional history, reaching their peak during the reign of the Sumatra-based Srivijaya civilisation, whose influence extended through Sumatra, Java, the Malay Peninsula and much of Borneo from the 7th to the 13th centuries.\n\nAlthough Muslims had passed through the Malay Peninsula as early as the 10th century, it was not until the 14th century that Islam first firmly established itself. The adoption of Islam in the 14th century saw the rise of a number of sultanates, the most prominent of which was the Sultanate of Malacca. Islam had a profound influence on the Malay people, but has also been influenced by them. The Portuguese were the first European colonial powers to establish themselves on the Malay Peninsula and Southeast Asia, capturing Malacca in 1511, followed by the Dutch in 1641. However, it was the British who, after initially establishing bases at Jesselton, Kuching, Penang and Singapore, ultimately secured their hegemony across the territory that is now Malaysia. The Anglo-Dutch Treaty of 1824 defined the boundaries between British Malaya and the Netherlands East Indies (which became Indonesia). A fourth phase of foreign influence was immigration of Chinese and Indian workers to meet the needs of the colonial economy created by the British in the Malay Peninsula and Borneo.\n\nJapanese invasion during World War II ended British domination in Malaysia. The subsequent occupation of Malaya, North Borneo and Sarawak from 1942 to 1945 unleashed nationalism. In the Peninsula, the Malayan Communist Party took up arms against the British. A tough military response was needed to end the insurgency and bring about the establishment of an independent, multi-racial Federation of Malaya on 31 August 1957. On 31 August 1963, the British territories in North Borneo and Singapore were granted independence and formed Malaysia with the Peninsular states on 16 September 1963. Approximately two years later, the Malaysian parliament passed a bill to separate Singapore from the Federation. A confrontation with Indonesia occurred in the early-1960s. Race riots in 1969 led to the imposition of emergency rule, and a curtailment of political life and civil liberties which has never been fully reversed. Since 1970 the \"National Front coalition\" headed by United Malays National Organisation (UMNO) has governed Malaysia. Economic growth dramatically increased living standards by the 1990s. This growing prosperity helped minimise political discontent.\n\nStone hand-axes from early hominoids, probably Homo erectus, have been unearthed in Lenggong. They date back 1.83 million years, the oldest evidence of hominid habitation in Southeast Asia. The earliest evidence of modern human habitation in Malaysia is the 40,000-year-old skull excavated from the Niah Caves in today's Sarawak, nicknamed \"Deep Skull\". It was excavated from a deep trench uncovered by Barbara and Tom Harrisson (a British ethnologist) in 1958. this is also the oldest modern human skull in Southeast Asia. The skull probably belongs to a 16-to 17-year-old adolescent girl. The first foragers visited the West Mouth of Niah Caves (located southwest of Miri) 40,000 years ago when Borneo was connected to the mainland of Southeast Asia. The landscape around the Niah Caves was drier and more exposed than it is now. Prehistorically, the Niah Caves were surrounded by a combination of closed forests with bush, parkland, swamps, and rivers. The foragers were able to survive in the rainforest through hunting, fishing, and gathering molluscs and edible plants. Mesolithic and Neolithic burial sites have also been found in the area. The area around the Niah Caves has been designated the Niah National Park.\n\nA study of Asian genetics points to the idea that the original humans in East Asia came from Southeast Asia. The oldest complete skeleton found in Malaysia is 11,000-year-old Perak Man unearthed in 1991. The indigenous groups on the peninsula can be divided into three ethnicities, the Negritos, the Senoi, and the proto-Malays. The first inhabitants of the Malay Peninsula were most probably Negritos. These Mesolithic hunters were probably the ancestors of the Semang, an ethnic Negrito group who have a long history in the Malay Peninsula.\n\nThe Senoi appear to be a composite group, with approximately half of the maternal mitochondrial DNA lineages tracing back to the ancestors of the Semang and about half to later ancestral migrations from Indochina. Scholars suggest they are descendants of early Austroasiatic-speaking agriculturalists, who brought both their language and their technology to the southern part of the peninsula approximately 4,000 years ago. They united and coalesced with the indigenous population.\n\nThe Proto Malays have a more diverse origin and had settled in Malaysia by 1000 BC. Although they show some connections with other inhabitants in Maritime Southeast Asia, some also have an ancestry in Indochina around the time of the Last Glacial Maximum about 20,000 years ago. Anthropologists support the notion that the Proto-Malays originated from what is today Yunnan, China. This was followed by an early-Holocene dispersal through the Malay Peninsula into the Malay Archipelago. Around 300 BC, they were pushed inland by the Deutero-Malays, an Iron Age or Bronze Age people descended partly from the Chams of Cambodia and Vietnam. The first group in the peninsula to use metal tools, the Deutero-Malays were the direct ancestors of today's Malaysian Malays, and brought with them advanced farming techniques. The Malays remained politically fragmented throughout the Malay archipelago, although a common culture and social structure was shared.\n\nIn the first millennium CE, Malays became the dominant race on the peninsula. The small early states that were established were greatly influenced by Indian culture. Indian influence in the region dates back to at least the 3rd century BCE. South Indian culture was spread to Southeast Asia by the south Indian Pallava dynasty in the 4th and 5th century.\n\nThe Malay Peninsula was known to ancient Tamils as \"Suvarnadvipa\" or the \"Golden Peninsula\". It was shown on Ptolemy's map as the \"Golden Khersonese\". He referred to the Straits of Melaka as \"Sinus Sabaricus\". Trade relations with China and India were established in the 1st century BC. Shards of Chinese pottery have been found in Borneo dating from the 1st century following the southward expansion of the Han Dynasty. In the early centuries of the first millennium, the people of the Malay Peninsula adopted the Indian religions of Hinduism and Buddhism, religions which had a major effect on the language and culture of those living in Malaysia. The Sanskrit writing system was used as early as the 4th century.\n\nThere were numerous Malay kingdoms in the 2nd and 3rd century, as many as 30, mainly based on the Eastern side of the Malay peninsula. Among the earliest kingdoms known to have been based in what is now Malaysia is the ancient empire of Langkasuka, located in the northern Malay Peninsula and based somewhere in Kedah. It was closely tied to Funan in Cambodia, which also ruled part of northern Malaysia until the 6th century. In the 5th century, the Kingdom of Pahang was mentioned in the \"Book of Song\". According to the Sejarah Melayu (\"Malay Annals\"), the Khmer prince Raja Ganji Sarjuna founded the kingdom of Gangga Negara (modern-day Beruas, Perak) in the 700s. Chinese chronicles of the 5th century CE speak of a great port in the south called Guantoli, which is thought to have been in the Straits of Malacca. In the 7th century, a new port called Shilifoshi is mentioned, and this is believed to be a Chinese rendering of Srivijaya.\n\nBetween the 7th and the 13th century, much of the Malay peninsula was under the Buddhist Srivijaya empire. The site of Srivijaya's centre is thought be at a river mouth in eastern Sumatra, based near what is now Palembang. For over six centuries the Maharajahs of Srivijaya ruled a maritime empire that became the main power in the archipelago. The empire was based around trade, with local kings (dhatus or community leaders) swearing allegiance to the central lord for mutual profit.\n\nThe relation between Srivijaya and the Chola Empire of south India was friendly during the reign of Raja Raja Chola I but during the reign of Rajendra Chola I the Chola Empire attacked Srivijaya cities.\nIn 1025 and 1026 Gangga Negara was attacked by Rajendra Chola I of the Chola Empire, the Tamil emperor who is now thought to have laid Kota Gelanggi to waste. Kedah—known as \"Kedaram\", \"Cheh-Cha\" (according to \"I-Ching\") or \"Kataha\", in ancient Pallava or Sanskrit—was in the direct route of the invasions and was ruled by the Cholas from 1025. A second invasion was led by Virarajendra Chola of the Chola dynasty who conquered Kedah in the late 11th century. The senior Chola's successor, Vira Rajendra Chola, had to put down a Kedah rebellion to overthrow other invaders. The coming of the Chola reduced the majesty of Srivijaya, which had exerted influence over Kedah, Pattani and as far as Ligor. During the reign of Kulothunga Chola I Chola overlordship was established over the Sri Vijaya province kedah in the late 11th century. The expedition of the Chola Emperors had such a great impression to the Malay people of the medieval period that their name was mentioned in the corrupted form as Raja Chulan in the medieval Malay chronicle Sejarah Melaya. Even today the Chola rule is remembered in Malaysia as many Malaysian princes have names ending with Cholan or Chulan, one such was the Raja of Perak called Raja Chulan.\nPattinapalai, a Tamil poem of the 2nd century CE, describes goods from Kedaram heaped in the broad streets of the Chola capital. A 7th-century Indian drama, \"Kaumudhimahotsva\", refers to Kedah as Kataha-nagari. The \"Agnipurana\" also mentions a territory known as Anda-Kataha with one of its boundaries delineated by a peak, which scholars believe is Gunung Jerai. Stories from the \"Katasaritasagaram\" describe the elegance of life in Kataha. The Buddhist kingdom of Ligor took control of Kedah shortly after. Its king Chandrabhanu used it as a base to attack Sri Lanka in the 11th century and ruled the northern parts, an event noted in a stone inscription in Nagapattinum in Tamil Nadu and in the Sri Lankan chronicles, \"Mahavamsa\".\n\nAt times, the Khmer kingdom, the Siamese kingdom, and even Cholas kingdom tried to exert control over the smaller Malay states. The power of Srivijaya declined from the 12th century as the relationship between the capital and its vassals broke down. Wars with the Javanese caused it to request assistance from China, and wars with Indian states are also suspected. In the 11th century, the centre of power shifted to Melayu, a port possibly located further up the Sumatran coast at near the Jambi River. The power of the Buddhist Maharajas was further undermined by the spread of Islam. Areas which were converted to Islam early, such as Aceh, broke away from Srivijaya’s control. By the late 13th century, the Siamese kings of Sukhothai had brought most of Malaya under their rule. In the 14th century, the Hindu Java-based Majapahit empire came into possession of the peninsula.\n\nAn excavation by Tom Harrisson in 1949 unearthed a series of Chinese ceramics at Santubong (near Kuching) that date to the Tang and the Song dynasties in the 8th to 13th century AD. It is possible that Santubong was an important seaport in Sarawak during the period, but its importance declined during the Yuan dynasty, and the port was deserted during the Ming dynasty. Other archaeological sites in Sarawak can be found inside the Kapit, Song, Serian, and Bau districts.\n\nIslam came to the Malay Archipelago through the Arab and Indian traders in the 13th century, ending the age of Hinduism and Buddhism. It arrived in the region gradually, and became the religion of the elite before it spread to the commoners. The Islam in Malaysia was influenced by previous religions and was originally not orthodox.\n\nThe port of Malacca on the west coast of the Malay Peninsula was founded in 1402 by Parameswara, a Srivijaya prince fleeing Temasek (now Singapore),<ref name=\"Marshall\"/Parameswara in particular sailed to Temasek to escape persecution. There he came under the protection of Temagi, a Malay chief from Patani who was appointed by the king of Siam as regent of Temasek. Within a few days, Parameswara killed Temagi and appointed himself regent. Some five years later he had to leave Temasek, due to threats from Siam. During this period, a Javanese fleet from Majapahit attacked Temasek.\nParameswara headed north to found a new settlement. At Muar, Parameswara considered siting his new kingdom at either Biawak Busuk or at Kota Buruk. Finding that the Muar location was not suitable, he continued his journey northwards. Along the way, he reportedly visited Sening Ujong (former name of present-day Sungai Ujong) before reaching a fishing village at the mouth of the Bertam River (former name of the Melaka River), and founded what would become the Malacca Sultanate. Over time this developed into modern-day Malacca Town. According to the \"Malay Annals\", here Parameswara saw a mouse deer outwitting a dog resting under a Malacca tree. Taking this as a good omen, he decided to establish a kingdom called Malacca. He built and improved facilities for trade. The Malacca Sultanate is commonly considered the first independent state in the peninsula.\n\nAt the time of Melaka's founding, the emperor of Ming Dynasty China was sending out fleets of ships to expand trade. Admiral Zheng He called at Malacca and brought Parameswara with him on his return to China, a recognition of his position as legitimate ruler of Malacca. In exchange for regular tribute, the Chinese emperor offered Melaka protection from the constant threat of a Siamese attack. The Chinese and Indians who settled in the Malay Peninsula before and during this period are the ancestors of today's Baba-Nyonya and Chetti community. According to one theory, Parameswara became a Muslim when he married a Princess of Pasai and he took the fashionable Persian title \"Shah\", calling himself Iskandar Shah. Chinese chronicles mention that in 1414, the son of the first ruler of Malacca visited the Ming emperor to inform them that his father had died. Parameswara's son was then officially recognised as the second ruler of Melaka by the Chinese Emperor and styled Raja Sri Rama Vikrama, Raja of Parameswara of Temasek and Malacca and he was known to his Muslim subjects as Sultan Sri Iskandar Zulkarnain Shah or Sultan Megat Iskandar Shah. He ruled Malacca from 1414 to 1424. Through the influence of Indian Muslims and, to a lesser extent, Hui people from China, Islam became increasingly common during the 15th century.\n\nAfter an initial period paying tribute to the Ayutthaya, the kingdom rapidly assumed the place previously held by Srivijaya, establishing independent relations with China, and exploiting its position dominating the Straits to control the China-India maritime trade, which became increasingly important when the Mongol conquests closed the overland route between China and the west.\nWithin a few years of its establishment, Malacca officially adopted Islam. Parameswara became a Muslim, and due to the fact Malacca was under a Muslim Prince the conversion of Malays to Islam accelerated in the 15th century. The political power of the Malacca Sultanate helped Islam’s rapid spread through the archipelago. Malacca was an important commercial centre during this time, attracting trade from around the region. By the start of the 16th century, with the Malacca Sultanate in the Malay peninsula and parts of Sumatra, the Demak Sultanate in Java, and other kingdoms around the Malay archipelago increasingly converting to Islam, it had become the dominant religion among Malays, and reached as far as the modern-day Philippines, leaving Bali as an isolated outpost of Hinduism today.\n\nMalacca's reign lasted little more than a century, but during this time became the established centre of Malay culture. Most future Malay states originated from this period. Malacca became a cultural centre, creating the matrix of the modern Malay culture: a blend of indigenous Malay and imported Indian, Chinese and Islamic elements. Malacca's fashions in literature, art, music, dance and dress, and the ornate titles of its royal court, came to be seen as the standard for all ethnic Malays. The court of Malacca also gave great prestige to the Malay language, which had originally evolved in Sumatra and been brought to Malacca at the time of its foundation. In time Malay came to be the official language of all the Malaysian states, although local languages survived in many places. After the fall of Malacca, the Sultanate of Brunei became the major centre of Islam.\n\nFrom the 15th century onwards, the Portuguese started seeking a maritime route towards Asia. In 1511, Afonso de Albuquerque led an expedition to Malaya which seized Malacca with the intent of using it as a base for activities in southeast Asia. This was the first colonial claim on what is now Malaysia. The son of the last Sultan of Malacca, Sultan Alauddin Riayat Shah II fled to the southern tip of the peninsula, where he founded a state that which became the Sultanate of Johor. Another son created the Perak Sultanate to the north. By the late 16th century, the tin mines of northern Malaya had been discovered by European traders, and Perak grew wealthy on the proceeds of tin exports. Portuguese influence was strong, as they aggressively tried to convert the population of Malacca to Catholicism. In 1571, the Spanish captured Manila and established a colony in the Philippines, reducing the Sultanate of Brunei's power.\nAfter the fall of Malacca to Portugal, the Johor Sultanate and the Sultanate of Aceh on northern Sumatra moved to fill the power vacuum left behind. The three powers struggled to dominate the Malay peninsula and the surrounding islands. Johor founded in the wake of Malacca's conquest grew powerful enough to rival the Portuguese, although it was never able to recapture the city. Instead it expanded in other directions, building in 130 years one of the largest Malay states. In this time the numerous attempts to recapture Malacca led to a strong backlash from the Portuguese, whose raids even reached Johor's capital of Johor Lama in 1587.\n\nIn 1607, the Sultanate of Aceh rose as the powerful and wealthiest state in the Malay archipelago. Under Iskandar Muda's reign, the sultanate's control was extended over a number of Malay states. A notable conquest was Perak, a tin-producing state on the Peninsula. In Iskandar Muda's disastrous campaign against Malacca in 1629, the combined Portuguese and Johor forces managed to destroy all the ships of his formidable fleet and 19,000 troops according to a Portuguese account. Aceh forces were not destroyed, however, as Aceh was able to conquer Kedah within the same year and took many of its citizens to Aceh. The Sultan's son-in-law, Iskandar Thani, the former prince of Pahang later became Iskandar Muda's successor. The conflict over control of the straits went on until 1641, when the Dutch (allied to Johor) gained control of Malacca.\n\nIn the early 17th century, the Dutch East India Company (\"Vereenigde Oost-Indische Compagnie\", or VOC) was established. During this time the Dutch were at war with Spain, which absorbed the Portuguese Empire due to the Iberian Union. The Dutch expanded across the archipelago, forming an alliance with Johor and using this to push the Portuguese out of Malacca in 1641. Backed by the Dutch, Johor established a loose hegemony over the Malay states, except Perak, which was able to play off Johor against the Siamese to the north and retain its independence. The Dutch did not interfere in local matters in Malacca, but at the same time diverted most trade to its colonies on Java.\n\nThe weakness of the small coastal Malay states led to the immigration of the Bugis, escaping from Dutch colonisation of Sulawesi, who established numerous settlements on the peninsula which they used to interfere with Dutch trade. They seized control of Johor following the assassination of the last Sultan of the old Melaka royal line in 1699. Bugis expanded their power in the states of Johor, Kedah, Perak, and Selangor. The Minangkabau from central Sumatra migrated into Malaya, and eventually established their own state in Negeri Sembilan. The fall of Johor left a power vacuum on the Malay Peninsula which was partly filled by the Siamese kings of Ayutthaya kingdom, who made the five northern Malay states—Kedah, Kelantan, Patani, Perlis, and Terengganu — their vassals. Johor’s eclipse also left Perak as the unrivalled leader of the Malay states.\n\nThe economic importance of Malaya to Europe grew rapidly during the 18th century. The fast-growing tea trade between China and United Kingdom increased the demand for high-quality Malayan tin, which was used to line tea-chests. Malayan pepper also had a high reputation in Europe, while Kelantan and Pahang had gold mines. The growth of tin and gold mining and associated service industries led to the first influx of foreign settlers into the Malay world — initially Arabs and Indians, later Chinese.\n\nDuring the 16th century, the Kuching area was known to Portuguese cartographers as \"Cerava\", one of the five great seaports on the island of Borneo. It was under the influence of the Bruneian Empire and was self-governed under Sultan Tengah. By the early 19th century, Sarawak had become a loosely governed territory under the control of the Brunei Sultanate. The Bruneian Empire had authority only along the coastal regions of Sarawak held by semi-independent Malay leaders. Meanwhile, the interior of Sarawak suffered from tribal wars fought by Iban, Kayan, and Kenyah peoples, who aggressively fought to expand their territories. Following the discovery of antimony ore in the Kuching region, Pangeran Indera Mahkota (a representative of the Sultan of Brunei) began to develop the territory between 1824 and 1830. When antimony production increased, the Brunei Sultanate demanded higher taxes from Sarawak; this led to civil unrest and chaos. In 1839, Sultan Omar Ali Saifuddin II (1827–1852), ordered his uncle Pangeran Muda Hashim to restore order. It was around this time that James Brooke (who would later become the first White Rajah of Sarawak) arrived in Sarawak, and Pangeran Muda Hashim requested his assistance in the matter, but Brooke refused. However, he agreed to a further request during his next visit to Sarawak in 1841. Pangeran Muda Hashim signed a treaty in 1841 surrendering Sarawak to Brooke. On 24 September 1841, Pangeran Muda Hashim bestowed the title of governor on James Brooke. This appointment was later confirmed by the Sultan of Brunei in 1842. In 1843, James Brooke decided to create a pro-British Brunei government by installing Pangeran Muda Hashim into the Brunei Court as he would be taking the Brooke's advice. James Brooke forced Brunei to appoint Hashim under the guns of East India Company's steamer \"Phlegethon\". The Brunei Court was unhappy with Hashim's appointment and had him assassinated in 1845. In retaliation, James Brooke attacked the Kampong Ayer, the capital of Brunei. After the incident, the Sultan of Brunei sent an apology letter to Queen Victoria. The sultan also confirmed James Brooke's possession of Sarawak and his mining rights of antimony without paying tribute to Brunei. In 1846 Brooke effectively became the Rajah of Sarawak and founded the White Rajah Dynasty of Sarawak.\n\nEnglish traders had been present in Malay waters since the 17th century. However, with the arrival of the British, European power became dominant in Malaysia. Before the mid-19th-century British interests in the region were predominantly economic, with little interest in territorial control. Already the most powerful coloniser in India, the British were looking towards southeast Asia for new resources. The growth of the China trade in British ships increased the East India Company’s desire for bases in the region. Various islands were used for this purpose, but the first permanent acquisition was Penang, leased from the Sultan of Kedah in 1786. This was followed soon after by the leasing of a block of territory on the mainland opposite Penang (known as Province Wellesley). In 1795, during the Napoleonic Wars, the British with the consent of the Netherlands occupied Dutch Melaka to forestall possible French encroachment in the area.\n\nWhen Malacca was handed back to the Dutch in 1815, the British governor, Stamford Raffles, looked for an alternative base, and in 1819 he acquired Singapore from the Sultan of Johor. The exchange of the British colony of Bencoolen for Malacca with the Dutch left the British as the sole colonial power on the peninsula. The territories of the British were set up as free ports, attempting to break the monopoly held by other colonial powers at the time, and making them large bases of trade. They allowed Britain to control all trade through the straits of Malacca. British influence was increased by Malayan fears of Siamese expansionism, to which Britain made a useful counterweight. During the 19th century the Malay Sultans aligned themselves with the British Empire, due to the benefits of associations with the British and the belief in superior British civilisation.\n\nIn 1824, British hegemony in Malaya (before the name Malaysia) was formalised by the Anglo-Dutch Treaty, which divided the Malay archipelago between Britain and the Netherlands. The Dutch evacuated Melaka and renounced all interest in Malaya, while the British recognised Dutch rule over the rest of the East Indies. By 1826 the British controlled Penang, Malacca, Singapore, and the island of Labuan, which they established as the crown colony of the Straits Settlements, administered first under the East India Company until 1867, when they were transferred to the Colonial Office in London.\n\nInitially, the British followed a policy of non-intervention in relations between the Malay states. The commercial importance of tin mining in the Malay states to merchants in the Straits Settlements led to infighting between the aristocracy on the peninsula. The destabilisation of these states damaged the commerce in the area, causing British intervention. The wealth of Perak’s tin mines made political stability there a priority for British investors, and Perak was thus the first Malay state to agree to the supervision of a British resident. British gunboat diplomacy was employed to bring about a peaceful resolution to civil disturbances caused by Chinese and Malay gangsters employed in a political fight between Ngah Ibrahim and Raja Muda Abdullah. The Pangkor Treaty of 1874 paved the way for the expansion of British influence in Malaya. The British concluded treaties with some Malay states, installing “residents” who advised the Sultans and soon became the effective rulers of their states. These advisors held power in everything except to do with Malay religion and customs.\n\nJohor alone resisted, by modernising and giving British and Chinese investors legal protection. By the turn of the 20th century, the states of Pahang, Selangor, Perak, and Negeri Sembilan, known together as the Federated Malay States, had British advisors. In 1909 the Siamese kingdom was compelled to cede Kedah, Kelantan, Perlis and Terengganu, which already had British advisors, over to the British. Sultan Abu Bakar of Johor and Queen Victoria were personal acquaintances who recognised each other as equals. It was not until 1914 that Sultan Abu Bakar's successor, Sultan Ibrahim, accepted a British adviser. The four previously Thai states and Johor were known as the Unfederated Malay States. The states under the most direct British control developed rapidly, becoming the largest suppliers in the world of first tin, then rubber.\n\nBy 1910, the pattern of British rule in the Malay lands was established. The Straits Settlements were a Crown colony, ruled by a governor under the supervision of the Colonial Office in London. Their population was about half Chinese, but all residents, regardless of race, were British subjects. The first four states to accept British residents, Perak, Selangor, Negeri Sembilan, and Pahang, were termed the Federated Malay States: while technically independent, they were placed under a Resident-General in 1895, making them British colonies in all but name. The Unfederated Malay States (Johore, Kedah, Kelantan, Perlis, and Terengganu) had a slightly larger degree of independence, although they were unable to resist the wishes of their British residents for long. Johor, as Britain's closest ally in Malay affairs, had the privilege of a written constitution, which gave the Sultan the right to appoint his own Cabinet, but he was generally careful to consult the British first.\n\nDuring the late 19th century the British also gained control of the north coast of Borneo, where Dutch rule had never been established. Development on the Peninsula and Borneo were generally separate until the 19th century. The eastern part of this region (now Sabah) was under the nominal control of the Sultan of Sulu, who later became a vassal of the Spanish East Indies. The rest was the territory of the Sultanate of Brunei. In 1841, British adventurer James Brooke helped the Sultan of Brunei suppress a revolt, and in return received the title of raja and the right to govern the Sarawak River District. In 1846, his title was recognised as hereditary, and the \"White Rajahs\" began ruling Sarawak as a recognised independent state. The Brookes expanded Sarawak at the expense of Brunei.\n\nIn 1881, the British North Borneo Company was granted control of the territory of British North Borneo, appointing a governor and legislature. It was ruled from the office in London. Its status was similar to that of a British Protectorate, and like Sarawak it expanded at the expense of Brunei. Until the Philippine independence on 1946, seven British-controlled islands in the north-eastern part of Borneo named Turtle Islands and Cagayan de Tawi-Tawi were ceded to the Philippine government by the Crown colony government of North Borneo. The Philippines then under its irredentism motive since the administration of President Diosdado Macapagal laying claim to eastern Sabah in a basis the territory was part of the present-defunct Sultanate of Sulu’s territory. In 1888, what was left of Brunei was made a British protectorate, and in 1891 another Anglo-Dutch treaty formalised the border between British and Dutch Borneo.\n\nUnlike some colonial powers, the British always saw their empire as primarily an economic concern, and its colonies were expected to turn a profit for British shareholders. Malaya’s obvious attractions were its tin and gold mines, but British planters soon began to experiment with tropical plantation crops—tapioca, gambier, pepper, and coffee. But in 1877 the rubber plant was introduced from Brazil, and rubber soon became Malaya’s staple export, stimulated by booming demand from European industry. Rubber was later joined by palm oil as an export earner. All these industries required a large and disciplined labour force, and the British did not regard the Malays as reliable workers. The solution was the importation of plantation workers from India, mainly Tamil-speakers from South India. The mines, mills and docks also attracted a flood of immigrant workers from southern China. Soon towns like Singapore, Penang, and Ipoh were majority Chinese, as was Kuala Lumpur, founded as a tin-mining centre in 1857. By 1891, when Malaya’s first census was taken, Perak and Selangor, the main tin-mining states, had Chinese majorities.\n\nThe Chinese mostly arrived poor; yet, their belief in industriousness and frugality, their emphasis in their children's education and their maintenance of Confucian family hierarchy, as well as their voluntary connection with tightly knit networks of mutual aid societies (run by \"Hui-Guan\" 會館, or non-profit organisations with nominal geographic affiliations from different parts of China) all contributed to their prosperity. In the 1890s Yap Ah Loy, who held the title of Kapitan China of Kuala Lumpur, was the richest man in Malaya, owning a chain of mines, plantations and shops. Malaya’s banking and insurance industries were run by the Chinese from the start, and Chinese businesses, usually in partnership with London firms, soon had a stranglehold on the economy. Since the Malay Sultans tended to spend well beyond their means, they were soon indebted to Chinese bankers, and this gave the Chinese political as well as economic leverage. At first the Chinese immigrants were mostly men, and many intended to return home when they had made their fortunes. Many did go home, but many more stayed. At first they married Malay women, producing a community of Sino-Malayans or baba people, but soon they began importing Chinese brides, establishing permanent communities and building schools and temples.\n\nThe Indians were initially less successful, since unlike the Chinese they came mainly as indentured labourers to work in the rubber plantations, and had few of the economic opportunities that the Chinese had. They were also a less united community, since they were divided between Hindus and Muslims and along lines of language and caste. An Indian commercial and professional class emerged during the early 20th century, but the majority of Indians remained poor and uneducated in rural ghettos in the rubber-growing areas.\n\nTraditional Malay society had great difficulty coping with both the loss of political sovereignty to the British and of economic power to the Chinese. By the early 20th century it seemed possible that the Malays would become a minority in their own country. The Sultans, who were seen as collaborators with both the British and the Chinese, lost some of their traditional prestige, particularly among the increasing number of Malays with a western education, but the mass of rural Malays continued to revere the Sultans and their prestige was thus an important prop for colonial rule. A small class of Malay nationalist intellectuals began to emerge during the early 20th century, and there was also a revival of Islam in response to the perceived threat of other imported religions, particularly Christianity. In fact few Malays converted to Christianity, although many Chinese did. The northern regions, which were less influenced by western ideas, became strongholds of Islamic conservatism, as they have remained.\n\nThe one consolation to Malay pride was that the British allowed them a virtual monopoly of positions in the police and local military units, as well as a majority of those administrative positions open to non-Europeans. While the Chinese mostly built and paid for their own schools and colleges, importing teachers from China, the colonial government fostered education for Malays, opening Malay College in 1905 and creating the Malay Administrative Service in 1910. (The college was dubbed “Bab ud-Darajat” – the Gateway to High Rank.) A Malay Teachers College followed in 1922, and a Malay Women’s Training College in 1935. All this reflected the official British policy that Malaya belonged to the Malays, and that the other races were but temporary residents. This view was increasingly out of line with reality, and contained the seeds of much future trouble.\n\nThe Malay teacher's college had lectures and writings that nurtured Malay nationalism and anti-colonialist sentiments. Due to this it is known as the birthplace of Malay nationalism. In 1938, Ibrahim Yaacob, an alumnus of Sultan Idris College, established the Kesatuan Melayu Muda (Young Malays Union or KMM) in Kuala Lumpur. It was the first nationalist political organisation in British Malaya, advocating for the union of all Malays regardless of origin, and fighting for Malay rights and against British Imperialism. A specific ideal the KMM held was \"Panji Melayu Raya\", which called for the unification of British Malaya and Dutch East Indies.\n\nIn the years before World War II, the British were concerned with finding the balance between a centralised state and maintaining the power of the Sultans in Malaya. There were no moves to give Malaya a unitary government, and in fact in 1935 the position of Resident-General of the Federated States was abolished, and its powers decentralised to the individual states. With their usual tendency to racial stereotyping, the British regarded the Malays as amiable but unsophisticated and rather lazy, incapable of self-government, although making good soldiers under British officers. They regarded the Chinese as clever but dangerous—and indeed during the 1920s and 1930s, reflecting events in China, the Chinese Nationalist Party (the Kuomintang) and the Communist Party of China built rival clandestine organisations in Malaya, leading to regular disturbances in the Chinese towns. The British saw no way that Malaya’s disparate collection of states and races could become a nation, let alone an independent one.\n\nMalaya saw little action during World War I, except for the sinking of the Russian cruiser Zhemchug by the German cruiser Emden on 28 October 1914 during the Battle of Penang.\n\nThe outbreak of war in the Pacific in December 1941 found the British in Malaya completely unprepared. During the 1930s, anticipating the rising threat of Japanese naval power, they had built a great naval base at Singapore, but never anticipated an invasion of Malaya from the north. Because of the demands of the war in Europe, there was virtually no British air capacity in the Far East. The Japanese were thus able to attack from their bases in French Indo-China with impunity, and despite stubborn resistance from British, Australian, and Indian forces, they overran Malaya in two months. Singapore, with no landward defences, no air cover, and no water supply, was forced to surrender in February 1942, doing irreparable damage to British prestige. British North Borneo and Brunei were also occupied.\n\nThe Japanese had a racial policy just as the British did. They regarded the Malays as a colonial people liberated from British imperialist rule, and fostered a limited form of Malay nationalism, which gained them some degree of collaboration from the Malay civil service and intellectuals. (Most of the Sultans also collaborated with the Japanese, although they maintained later that they had done so unwillingly.) The Malay nationalist Kesatuan Melayu Muda, advocates of \"Melayu Raya\", collaborated with the Japanese, based on the understanding that Japan would unite the Dutch East Indies, Malaya and Borneo and grant them independence. The occupiers regarded the Chinese, however, as enemy aliens, and treated them with great harshness: during the so-called \"sook ching\" (purification through suffering), up to 80,000 Chinese in Malaya and Singapore were killed. Chinese businesses were expropriated and Chinese schools either closed or burned down. Not surprisingly the Chinese, led by the Malayan Communist Party (MCP), became the backbone of the Malayan Peoples' Anti-Japanese Army (MPAJA), which with British assistance became the most effective resistance force in the occupied Asian countries.\n\nAlthough the Japanese argued that they supported Malay nationalism, they offended Malay nationalism by allowing their ally Thailand to re-annex the four northern states, Kedah, Perlis, Kelantan, and Terengganu that had been surrendered to the British in 1909. The loss of Malaya’s export markets soon produced mass unemployment which affected all races and made the Japanese increasingly unpopular.\n\nDuring occupation, ethnic tensions were raised and nationalism grew. The Malayans were thus on the whole glad to see the British back in 1945, but things could not remain as they were before the war, and a stronger desire for independence grew. Britain was bankrupt and the new Labour government was keen to withdraw its forces from the East as soon as possible. Colonial self-rule and eventual independence were now British policy. The tide of colonial nationalism sweeping through Asia soon reached Malaya. But most Malays were more concerned with defending themselves against the MCP which was mostly made up of Chinese, than with demanding independence from the British; indeed, their immediate concern was that the British not leave and abandon the Malays to the armed Communists of the MPAJA, which was the largest armed force in the country.\n\nIn 1944, the British drew up plans for a Malayan Union, which would turn the Federated and Unfederated Malay States, plus Penang and Malacca (but not Singapore), into a single Crown colony, with a view towards independence. The Bornean territories and Singapore were left out as it was thought this would make union more difficult to achieve. There was however strong opposition from the Malays, who opposed the weakening of the Malay rulers and the granting of citizenship to the ethnic Chinese and other minorities. The British had decided on equality between races as they perceived the Chinese and Indians as more loyal to the British during the war than the Malays. The Sultans, who had initially supported it, backed down and placed themselves at the head of the resistance.\n\nIn 1946, the United Malays National Organisation (UMNO) was founded by Malay nationalists led by Dato Onn bin Jaafar, the Chief Minister of Johor. UMNO favoured independence for Malaya, but only if the new state was run exclusively by the Malays. Faced with implacable Malay opposition, the British dropped the plan for equal citizenship. The Malayan Union was thus established in 1946, and was dissolved in 1948 and replaced by the Federation of Malaya, which restored the autonomy of the rulers of the Malay states under British protection.\n\nMeanwhile, the Communists were moving towards open insurrection. The MPAJA had been disbanded in December 1945, and the MCP organised as a legal political party, but the MPAJA’s arms were carefully stored for future use. The MCP policy was for immediate independence with full equality for all races. This meant it recruited very few Malays. The Party’s strength was in the Chinese-dominated trade unions, particularly in Singapore, and in the Chinese schools, where the teachers, mostly born in China, saw the Communist Party of China as the leader of China’s national revival. In March 1947, reflecting the international Communist movement’s “turn to left” as the Cold War set in, the MCP leader Lai Tek was purged and replaced by the veteran MPAJA guerrilla leader Chin Peng, who turned the party increasingly to direct action. These rebels, under the leadership of the MCP, launched guerrilla operations designed to force the British out of Malaya. In July, following a string of assassinations of plantation managers, the colonial government struck back, declaring a State of Emergency, banning the MCP and arresting hundreds of its militants. The Party retreated to the jungle and formed the Malayan Peoples’ Liberation Army, with about 13,000 men under arms, all Chinese.\n\nThe Malayan Emergency as it was known, lasted from 1948 to 1960, and involved a long anti-insurgency campaign by Commonwealth troops in Malaya. The British strategy, which proved ultimately successful, was to isolate the MCP from its support base by a combination of economic and political concessions to the Chinese and the resettlement of Chinese squatters into “New Villages” in “white areas” free of MCP influence. The effective mobilisation of the Malays against the MCP was also an important part of the British strategy. From 1949 the MCP campaign lost momentum and the number of recruits fell sharply. Although the MCP succeeded in assassinating the British High Commissioner, Sir Henry Gurney, in October 1951, this turn to terrorist tactics alienated many moderate Chinese from the Party. The arrival of Lt.-Gen Sir Gerald Templer as British commander in 1952 was the beginning of the end of the Emergency. Templer invented the techniques of counter-insurgency warfare in Malaya and applied them ruthlessly. Although the insurgency was defeated Commonwealth troops remained with the backdrop of the Cold War. Against this backdrop, independence for the Federation within the Commonwealth was granted on 31 August 1957, with Tunku Abdul Rahman as the first prime minister.\n\nChinese reaction against the MCP was shown by the formation of the Malayan Chinese Association (MCA) in 1949 as a vehicle for moderate Chinese political opinion. Its leader Tan Cheng Lock favoured a policy of collaboration with UMNO to win Malayan independence on a policy of equal citizenship, but with sufficient concessions to Malay sensitivities to ease nationalist fears. Tan formed a close collaboration with Tunku (Prince) Abdul Rahman, the Chief Minister of Kedah and from 1951 successor to Datuk Onn as leader of UMNO. Since the British had announced in 1949 that Malaya would soon become independent whether the Malayans liked it or not, both leaders were determined to forge an agreement their communities could live with as a basis for a stable independent state. The UMNO-MCA Alliance, which was later joined by the Malayan Indian Congress (MIC), won convincing victories in local and state elections in both Malay and Chinese areas between 1952 and 1955.\n\nThe introduction of elected local government was another important step in defeating the Communists. After Joseph Stalin’s death in 1953, there was a split in the MCP leadership over the wisdom of continuing the armed struggle. Many MCP militants lost heart and went home, and by the time Templer left Malaya in 1954 the Emergency was over, although Chin Peng led a diehard group that lurked in the inaccessible country along the Thai border for many years.\n\nDuring 1955 and 1956 UMNO, the MCA and the British hammered out a constitutional settlement for a principle of equal citizenship for all races. In exchange, the MCA agreed that Malaya’s head of state would be drawn from the ranks of the Malay Sultans, that Malay would be the official language, and that Malay education and economic development would be promoted and subsidised. In effect this meant that Malaya would be run by the Malays, particularly since they continued to dominate the civil service, the army and the police, but that the Chinese and Indians would have proportionate representation in the Cabinet and the parliament, would run those states where they were the majority, and would have their economic position protected. The difficult issue of who would control the education system was deferred until after independence. This came on 31 August 1957, when Tunku Abdul Rahman became the first Prime Minister of independent Malaya.\n\nThis left the unfinished business of the other British-ruled territories in the region. After the Japanese surrender the Brooke family and the British North Borneo Company gave up their control of Sarawak and North Borneo respectively, and these became British Crown Colonies. They were much less economically developed than Malaya, and their local political leaderships were too weak to demand independence. Singapore, with its large Chinese majority, achieved autonomy in 1955, and in 1959 the young socialist leader Lee Kuan Yew became Prime Minister. The Sultan of Brunei remained as a British client in his oil-rich enclave. Between 1959 and 1962 the British government orchestrated complex negotiations between these local leaders and the Malayan government.\n\nOn 24 April 1961 Lee Kuan Yew proposed the idea of forming Malaysia during a meeting to Tunku Abdul Rahman, after which Tunku invited Lee to prepare a paper elaborating on this idea. On 9 May, Lee sent the final version of the paper to Tunku and then deputy Malayan Prime Minister Abdul Razak. There were doubts about the practicality of the idea but Lee assured the Malayan government of continued Malay political dominance in the new federation. Razak supported the idea of the new federation and worked to convince Tunku to back it. On 27 May 1961, Abdul Rahman proposed the idea of forming \"Malaysia\", which would consist of Brunei, Malaya, North Borneo, Sarawak, and Singapore, all except Malaya still under British rule. It was states that this would allow the central government to better control and combat communist activities, especially in Singapore. It was also feared that if Singapore became independent, it would become a base for Chinese chauvinists to threaten Malayan sovereignty. The proposed inclusion of British territories besides Singapore was intended to keep the ethnic composition of the new nation similar to that of Malaya, with the Malay and indigenous populations of the other territories cancelling out the Chinese majority in Singapore.\n\nAlthough Lee Kuan Yew supported the proposal, his opponents from the Singaporean Socialist Front resisted, arguing that this was a ploy for the British to continue controlling the region. Most political parties in Sarawak were also against the merger, and in North Borneo, where there were no political parties, community representatives also stated their opposition. Although the Sultan of Brunei supported the merger, the Parti Rakyat Brunei opposed it as well. At the Commonwealth Prime Ministers Conference in 1961, Abdul Rahman explained his proposal further to its opponents. In October, he obtained agreement from the British government to the plan, provided that feedback be obtained from the communities involved in the merger.\nThe Cobbold Commission, named after its head, Lord Cobbold, conducted a study in the Borneo territories and approved a merger with North Borneo and Sarawak; however, it was found that a substantial number of Bruneians opposed merger. North Borneo drew up a list of points, referred to as the 20-point agreement, proposing terms for its inclusion in the new federation. Sarawak prepared a similar memorandum, known as the 18-point agreement. Some of the points in these agreements were incorporated into the eventual constitution, some were instead accepted orally. These memoranda are often cited by those who believe that Sarawak's and North Borneo's rights have been eroded over time. A referendum was conducted in Singapore to gauge opinion, and 70% supported merger with substantial autonomy given to the state government. The Sultanate of Brunei withdrew from the planned merger due to opposition from certain segments of its population as well as arguments over the payment of oil royalties and the status of the sultan in the planned merger. Additionally, the Bruneian Parti Rakyat Brunei staged an armed revolt, which, though it was put down, was viewed as potentially destabilising to the new nation.\n\nAfter reviewing the Cobbold Commission's findings, the British government appointed the Landsdowne Commission to draft a constitution for Malaysia. The eventual constitution was essentially the same as the 1957 constitution, albeit with some rewording; for instance, giving recognition to the special position of the natives of the Borneo States. North Borneo, Sarawak and Singapore were also granted some autonomy unavailable to the states of Malaya. After negotiations in July 1963, it was agreed that Malaysia would come into being on 31 August 1963, consisting of Malaya, North Borneo, Sarawak and Singapore. The date was to coincide with the independence day of Malaya and the British giving self-rule to Sarawak and North Borneo. However, the Philippines and Indonesia strenuously objected to this development, with Indonesia claiming Malaysia represented a form of \"neocolonialism\" and the Philippines claiming North Borneo as its territory. The opposition from the Indonesian government led by Sukarno and attempts by the Sarawak United People's Party delayed the formation of Malaysia. Due to these factors, an eight-member UN team was formed to re-ascertain whether North Borneo and Sarawak truly wanted to join Malaysia. Malaysia formally came into being on 16 September 1963, consisting of Malaya, North Borneo, Sarawak, and Singapore. In 1963 the total population of Malaysia was about 10 million.\n\nAt the time of independence Malaya had great economic advantages. It was among the world’s leading producers of three valuable commodities, rubber, tin, and palm oil, and also a significant iron ore producer. These export industries gave the Malayan government a healthy surplus to invest in industrial development and infrastructure projects. Like other developing nations in the 1950s and 1960s, Malaya (and later Malaysia) placed great stress on state planning, although UMNO was never a socialist party. The First and Second Malayan Plans (1956–60 and 1961–65 respectively) stimulated economic growth through state investment in industry and repairing infrastructure such as roads and ports, which had been damaged and neglected during the war and the Emergency. The government was keen to reduce Malaya’s dependence on commodity exports, which put the country at the mercy of fluctuating prices. The government was also aware that demand for natural rubber was bound to fall as the production and use of synthetic rubber expanded. Since a third of the Malay workforce worked in the rubber industry it was important to develop alternative sources of employment. Competition for Malaya’s rubber markets meant that the profitability of the rubber industry increasingly depended on keeping wages low, which perpetuated rural Malay poverty.\n\nBoth Indonesia and the Philippines withdrew their ambassadors from Malaya on 15 September 1963, the day before Malaysia's formation. In Jakarta the British and Malayan embassies were stoned, and the British consulate in Medan was ransacked with Malaya's consul taking refuge in the US consulate. Malaysia withdrew its ambassadors in response, and asked Thailand to represent Malaysia in both countries.\n\nIndonesian President Sukarno, backed by the powerful Communist Party of Indonesia (PKI), chose to regard Malaysia as a \"neocolonialist\" plot against his country, and backed a Communist insurgency in Sarawak, mainly involving elements of the local Chinese community. Indonesian irregular forces were infiltrated into Sarawak, where they were contained by Malaysian and Commonwealth of Nations forces. This period of \"Konfrontasi\", an economic, political, and military confrontation lasted until the downfall of Sukarno in 1966. The Philippines objected to the formation of the federation, claiming North Borneo was part of Sulu, and thus the Philippines. In 1966 the new president, Ferdinand Marcos, dropped the claim, although it has since been revived and is still a point of contention marring Philippine-Malaysian relations.\n\nThe Depression of the 1930s, followed by the outbreak of the Sino-Japanese War, had the effect of ending Chinese emigration to Malaya. This stabilised the demographic situation and ended the prospect of the Malays becoming a minority in their own country. At the time of independence in 1957, Malays comprised 55% of the population, Chinese 35% and Indians 10%. This balance was altered by the inclusion of the majority-Chinese Singapore, upsetting many Malays. The federation increased the Chinese proportion to close to 40%. Both UMNO and the MCA were nervous about the possible appeal of Lee's People's Action Party (then seen as a radical socialist party) to voters in Malaya, and tried to organise a party in Singapore to challenge Lee's position there. Lee in turn threatened to run PAP candidates in Malaya at the 1964 federal elections, despite an earlier agreement that he would not do so (see PAP-UMNO Relations). Racial tensions intensified as PAP created an opposition alliance aiming for equality between races. This provoked Tunku Abdul Rahman to demand that Singapore withdraw from Malaysia. While the Singaporean leaders attempted to keep Singapore as a part of the Federation, the Malaysian Parliament voted 126–0 on 9 August 1965 in favor of the expulsion of Singapore.\n\nThe most vexed issues of independent Malaysia were education and the disparity of economic power among the ethnic communities. The Malays felt unhappy with the wealth of the Chinese community, even after the expulsion of Singapore. Malay political movements emerged based around this. However, since there was no effective opposition party, these issues were contested mainly within the coalition government, which won all but one seat in the first post-independence Malayan Parliament. The two issues were related, since the Chinese advantage in education played a large part in maintaining their control of the economy, which the UMNO leaders were determined to end. The MCA leaders were torn between the need to defend their own community's interests and the need to maintain good relations with UMNO. This produced a crisis in the MCA in 1959, in which a more assertive leadership under Lim Chong Eu defied UMNO over the education issue, only to be forced to back down when Tunku Abdul Rahman threatened to break up the coalition.\n\nThe Education Act of 1961 put UMNO's victory on the education issue into legislative form. Henceforward Malay and English would be the only teaching languages in secondary schools, and state primary schools would teach in Malay only. Although the Chinese and Indian communities could maintain their own Chinese and Tamil-language primary schools, all their students were required to learn Malay, and to study an agreed \"Malayan curriculum\". Most importantly, the entry exam to the University of Malaya (which moved from Singapore to Kuala Lumpur in 1963) would be conducted in Malay, even though most teaching at the university was in English until the 1970s. This had the effect of excluding many Chinese students. At the same time Malay schools were heavily subsidised, and Malays were given preferential treatment. This obvious defeat for the MCA greatly weakened its support in the Chinese community.\n\nAs in education, the UMNO government's unspoken agenda in the field of economic development aimed to shift economic power away from the Chinese and towards the Malays. The two Malayan Plans and the First Malaysian Plan (1966–1970) directed resources heavily into developments which would benefit the rural Malay community, such as village schools, rural roads, clinics, and irrigation projects. Several agencies were set up to enable Malay smallholders to upgrade their production and to increase their incomes. The Federal Land Development Authority (FELDA) helped many Malays to buy farms or to upgrade ones they already owned. The state also provided a range of incentives and low-interest loans to help Malays start businesses, and government tendering systematically favoured Malay companies, leading many Chinese-owned businesses to \"Malayanise\" their management. All this certainly tended to reduce to gap between Chinese and Malay standards of living, although some argued that this would have happened anyway as Malaysia's trade and general prosperity increased.\n\nThe collaboration of the MCA and the MIC in these policies weakened their hold on the Chinese and Indian electorates. At the same time, the effects of the government’s affirmative action policies of the 1950s and 1960s had been to create a discontented class of educated but underemployed Malays. This was a dangerous combination, and led to the formation of a new party, the Malaysian People’s Movement (Gerakan Rakyat Malaysia) in 1968. Gerakan was a deliberately non-communal party, bringing in Malay trade unionists and intellectuals as well as Chinese and Indian leaders. At the same time, an Islamist party, the Islamic Party of Malaysia (PAS) and a Chinese socialist party, the Democratic Action Party (DAP), gained increasing support, at the expense of UMNO and the MCA respectively.\n\nAt the May 1969 federal elections, the UMNO-MCA-MIC Alliance polled only 48% of the vote, although it retained a majority in the legislature. The MCA lost most of the Chinese-majority seats to Gerakan or DAP candidates. The victorious opposition celebrated by holding a motorcade on the main streets of Kuala Lumpur with supporters holding up brooms as a signal of its intention to make sweeping changes. Fear of what the changes might mean for them (as much of the country's businesses were Chinese-owned), a Malay backlash resulted, leading rapidly to riots and inter-communal violence in which about 6,000 Chinese homes and businesses were burned and at least 184 people were killed. The government declared a state of emergency, and a National Operations Council, headed by Deputy Prime Minister Tun Abdul Razak, took power from the government of Tunku Abdul Rahman, who, in September 1970, was forced to retire in favour of Abdul Razak. It consisted of nine members, mostly Malay, and wielded full political and military power.\n\nUsing the Emergency-era Internal Security Act (ISA), the new government suspended Parliament and political parties, imposed press censorship and placed severe restrictions on political activity. The ISA gave the government power to intern any person indefinitely without trial. These powers were widely used to silence the government’s critics, and have never been repealed. The Constitution was changed to make illegal any criticism, even in Parliament, of the Malaysian monarchy, the special position of Malays in the country, or the status of Malay as the national language.\n\nIn 1971 Parliament reconvened, and a new government coalition, the National Front (Barisan Nasional), was formed in 1973 to replace the Alliance party. The coalition consisted of UMNO, the MCA, the MIC, Gerakan, PPP, and regional parties in Sabah and Sarawak. The PAS also joined the Front but was expelled in 1977. The DAP was left outside as the only significant opposition party. Abdul Razak held office until his death in 1976. He was succeeded by Datuk Hussein Onn, the son of UMNO’s founder Onn Jaafar, and then by Tun Mahathir Mohamad, who had been Education Minister since 1981, and who held power for 22 years. During these years policies were put in place which led to the rapid transformation of Malaysia’s economy and society, such as the controversial New Economic Policy, which was intended to increase proportionally the share of the economic \"pie\" of the bumiputras as compared to other ethnic groups—was launched by Prime Minister Tun Abdul Razak. Malaysia has since maintained a delicate ethno-political balance, with a system of government that has attempted to combine overall economic development with political and economic policies that promote equitable participation of all races.\n\nIn 1970 three quarters of Malaysians living below the poverty line were Malays, the majority of Malays were still rural workers, and Malays were still largely excluded from the modern economy. The government’s response was the New Economic Policy of 1971, which was to be implemented through a series of four five-year plans from 1971 to 1990. The plan had two objectives: the elimination of poverty, particularly rural poverty, and the elimination of the identification between race and prosperity. This latter policy was understood to mean a decisive shift in economic power from the Chinese to the Malays, who until then made up only 5% of the professional class.\n\nPoverty was tackled through an agricultural policy which resettled 250,000 Malays on newly cleared farmland, more investment in rural infrastructure, and the creation of free trade zones in rural areas to create new manufacturing jobs. Little was done to improve the living standards of the low-paid workers in plantation agriculture, although this group steadily declined as a proportion of the workforce. By 1990 the poorest parts of Malaysia were rural Sabah and Sarawak, which lagged significantly behind the rest of the country. During the 1970s and ‘80s rural poverty did decline, particularly in the Malayan Peninsula, but critics of the government’s policy contend that this was mainly due to the growth of overall national prosperity (due in large part to the discovery of important oil and gas reserves) and migration of rural people to the cities rather than to state intervention. These years saw rapid growth in Malaysian cities, particularly Kuala Lumpur, which became a magnet for immigration both from rural Malaya and from poorer neighbours such as Indonesia, Bangladesh, Thailand and the Philippines. Urban poverty became a problem for the first time, with shanty towns growing up around the cities.\n\nThe second arm of government policy, driven mainly by Mahathir first as Education Minister and then as Prime Minister, was the transfer of economic power to the Malays. Mahathir greatly expanded the number of secondary schools and universities throughout the country, and enforced the policy of teaching in Malay rather than English. This had the effect of creating a large new Malay professional class. It also created an unofficial barrier against Chinese access to higher education, since few Chinese are sufficiently fluent in Malay to study at Malay-language universities. Chinese families therefore sent their children to universities in Singapore, Australia, Britain or the United States – by 2000, for example, 60,000 Malaysians held degrees from Australian universities. This had the unintended consequence of exposing large numbers of Malaysians to life in Western countries, creating a new source of discontent. Mahathir also greatly expanded educational opportunities for Malay women – by 2000 half of all university students were women.\nTo find jobs for all these new Malay graduates, the government created several agencies for intervention in the economy. The most important of these were PERNAS (National Corporation Ltd.), PETRONAS (National Petroleum Ltd.), and HICOM (Heavy Industry Corporation of Malaysia), which not only directly employed many Malays but also invested in growing areas of the economy to create new technical and administrative jobs which were preferentially allocated to Malays. As a result, the share of Malay equity in the economy rose from 1.5% in 1969 to 20.3% in 1990, and the percentage of businesses of all kinds owned by Malays rose from 39 percent to 68 percent. This latter figure was deceptive because many businesses that appeared to be Malay-owned were still indirectly controlled by Chinese, but there is no doubt that the Malay share of the economy considerably increased. The Chinese remained disproportionately powerful in Malaysian economic life, but by 2000 the distinction between Chinese and Malay business was fading as many new corporations, particularly in growth sectors such as information technology, were owned and managed by people from both ethnic groups.\n\nMalaysia’s rapid economic progress since 1970, which was only temporarily disrupted by the Asian financial crisis of 1997, has not been matched by change in Malaysian politics. The repressive measures passed in 1970 remain in place. Malaysia has had regular elections since 1974, and although campaigning is reasonably free at election time, it is in effect a one-party state, with the UMNO-controlled National Front usually winning nearly all the seats, while the DAP wins some Chinese urban seats and the PAS some rural Malay ones. Since the DAP and the PAS have diametrically opposed policies, they have been unable to form an effective opposition coalition. There is almost no criticism of the government in the media and public protest remains severely restricted. The ISA continues to be used to silence dissidents, and the members of the UMNO youth movement are deployed to physically intimidate opponents.\n\nThe restoration of democracy after the 1969 crisis caused disputes in the UMNO, a struggle of power which increased after the death of Tun Abdul Razak. The ailing Datuk Hussein Bin Onn replaced him, but the fight for control shifted to appointing the deputy prime minister. Mahathir Mohamad was chosen, an advocate of Bumiputra who also tried to benefit the other ethnic communities.\n\nUnder the premiership of Mahathir Mohamad, Malaysia experienced economic growth from the 1980s, a 1985–86 property market depression, and returned to growth through to the mid-1990s. Mahathir increased privatisation and introduced the New Development Policy (NDP), designed to increase economic wealth for all Malaysians, rather than just Malays. The period saw a shift from an agriculture-based economy to one based on manufacturing and industry in areas such as computers and consumer electronics. It was during this period, too, that the physical landscape of Malaysia changed with the emergence of numerous mega-projects. Notable amongst these projects were the construction of the Petronas Twin Towers (at the time the tallest building in the world, and, as of 2016, still the tallest twin building), Kuala Lumpur International Airport (KLIA), the North-South Expressway, the Sepang International Circuit, the Multimedia Super Corridor (MSC), the Bakun hydroelectric dam, and Putrajaya, the new federal administrative capital.\n\nUnder Mahathir Mohamad’s long Prime Ministership (1981–2003), Malaysia’s political culture became increasingly centralised and authoritarian, due to Mahathir's belief that the multiethnic Malaysia could only remain stable through controlled democracy. In 1986–87, he faced leadership challenges among his own party. There were also attacks by the government on several non-governmental organisations (NGO) which were critical of various government policies. There were also issues such the questioning by MCA's Lee Kim Sai over the use of the term \"pendatang\" (immigrants) that was seen as challenging Malay's bumiputra status, as well as rumours of forced conversion to or from Islam. Mahathir initiated a crackdown on opposition dissidents with the use of the Internal Security Act named Operation Lalang. The Internal Security Act was invoked in October 1987 arresting 106 people, including opposition leaders. The head of the judiciary and five members of the supreme court who had questioned his use of the ISA were also arrested, and a clampdown on Malaysia's press occurred.\n\nThis culminated in the dismissal and imprisonment on unsubstantiated charges of the Deputy Prime Minister, Anwar Ibrahim, in 1997 after an internal dispute within the government. The complicity of the judiciary in this piece of persecution was seen as a particularly clear sign of the decline of Malaysian democracy. The Anwar affair led to the formation of a new party, the People's Justice Party, or Keadilan, led by Anwar’s wife, Wan Azizah Wan Ismail. At the 1999 elections Keadilan formed a coalition with the DAP and the PAS known as the Alternative Front (Barisan Alternatif). The result of this was that the PAS won a number of Malay seats from UMNO, but many Chinese voters disapproved of this unnatural alliance with the Islamist PAS, causing the DAP to lose many of its seats to the MCA, including that of its veteran leader, Lim Kit Siang. Wan Azizah won her husband’s former constituency in Penang but otherwise Keadilan made little impact.\n\nIn the late 1990s, Malaysia was shaken by the Asian financial crisis, which damaged Malaysia's assembly line-based economy. Mahathir combated it initially with IMF approved policies. However, the devaluation of the Ringgit and the deepening recession caused him to create his own programme, based on protecting Malaysia from foreign investors and reinvigorating the economy through construction projects and the lowering of interest rates. The policies caused Malaysia's economy to rebound by 2002, but brought disagreement between Mahathir and his deputy, Anwar Ibrahim, who backed the IMF policies. This led to the sacking of the Anwar, causing political unrest. Anwar was arrested and banned from politics on what are considered trumped up charges. In 2003 Mahathir, Malaysia's longest serving prime minister, voluntarily retired in favour of his new deputy, Abdullah Ahmad Badawi. In November 2007 two anti-government rallies occurred, precipitated by allegations of corruption and discrepancies in the election system that heavily favoured the ruling political party, National Front, which has been in power since Malaya achieved independence.\n\nDato Seri Abdullah Ahmad Badawi freed Anwar, which was seen as a portent of a mild liberalisation. At the 2004 election, the National Front led by Abdullah had a massive victory, virtually wiping out the PAS and Keadilan, although the DAP recovered the seats it had lost in 1999. This victory was seen as the result mainly of Abdullah's personal popularity and the strong recovery of Malaysia’s economy, which has lifted the living standards of most Malaysians to almost first world standards, coupled with an ineffective opposition. The government's objective is for Malaysia to become a fully developed country by 2020 as expressed in \"Wawasan 2020\". It leaves unanswered, however, the question of when and how Malaysia will acquire a first world political system (a multi-party democracy, a free press, an independent judiciary and the restoration of civil and political liberties) to go with its new economic maturity.\n\nIn November 2007, Malaysia was rocked by two anti-government rallies. The 2007 Bersih Rally which was attended by 40,000 people was held in Kuala Lumpur on 10 November 2007, to campaign for electoral reform. It was precipitated by allegations of corruption and discrepancies in the Malaysian election system that heavily favour the ruling political party, Barisan Nasional, which has been in power since Malaysia achieved its independence in 1957. Another rally was held on 25 November 2007, in Kuala Lumpur led by HINDRAF. The rally organiser, the Hindu Rights Action Force, had called the protest over alleged discriminatory policies favouring ethnic Malays. The crowd was estimated to be between 5,000 and 30,000. In both cases the government and police tried to prevent the gatherings from taking place.\n\nOn 16 October 2008, HINDRAF was banned when the government labelled the organisation as \"a threat to national security\".\n\nNajib Razak entered office as Prime Minister with a sharp focus on domestic economic issues and political reform. On his first day as Prime Minister, Najib announced as his first actions the removal of bans on two opposition newspapers, \"Suara Keadilan\" and \"Harakahdaily\", run by the opposition leader Datuk Seri Anwar Ibrahim-led People's Justice Party and the Pan Islamic Party, respectively, and the release of 13 people held under the Internal Security Act. Among the released detainees were two ethnic Indian activists who were arrested in December 2007 for leading an anti-government campaign, three foreigners and eight suspected Islamic militants. Najib also pledged to conduct a comprehensive review of the much-criticised law which allows for indefinite detention without trial. In the speech, he emphasised his commitment to tackling poverty, restructuring Malaysian society, expanding access to quality education for all, and promoting renewed \"passion for public service\". He also deferred and abandoned the digital television transition plan of all free-to-air broadcasters such as Radio Televisyen Malaysia.\n\nMalaysia Day, celebrating the formation of Malaysia on 16 September 1963, was declared a public holiday in 2010 in complement to the existing 31 August celebration of Hari Merdeka.\n\n\n\n", "id": "13806", "title": "History of Malaysia"}
