{"url": "https://en.wikipedia.org/wiki?curid=13959", "text": "Hannibal\n\nHannibal (247 – between 183 and 181 BC), fully Hannibal Barca, was a Punic military commander from Carthage, generally considered one of the greatest military commanders in history. His father Hamilcar Barca was the leading Carthaginian commander during the First Punic War. His younger brothers were Mago and Hasdrubal, and he was brother-in-law to Hasdrubal the Fair.\n\nHannibal lived during a period of great tension in the Mediterranean Basin, when the Roman Republic established its supremacy over other great powers such as ancient Carthage and the Greek kingdoms of Macedonia, Syracuse, and the Seleucid Empire. One of his most famous achievements was at the outbreak of the Second Punic War, when he marched an army which included war elephants from Iberia over the Pyrenees and the Alps into Italy. In his first few years in Italy, he won three dramatic victories—the Trebia, Lake Trasimene, and Cannae, in which he distinguished himself for his ability to determine his and his opponent's strengths and weaknesses, and to play the battle to his strengths and the enemy's weaknesses—and won over many allies of Rome. Hannibal occupied much of Italy for 15 years but was unable to march on Rome. An enemy counter-invasion of North Africa forced him to return to Carthage, where he was decisively defeated by Scipio Africanus at the Battle of Zama. Scipio had studied Hannibal's tactics and brilliantly devised some of his own, and finally defeated Rome's nemesis at Zama, having previously driven Hannibal's brother Hasdrubal out of the Iberian Peninsula.\n\nAfter the war, Hannibal successfully ran for the office of sufet. He enacted political and financial reforms to enable the payment of the war indemnity imposed by Rome; however, Hannibal's reforms were unpopular with members of the Carthaginian aristocracy and in Rome, and he fled into voluntary exile. During this time, he lived at the Seleucid court, where he acted as military advisor to Antiochus III the Great in his war against Rome. Antiochus met defeat at the Battle of Magnesia and was forced to accept Rome's terms, and Hannibal fled again, making a stop in the Kingdom of Armenia. His flight ended in the court of Bithynia, where he achieved an outstanding naval victory against a fleet from Pergamon. He was afterwards betrayed to the Romans and committed suicide by poisoning himself.\n\nHannibal is often regarded as one of the greatest military strategists in history and one of the greatest generals of antiquity, together with Alexander the Great, Julius Caesar, Scipio Africanus, and Pyrrhus of Epirus. Plutarch states that Hannibal was questioned by Scipio as to who was the greatest general, and Hannibal replied either Alexander or Pyrrhus, then himself, or, according to another version of the event, Pyrrhus, Scipio, then himself. Military historian Theodore Ayrault Dodge called Hannibal the \"father of strategy\", because his greatest enemy, Rome, came to adopt elements of his military tactics in its own strategic arsenal. This praise has earned him a strong reputation in the modern world, and he was regarded as a great strategist by Napoleon and others.\n\nThe English form of the name is derived from the Latin. Greek historians rendered the name as \"Anníbas Bárkas\" ().\n\nHannibal was his given name. Hannibal's name was recorded in Carthaginian sources as ḤNBʻL (in Punic:). Its precise vocalization remains a matter of debate. Suggested readings include \"Ḥannibaʻl\" or \"Ḥannibaʻal\", meaning \"grace of Baʻal\", \"Ba'al is gracious\", or \"Ba'al has been gracious\"; or \"Ḥannobaʻal\", with the same meaning.\n\nBarca (, BRQ) was the surname of his aristocratic family, meaning \"shining\" or \"lightning\". It is thus equivalent to the Arabic name \"Barq\" or the Hebrew name Barak or the ancient Greek epithet \"keraunos\", which was commonly given to military commanders in the Hellenistic period.\n\nIn English, his clan are sometimes collectively known as the Barcids. As with Greek and Roman practice, patronymics were a common part of Carthaginian nomenclature, so that Hannibal would also have been known as \"Hannibal son of Hamilcar\".\n\nHannibal was one of the sons of Hamilcar Barca, a Carthaginian leader. He was born in what is present day Tunisia. He had several sisters and two brothers, Hasdrubal and Mago. His brothers-in-law were Hasdrubal the Fair and the Numidian king Naravas. He was still a child when his sisters married, and his brothers-in-law were close associates during his father's struggles in the Mercenary War and the Punic conquest of the Iberian Peninsula. In light of Hamilcar Barca's cognomen, historians refer to Hamilcar's family as the Barcids. However, there is debate as to whether the cognomen Barca (meaning \"thunderbolt\") was applied to Hamilcar alone or was hereditary within his family. If the latter, then Hannibal and his brothers also bore the name \"Barca\".\n\nAfter Carthage's defeat in the First Punic War, Hamilcar set out to improve his family's and Carthage's fortunes. With that in mind and supported by Gades, Hamilcar began the subjugation of the tribes of the Iberian Peninsula. Carthage at the time was in such a poor state that its navy was unable to transport his army; instead, Hamilcar had to march it towards the Pillars of Hercules and then cross the Strait of Gibraltar.\n\nAccording to Polybius, Hannibal much later said that when he came upon his father and begged to go with him, Hamilcar agreed and demanded that he swear that as long as he lived he would never be a friend of Rome. There is even an account of him at a very young age begging his father to take him to an overseas war. In the story, Hannibal's father took him up and brought him to a sacrificial chamber. Hamilcar held Hannibal over the fire roaring in the chamber and made him swear that he would never be a friend of Rome. Other sources report that Hannibal told his father, \"I swear so soon as age will permit...I will use fire and steel to arrest the destiny of Rome.\" According to the tradition, Hannibal's oath took place in the town of Peñíscola, today part of the Valencian Community, Spain.\n\nHannibal's father went about the conquest of Hispania. When his father drowned in battle, Hannibal's brother-in-law Hasdrubal the Fair succeeded to his command of the army with Hannibal serving as an officer under him. Hasdrubal pursued a policy of consolidation of Carthage's Iberian interests, even signing a treaty with Rome whereby Carthage would not expand north of the Ebro so long as Rome did not expand south of it. Hasdrubal also endeavoured to consolidate Carthaginian power through diplomatic relationships with native tribes.\n\nUpon the assassination of Hasdrubal in 221 BC, Hannibal was proclaimed commander-in-chief by the army and confirmed in his appointment by the Carthaginian government. Livy, a Roman scholar, gives a depiction of the young Carthaginian: \"No sooner had he arrived...the old soldiers fancied they saw Hamilcar in his youth given back to them; the same bright look; the same fire in his eye, the same trick of countenance and features. Never was one and the same spirit more skillful to meet opposition, to obey, or to command[.]\"\n\nAfter he assumed command, Hannibal spent two years consolidating his holdings and completing the conquest of Hispania, south of the Ebro. In his first campaign, Hannibal attacked and stormed the Olcades' strongest centre, Alithia, which promptly led to their surrender, and brought Punic power close to the River Tagus. His following campaign in 220 BC was against the Vaccaei to the west, where he stormed the Vaccaen strongholds of Helmantice and Arbucala. On his return home, laden with many spoils, a coalition of Spanish tribes, led by the Carpetani, attacked, and Hannibal won his first major battlefield success and showed off his tactical skills at the battle of the River Tagus. However, Rome, fearing the growing strength of Hannibal in Iberia, made an alliance with the city of Saguntum, which lay a considerable distance south of the River Ebro and claimed the city as its protectorate. Hannibal not only perceived this as a breach of the treaty signed with Hasdrubal, but as he was already planning an attack on Rome, this was his way to start the war. So he laid siege to the city, which fell after eight months. Rome reacted to this apparent violation of the treaty and demanded justice from Carthage. In view of Hannibal's great popularity, the Carthaginian government did not repudiate Hannibal's actions, and the war he sought was declared at the end of the year. Hannibal was now determined to carry the war into the heart of Italy by a rapid march through Hispania and southern Gaul.\n\nThis journey was originally planned by Hannibal's brother-in-law Hasdrubal the Fair, who became a Carthaginian general in the Iberian Peninsula in 229 BC. He maintained this post for eight years until 221 BC. Soon the Romans became aware of an alliance between Carthage and the Celts of the Po Valley in Northern Italy. The Celts were amassing forces to invade farther south in Italy, presumably with Carthaginian backing. Therefore, the Romans preemptively invaded the Po region in 225 BC. By 220 BC, the Romans had annexed the area as Cisalpine Gaul. Hasdrubal was assassinated around the same time (221 BC), bringing Hannibal to the fore. It seems that the Romans lulled themselves into a false sense of security, having dealt with the threat of a Gallo-Carthaginian invasion, and perhaps knowing that the original Carthaginian commander had been killed.\n\nHannibal departed New Carthage in late spring of 218 BC. He fought his way through the northern tribes to the foothills of the Pyrenees, subduing the tribes through clever mountain tactics and stubborn fighting. He left a detachment of 20,000 troops to garrison the newly conquered region. At the Pyrenees, he released 11,000 Iberian troops who showed reluctance to leave their homeland. Hannibal reportedly entered Gaul with 40,000 foot soldiers and 12,000 horsemen.\n\nHannibal recognized that he still needed to cross the Pyrenees, the Alps, and many significant rivers. Additionally, he would have to contend with opposition from the Gauls, whose territory he passed through. Starting in the spring of 218 BC, he crossed the Pyrenees and reached the Rhône by conciliating the Gaulish chiefs along his passage before the Romans could take any measures to bar his advance, arriving at the Rhône in September. Hannibal's army numbered 38,000 infantry, 8,000 cavalry, and 38 elephants, almost none of which would survive the harsh conditions of the Alps.\n\nHannibal outmaneuvered the natives who had tried to prevent his crossing, then evaded a Roman force marching from the Mediterranean coast by turning inland up the valley of the Rhône. His exact route over the Alps has been the source of scholarly dispute ever since. (Polybius, the surviving ancient account closest in time to Hannibal's campaign, reports that the route was already debated.) The most influential modern theories favor either a march up the valley of the Drôme and a crossing of the main range to the south of the modern highway over the Col de Montgenèvre or a march farther north up the valleys of the Isère and Arc crossing the main range near the present Col de Mont Cenis or the Little St Bernard Pass. Recent numismatic evidence suggests that Hannibal's army may have passed within sight of the Matterhorn.\n\nBy Livy's account, the crossing was accomplished in the face of huge difficulties. These Hannibal surmounted with ingenuity, such as when he used vinegar and fire to break through a rockfall. According to Polybius, he arrived in Italy accompanied by 20,000 foot soldiers, 4,000 horsemen, and only a few elephants. The fired rockfall event is mentioned only by Livy; Polybius is mute on the subject and there is no evidence of carbonized rock at the only two-tier rockfall in the Western Alps, located below the Col de la Traversette (Mahaney, 2008). If Polybius is correct in his figure for the number of troops that he commanded after the crossing of the Rhône, this would suggest that he had lost almost half of his force. Historians such as Serge Lancell have questioned the reliability of the figures for the number of troops that he had when he left Hispania. From the start, he seems to have calculated that he would have to operate without aid from Hispania.\n\nHannibal's vision of military affairs was derived partly from the teaching of his Greek tutors and partly from experience gained alongside his father, and it stretched over most of the Hellenistic World of his time. Indeed, the breadth of his vision gave rise to his grand strategy of conquering Rome by opening a northern front and subduing allied city-states on the peninsula, rather than by attacking Rome directly. Historical events which led to the defeat of Carthage during the First Punic War when his father commanded the Carthaginian Army also led Hannibal to plan the invasion of Italy by land across the Alps.\n\nThe task was daunting, to say the least. It involved the mobilization of between 60,000 and 100,000 troops (see Proctor, 1971) and the training of a war-elephant corps, all of which had to be provisioned along the way. The alpine invasion of Italy was a military operation that would shake the Mediterranean World of 218 BC with repercussions for more than two decades.\n\nHannibal's perilous march brought him into the Roman territory and frustrated the attempts of the enemy to fight out the main issue on foreign ground. His sudden appearance among the Gauls of the Po Valley, moreover, enabled him to detach those tribes from their new allegiance to the Romans before the Romans could take steps to check the rebellion. Publius Cornelius Scipio was the consul who commanded the Roman force sent to intercept Hannibal (he was also Scipio Africanus' father). He had not expected Hannibal to make an attempt to cross the Alps, since the Romans were prepared to fight the war in the Iberian Peninsula. With a small detachment still positioned in Gaul, Scipio made an attempt to intercept Hannibal. He succeeded, through prompt decision and speedy movement, in transporting his army to Italy by sea in time to meet Hannibal. Hannibal's forces moved through the Po Valley and were engaged in the Battle of Ticinus. Here, Hannibal forced the Romans to evacuate the plain of Lombardy, by virtue of his superior cavalry. The victory was minor, but it encouraged the Gauls and Ligurians to join the Carthaginian cause, whose troops bolstered his army back to around 40,000 men. Scipio was severely injured, his life only saved by the bravery of his son who rode back onto the field to rescue his fallen father. Scipio retreated across the Trebia to camp at Placentia with his army mostly intact.\n\nThe other Roman consular army was rushed to the Po Valley. Even before news of the defeat at Ticinus had reached Rome, the Senate had ordered Consul Tiberius Sempronius Longus to bring his army back from Sicily to meet Scipio and face Hannibal. Hannibal, by skillful maneuvers, was in position to head him off, for he lay on the direct road between Placentia and Arminum, by which Sempronius would have to march to reinforce Scipio. He then captured Clastidium, from which he drew large amounts of supplies for his men. But this gain was not without loss, as Sempronius avoided Hannibal's watchfulness, slipped around his flank, and joined his colleague in his camp near the Trebia River near Placentia. There Hannibal had an opportunity to show his masterful military skill at the Trebia in December of the same year, after wearing down the superior Roman infantry, when he cut it to pieces with a surprise attack and ambush from the flanks.\n\nHannibal quartered his troops for the winter with the Gauls, whose support for him had abated. In the spring of 217 BC, Hannibal decided to find a more reliable base of operations farther south. Gnaeus Servilius and Gaius Flaminius (the new consuls of Rome) were expecting Hannibal to advance on Rome, and they took their armies to block the eastern and western routes that Hannibal could use.\n\nThe only alternative route to central Italy lay at the mouth of the Arno. This area was practically one huge marsh, and happened to be overflowing more than usual during this particular season. Hannibal knew that this route was full of difficulties, but it remained the surest and certainly the quickest way to central Italy. Polybius claims that Hannibal's men marched for four days and three nights, \"through a land that was under water\", suffering terribly from fatigue and enforced want of sleep. He crossed without opposition over both the Apennines (during which he lost his right eye because of conjunctivitis) and the seemingly impassable Arno, but he lost a large part of his force in the marshy lowlands of the Arno.\n\nHe arrived in Etruria in the spring of 217 BC and decided to lure the main Roman army under Flaminius into a pitched battle by devastating the region that Flaminius had been sent to protect. As Polybius recounts, \"he [Hannibal] calculated that, if he passed the camp and made a descent into the district beyond, Flaminius (partly for fear of popular reproach and partly of personal irritation) would be unable to endure watching passively the devastation of the country but would spontaneously follow him... and give him opportunities for attack.\" At the same time, Hannibal tried to break the allegiance of Rome's allies by proving that Flaminius was powerless to protect them. Despite this, Flaminius remained passively encamped at Arretium. Hannibal marched boldly around Flaminius' left flank, unable to draw him into battle by mere devastation, and effectively cut him off from Rome (thus executing the first recorded turning movement in military history). He then advanced through the uplands of Etruria, provoking Flaminius into a hasty pursuit and catching him in a defile on the shore of Lake Trasimenus. There Hannibal destroyed Flaminius' army in the waters or on the adjoining slopes, killing Flaminius as well (see Battle of Lake Trasimene). This was the most costly ambush that the Romans ever sustained until the Battle of Carrhae against the Parthian Empire.\n\nHannibal had now disposed of the only field force that could check his advance upon Rome, but he realized that, without siege engines, he could not hope to take the capital. He preferred to exploit his victory by entering into central and southern Italy and encouraging a general revolt against the sovereign power.\n\nThe Romans appointed Fabius Maximus as their dictator. Departing from Roman military traditions, Fabius adopted the strategy named after him, avoiding open battle while placing several Roman armies in Hannibal's vicinity in order to watch and limit his movements.\n\nHannibal ravaged Apulia but was unable to bring Fabius to battle, so he decided to march through Samnium to Campania, one of the richest and most fertile provinces of Italy, hoping that the devastation would draw Fabius into battle. Fabius closely followed Hannibal's path of destruction, yet still refused to let himself be drawn out of the defensive. This strategy was unpopular with many Romans, who believed that it was a form of cowardice.\n\nHannibal decided that it would be unwise to winter in the already devastated lowlands of Campania, but Fabius had ensured that all the passes were blocked out of Campania. To avoid this, Hannibal deceived the Romans into thinking that the Carthaginian army was going to escape through the woods. As the Romans moved off towards the woods, Hannibal's army occupied the pass, and then made their way through the pass unopposed. Fabius was within striking distance but in this case his caution worked against him. Smelling a stratagem (rightly), he stayed put. For the winter, Hannibal found comfortable quarters in the Apulian plain. What Hannibal achieved in extricating his army was, as Adrian Goldsworthy puts it, \"a classic of ancient generalship, finding its way into nearly every historical narrative of the war and being used by later military manuals\". This was a severe blow to Fabius' prestige and soon after this his period of dictatorial power ended.\n\nIn the spring of 216 BC, Hannibal took the initiative and seized the large supply depot at Cannae in the Apulian plain. By capturing Cannae, Hannibal had placed himself between the Romans and their crucial sources of supply. Once the Roman Senate resumed their consular elections in 216 BC, they appointed Gaius Terentius Varro and Lucius Aemilius Paullus as consuls. In the meantime, the Romans hoped to gain success through sheer strength and weight of numbers, and they raised a new army of unprecedented size, estimated by some to be as large as 100,000 men, but more likely around 50-80,000.\n\nThe Romans and allied legions resolved to confront Hannibal and marched southward to Apulia. They eventually found him on the left bank of the Aufidus River, and encamped six miles (10 km) away. On this occasion, the two armies were combined into one, the consuls having to alternate their command on a daily basis. Varro was in command on the first day, a man of reckless and hubristic nature (according to Livy) and determined to defeat Hannibal. Hannibal capitalized on the eagerness of Varro and drew him into a trap by using an envelopment tactic. This eliminated the Roman numerical advantage by shrinking the combat area. Hannibal drew up his least reliable infantry in a semicircle in the center with the wings composed of the Gallic and Numidian horse. The Roman legions forced their way through Hannibal's weak center, but the Libyan mercenaries on the wings, swung around by the movement, menaced their flanks. The onslaught of Hannibal's cavalry was irresistible. Hannibal's chief cavalry commander Maharbal led the mobile Numidian cavalry on the right, and they shattered the Roman cavalry opposing them. Hannibal's Iberian and Gallic heavy cavalry, led by Hanno on the left, defeated the Roman heavy cavalry, and then both the Carthaginian heavy cavalry and the Numidians attacked the legions from behind. As a result, the Roman army was hemmed in with no means of escape.\n\nDue to these brilliant tactics, Hannibal managed to surround and destroy all but a small remnant of his enemy, despite his own inferior numbers. Depending upon the source, it is estimated that 50,000-70,000 Romans were killed or captured. Among the dead were Roman Consul Lucius Aemilius Paullus, as well as two consuls for the preceding year, two quaestors, twenty-nine out of the forty-eight military tribunes, and an additional eighty senators (at a time when the Roman Senate was composed of no more than 300 men, this constituted 25%–30% of the governing body). This makes the battle one of the most catastrophic defeats in the history of Ancient Rome, and one of the bloodiest battles in all of human history (in terms of the number of lives lost within a single day).\n\nAfter Cannae, the Romans were very hesitant to confront Hannibal in pitched battle, preferring instead to weaken him by attrition, relying on their advantages of interior lines, supply, and manpower. As a result, Hannibal fought no more major battles in Italy for the rest of the war. It is believed that his refusal to bring the war to Rome itself was due to a lack of commitment from Carthage of men, money, and materiel — principally siege equipment. Whatever the reason, the choice prompted Maharbal to say, \"Hannibal, you know how to gain a victory, but not how to use one.\"\n\nAs a result of this victory, many parts of Italy joined Hannibal's cause. As Polybius notes, \"How much more serious was the defeat of Cannae, than those that preceded it can be seen by the behavior of Rome's allies; before that fateful day, their loyalty remained unshaken, now it began to waver for the simple reason that they despaired of Roman Power.\" During that same year, the Greek cities in Sicily were induced to revolt against Roman political control, while Macedonian King Philip V pledged his support to Hannibal – thus initiating the First Macedonian War against Rome. Hannibal also secured an alliance with newly appointed Hieronymus of Syracuse. It is often argued that, if Hannibal had received proper material reinforcements from Carthage, he might have succeeded with a direct attack upon Rome. Instead, he had to content himself with subduing the fortresses that still held out against him, and the only other notable event of 216 BC was the defection of certain Italian territories, including Capua, the second largest city of Italy, which Hannibal made his new base. However, only a few of the Italian city-states defected to him that he had expected to gain as allies.\n\nThe war in Italy settled into a strategic stalemate. The Romans used the attritional strategy that Fabius had taught them, and which, they finally realized, was the only feasible means of defeating Hannibal. Indeed, Fabius received the surname \"Cunctator\" (\"the Delayer\") because of his policy of not meeting Hannibal in open battle but through guerilla, scorched earth tactics.\nThe Romans deprived Hannibal of a large-scale battle and instead assaulted his weakening army with multiple smaller armies in an attempt to both weary him and create unrest in his troops. For the next few years, Hannibal was forced to sustain a scorched earth policy and obtain local provisions for protracted and ineffectual operations throughout southern Italy. His immediate objectives were reduced to minor operations centered mainly round the cities of Campania.\n\nThe forces detached to his lieutenants were generally unable to hold their own, and neither his home government nor his new ally Philip V of Macedon helped to make up his losses. His position in southern Italy, therefore, became increasingly difficult and his chance of ultimately conquering Rome grew ever more remote. Hannibal still won a number of notable victories: completely destroying two Roman armies in 212 BC, and killing two consuls (including the famed Marcus Claudius Marcellus) in a battle in 208 BC. However, Hannibal slowly began losing ground—inadequately supported by his Italian allies, abandoned by his government (either because of jealousy or simply because Carthage was overstretched), and unable to match Rome's resources. He was never able to bring about another grand decisive victory that could produce a lasting strategic change.\n\nCarthaginian political will was embodied in the ruling oligarchy. There was a Carthaginian Senate, but the real power was with the inner \"Council of 30 Nobles\" and the board of judges from ruling families known as the \"Hundred and Four\". These two bodies came from the wealthy, commercial families of Carthage. Two political factions operated in Carthage: the war party, also known as the \"Barcids\" (Hannibal's family name); and the peace party led by Hanno II the Great. Hanno had been instrumental in denying Hannibal's requested reinforcements following the battle at Cannae.\n\nHannibal started the war without the full backing of Carthaginian oligarchy. His attack of Saguntum had presented the oligarchy with a choice of war with Rome or loss of prestige in Iberia. The oligarchy, not Hannibal, controlled the strategic resources of Carthage. Hannibal constantly sought reinforcements from either Iberia or North Africa. Hannibal's troops who were lost in combat were replaced with less well-trained and motivated mercenaries from Italy or Gaul. The commercial interests of the Carthaginian oligarchy dictated the reinforcement and supply of Iberia rather than Hannibal throughout the campaign.\n\nIn 212 BC, Hannibal captured Tarentum but he failed to obtain control of its harbour. The tide was slowly turning against him, and in favor of Rome.\n\nThe Romans then mounted two sieges of Capua, which fell in 211 BC, and completed their conquest of Syracuse and destruction of the Carthaginian army in Sicily. Shortly thereafter, the Romans pacified Sicily and entered into an alliance with the Aetolian League to counter Philip V of Macedon. Philip, who attempted to exploit Rome's preoccupation in Italy to conquer Illyria, now found himself under attack from several sides at once and was quickly subdued by Rome and her Greek allies. Meanwhile, Hannibal had defeated Fulvius at the battle of Herdonia in Apulia, but lost Tarentum the following year.\n\nIn 210 BC, Hannibal again proved his superiority in tactics by inflicting a severe defeat at Herdonia (modern Ordona) in Apulia upon a proconsular army and, in 208 BC, destroyed a Roman force engaged in the siege of Locri. But with the loss of Tarentum in 209 BC and the gradual reconquest by the Romans of Samnium and Lucania, his hold on south Italy was almost lost. In 207 BC, he succeeded in making his way again into Apulia, where he waited to concert measures for a combined march upon Rome with his brother Hasdrubal Barca. On hearing, however, of his brother's defeat and death at the battle of the Metaurus, he retired to Calabria, where he maintained himself for the ensuing years. His brother's head had been cut off, carried across Italy, and tossed over the palisade of Hannibal's camp as a cold message of the iron-clad will of the Roman Republic. The combination of these events marked the end to Hannibal's success in Italy. With the failure of his brother Mago in Liguria (205–203 BC) and of his own negotiations with Phillip V, the last hope of recovering his ascendancy in Italy was lost. In 203 BC, after nearly fifteen years of fighting in Italy, and with the military fortunes of Carthage rapidly declining, Hannibal was recalled to Carthage to direct the defense of his native country against a Roman invasion under Scipio Africanus.\n\nIn 203 BC, Hannibal was recalled from Italy by the war party in Carthage. After leaving a record of his expedition engraved in Punic and Greek upon bronze tablets in the temple of Juno at Crotona, he sailed back to Africa. His arrival immediately restored the predominance of the war party, which placed him in command of a combined force of African levies and his mercenaries from Italy. In 202 BC, Hannibal met Scipio in a fruitless peace conference. Despite mutual admiration, negotiations floundered due to Roman allegations of \"Punic Faith,\" referring to the breach of protocols that ended the First Punic War by the Carthaginian attack on Saguntum, and a Carthaginan attack on a stranded Roman fleet. Scipio and Carthage had worked out a peace plan, which was approved by Rome. The terms of the treaty were quite modest, but the war had been long for the Romans. Carthage could keep its African territory but would lose its overseas empire. Masinissa (Numidia) was to be independent. Also, Carthage was to reduce its fleet and pay a war indemnity. But Carthage then made a terrible blunder. Its long-suffering citizens had captured a stranded Roman fleet in the Gulf of Tunis and stripped it of supplies, an action that aggravated the faltering negotiations. Meanwhile, Hannibal, recalled from Italy by the Carthaginian Senate, had returned with his army. Fortified by both Hannibal and the supplies, the Carthaginians rebuffed the treaty and Roman protests. The decisive battle of Zama soon followed; the defeat removed Hannibal's air of invincibility.\n\nUnlike most battles of the Second Punic War, at Zama, the Romans were superior in cavalry and the Carthaginians had the edge in infantry. This Roman cavalry superiority was due to the betrayal of Masinissa, who had earlier assisted Carthage in Iberia, but changed sides in 206 BC with the promise of land and due to his personal conflicts with Syphax, a Carthaginian ally. Although the aging Hannibal was suffering from mental exhaustion and deteriorating health after years of campaigning in Italy, the Carthaginians still had the advantage in numbers and were boosted by the presence of 80 war elephants.\n\nThe Roman cavalry won an early victory by swiftly routing the Carthaginian horse, and standard Roman tactics for limiting the effectiveness of the Carthaginian war elephants were successful, including playing trumpets to frighten the elephants into running into the Carthaginian lines. Some historians say that the elephants routed the Carthaginian cavalry and not the Romans, whilst others suggest that it was actually a tactical retreat planned by Hannibal. Whatever the truth, the battle remained closely fought. At one point, it seemed that Hannibal was on the verge of victory, but Scipio was able to rally his men, and his cavalry, having routed the Carthaginian cavalry, attacked Hannibal's rear. This two-pronged attack caused the Carthaginian formation to collapse.\n\nWith their foremost general defeated, the Carthaginians had no choice but to surrender. Carthage lost approximately 20,000 troops with an additional 15,000 wounded. In contrast, the Romans suffered only 2,500 casualties. The last major battle of the Second Punic War resulted in a loss of respect for Hannibal by his fellow Carthaginians. The conditions of defeat were such that Carthage could no longer battle for Mediterranean supremacy.\n\nHannibal was still only 43 and soon showed that he could be a statesman as well as a soldier. Following the conclusion of a peace that left Carthage stripped of its formerly mighty empire, Hannibal prepared to take a back seat for a time. However, the blatant corruption of the oligarchy gave Hannibal a chance to re-emerge and he was elected suffete (chief magistrate). The office had become rather insignificant, but Hannibal restored its power and authority. The oligarchy, always jealous of him, had even charged him with having betrayed the interests of his country while in Italy, for neglecting to take Rome when he might have done so. So effectively did Hannibal reform abuses that the heavy tribute imposed by Rome could be paid by installments without additional and extraordinary taxation. He also reformed the Hundred and Four, stipulating that its membership be chosen by direct election rather than co-option. He also used citizen support to change the term of office in the Hundred and Four from life to a year, with a term limit of two years.\nSeven years after the victory of Zama, the Romans, alarmed by Carthage's renewed prosperity, demanded Hannibal's surrender. Hannibal thereupon went into voluntary exile. He journeyed to Tyre, the mother city of Carthage, and then to Ephesus, where he was honorably received by Antiochus III of Syria, who was preparing for war with Rome. Hannibal soon saw that the king's army was no match for the Romans. He advised equipping a fleet and landing a body of troops in the south of Italy, offering to take command himself. But he could not make much impression on Antiochus, who listened to his courtiers and would not entrust Hannibal with any important office. According to Cicero, while at the court of Antiochus, Hannibal attended a lecture by Phormio, a philosopher, that ranged through many topics. When Phormio finished a discourse on the duties of a general, Hannibal was asked his opinion. He replied, \"I have seen during my life many old fools; but this one beats them all.\" Another story, according to Aulus Gellius, is that when Antiochus III showed off the gigantic and elaborately equipped army he had created to invade Greece to Hannibal, he asked him if they would be enough for the Roman Republic, to which Hannibal replied, \"I think all this will be enough, yes, quite enough, for the Romans, even though they are most avaricious.\" In 191 BC, the Romans under Manius Acilius Glabrio routed Antiochus at the battle of Thermopylae and obliged him to withdraw to Asia. The Romans followed up their success by attacking Antiochus in Anatolia and the Seleucid Empire was decisively defeated at the battle of Magnesia in 190 BC by Lucius Cornelius Scipio Asiaticus.\n\nIn 190 BC, Hannibal was placed in command of a Seleucid fleet but was defeated in the battle of the Eurymedon. According to Strabo and Plutarch, Hannibal also received hospitality at the Armenian royal court of Artaxias I. The authors add an apocryphal story of how Hannibal planned and supervised the building of the new royal capital Artaxata. When Antiochus seemed prepared to surrender him to the Romans, Hannibal fled to Crete, but he soon went back to Anatolia and sought refuge with Prusias I of Bithynia, who was engaged in warfare with Rome's ally, King Eumenes II of Pergamon. Hannibal went on to serve Prusias in this war. During one of the naval victories he gained over Eumenes, Hannibal had large pots filled with venomous snakes thrown onto Eumenes' ships. Hannibal also went on to defeat Eumenes in two other battles on land until the Romans interfered and threatened Bithynia into giving up Hannibal.\n\nPrusias agreed to give him up, but Hannibal was determined not to fall into his enemy's hands. The precise year and cause of Hannibal's death are unknown. Pausanias wrote that Hannibal's death occurred upon when mounting his horse, his finger becoming wounded by his drawn sword resulted in a fever and then his death three days later. Juvenal asserts that his death was at Libyssa on the eastern shore of the Sea of Marmara, after having taken poison, which, it was said, he had long carried about with him in a ring. Before dying, he left behind a letter declaring, \"Let us relieve the Romans from the anxiety they have so long experienced, since they think it tries their patience too much to wait for an old man's death\".\n\nIn his \"Annales\", Titus Pomponius Atticus reports it occurred in 183 BC, and Livy implies the same. Polybius, who wrote nearest the event, gives 182 BC. Sulpicius Blitho records it under 181 BC.\n\nHannibal caused great distress to many in Roman society. Hannibal became such a figure of terror that whenever disaster struck, the Roman Senators would exclaim \"\" (\"Hannibal is at the gates!\") to express their fear or anxiety. This famous Latin phrase became a common expression that is often still used when a client arrives through the door or when one is faced with calamity.\n\nThe works of Roman writers such as Livy, Frontinus, and Juvenal show a grudging admiration for Hannibal. The Romans even built statues of the Carthaginian in the very streets of Rome to advertise their defeat of such a worthy adversary. It is plausible to suggest that Hannibal engendered the greatest fear Rome had towards an enemy. Nevertheless, they grimly refused to admit the possibility of defeat and rejected all overtures for peace; they even refused to accept the ransom of prisoners after Cannae.\n\nDuring the war there are no reports of revolutions among the Roman citizens, no factions with the Senate desiring peace, no pro-Carthaginian Roman turncoats, no coups. Indeed, throughout the war Roman aristocrats ferociously competed with each other for positions of command to fight against Rome's most dangerous enemy. Hannibal's military genius was not enough to really disturb the Roman political process and the collective political and military capacity of the Roman people. As Lazenby states,\nIt says volumes, too, for their political maturity and respect for constitutional forms that the complicated machinery of government continued to function even amidst disaster—there are few states in the ancient world in which a general who had lost a battle like Cannae would have dared to remain, let alone would have continued to be treated respectfully as head of state.\nAccording to the historian Livy, the Romans feared Hannibal's military genius, and during Hannibal's march against Rome in 211 BC\n\"a messenger who had travelled from Fregellae for a day and a night without stopping created great alarm in Rome, and the excitement was increased by people running about the City with wildly exaggerated accounts of the news he had brought. The wailing cry of the matrons was heard everywhere, not only in private houses but even in the temples. Here they knelt and swept the temple-floors with their dishevelled hair and lifted up their hands to heaven in piteous entreaty to the gods that they would deliver the City of Rome out of the hands of the enemy and preserve its mothers and children from injury and outrage.\" In the Senate the news was \"received with varying feelings as men's temperaments differed,\" so it was decided to keep Capua under siege, but to send 15,000 infantry and 1,000 cavalry as reinforcements to Rome.\n\nAccording to Livy, the land occupied by Hannibal's army outside Rome in 211 BC was sold at the very time of its occupation and for the same price. This may not be true but as Lazenby states, \"could well be, exemplifying as it does not only the supreme confidence felt by the Romans in ultimate victory, but also the way in which something like normal life continued. After Cannae the Romans showed a considerable steadfastness in adversity. An undeniable proof of Rome's confidence is demonstrated by the fact that after the Cannae disaster she was left virtually defenseless, but the Senate still chose not to withdraw a single garrison from an overseas province to strengthen the city. In fact, they were reinforced and the campaigns there maintained until victory was secured; beginning first in Sicily under the direction of Claudius Marcellus, and later in Hispania under Scipio Africanus. Although the long-term consequences of Hannibal's war are debatable, this war was undeniably Rome's \"finest hour\".\n\nMost of the sources available to historians about Hannibal are from Romans. They considered him the greatest enemy Rome had ever faced. Livy gives us the idea that he was extremely cruel. Even Cicero, when he talked of Rome and its two great enemies, spoke of the \"honourable\" Pyrrhus and the \"cruel\" Hannibal. Yet a different picture is sometimes revealed. When Hannibal's successes had brought about the death of two Roman consuls, he vainly searched for the body of Gaius Flaminius Nepos on the shores of Lake Trasimene, held ceremonial rituals in recognition of Lucius Aemilius Paullus, and sent Marcellus' ashes back to his family in Rome. Any bias attributed to Polybius, however, is more troublesome, since he was clearly sympathetic towards Hannibal. Nevertheless, Polybius spent a long period as a hostage in Italy and relied heavily on Roman sources, so there remains the possibility that he reproduced elements of Roman propaganda.\n\nHannibal is generally regarded as one of the best military strategists and tacticians of all time, the double envelopment at Cannae an enduring legacy of tactical brilliance. According to Appian, several years after the Second Punic War, Hannibal served as a political advisor in the Seleucid Kingdom and Scipio was sent there on a diplomatic mission from Rome.\nMilitary academies all over the world continue to study Hannibal's exploits (especially his victory at Cannae).\n\nMaximilian Otto Bismarck Caspari, in his article in the Encyclopædia Britannica Eleventh Edition, praises Hannibal in these words:\nEven the Roman chroniclers acknowledged Hannibal's supreme military leadership, writing that, \"he never required others to do what he could and would not do himself\".\nAccording to Polybius 23, 13, p. 423:\n\nCount Alfred von Schlieffen developed his eponymously titled \"Schlieffen Plan\" (1905/1906) from his military studies, with a particularly heavy emphasis on the envelopment technique which Hannibal employed to surround and destroy the Roman army in the battle of Cannae. George S. Patton believed himself a reincarnation of Hannibal as well as of many other people, including a Roman legionary and a Napoleonic soldier. Norman Schwarzkopf Jr., the commander of the Coalition of the Gulf War, claimed, \"The technology of war may change, the sophistication of weapons certainly changes. But those same principles of war that applied to the days of Hannibal apply today.\"\n\nAccording to the military historian Theodore Ayrault Dodge,\nHannibal's name occurs commonly in later art and popular culture, an objective measure of his considerable influence on Western history.\n\nAs with other military leaders, Hannibal's victories against superior forces in an ultimately losing cause won him enduring fame that outlasted his native North Africa. His crossing of the Alps remains one of the most monumental military feats of ancient warfare and has since captured the imagination of the world (romanticized by several artworks).\n\nList of derivative works (novels unless otherwise noted):\n\n\n\n\n\n", "id": "13959", "title": "Hannibal"}
{"url": "https://en.wikipedia.org/wiki?curid=13961", "text": "Hansie Cronje\n\nWessel Johannes \"Hansie\" Cronje (25 September 1969 – 1 June 2002) was a South African cricketer and captain of the South African national cricket team in the 1990s. He died in a plane crash in 2002. He was voted the 11th greatest South African in 2004 despite having been banned from cricket for life due to his role in a match-fixing scandal.\n\nCronje was born in Bloemfontein, South Africa to Ewie Cronje and San-Marie Cronje on 25 September 1969. He graduated in 1987 from Grey College in Bloemfontein, where he was the head boy. An excellent all round sportsman, he represented the then Orange Free State Province in cricket and rugby at schools level. He was the captain of his school's cricket and rugby teams. Cronje earned a Bachelor of Commerce degree from the University of the Free State. He had an older brother, Frans Cronje, and a younger sister, Hester Parsons.\n\nHis father Ewie had played for Orange Free State in the 1960s, and Frans had also played first-class cricket.\n\nCronje made his first-class debut for Orange Free State against Transvaal at Johannesburg in January 1988 at the age of 18. In the following season, he was a regular, appearing in all eight Currie Cup matches plus being part of the Benson and Hedges Series winning team, scoring 73 as an opener in the final. In 1989–90, despite playing all the Currie Cup matches, he failed to make a century, and averaged only 19.76; however, in one-day games he averaged 60.12. During that season he scored his maiden century for South African Universities against Mike Gatting's rebels.\n\nDespite having just turned 21, Cronje was made captain of Orange Free State for the 1990–91 season. He scored his maiden century for them against Natal in December 1990, and finished the season with another century and a total of 715 runs at 39.72. That season he also scored 159* in a 40-over match against Griqualand West.\n\nIn 1992–93, he captained Orange Free State to the Castle Cup/Total Power Series double.\n\nIn 1995, Cronje appeared for Leicestershire where he scored 1301 runs at 52.04 finishing the season as the county's leading scorer.\n\nIn 1995–96, he finished the season top of the batting averages in the Currie Cup, his top score of 158 helped Free State chase down 389 to beat Northern Transvaal.\n\nIn 1997, Cronje played for Ireland as an overseas player in the Benson and Hedges Cup and helped them to a 46-run win over Middlesex by scoring 94 not out and taking three wickets. This was Ireland's first ever win against English county opposition. Later in the same competition, he scored 85 and took one wicket against Glamorgan.\n\nCronje's form in 1991/92 was impressive especially in the one-day format where he averaged 61.40. He earned an international call up for the 1992 World Cup, making his One Day International debut against Australia at Sydney. During the tournament he played in eight of the team's nine games, averaging 34.00 with the bat, while his medium pace was used in bowling 20 overs.\n\nAfter the World Cup Cronje was part of the tour to the West Indies; he featured in the three ODI's, and in the Test match at Bridgetown that followed, he made his Test debut. This was South Africa's first Test since readmission and they came close to beating a strong West Indian side, going into the final day at 122/2 chasing 200 they collapsed to 148.\n\nIndia toured South Africa in 1992/93. In the first one-day international, he hit the famous six when his team needed 6 runs off only 4 balls, and was awarded Man of the match for his bowling. In the one-day series, Cronje managed just one fifty but with the ball he was economical and took his career best figures of 5/32, becoming the second South African to take five wickets in an ODI. In the Test series that followed he scored his maiden test century, 135 off 411 balls, after coming in at 0–1 in the second over he was last man out, after eight and three-quarter hours, in a total of 275. This contributed to South Africa's first Test win since readmission. At the end of the season in a triangular tournament with Pakistan and West Indies he scored 81 off 70 balls against Pakistan.\n\nIn South Africa's next Test series against Sri Lanka, Cronje scored his second Test century, 122 in the second Test in Colombo; the victory margin of an innings and 208 runs is a South African record. He finished the series with 237 runs at 59.25 after scoring 73* in the drawn third Test.\n\nIn 1993–94, there was another Castle Cup/Total Power Series double for Orange Free State. In international cricket, he was named as vice-captain for the tour of Australia despite being the youngest member of the squad. In the first ODI of the triangular tournament with New Zealand and Australia, he guided South Africa to victory against Australia at the MCG with 91*, which won him the man of the match award. He scored 71 in a rain-affected first test at Melbourne before a tense second test that South Africa won by 5 runs. An injury to captain Kepler Wessels meant Cronje was captain for the final day of the match. Between the second and third tests, the one-day tournament continued, now with Cronje as captain, South Africa made the final series but lost it 2–1 to Australia. He became South Africa's second youngest Test captain, after Murray Bisset in 1898–99, when he led the team for the third test at Adelaide but it was an unsuccessful start to his captaincy career as the series was squared.\n\nIn February 1994, there was the return series as Australia toured South Africa. Cronje started the ODI series with scores of 112, 97, 45 and 50* and when Australia played Orange Free State in their final match before the first Test, Cronje hit 251 off 306 balls, 200 of these came on the final day in which 294 runs were added. Despite this, Orange Free State lost the match. In the first test at Johannesburg, he added another century as South Africa won by 197 runs. This innings was the end of a 14-day period in which he'd scored 721 runs against the Aussies. However, he failed to reach fifty in the next two tests and four ODIs as both series were drawn.\n\nThere was another drawn series when South Africa toured England in 1994. Cronje scored only one century on the whole tour and scored only 90 runs in the three-test series. In October 1994, South Africa again came up against Australia in a triangular one day series also featuring Pakistan. Cronje scored 354 runs at an average of 88.50. Despite this, South Africa lost all their matches. This series was Bob Woolmer's first as coach and Kepler Wessels' last as captain. Cronje, who'd previously been vice-captain, was named as captain for the test series with New Zealand in 1994–95.\n\nSouth Africa lost the first Test in Johannesburg but before the second test the two teams plus Pakistan and Sri Lanka competed for the Mandela Trophy, New Zealand failed to gain a win in the six match round robin stage while South Africa beat Pakistan in the final. This changed the momentum as South Africa secured wins in Durban and Cape Town, where Cronje scored his fourth test century, he was the first captain since W. G. Grace to win a three-match rubber after being one down.\n\nIn early 1995, South Africa won one-off tests against both Pakistan and New Zealand, in Auckland Cronje scored the only century of the match before a final day declaration left his bowlers barely enough time to dismiss the Kiwis.\n\nIn October 1995, South Africa won a one-off Test with Zimbabwe. Cronje scored a second innings 54* to guide them to seven wicket win. In two one-dayers that followed, he took five wickets as South Africa comfortably won both. South Africa won the five Test series against England 1–0 despite Cronje struggling, scoring 113 runs at 18.83. However, he top scored in the one-day series that they won 6–1.\n\nIn the 1996 World Cup, he scored 78 and 45* against New Zealand and Pakistan respectively as South Africa won their group but in the Quarter final with West Indies a Brian Lara century ended their ten-game winning streak.\n\nThe 1996–97 season featured back-to-back series with India. The first away was lost 2–1. The home series was won 2–0. In the six tests combined, Cronje managed one fifty. Cronje produced better form against Australia, averaging over 50 in both test and ODI series although both were lost.\n\nCronje started 1997–98 by leading South Africa to their first series victory in Pakistan, his batting continued to struggle with his biggest contribution being taking the wickets of Inzamam-ul-Haq and Moin Khan in the Third Test.\n\nCronje once again came up against Australia and once again ended on the losing side. In the triangular one day series they won the group with Australia just scraping through, they also won the first 'final' but South Africa lost the last two finals. During the group matches Cronje had threatened to lead his team off after Pat Symcox had missiles thrown at him, Symcox had the last laugh ending the match with 4/24. Before the Test series started he scored consecutive centuries against Tasmania and Australia A these were his first in two years.\n\nIn the first Test, Cronje scored 70 as South Africa saved the match; in the second Test, he lasted 335 minutes for his 88. Despite this, they lost by an innings. In the third Test, they scored 517 and although Mark Taylor carried his bat for 169, Australia needed to bat 109 overs to save the match. Mark Waugh batted 404 minutes, and, despite controversy when Waugh hit one of his bails off (under Law 35 he was adjudged to have finished his stroke and therefore given not out), South Africa fell three wickets short. Cronje put a stump through the umpires` dressing room door after the match and was lucky to avoid a ban.\n\nCronje missed the first Test of the series with Pakistan because of a knee injury. The second Test at Durban was lost, but he top scored at Port Elizabeth with 85, to help square the three Test series 1–1. There was still time in the season for a two-Test series with Sri Lanka. The first was won with Cronje scoring 49 and 74; in the second Test, he took 3/14, his best bowling in Tests, and smashed 82 off 63 balls, his fifty being brought up with three consecutive sixes off Muttiah Muralitharan, and was reached off just 31 balls; at the time, it was the second fastest in Tests after Kapil Dev's. In the triangular series, which South Africa won, he scored only one fifty at East London where he also took 2/17 off 10 overs.\n\nDuring the 1998 Test series against England, Cronje scored five consecutive fifties, having failed to score one in the nine previous Tests against them. In his fiftieth Test, at Trent Bridge he scored 126, his sixth and last Test century and his first in 29 matches. During his second innings of 67, he passed 3,000 runs – only the second South African to do so. However, England won the Test, and the one at Headingley, to win the series 2–1, Cronje finished the series as South Africa's top scorer with 401 runs at 66.83.\n\nIn the West Indies series of 1998–99, Cronje captained South Africa to their only whitewash in a five Test series. His best batting against West Indies came when playing for Free State; he scored 158* as they chased down 438 and made up a first innings deficit of 249. In the ODI series he was South Africa's top scorer and took 11 wickets at 14.72 as South Africa won 6–1.\n\nIn March 1999, they toured New Zealand, beating them 1–0 in the Test series and 3–2 in the one-dayers.\n\nAt the 1999 World Cup, Cronje finished with 98 runs at 12.25 as South Africa were eliminated after the famous tied semi-final against Australia at Edgbaston. In the first match of the tournament versus India, Cronje came onto the field with an earpiece wired to coach Bob Woolmer, but at the first drinks break match referee Talat Ali ordered him to remove it.\n\nIn October 1999, Cronje became South Africa's highest Test run scorer during the first Test against Zimbabwe. The two Test series was won 2–0 thanks to innings victories. South Africa won the series with England in the fourth Test at Cape Town, Cronje's fiftieth as captain.\n\nThe fifth test of the 1999–2000 South Africa versus England series at Centurion was ruined by rain, entering the final day only 45 overs had been possible with South Africa 155/6. On the final morning as they batted on, news filtered through that the captains had met and were going to \"make a game of it\". A target of 250 from 70 overs was agreed. When South Africa reached 248/8, Cronje declared; both teams then forfeited an innings leaving England a target of 249 to win the Test, which they did with two wickets left and only five balls remaining. It ended South Africa's 14 game unbeaten streak in Test cricket. It was later learnt Cronje accepted money and a gift from a bookmaker in return for making an early declaration in this Test. (See below).\n\nCronje top scored with 56 after South Africa were left reeling at 21–5 in the Final of the triangular tournament which featured England and Zimbabwe.\n\nOn 31 March 2000, his cricket career finished with a 73-ball 79 against Pakistan in the final of Sharjah Cup 1999/2000.\n\nUnder Cronje's captaincy, South Africa won 27 Tests and lost 11, completing series victories against every team except Australia.\n\nHe captained the One Day International team to 99 wins out of 138 matches with one tied match and three no results. He holds the South African record for matches won as captain, and his record of captaining his side in 138 matches stands bettered only by Graeme Smith's 149 matches as ODI captain. His 99 wins as captain makes him the fourth most successful captain worldwide in terms of matches won, behind Ricky Ponting, Allan Border and Mahendra Singh Dhoni and in terms of percentage of wins (73.70), behind Ponting and Clive Lloyd.\n\nBetween September 1993 and March 2000, he played in 162 consecutive ODIs, a South African record.\n\nOn 7 April 2000, it was revealed there was a conversation between Cronje and Sanjay Chawla, a representative of an Indian betting syndicate, over match-fixing allegations. Three other players, Herschelle Gibbs, Nicky Boje, and Pieter Strydom, were also implicated. After an enquiry by the King Commission, Cronje was banned from any involvement in cricket for life. He challenged his life ban in September 2001 but on 17 October 2001, his application was dismissed.\n\nAfter 13 years, on 22 July 2013, the Indian Police registered a First Information Report for match-fixing in 2000; the charge sheet in the case involving a few South African cricketers including its former captain Hansie Cronje, was finally filed. The scandal was one of the biggest ever to have hit international cricket until the Pakistan cricket spot-fixing scandal.\n\nOn 1 June 2002, Cronje's scheduled flight home from Johannesburg to George was grounded. So he hitched a ride as the only passenger aboard a Hawker Siddeley HS 748 turboprop aircraft. Near George airport, the pilots lost visibility in clouds and were unable to land, partly due to unusable navigational equipment. While circling, the plane crashed into the Outeniqua Mountains northeast of the airport. Cronje, aged 32, and the two pilots were killed instantly.\n\nIn August 2006, an inquest into the plane crash was opened by South Africa's High Court. The inquest concluded that \"the death of the deceased Wessel Johannes Cronje was brought about by an act or omission prima facie amounting to an offence on the part of pilots.\"\n\nTheories that Cronje was murdered on the orders of a cricket betting syndicate flourished after his death and were most recently re-floated by former Nottinghamshire coach Clive Rice in the wake of the death of Pakistan coach Bob Woolmer in March 2007.\n\nHansie Cronje married Bertha Hans on 8 April 1995. They had no children. Hansie's widow later married Jacques Du Plessis, a financial auditor, in 2003. It was reported that the private ceremony was attended by Hansie's parents and siblings.\n\n\n", "id": "13961", "title": "Hansie Cronje"}
{"url": "https://en.wikipedia.org/wiki?curid=13963", "text": "Hultsfred Municipality\n\nHultsfred Municipality (\"Hultsfreds kommun\") is a municipality in Kalmar County, in south-eastern Sweden. The seat is in the town of Hultsfred.\n\nThe present municipality was created in 1971 through the amalgamation of the market town (\"köping\") of Hultsfred (instituted in 1927) with a number of surrounding municipalities. In 1863 there were eight entities in the area.\n\nHultsfred is known as the site of a major rock festival in Sweden, the Hultsfred Festival.\n\nBasically every one of the localities of Hultsfred Municipality are situated on the railway. Besides Hultsfred, in the mid north of the municipality, there are the towns of Virserum in the south-west and other ever smaller settlements such as Lönneberga, Silverdalen and Målilla. The population of the municipality has however been decreasing with some 2,000 people in the last 10 years, as many people move to larger cities, causing a decrease in nativity.\n\nMuch of the geography is taken up with forests, a notability for the entire province of Småland, with some few scattered areas suitable for agriculture.\n\nIn the age known as the Nordic Bronze Age, the area had some shipping of furs to northern Germany and the Roman army, but not much is known from that time other than the area being inhabited; there has also been older finds from 3000-4000 BC. However, from the medieval age, around 1100 AD, there still remains a few churches.\n\nThe area continued to be inhabited mainly by farmers until the 20th century. In the 17th and 18th there was some production of iron in Kalmar County, totalling about 10 mines; of those 2 were located to the municipality of Hultsfred. Hultsfred was a center for some military exercising companies during the 19th century, and some remaining building can be visited in the vicinity of Silverån. When the railroads through Sweden were built late in that century, Hultsfred received a population boost.\n\nThere are several folks museums around the area that keeps trace of its history.\n\nThere are eight urban areas (also called a Tätort or locality) in Hultsfred Municipality.\n\nIn the table the localities are listed according to the size of the population as of December 31, 2005. The municipal seat is in bold characters.\n\nHultsfred Municipality is twinned with:\n\n", "id": "13963", "title": "Hultsfred Municipality"}
{"url": "https://en.wikipedia.org/wiki?curid=13964", "text": "Parliament of the United Kingdom\n\nThe Parliament of the United Kingdom, commonly known as the UK Parliament or British Parliament, is the supreme legislative body in the United Kingdom, British Crown dependencies and British overseas territories. It alone possesses legislative supremacy and thereby ultimate power over all other political bodies in the UK and its territories. Its head is the Sovereign of the United Kingdom (currently Queen Elizabeth II) and its seat is the Palace of Westminster in the City of Westminster, London.\n\nThe parliament is bicameral, consisting of an upper house (the House of Lords) and a lower house (the House of Commons). The Sovereign forms the third component of the legislature (the Queen-in-Parliament). The House of Lords includes two different types of members: the Lords Spiritual, consisting of the most senior bishops of the Church of England, and the Lords Temporal, consisting mainly of life peers, appointed by the Sovereign on the advice of the Prime Minister, and of 92 hereditary peers, sitting either by virtue of holding a royal office, or by being elected by their fellow hereditary peers. Prior to the opening of the Supreme Court in October 2009, the House of Lords also performed a judicial role through the Law Lords.\n\nThe House of Commons is a democratically elected chamber with elections held at least every five years. The two Houses meet in separate chambers in the Palace of Westminster (commonly known as the Houses of Parliament) in London. By constitutional convention, all government ministers, including the Prime Minister, are members of the House of Commonsor, less commonly, the House of Lordsand are thereby accountable to the respective branches of the legislature. Most cabinet ministers (Secretaries of State) tend to be from the Commons, whilst junior ministers are from both Houses. \n\nThe Parliament of Great Britain was formed in 1707 following the ratification of the Treaty of Union by Acts of Union passed by the Parliament of England and the Parliament of Scotland. At the start of the 19th century, Parliament was further enlarged by Acts of Union ratified by the Parliament of Great Britain and the Parliament of Ireland that abolished the latter and added 100 Irish MPs and 32 Lords to the former to create the Parliament of the United Kingdom of Great Britain and Ireland. The Royal and Parliamentary Titles Act 1927 formally amended the name to the \"Parliament of the United Kingdom of Great Britain and Northern Ireland\", five years after the secession of the Irish Free State.\n\nThe UK parliament and its institutions have set the pattern for many democracies throughout the world, and it has been called \"the mother of parliaments\". However, John Brightwho coined the epithetused it with reference to a country (England) rather than a parliament.\n\nIn theory, the UK's supreme legislative power is vested in the Crown-in-Parliament. However, the Crown acts on the advice of the Prime Minister and the powers of the House of Lords have been curtailed; thus power is \"de facto\" vested in the House of Commons.\n\nThe United Kingdom of Great Britain and Ireland was created in 1801, by the merger of the Kingdoms of Great Britain and Ireland under the Acts of Union.\n\nThe principle of ministerial responsibility to the lower House did not develop until the 19th century—the House of Lords was superior to the House of Commons both in theory and in practice. Members of the House of Commons (MPs) were elected in an antiquated electoral system, under which constituencies of vastly different sizes existed. Thus, the borough of Old Sarum, with seven voters, could elect two members, as could the borough of Dunwich, which had almost completely disappeared into the sea due to land erosion.\n\nMany small constituencies, known as pocket or rotten boroughs, were controlled by members of the House of Lords, who could ensure the election of their relatives or supporters. During the reforms of the 19th century, beginning with the Reform Act 1832, the electoral system for the House of Commons was progressively regularised. No longer dependent on the Lords for their seats, MPs grew more assertive.\nThe supremacy of the British House of Commons was established in the early 20th century. In 1909, the Commons passed the so-called \"People's Budget\", which made numerous changes to the taxation system which were detrimental to wealthy landowners. The House of Lords, which consisted mostly of powerful landowners, rejected the Budget. On the basis of the Budget's popularity and the Lords' consequent unpopularity, the Liberal Party narrowly won two general elections in 1910.\n\nUsing the result as a mandate, the Liberal Prime Minister, Herbert Henry Asquith, introduced the Parliament Bill, which sought to restrict the powers of the House of Lords. (He did not reintroduce the land tax provision of the People's Budget.) When the Lords refused to pass the bill, Asquith countered with a promise extracted from the King in secret before the second general election of 1910 and requested the creation of several hundred Liberal peers, so as to erase the Conservative majority in the House of Lords. In the face of such a threat, the House of Lords narrowly passed the bill.\n\nThe Parliament Act 1911, as it became, prevented the Lords from blocking a money bill (a bill dealing with taxation), and allowed them to delay any other bill for a maximum of three sessions (reduced to two sessions in 1949), after which it could become law over their objections. However, regardless of the Parliament Acts of 1911 and 1949, the House of Lords has always retained the unrestricted power to veto any bill outright which attempts to extend the life of a parliament.\n\nThe Government of Ireland Act 1920 created the parliaments of Northern Ireland and Southern Ireland and reduced the representation of both parts at Westminster. The number of Northern Ireland seats was increased again after the introduction of direct rule in 1973. The Irish Free State became independent in 1922, and in 1927 parliament was renamed the Parliament of the United Kingdom of Great Britain and Northern Ireland.\n\nFurther reforms to the House of Lords were made in the 20th century. The Life Peerages Act 1958 authorised the regular creation of life peerage dignities. By the 1960s, the regular creation of hereditary peerage dignities had ceased; thereafter, almost all new peers were life peers only.\n\nThe House of Lords Act 1999 removed the automatic right of hereditary peers to sit in the Upper House, although it made an exception for 92 of them to be elected to life-terms by the other hereditary peers, with by-elections upon their death. The House of Lords is now a chamber that is subordinate to the House of Commons. Additionally, the Constitutional Reform Act 2005 led to abolition of the judicial functions of the House of Lords with the creation of the new Supreme Court of the United Kingdom in October 2009.\n\nThe legislative authority, the Crown-in-Parliament, has three separate elements: the Monarch, the House of Lords, and the House of Commons. No individual may be a member of both Houses, and members of the House of Lords are legally barred from voting in elections for members of the House of Commons.\n\nRoyal Assent of the Monarch is required for all Bills to become law, and certain Delegated Legislation must be made by the Monarch by Order in Council. The Crown also has executive powers which do not depend on Parliament, through prerogative powers, including the power to make treaties, declare war, award honours, and appoint officers and civil servants. In practice these are always exercised by the monarch on the advice of the Prime Minister and the other ministers of HM Government. The Prime Minister and government are directly accountable to Parliament, through its control of public finances, and to the public, through the election of Members of Parliament.\n\nThe Monarch also appoints the Prime Minister, who then forms a government from members of the Houses of Parliament. This must be someone who could command a majority in a confidence vote in the House of Commons. In the past the monarch has occasionally had to make a judgment, as in the appointment of Alec Douglas-Home in 1963 when it was thought that the incumbent Prime Minister, Harold Macmillan, had become ill with terminal cancer. However, today the monarch is advised by the outgoing Prime Minister as to whom he or she should offer the position next.\n\nThe Upper House is formally styled \"The Right Honourable The Lords Spiritual and Temporal in Parliament Assembled\", the Lords Spiritual being bishops of the Church of England and the Lords Temporal being Peers of the Realm. The Lords Spiritual and Lords Temporal are considered separate \"estates\", but they sit, debate and vote together.\n\nSince the Parliament Acts 1911 and 1949, the powers of the House of Lords have been very much less than those of the House of Commons. All bills except money bills are debated and voted upon in the House of Lords; however, by voting against a bill, the House of Lords can only delay it for a maximum of two parliamentary sessions over a year. After that time, the House of Commons can force the Bill through without the Lords' consent, under the Parliament Acts. The House of Lords can also hold the government to account through questions to government ministers and the operation of a small number of select committees. The highest court in England & Wales and in Northern Ireland used to be a committee of the House of Lords, but it became an independent supreme court in 2009.\n\nThe Lords Spiritual formerly included all of the senior clergymen of the Church of England—archbishops, bishops, abbots and mitred priors. Upon the Dissolution of the Monasteries under Henry VIII the abbots and mitred priors lost their positions in Parliament. All diocesan bishops continued to sit in Parliament, but the Bishopric of Manchester Act 1847, and later Acts, provide that only the 26 most senior are Lords Spiritual. These always include the incumbents of the \"five great sees\", namely the Archbishop of Canterbury, the Archbishop of York, the Bishop of London, the Bishop of Durham and the Bishop of Winchester. The remaining 21 Lords Spiritual are the most senior diocesan bishops, ranked in order of consecration, although the Lords Spiritual (Women) Act 2015 makes time-limited provision for vacancies to be filled by women who are bishops.\n\nThe Lords Temporal are all members of the Peerage. Formerly, they were all hereditary peers. The right of some hereditary peers to sit in Parliament was not automatic: after Scotland and England united into Great Britain in 1707, it was provided that all peers whose dignities had been created by English kings could sit in Parliament, but those whose dignities had been created by Scottish kings were to elect a limited number of \"representative peers\". A similar arrangement was made in respect of Ireland when it was united with Great Britain in 1801, but when southern Ireland left the United Kingdom in 1922 the election of Irish representative peers ceased. By the Peerage Act 1963, the election of Scottish representative peers also ended, and all Scottish peers were granted the right to sit in Parliament. Under the House of Lords Act 1999, only life peerages (that is to say, peerage dignities which cannot be inherited) automatically entitle their holders to seats in the House of Lords. Of the hereditary peers, only 92—the Earl Marshal, the Lord Great Chamberlain and the 90 elected by other peers—retain their seats in the House.\n\nThe Commons, the last of the \"estates\" of the Kingdom, are represented in the House of Commons, which is formally styled \"The Honourable The Commons in Parliament Assembled\" (\"commons\" coming not from the term \"commoner\", but from \"\", the old French term for a district). In 2016, the House consists of 650 members. Each \"Member of Parliament\" or \"MP\" is chosen by a single constituency by the First-Past-the-Post electoral system. Universal adult suffrage exists for those 18 and over; citizens of the United Kingdom, and those of the Republic of Ireland and Commonwealth nations resident in the United Kingdom, are qualified to vote, unless they are in prison at the time of the elections. The term of members of the House of Commons depends on the term of Parliament, a maximum of five years; a general election, during which all the seats are contested, occurs after each dissolution (see below).\n\nAll legislation must be passed by the House of Commons to become law and it controls taxation and the supply of money to the government. Government ministers (including the Prime Minister) must regularly answer questions in the House of Commons and there are a number of select committees that scrutinise particular issues and the workings of the government. There are also mechanisms that allow members of the House of Commons to bring to the attention of the government particular issues affecting their constituents.\n\nThe State Opening of Parliament is an annual event that marks the commencement of a session of the Parliament of the United Kingdom. It is held in the House of Lords Chamber and, before 2012, took place in November or December, or in a general election year, when the new Parliament first assembled. From 2012 onwards, the ceremony has taken place in May or June.\nMotioned by the Monarch, the Lord Great Chamberlain raises his wand of office to signal to Black Rod, who is charged with summoning the House of Commons and has been waiting in the Commons lobby. Black Rod turns and, under the escort of the Door-keeper of the House of Lords and an inspector of police, approaches the doors to the Chamber of the Commons. In 1642, King Charles I stormed into the House of Commons in an unsuccessful attempt to arrest the Five Members, who included the celebrated English patriot and leading Parliamentarian John Hampden. This action sparked the English Civil War. The wars established the constitutional rights of Parliament, a concept legally established in the Glorious Revolution in 1688 and the subsequent Bill of Rights 1689. Since then, no British monarch has entered the House of Commons when it is in session. On Black Rod's approach, the doors are slammed shut against him, symbolising the rights of parliament and its independence from the monarch. He then strikes with the end of his ceremonial staff (the Black Rod) three times on the closed doors of the Commons Chamber. He is then admitted, and announces the command of the monarch for the attendance of the Commons.\n\nThe monarch reads a speech, known as the Speech from the Throne, which is prepared by the Prime Minister and his or her cabinet members, outlining the Government's agenda for the coming year. The speech reflects the legislative agenda for which the Cabinet seeks the agreement of both Houses of Parliament.\n\nAfter the monarch leaves, each Chamber proceeds to the consideration of an \"Address in Reply to Her Majesty's Gracious Speech\". But first, each House considers a bill \"pro forma\" to symbolise their right to deliberate independently of the monarch. In the House of Lords, the bill is called the Select Vestries Bill, while the Commons equivalent is the Outlawries Bill. The Bills are considered for the sake of form only, and do not make any actual progress.\n\nBoth houses of the British Parliament are presided over by a speaker, the Speaker of the House for the Commons and the Lord Speaker in the House of Lords.\n\nFor the Commons, the approval of the Sovereign is theoretically required before the election of the Speaker becomes valid, but it is, by modern convention, always granted. The Speaker's place may be taken by three deputies, known as the Chairman, First Deputy Chairman and Second Deputy Chairman of Ways and Means. (They take their name from the Committee of Ways and Means, of which they were once presiding officers, but which no longer exists.)\n\nPrior to July 2006, the House of Lords was presided over by a Lord Chancellor (a Cabinet member), whose influence as Speaker was very limited (whilst the powers belonging to the Speaker of the House of Commons are vast). However, as part of the Constitutional Reform Act 2005, the position of Speaker of the House of Lords (as it is termed in the Act) was separated from the office of Lord Chancellor (the office which has control over the judiciary as a whole), though the Lords remain largely self-governing. Decisions on points of order and on the disciplining of unruly members are made by the whole body in the Upper House, but by the Speaker alone in the Lower House. Speeches in the House of Lords are addressed to the House as a whole (using the words \"My Lords\"), but those in the House of Commons are addressed to the Speaker alone (using \"Mr Speaker\" or \"Madam Speaker\"). Speeches may be made to both Houses simultaneously.\n\nBoth Houses may decide questions by voice vote; members shout out \"Aye!\" and \"No!\" in the Commons—or \"Content!\" and \"Not-Content!\" in the Lords—and the presiding officer declares the result. The pronouncement of either Speaker may be challenged, and a recorded vote (known as a division) demanded. (The Speaker of the House of Commons may choose to overrule a frivolous request for a division, but the Lord Speaker does not have that power.) In each House, a division requires members to file into one of the two lobbies alongside the Chamber; their names are recorded by clerks, and their votes are counted as they exit the lobbies to re-enter the Chamber. The Speaker of the House of Commons is expected to be non-partisan, and does not cast a vote except in the case of a tie; the Lord Speaker, however, votes along with the other Lords.\n\nBoth Houses normally conduct their business in public, and there are galleries where visitors may sit.\n\nAs at 2016, Parliament has a fixed term of 5 years.\n\nOriginally there was no fixed limit on the length of a Parliament, but the Triennial Act 1694 set the maximum duration at three years. As the frequent elections were deemed inconvenient, the Septennial Act 1715 extended the maximum to seven years, but the Parliament Act 1911 reduced it to five. During the Second World War, the term was temporarily extended to ten years by Acts of Parliament. Since the end of the war the maximum has remained five years. Modern Parliaments, however, rarely continued for the maximum duration; normally, they were dissolved earlier. For instance, the 52nd, which assembled in 1997, was dissolved after four years. The Septennial Act was repealed by the Fixed-term Parliaments Act 2011.\n\nSummary history of terms of the Parliament of the United Kingdom\n\nFollowing a general election, a new Parliamentary session begins. Parliament is formally summoned 40 days in advance by the Sovereign, who is the source of parliamentary authority. On the day indicated by the Sovereign's proclamation, the two Houses assemble in their respective chambers. The Commons are then summoned to the House of Lords, where Lords Commissioners (representatives of the Sovereign) instruct them to elect a Speaker. The Commons perform the election; on the next day, they return to the House of Lords, where the Lords Commissioners confirm the election and grant the new Speaker the royal approval in the Sovereign's name.\n\nThe business of Parliament for the next few days of its session involves the taking of the oaths of allegiance. Once a majority of the members have taken the oath in each House, the State Opening of Parliament may take place. The Lords take their seats in the House of Lords Chamber, the Commons appear at the Bar (at the entrance to the Chamber), and the Sovereign takes his or her seat on the throne. The Sovereign then reads the Speech from the Throne—the content of which is determined by the Ministers of the Crown—outlining the Government's legislative agenda for the upcoming year. Thereafter, each House proceeds to the transaction of legislative business.\n\nBy custom, before considering the Government's legislative agenda, a bill is introduced \"pro forma\" in each House—the Select Vestries Bill in the House of Lords and the Outlawries Bill in the House of Commons. These bills do not become laws; they are ceremonial indications of the power of each House to debate independently of the Crown. After the \"pro forma\" bill is introduced, each House debates the content of the Speech from the Throne for several days. Once each House formally sends its reply to the Speech, legislative business may commence, appointing committees, electing officers, passing resolutions and considering legislation.\n\nA session of Parliament is brought to an end by a prorogation. There is a ceremony similar to the State Opening, but much less well-known. Normally, the Sovereign does not personally attend the prorogation ceremony in the House of Lords; he or she is represented by Lords Commissioners. The next session of Parliament begins under the procedures described above, but it is not necessary to conduct another election of a Speaker or take the oaths of allegiance afresh at the beginning of such subsequent sessions. Instead, the State Opening of Parliament proceeds directly. To avoid the delay of opening a new session in the event of an emergency during the long summer recess, Parliament is no longer prorogued beforehand, but only after the Houses have reconvened in the autumn; the State Opening follows a few days later.\n\nEach Parliament comes to an end, after a number of sessions, in anticipation of a general election. Parliament is dissolved by virtue of the Fixed-term Parliaments Act 2011. Prior to that, dissolution was effected by the Sovereign, always on the advice of the Prime Minister. The Prime Minister could seek dissolution at a time politically advantageous to his or her party. If the Prime Minister loses the support of the House of Commons, Parliament will dissolve and a new election will be held. Parliaments can also be dissolved if two-thirds of the House of Commons votes for an early election.\n\nFormerly, the demise of the Sovereign automatically brought a Parliament to an end, the Crown being seen as the \"\" (beginning, basis and end) of the body, but this is no longer the case. The first change was during the reign of William and Mary, when it was seen to be inconvenient to have no Parliament at a time when succession to the Crown could be disputed, and an Act was passed that provided that a Parliament was to continue for six months after the death of a Sovereign, unless dissolved earlier. Under the Representation of the People Act 1867 Parliament can now continue for as long as it would otherwise have done in the event of the death of the Sovereign.\n\nAfter each Parliament concludes, the Crown issues writs to hold a general election and elect new members of the House of Commons, though membership of the House of Lords does not change.\n\nLaws can be made by Acts of the United Kingdom Parliament. While Acts can apply to the whole of the United Kingdom including Scotland, due to the continuing separation of Scots law many Acts do not apply to Scotland and may be matched either by equivalent Acts that apply to Scotland alone or, since 1999, by legislation set by the Scottish Parliament relating to devolved matters.\n\nThis has led to a paradox known as the West Lothian question. The existence of a devolved Scottish Parliament means that while Westminster MPs from Scotland may vote directly on matters that affect English constituencies, they may not have much power over their laws affecting their own constituency. While any Act of the Scottish Parliament may be overturned, amended or ignored by Westminster, in practice this has yet to happen. Furthermore, the existence of the Legislative Consent Motion enables English MPs to vote on issues nominally devolved to Scotland, as part of United Kingdom legislation. Since there is no devolved \"English Parliament\", the converse is not true.\n\nLaws, in draft form known as bills, may be introduced by any member of either House. A bill introduced by a Minister is known as a \"Government Bill\"; one introduced by another member is called a \"Private Member's Bill\". A different way of categorising bills involves the subject. Most bills, involving the general public, are called \"Public Bills\". A bill that seeks to grant special rights to an individual or small group of individuals, or a body such as a local authority, is called a \"Private Bill\". A Public Bill which affects private rights (in the way a Private Bill would) is called a \"Hybrid Bill\", although those that draft bills take pains to avoid this.\n\nPrivate Members' Bills make up the majority of bills, but are far less likely to be passed than government bills. There are three methods for an MP to introduce a Private Member's Bill. The Private Members' Ballot (once per Session) put names into a ballot, and those who win are given time to propose a bill. The Ten Minute Rule is another method, where MPs are granted ten minutes to outline the case for a new piece of legislation. Standing Order 57 is the third method, which allows a bill to be introduced without debate if a day's notice is given to the Table Office. Filibustering is a danger, as an opponent of a bill can waste much of the limited time allotted to it. Private Members' Bills have no chance of success if the current government opposes them, but they are used in moral issues: the bills to decriminalise homosexuality and abortion were Private Members' Bills, for example. Governments can sometimes attempt to use Private Members' Bills to pass things it would rather not be associated with. \"Handout bills\" are bills which a government hands to MPs who win Private Members' Ballots.\n\nEach Bill goes through several stages in each House. The first stage, called the first reading, is a formality. At the second reading, the general principles of the bill are debated, and the House may vote to reject the bill, by not passing the motion \"That the Bill be now read a second time\". Defeats of Government Bills are extremely rare, the last being in 2005.\n\nFollowing the second reading, the bill is sent to a committee. In the House of Lords, the Committee of the Whole House or the Grand Committee are used. Each consists of all members of the House; the latter operates under special procedures, and is used only for uncontroversial bills. In the House of Commons, the bill is usually committed to a Public Bill Committee, consisting of between 16 and 50 members, but the Committee of the Whole House is used for important legislation. Several other types of committees, including Select Committees, may be used, but rarely. A committee considers the bill clause by clause, and reports the bill as amended to the House, where further detailed consideration (\"consideration stage\" or \"report stage\") occurs. However, a practice which used to be called the \"kangaroo\" (Standing Order 32) allows the Speaker to select which amendments are debated. This device is also used under Standing Order 89 by the committee chairman, to restrict debate in committee.\n\nOnce the House has considered the bill, the third reading follows. In the House of Commons, no further amendments may be made, and the passage of the motion \"That the Bill be now read a third time\" is passage of the whole bill. In the House of Lords further amendments to the bill may be moved. After the passage of the third reading motion, the House of Lords must vote on the motion \"That the Bill do now pass\". Following its passage in one House, the bill is sent to the other House. If passed in identical form by both Houses, it may be presented for the Sovereign's Assent. If one House passes amendments that the other will not agree to, and the two Houses cannot resolve their disagreements, the bill fails.\n\nHowever, since the passage of the Parliament Act 1911 the power of the House of Lords to reject bills passed by the House of Commons has been restricted, and further restrictions were placed by the Parliament Act 1949. If the House of Commons passes a public bill in two successive sessions, and the House of Lords rejects it both times, the Commons may direct that the bill be presented to the Sovereign for his or her Assent, disregarding the rejection of the Bill in the House of Lords. In each case, the bill must be passed by the House of Commons at least one calendar month before the end of the session. The provision does not apply to bills originated in the House of Lords, to bills seeking to extend the duration of a Parliament beyond five years, or to Private Bills. A special procedure applies in relation to bills classified by the Speaker of the House of Commons as \"Money Bills\". A Money Bill concerns \"solely\" national taxation or public funds; the Speaker's certificate is deemed conclusive under all circumstances. If the House of Lords fails to pass a Money Bill within one month of its passage in the House of Commons, the Lower House may direct that the Bill be submitted for the Sovereign's Assent immediately.\n\nEven before the passage of the Parliament Acts, the Commons possessed pre-eminence in cases of financial matters. By ancient custom, the House of Lords may not introduce a bill relating to taxation or Supply, nor amend a bill so as to insert a provision relating to taxation or Supply, nor amend a Supply Bill in any way. The House of Commons is free to waive this privilege, and sometimes does so to allow the House of Lords to pass amendments with financial implications. The House of Lords remains free to reject bills relating to Supply and taxation, but may be overruled easily if the bills are Money Bills. (A bill relating to revenue and Supply may not be a Money Bill if, for example, it includes subjects other than national taxation and public funds).\n\nThe last stage of a bill involves the granting of the Royal Assent. Theoretically, the Sovereign may either grant the Royal Assent (that is, make the bill a law) or withhold it (that is, veto the bill). In modern times the Sovereign always grants the Royal Assent, using the Norman French words \"La Reyne le veult\" (the Queen wishes it; \"Le Roy\" instead in the case of a king). The last refusal to grant the Assent was in 1708, when Queen Anne withheld her Assent from a bill \"for the settling of Militia in Scotland\", in the words \"\" (the Queen will think it over).\n\nThus, every bill obtains the assent of all three components of Parliament before it becomes law (except where the House of Lords is over-ridden under the Parliament Acts 1911 and 1949). The words \"BE IT ENACTED by the Queen's [King's] most Excellent Majesty, by and with the advice and consent of the Lords Spiritual and Temporal, and Commons, in this present Parliament assembled, and by the authority of the same, as follows:-\", or, where the House of Lords' authority has been overridden by use of the Parliament Acts, the words \"BE IT ENACTED by The Queen's [King's] most Excellent Majesty, by and with the advice and consent of the Commons in this present Parliament assembled, in accordance with the provisions of the Parliament Acts 1911 and 1949, and by the authority of the same, as follows:-\" appear near the beginning of each Act of Parliament. These words are known as the enacting formula.\n\nPrior to the creation of the Supreme Court of the United Kingdom in October 2009, Parliament also used to perform several judicial functions. The Queen-in-Parliament constituted the highest court in the realm for most purposes, but the Privy Council had jurisdiction in some cases (for instance, appeals from ecclesiastical courts). The jurisdiction of Parliament arose from the ancient custom of petitioning the Houses to redress grievances and to do justice. The House of Commons ceased considering petitions to reverse the judgements of lower courts in 1399, effectively leaving the House of Lords as the court of last resort. In modern times, the judicial functions of the House of Lords were performed not by the whole House, but by a group of \"Lords of Appeal in Ordinary\" (judges granted life peerage dignities under the Appellate Jurisdiction Act 1876 by the Sovereign) and by \"Lords of Appeal\" (other peers with experience in the judiciary). However, under the Constitutional Reform Act 2005, these judicial functions were transferred to the newly created Supreme Court in 2009, and the Lords of Appeal in Ordinary became the first Justices of the Court. Peers who hold high judicial office are no longer allowed to vote or speak in the Lords until they retire as Justices.\n\nIn the late 19th century, Acts allowed for the appointment of \"Scottish Lords of Appeal in Ordinary\" and ended appeal in Scottish criminal matters to the House of Lords, so that the High Court of Justiciary became the highest criminal court in Scotland. There is an argument that the provisions of Article XIX of the Union with England Act 1707 prevent any Court outside Scotland from hearing any appeal in criminal cases: \"\"And that the said Courts or any other of the like nature after the Unions shall have no power to Cognosce Review or Alter the Acts or Sentences of the Judicatures within Scotland or stop the Execution of the same.\"\" Nowadays the House of Lords legislative committee usually has a minimum of two Scottish Judges to ensure that some experience of Scots law is brought to bear on Scottish appeals in civil cases, from the Court of Session.\n\nCertain other judicial functions have historically been performed by the House of Lords. Until 1948, it was the body in which peers had to be tried for felonies or high treason; now, they are tried by normal juries. When the House of Commons impeaches an individual, the trial takes place in the House of Lords. Impeachments are now rare; the last one occurred in 1806. In 2006, a number of MPs attempted to revive the custom, having signed a motion for the impeachment of Tony Blair, but this was unsuccessful.\n\nThe British Government is answerable to the House of Commons. However, neither the Prime Minister nor members of the Government are elected by the House of Commons. Instead, the Queen requests the person most likely to command the support of a majority in the House, normally the leader of the largest party in the House of Commons, to form a government. So that they may be accountable to the Lower House, the Prime Minister and most members of the Cabinet are, by convention, members of the House of Commons. The last Prime Minister to be a member of the House of Lords was Alec Douglas-Home, 14th Earl of Home, who became Prime Minister in 1963. To adhere to the convention under which he was responsible to the Lower House, he disclaimed his peerage and procured election to the House of Commons within days of becoming Prime Minister.\n\nGovernments have a tendency to dominate the legislative functions of Parliament, by using their in-built majority in the House of Commons, and sometimes using their patronage power to appoint supportive peers in the Lords. Formerly, no-one could be a member of Parliament while holding an Office of profit under the Crown, thus maintaining the separation of powers, but the principle has been gradually eroded. Until 1919, Members of Parliament who were appointed to ministerial office lost their right to sit in the House of Commons and had to seek re-election. The rule survives in the House of Commons Disqualification Act 1975 which specifies a number of state positions that make an individual ineligible to serve as a Member of Parliament. The only vestige of the principle is the process of resignation from the House of Commons.\nIn practice, governments can pass any legislation (within reason) in the Commons they wish, unless there is major dissent by MPs in the governing party.\nBut even in these situations, it is highly unlikely a bill will be defeated, though dissenting MPs may be able to extract concessions from the government. In 1976, Lord Hailsham created a now widely used name for this behaviour, in an academic paper called \"elective dictatorship\".\n\nParliament controls the executive by passing or rejecting its Bills and by forcing Ministers of the Crown to answer for their actions, either at \"Question Time\" or during meetings of the parliamentary committees. In both cases, Ministers are asked questions by members of their Houses, and are obliged to answer.\n\nAlthough the House of Lords may scrutinise the executive through Question Time and through its committees, it cannot bring down the Government. A ministry must always retain the confidence and support of the House of Commons. The Lower House may indicate its lack of support by rejecting a Motion of Confidence or by passing a Motion of No Confidence. Confidence Motions are generally originated by the Government in order to reinforce its support in the House, whilst No Confidence Motions are introduced by the Opposition. The motions sometimes take the form \"That this House has [no] confidence in Her Majesty's Government\" but several other varieties, many referring to specific policies supported or opposed by Parliament, are used. For instance, a Confidence Motion of 1992 used the form, \"That this House expresses the support for the economic policy of Her Majesty's Government.\" Such a motion may theoretically be introduced in the House of Lords, but, as the Government need not enjoy the confidence of that House, would not be of the same effect as a similar motion in the House of Commons; the only modern instance of such an occurrence involves the 'No Confidence' motion that was introduced in 1993 and subsequently defeated.\n\nMany votes are considered votes of confidence, although not including the language mentioned above. Important bills that form part of the Government's agenda (as stated in the Speech from the Throne) are generally considered matters of confidence. The defeat of such a bill by the House of Commons indicates that a Government no longer has the confidence of that House. The same effect is achieved if the House of Commons \"withdraws Supply\", that is, rejects the budget.\n\nWhere a Government has lost the confidence of the House of Commons, the Prime Minister is obliged either to resign, or seek the dissolution of Parliament and a new general election. Where a Prime Minister has ceased to retain a majority in that vote and requests a dissolution, the Sovereign can in theory reject his request, forcing his resignation and allowing the Leader of the Opposition to be asked to form a new government. This power is used extremely rarely. The conditions that should be met to allow such a refusal are known as the Lascelles Principles. These conditions and principles are constitutional conventions arising from the Sovereign's reserve powers as well as longstanding tradition and practice, not laid down in law.\n\nIn practice, the House of Commons' scrutiny of the Government is very weak. Since the first-past-the-post electoral system is employed in elections, the governing party tends to enjoy a large majority in the Commons; there is often limited need to compromise with other parties. Modern British political parties are so tightly organised that they leave relatively little room for free action by their MPs. In many cases, MPs may be expelled from their parties for voting against the instructions of party leaders. During the 20th century, the Government has lost confidence issues only three times—twice in 1924, and once in 1979.\n\nIn the United Kingdom, question time in the House of Commons lasts for an hour each day from Monday to Thursday (2:30 to 3:30pm on Mondays, 11:30am to 12:30pm on Tuesdays and Wednesdays, and 9:30 to 10:30am on Thursdays). Each Government department has its place in a rota which repeats every five weeks. The exception to this sequence are the Business Questions (Questions to the Leader of House of Commons), in which questions are answered each Thursday about the business of the House the following week. Also, Questions to the Prime Minister takes place each Wednesday from noon to 12:30pm.\n\nIn addition to government departments, there are also questions to the Church commissioners. Additionally, each Member of Parliament is entitled to table questions for written answer. Written questions are addressed to the Ministerial head of a government department, usually a Secretary of State, but they are often answered by a Minister of State or Parliamentary Under Secretary of State. Written Questions are submitted to the Clerks of the Table Office, either on paper or electronically, and answers are recorded in \"The Official Report (Hansard)\" so as to be widely available and accessible.\n\nIn the House of Lords, a half-hour is set aside each afternoon at the start of the day's proceedings for Lords' oral questions. A peer submits a question in advance, which then appears on the Order Paper for the day's proceedings. The Lord shall say: \"\"My Lords, I beg leave to ask the Question standing in my name on the Order Paper\"\". The Minister responsible then answers the question. The Lord is then allowed to ask a supplementary question and other peers ask further questions on the theme of the original put down on the order paper. (For instance, if the question regards immigration, Lords can ask the Minister any question related to immigration during the allowed period).\n\nSeveral different views have been taken of Parliament's sovereignty. According to the jurist Sir William Blackstone, \"It has sovereign and uncontrollable authority in making, confirming, enlarging, restraining, abrogating, repealing, reviving, and expounding of laws, concerning matters of all possible denominations, ecclesiastical, or temporal, civil, military, maritime, or criminal ... it can, in short, do every thing that is not naturally impossible.\"\n\nA different view has been taken by the Scottish judge Lord Cooper of Culross. When he decided the 1953 case of \"MacCormick v. Lord Advocate\" as Lord President of the Court of Session, he stated, \"The principle of unlimited sovereignty of Parliament is a distinctively English principle and has no counterpart in Scottish constitutional law.\" He continued, \"Considering that the Union legislation extinguished the Parliaments of Scotland and England and replaced them by a new Parliament, I have difficulty in seeing why the new Parliament of Great Britain must inherit all the peculiar characteristics of the English Parliament but none of the Scottish.\" Nevertheless, he did not give a conclusive opinion on the subject.\n\nThus, the question of Parliamentary sovereignty appears to remain unresolved. Parliament has not passed any Act defining its own sovereignty. A related possible limitation on Parliament relates to the Scottish legal system and Presbyterian faith, preservation of which were Scottish preconditions to the creation of the unified Parliament. Since the Parliament of the United Kingdom was set up in reliance on these promises, it may be that it has no power to make laws that break them.\n\nParliament's power has often been eroded by its own Acts. Acts passed in 1921 and 1925 granted the Church of Scotland complete independence in ecclesiastical matters. More recently, its power has been restricted by membership of the European Union, which has the power to make laws enforceable in each member state. In the Factortame case, the European Court of Justice ruled that British courts could have powers to overturn British legislation contravening European law.\n\nParliament has also created national devolved parliaments and assemblies with differing degrees of legislative authority in Scotland, Wales and Northern Ireland. Parliament still has the power over areas for which responsibility lies with the devolved institutions, but would gain the agreement of those institutions to act on their behalf. Similarly, it has granted the power to make regulations to Ministers of the Crown, and the power to enact religious legislation to the General Synod of the Church of England. (Measures of the General Synod and, in some cases proposed statutory instruments made by ministers, must be approved by both Houses before they become law.)\n\nIn every case aforementioned, authority has been conceded by Act of Parliament and may be taken back in the same manner. It is entirely within the authority of Parliament, for example, to abolish the devolved governments in Scotland, Wales and Northern Ireland or to leave the EU. However, Parliament also revoked its legislative competence over Australia and Canada with the Australia and Canada Acts: although the Parliament of the United Kingdom could pass an Act reversing its action, it would not take effect in Australia or Canada as the competence of the Imperial Parliament is no longer recognised there in law.\n\nOne well-recognised exception to Parliament's power involves binding future Parliaments. No Act of Parliament may be made secure from amendment or repeal by a future Parliament. For example, although the Act of Union 1800 states that the Kingdoms of Great Britain and Ireland are to be united \"forever\", Parliament permitted southern Ireland to leave the United Kingdom in 1922.\n\nEach House of Parliament possesses and guards various ancient privileges. The House of Lords relies on inherent right. In the case of the House of Commons, the Speaker goes to the Lords' Chamber at the beginning of each new Parliament and requests representatives of the Sovereign to confirm the Lower House's \"undoubted\" privileges and rights. The ceremony observed by the House of Commons dates to the reign of King Henry VIII. Each House is the guardian of its privileges, and may punish breaches thereof. The extent of parliamentary privilege is based on law and custom. Sir William Blackstone states that these privileges are \"very large and indefinite\", and cannot be defined except by the Houses of Parliament themselves.\n\nThe foremost privilege claimed by both Houses is that of freedom of speech in debate; nothing said in either House may be questioned in any court or other institution outside Parliament. Another privilege claimed is that of freedom from arrest; at one time this was held to apply for any arrest except for high treason, felony or breach of the peace but it now excludes any arrest on criminal charges; it applies during a session of Parliament, and 40 days before or after such a session. Members of both Houses are no longer privileged from service on juries.\n\nBoth Houses possess the power to punish breaches of their privilege. Contempt of Parliament—for example, disobedience of a subpoena issued by a committee—may also be punished. The House of Lords may imprison an individual for any fixed period of time, but an individual imprisoned by the House of Commons is set free upon prorogation. The punishments imposed by either House may not be challenged in any court, and the Human Rights Act does not apply.\n\nUntil at least 2015, members of the House of Commons also had the privilege of a separate seating area in the Palace of Westminster canteen, protected by a false partition labelled \"MPs only beyond this point\", so that they did not have to sit with canteen staff taking a break. This provoked mockery from a newly elected 20-year-old MP who described it as \"ridiculous\" snobbery.\n\nThe quasi-official emblem of the Houses of Parliament is a crowned portcullis. The portcullis was originally the badge of various English noble families from the 14th century. It went on to be adopted by the kings of the Tudor dynasty in the 16th century, under whom the Palace of Westminster became the regular meeting place of Parliament. The crown was added to make the badge a specifically royal symbol.\n\nThe portcullis probably first came to be associated with the Palace of Westminster through its use as decoration in the rebuilding of the Palace after the fire of 1512. However, at the time it was only one of many symbols. The widespread use of the portcullis throughout the Palace dates from the 19th century, when Charles Barry and Augustus Pugin used it extensively as a decorative feature in their designs for the new Palace built following the disastrous 1834 fire.\n\nThe crowned portcullis came to be accepted during the 20th century as the emblem of both houses of parliament. This was simply a result of custom and usage rather than a specific decision. The emblem now appears on official stationery, publications and papers, and is stamped on various items in use in the Palace of Westminster, such as cutlery, silverware and china. Various shades of red and green are used for visual identification of the House of Lords and the House of Commons.\n\n\nLists of MPs elected:\n\nBibliography\n\n", "id": "13964", "title": "Parliament of the United Kingdom"}
{"url": "https://en.wikipedia.org/wiki?curid=13966", "text": "Hosea\n\nIn the Hebrew Bible, Hosea ( or ; ; Greek , \"Ōsēe\"), son of Beeri, was an 8th-century BC prophet in Israel who authored the book of prophecies bearing his name. He is one of the Twelve Prophets of the Jewish Hebrew Bible, also known as the Minor Prophets of the Christian Old Testament. Hosea is often seen as a \"prophet of doom\", but underneath his message of destruction is a promise of restoration. The Talmud (\"Pesachim\" 87a) claims that he was the greatest prophet of his generation. The period of Hosea's ministry extended to some sixty years and he was the only prophet of Israel of his time who left any written prophecy.\n\nThe name \"Hosea\", meaning \"salvation\", or \"He saves\", or \"He helps\", seems to have been not uncommon, being derived from the auspicious verb from which we have the frequently recurring word \"salvation\". It may be a contraction of a larger form of which the divine name (YHWH) or its abbreviation formed a part, so as to signify \"YHWH helps\". According to the Bible\nNumbers 13:8, 13:16 that was the original name of Joshua, son of Nun, until Moses gave him the longer, theophoric name \"Yehoshua\", \"YHWH is salvation\".\n\nAlthough it is not expressly stated in the Book of Hosea, it is apparent from the level of detail and familiarity focused on northern geography, that Hosea conducted his prophetic ministries in the Northern Israel (Samaria) of which he was a native.\n\nLittle is known about the life or social status of Hosea. According to the Book of Hosea, he married the prostitute Gomer, the daughter of Diblaim, at God's command. In ff., there is a reference to the wars which led to the capture of the kingdom by the Assyrians (ca. 734–732 BC). It is not certain if he had also experienced the destruction of Samaria, which is foreseen in .\n\nHosea's family life reflected the \"adulterous\" relationship which Israel had built with polytheistic gods. The relationship between Hosea and Gomer parallels the relationship between God and Israel. Even though Gomer runs away from Hosea and sleeps with another man, he loves her anyway and forgives her. Likewise, even though the people of Israel worshipped false gods, God continued to love them and did not abandon his covenant with them.\n\nSimilarly, his children's names made them like walking prophecies of the fall of the ruling dynasty and the severed covenant with God – much like the prophet Isaiah a generation later. The name of Hosea's daughter, Lo-ruhamah, which translates as \"not pitied\", is chosen by God as a sign of displeasure with the people of Israel for following false gods. (In Hosea 2:23 she is redeemed, shown mercy with the term \"Ruhamah\".) The name of Hosea's son, Lo-ammi, which translates as \"not my people\", is chosen by the Lord as a sign of the Lord's displeasure with the people of Israel for following those false gods (see Hosea 1:8-9).\n\nOne of the early writing prophets, Hosea used his own experience as a symbolic representation of God and Israel: God the husband, Israel the wife. Hosea's wife left him to go with other men; Israel left the Lord to go with false gods. Hosea searched for his wife, found her and brought her back; God would not abandon Israel and brought them back even though they had forsaken him.\n\nThe Book of Hosea was a severe warning to the northern kingdom against the growing idolatry being practiced there; the book was a dramatic call to repentance. Christians extend the analogy of Hosea to Christ and the church: Christ the husband, his church the bride. Christians see in this book a comparable call to the church not to forsake the Lord Jesus Christ. Christians also take the buying back of Gomer as the redemptive qualities of Jesus Christ's sacrifice on the cross.\n\nOther preachers, like Charles Spurgeon, saw Hosea as a striking presentation of the mercy of God in his sermon on Hosea 1:7 titled The LORD's Own Salvation. “But I will have mercy upon the house of Judah, and will save them by the Lord their God, and will not save them by bow, nor by sword, nor by battle, by horses, nor by horsemen.” – Hosea 1:7 in his sermon NO. 2057, December 16TH, 1888.\n\nThe Qur'an mentions only some prophets by name, but makes it clear that many were sent who are not mentioned. Therefore, many Muslim scholars, such as (Ibn Ishaq), speak of Hosea as one of the true Hebrew prophets of Israel. The Book of Hosea has also been used in Qur'anic exegesis by Abdullah Yusuf Ali, especially in reference to Qur'anic verses which speak of the backsliding of Israel.\n\nHe is commemorated with the other Minor prophets in the Calendar of saints of the Armenian Apostolic Church on July 31. He is commemorated on the Eastern Orthodox liturgical calendar, with a feast day on October 17 (for those churches which follow the Julian Calendar, October 17 currently falls on October 30 of the modern Gregorian Calendar). He is also commemorated on the Sunday of the Holy Fathers (the Sunday before the Nativity of the Lord).\n\nThe tomb of Hosea is a structure located in the Jewish cemetery of Safed, believed to be the final resting place of Hosea.\n\n", "id": "13966", "title": "Hosea"}
{"url": "https://en.wikipedia.org/wiki?curid=13967", "text": "Habakkuk\n\nHabakkuk ( or ; ; also spelled Habacuc) was a prophet in the Hebrew Bible. He is the author of the Book of Habakkuk, the eighth of the collected twelve minor prophets.\n\nAlmost nothing is known about Habakkuk, aside from what few facts are stated within the book of the Bible bearing his name, or those inferences that may be drawn from that book. His name appears in the Bible only in Habakkuk 1:1 and 3:1, with no biographical details provided other than his title \"the prophet.\" Even the origin of his name is uncertain.\n\nFor almost every other prophet, more information is given, such as the name of the prophet's hometown, his occupation, or information concerning his parentage or tribe. For Habakkuk, however, there is no reliable account of any of these. Although his home is not identified, scholars conclude that Habakkuk lived in Jerusalem at the time he wrote his prophecy. Further analysis has provided an approximate date for his prophecy and possibilities concerning his activities and background.\n\nBeyond the Bible, considerable conjecture has been put forward over the centuries in the form of Christian and Rabbinic tradition, but such accounts are dismissed by modern scholars as speculative and apocryphal.\n\nBecause the book of Habakkuk consists of five oracles about the Chaldeans (Babylonians), and the Chaldean rise to power is dated circa 612 BC, it is assumed he was active about that time, making him an early contemporary of Jeremiah and Zephaniah. Jewish sources, however, do not group him with those two prophets, who are often placed together, so it is possible that he was slightly earlier than the pair.\n\nBecause the final chapter of his book is a song, it is sometimes assumed that he was a member of the tribe of Levi, which served as musicians in Solomon's Temple.\n\nThe name Habakkuk, or Habacuc, appears in the Hebrew Bible only in Habakkuk 1:1 and 3:1. In the Masoretic Text, it is written in ( \"Ḥavaqquq\" \"Ḥăḇaqqûq\"). This name does not occur elsewhere. The Septuagint transcribes his name into Greek as (\"Ambakoum\"), and the Vulgate transcribes it into Latin as \"Abacuc\".\n\nThe etymology of the name is not clear, and its form has no parallel in Hebrew. The name is possibly related to the Akkadian \"khabbaququ\", the name of a fragrant plant, or the Hebrew root , meaning \"embrace\".\n\nHabakkuk appears in Bel and the Dragon, which is part of the deuterocanonical Additions to Daniel. Verses 33–39 state that Habakkuk is in Judea and after making some stew, he's told by the angel of the Lord to take the stew to Daniel, who is in Babylon in the lion's den. After proclaiming he is unaware of both the den and Babylon, the angel transports Habakkuk to the lion's den. Habakkuk gives Daniel the food to sustain him, and is immediately taken back to \"his own place\".\n\nHabakkuk is also mentioned in Lives of the Prophets, which also notes his time in Babylon.\n\nAccording to the Zohar (Volume 1, page 8b) Habakkuk is the boy born to the Shunamite woman through Elisha's blessing:\n\n() And he said, About this season, according to the time of life, thou shalt embrace ( – hoveket, therefore Habakkuk) a son. And she said, Nay, my lord, [thou] man of God, do not lie unto thine handmaid.\n\nThe only work attributed to Habakkuk is the short book of the Bible that bears his name. The book of Habbakuk consists of five oracles about the Chaldeans (Babylonians) and a song of praise to God.\n\nThe style of the book has been praised by many scholars, suggesting that its author was a man of great literary talent. The entire book follows the structure of a chiasmus in which parallelism of thought is used to bracket sections of the text.\n\nHabakkuk is unusual among the prophets in that he openly questions the working of God (1:3a, 1:13b). In the first part of the first chapter, the Prophet sees the injustice among his people and asks why God does not take action: \"1:2 Yahweh, how long will I cry, and you will not hear? I cry out to you 'Violence!' and will you not save?\" (World English Bible).\n\nThe final resting place of Habakkuk has been claimed at multiple locations. The fifth-century Christian historian Sozomen claimed that the relics of Habakkuk were found at Cela, when God revealed their location to Zebennus, bishop of Eleutheropolis, in a dream. Currently, one location in Israel and one in Iran lay claim to being the burial site of the prophet.\n\nThe burial place of Habakkuk is identified by Jewish tradition as a hillside in the Upper Galilee region of northern Israel, close to the villages Kadarim and Hukok, about six miles southwest of Safed and twelve miles north of Mount Tabor. A small stone building, erected during the 20th century, protects the tomb. Tradition dating as early as the 12th century AD holds that Habakkuk's tomb is at this location, but the tomb may also be of a local sheikh of Yaquq, a name related to the biblical place named \"Hukkok\" (mentioned in ), whose pronunciation and spelling in Hebrew are close to \"Habakkuk\". Archaeological findings in this location include several burial places dated to the Second Temple period.\n\nA mausoleum southeast of the city of Tuyserkan in the west of Iran is also believed to be Habakkuk's burial place. It is protected by Iran's Cultural Heritage, Handcrafts and Tourism Organization. The Organization's guide to the Hamadan Province states that Habakkuk was believed to be a guardian to Solomon's Temple, and that he was captured by the Babylonians and remained in their prison for some years. After being freed by Cyrus the Great, he went to Ecbatana and remained there until he died, and was buried somewhere nearby, in what is today Tuyserkan. Habakkuk is called both Habaghugh and Hayaghugh by the locals.\n\nThe surrounding shrine may date to the period of the Seljuq Empire (11–12th century); it consists of an octagonal wall and conical dome. Underneath the shrine is a hidden basement with three floors. In the center of the shrine's courtyard is the grave where Habakkuk is said to be buried. A stone upon the grave is inscribed in both Hebrew and Persian stating that the prophet's father was Shioua Lovit, and his mother was Lesho Namit.\n\nOn the Eastern Orthodox liturgical calendar, his feast day is December 2. In the Roman Catholic Church, the twelve minor prophets are read in the Roman Breviary during the fourth and fifth weeks of November, which are the last two weeks of the liturgical year, and his feast day is January 15. This day is also celebrated as his feast by the Greek Orthodox Church. In 2011, he was commemorated with the other Minor Prophets in the calendar of saints of the Armenian Apostolic Church on February 8.\n\nHabakkuk has also been commemorated in sculpture. In 1435, the Florentine artist Donatello created a sculpture of the prophet for the bell tower of Florence. This statue, nicknamed \"Zuccone\" (\"Big Pumpkin\") because of the shape of the head, now resides in the Museo dell'Opera del Duomo. The Basilica of Santa Maria del Popolo in Rome contains a Baroque sculpture of Habakkuk by the 17th-century artist Bernini. Between 1800 and 1805, the Brazilian sculptor Aleijadinho completed a soapstone sculpture of Habakkuk as part of his \"Twelve Prophets\". The figures are arranged around the forecourt and monumental stairway in front of the \"Santuário do Bom Jesus do Matosinhos\" at Congonhas.\n\n\n\n", "id": "13967", "title": "Habakkuk"}
{"url": "https://en.wikipedia.org/wiki?curid=13968", "text": "Haggai\n\nHaggai (, \"Ḥaggay\" or \"Hag-i\", Koine Greek: Ἀγγαῖος; ) was a Hebrew prophet during the building of the Second Temple in Jerusalem, and one of the twelve minor prophets in the Hebrew Bible and the author of the Book of Haggai. His name means \"my holiday\". He was the first of three post-exile prophets from the Neo-Babylonian Exile of the House of Judah (with Zechariah, his contemporary, and Malachi, who lived about one hundred years later), who belonged to the period of Jewish history which began after the return from captivity in Babylon.\n\nScarcely anything is known of his personal history. He may have been one of the captives taken to Babylon by Nebuchadnezzar. He began his ministry about sixteen years after the return of the Jews to Judah (ca. 520 BC). The work of rebuilding the temple had been put to a stop through the intrigues of the Samaritans. After having been suspended for eighteen years, the work was resumed through the efforts of Haggai and Zechariah. They exhorted the people, which roused them from their lethargy, and induced them to take advantage of a change in the policy of the Persian government under Darius the Great.\n\nThe name Haggai, with various vocalizations, is also found in the Book of Esther, as a eunuch servant of the Queen.\n\nHaggai supported the officials of his time, specifically Zerubbabel, the governor, and Joshua the High Priest. In the Book of Haggai, God refers to Zerubbabel as \"my servant\" as King David was, and says he will make him as a \"signet ring,\" as King Jehoiachin was (Haggai 2:23; cf. Jer 22:24). The signet ring symbolized a ring worn on the hand of Yahweh, showing that a king held divine favour. Thus, Haggai is implicitly, but not explicitly, saying that Zerubbabel would preside over a restored Davidic kingdom.\n\nHaggai, in rabbinic writing, is often referred to as one of the men of the Great Assembly. The Babylonian Talmud (5th century CE) mentions a tradition concerning the prophet Haggai, saying that he gave instruction concerning three things: (a) that it is not lawful for a man whose brother married his daughter (as a co-wife in a polygamous relationship) to consummate a levirate marriage with one of his deceased brother's co-wives (a teaching accepted by the School of Hillel, but rejected by the School of Shammai); (b) that Jews living in the regions of Ammon and Moab separate from their produce the poor man's tithe during the Sabbatical year; (c) that they accept of proselytes from the peoples of Tadmor (Palmyra) and from the people of Ḳardu.\n\nOn the Eastern Orthodox liturgical calendar, Haggai is commemorated as a saint and prophet. His feast day is December 16 (for those churches which follow the traditional Julian Calendar, December 16 currently falls on December 29 of the modern Gregorian Calendar). He is also commemorated, in common with the other righteous persons of the Old Testament, on the Sunday of the Holy Fathers (the Sunday before the Nativity of the Lord).\n\nHaggai is commemorated with the other Minor prophets in the Calendar of saints of the Armenian Apostolic Church on July 31.\n\nIn the Masonic degree of Holy Royal Arch Haggai is one of the Three Principals of the Chapter. Named after Haggai the prophet and is supported by Zerubbabel, Prince of the People, and Joshua, the son of Josedech, the High Priest.\n\n\n", "id": "13968", "title": "Haggai"}
{"url": "https://en.wikipedia.org/wiki?curid=13969", "text": "Herman Hollerith\n\nHerman Hollerith (February 29, 1860 – November 17, 1929) was an American inventor who developed an electromechanical punched card tabulator to assist in summarizing information and, later, accounting. He was the founder of the Tabulating Machine Company that was consolidated in 1911 with three other companies to form the Computing-Tabulating-Recording Company, later renamed IBM. Hollerith is regarded as one of the seminal figures in the development of data processing. His invention of the punched card tabulating machine marks the beginning of the era of semiautomatic data processing systems, and his concept dominated that landscape for nearly a century.\n\nHerman Hollerith was born the son of German immigrant Prof. Georg Hollerith from Großfischlingen (near Neustadt an der Weinstraße) in Buffalo, New York, where he spent his early childhood. He entered the City College of New York in 1875, graduated from the Columbia University School of Mines with an \"Engineer of Mines\" degree in 1879 at age 19, and in 1890 asked for (and was awarded) a Ph.D based on his development of the tabulating system. In 1882 Hollerith joined the Massachusetts Institute of Technology where he taught mechanical engineering and conducted his first experiments with punched cards. He eventually moved to Washington, D.C., living in Georgetown, with a home on 29th Street and a business building at 31st Street and the C&O Canal, where today there is a commemorative plaque installed by IBM. He died in Washington D.C. of a heart attack.\n\nAt the urging of John Shaw Billings, Hollerith developed a mechanism using electrical connections to increment a counter, recording information. A key idea was that a datum could be recorded by the presence or absence of a hole at a specific location on a card. For example, if a specific hole location indicates \"marital status\", then a hole there can indicate \"married\" while not having a hole indicates \"single\". Hollerith determined that data in specified locations on a card, the now-familiar rows and columns, could be counted or sorted electromechanically. A description of this system, \"An Electric Tabulating System (1889)\", was submitted by Hollerith to Columbia University as his doctoral thesis, and is reprinted in Randell's book. On January 8, 1889, Hollerith was issued U.S. Patent 395,782, claim 2 of which reads:\n\nThe herein-described method of compiling statistics, which consists in recording separate statistical items pertaining to the individual by holes or combinations of holes punched in sheets of electrically non-conducting material, and bearing a specific relation to each other and to a standard, and then counting or tallying such statistical items separately or in combination by means of mechanical counters operated by electro-magnets the circuits through which are controlled by the perforated sheets, substantially as and for the purpose set forth.\n\nHollerith had left teaching and begun working for the United States Census Bureau in the year he filed his first patent application. Titled \"Art of Compiling Statistics\", it was filed on September 23, 1884; U.S. Patent 395,782 was granted on January 8, 1889.\n\nHollerith initially did business under his own name, as \"The Hollerith Electric Tabulating System\", specializing in punched card data processing equipment. He built tabulators and other machines under contract for the Census Office, which used them for the 1890 census. The net effect of the many changes from the 1880 census: the larger population, the data items to be collected, the Census Bureau headcount, the scheduled publications, and the use of Hollerith's electromechanical tabulators, was to reduce the time required to process the census from eight years for the 1880 census to six years for the 1890 census.\n\nIn 1896 Hollerith founded the \"Tabulating Machine Company\" (in 1905 renamed \"The Tabulating Machine Company\"). Many major census bureaus around the world leased his equipment and purchased his cards, as did major insurance companies. Hollerith's machines were used for censuses in England, Italy, Germany, Russia, Austria, Canada, France, Norway, Puerto Rico, Cuba, and the Philippines, and again in the 1900 census. \n\nHe invented the first automatic card-feed mechanism and the first keypunch. The 1890 Tabulator was hardwired to operate on 1890 Census cards. A control panel in his 1906 Type I Tabulator simplified rewiring for different jobs. The 1920s removable control panel supported prewiring and near instant job changing. These inventions were among the foundations of the data processing industry and Hollerith's punched cards (later used for computer input/output) continued in use for almost a century.\n\nIn 1911 four corporations, including Hollerith's firm, were consolidated to form the Computing-Tabulating-Recording Company (CTR). Under the presidency of Thomas J. Watson, it was renamed International Business Machines Corporation (IBM) in 1924. By 1933 the \"The Tabulating Machine Company\" name had disappeared as subsidiary companies were subsumed by IBM.\n\nHollerith is buried at Oak Hill Cemetery in the Georgetown neighborhood of Washington, D.C.\n\nHollerith cards were named after the elder Herman Hollerith, as were Hollerith constants (also sometimes called Hollerith strings), an early type of string constant declaration (in computer programming).\n\nHis great-grandson, the Rt. Rev. Herman Hollerith IV is the Episcopal bishop of the Diocese of Southern Virginia, and another great-grandson, Randolph Marshall Hollerith, is an Episcopal priest and the dean of Washington National Cathedral in Washington D.C..\n\n\n\n", "id": "13969", "title": "Herman Hollerith"}
{"url": "https://en.wikipedia.org/wiki?curid=13971", "text": "History of painting\n\nThe history of painting reaches back in time to artifacts from pre-historic humans, and spans all cultures. It represents a continuous, though periodically disrupted, tradition from Antiquity. Across cultures, and spanning continents and millennia, the history of painting is an ongoing river of creativity, that continues into the 21st century. Until the early 20th century it relied primarily on representational, religious and classical motifs, after which time more purely abstract and conceptual approaches gained favor.\n\nDevelopments in Eastern painting historically parallel those in Western painting, in general, a few centuries earlier. African art, Jewish art, Islamic art, Indian art, Chinese art, and Japanese art each had significant influence on Western art, and vice versa.\n\nInitially serving utilitarian purpose, followed by imperial, private, civic, and religious patronage, Eastern and Western painting later found audiences in the aristocracy and the middle class. From the Modern era, the Middle Ages through the Renaissance painters worked for the church and a wealthy aristocracy. Beginning with the Baroque era artists received private commissions from a more educated and prosperous middle class. Finally in the West the idea of \"art for art's sake\" began to find expression in the work of the Romantic painters like Francisco de Goya, John Constable, and J.M.W. Turner. The 19thcentury saw the rise of the commercial art gallery, which provided patronage in the 20th century.\n\nThe oldest known paintings are approximately 40,000 years old. José Luis Sanchidrián at the University of Cordoba, Spain, believes the paintings are more likely to have been painted by Neanderthals than early modern humans. Images at the Chauvet cave in France are thought to be about 32,000 years old. They are engraved and painted using red ochre and black pigment and show horses, rhinoceros, lions, buffalo, mammoth or humans often hunting. There are examples of cave paintings all over the world—in France, India, Spain, Portugal, China, Australia etc.\n\nVarious conjectures have been made as to the meaning these paintings had to the people that made them. Prehistoric men may have painted animals to \"catch\" their soul or spirit in order to hunt them more easily or the paintings may represent an animistic vision and homage to surrounding nature. They may be the result of a basic need of expression that is innate to human beings, or they could have been for the transmission of practical information.\nIn Paleolithic times, the representation of humans in cave paintings was rare. Mostly, animals were painted, not only animals that were used as food but also animals that represented strength like the rhinoceros or large Felidae, as in the Chauvet Cave. Signs like dots were sometimes drawn. Rare human representations include handprints and stencils, and figures depicting human / animal hybrids. The Chauvet Cave in the Ardèche Departments of France contains the most important preserved cave paintings of the Paleolithic era, painted around 31,000 BC. The Altamira cave paintings in Spain were done 14,000 to 12,000 BC and show, among others, bisons. The hall of bulls in Lascaux, Dordogne, France, is one of the best known cave paintings and dates to about 15,000 to 10,000 BC.\n\nIf there is meaning to the paintings, it remains unknown. The caves were not in an inhabited area, so they may have been used for seasonal rituals. The animals are accompanied by signs which suggest a possible magic use. Arrow-like symbols in Lascaux are sometimes interpreted as being used as calendars or almanacs, but the evidence remains inconclusive. The most important work of the Mesolithic era were the \"marching warriors\", a rock painting at Cingle de la Mola, Castellón, Spain dated to about 7000 to 4000 BC. The technique used was probably spitting or blowing the pigments onto the rock. The paintings are quite naturalistic, though stylized. The figures are not three-dimensional, even though they overlap\n\nThe earliest known Indian paintings (see section below) were the rock paintings of prehistoric times, the petroglyphs as found in places like the Rock Shelters of Bhimbetka, (see above) and some of them are older than 5500 BC. Such works continued and after several millennia, in the 7th century, carved pillars of Ajanta, Maharashtra state present a fine example of Indian paintings. The colors, mostly various shades of red and orange, were derived from minerals.\n\nThe history of Eastern painting includes a vast range of influences from various cultures and religions. Developments in Eastern painting historically parallel those in Western painting, in general a few centuries earlier. African art, Jewish art, Islamic art, Indian art, Chinese art, Korean Art, and Japanese art each had significant influence on Western art, and, vice versa.\n\nChinese painting is one of the oldest continuous artistic traditions in the world. The earliest paintings were not representational but ornamental; they consisted of patterns or designs rather than pictures. Early pottery was painted with spirals, zigzags, dots, or animals. It was only during the Warring States period (403–221 B.C.) that artists began to represent the world around them. Japanese painting is one of the oldest and most highly refined of the Japanese arts, encompassing a wide variety of genre and styles. The history of Japanese painting is a long history of synthesis and competition between native Japanese aesthetics and adaptation of imported ideas. Korean painting, as an independent form, began around 108 B.C., around the fall of Gojoseon, making it one of the oldest in the world. The artwork of that time period evolved into the various styles that characterized the Three Kingdoms of Korea period, most notably the paintings and frescoes that adorn the tombs of Goguryeo's royalty. During the Three Kingdoms period and through the Goryeo dynasty, Korean painting was characterized primarily by a combination of Korean-style landscapes, facial features, Buddhist-centered themes, and an emphasis on celestial observation that was facilitated by the rapid development of Korean astronomy.\n\n\"See also Chinese painting, Japanese painting, Korean painting.\"\n\nChina, Japan and Korea have a strong tradition in painting which is also highly attached to the art of calligraphy and printmaking (so much that it is commonly seen as painting). Far east traditional painting is characterized by water based techniques, less realism, \"elegant\" and stylized subjects, graphical approach to depiction, the importance of white space (or negative space) and a preference for landscape (instead of the human figure) as a subject. Beyond ink and color on silk or paper scrolls, gold on lacquer was also a common medium in painted East Asian artwork. Although silk was a somewhat expensive medium to paint upon in the past, the invention of paper during the 1st century AD by the Han court eunuch Cai Lun provided not only a cheap and widespread medium for writing, but also a cheap and widespread medium for painting (making it more accessible to the public).\n\nThe ideologies of Confucianism, Daoism, and Buddhism played important roles in East Asian art. Medieval Song dynasty painters such as Lin Tinggui and his \"Luohan Laundering\" (housed in the Smithsonian Freer Gallery of Art) of the 12th century are excellent examples of Buddhist ideas fused into classical Chinese artwork. In the latter painting on silk (image and description provided in the link), bald-headed Buddhist Luohan are depicted in a practical setting of washing clothes by a river. However, the painting itself is visually stunning, with the Luohan portrayed in rich detail and bright, opaque colors in contrast to a hazy, brown, and bland wooded environment. Also, the tree tops are shrouded in swirling fog, providing the common \"negative space\" mentioned above in East Asian Art.\n\nIn Japonisme, late 19th-century Post-Impressionists like Van Gogh and Henri de Toulouse-Lautrec, and tonalists such as James McNeill Whistler, admired early 19th-century Japanese Ukiyo-e artists like Hokusai (1760-1849) and Hiroshige (1797-1858) and were influenced by them.\n\nThe earliest surviving examples of Chinese painted artwork date to the Warring States Period (481 – 221 BC), with paintings on silk or tomb murals on rock, brick, or stone. They were often in simplistic stylized format and in more-or-less rudimentary geometric patterns. They often depicted mythological creatures, domestic scenes, labor scenes, or palatial scenes filled with officials at court. Artwork during this period and the subsequent Qin Dynasty (221 – 207 BC) and Han Dynasty (202 BC – 220 AD) was made not as a means in and of itself or for higher personal expression; rather artwork was created to symbolize and honor funerary rites, representations of mythological deities or spirits of ancestors, etc. Paintings on silk of court officials and domestic scenes could be found during the Han Dynasty, along with scenes of men hunting on horseback or partaking in military parade. There was also painting on three dimensional works of art like figurines and statues, such as the original-painted colors covering the soldier and horse statues of the Terracotta Army. During the social and cultural climate of the ancient Eastern Jin Dynasty (316 – 420 AD) based at Nanjing in the south, painting became one of the official pastimes of Confucian-taught bureaucratic officials and aristocrats (along with music played by the guqin zither, writing fanciful calligraphy, and writing and reciting of poetry). Painting became a common form of artistic self-expression, and during this period painters at court or amongst elite social circuits were judged and ranked by their peers.\nThe establishment of classical Chinese landscape painting is accredited largely to the Eastern Jin Dynasty artist Gu Kaizhi (344 – 406 AD), one of the most famous artists of Chinese history. Like the elongated scroll scenes of Kaizhi, Tang dynasty (618 – 907 AD) Chinese artists like Wu Daozi painted vivid and highly detailed artwork on long horizontal handscrolls (which were very popular during the Tang), such as his \"Eighty Seven Celestial People\". Painted artwork during the Tang period pertained the effects of an idealized landscape environment, with sparse amount of objects, persons, or activity, as well as monochromatic in nature (example: the murals of Price Yide's tomb in the Qianling Mausoleum). There were also figures such as early Tang-era painter Zhan Ziqian, who painted superb landscape paintings that were well ahead of his day in portrayal of realism. However, landscape art did not reach greater level of maturity and realism in general until the Five Dynasties and Ten Kingdoms period (907 – 960 AD). During this time, there were exceptional landscape painters like Dong Yuan (refer to this article for an example of his artwork), and those who painted more vivid and realistic depictions of domestic scenes, like Gu Hongzhong and his \"Night Revels of Han Xizai\".\nDuring the Chinese Song dynasty (960 – 1279 AD), not only landscape art was improved upon, but portrait painting became more standardized and sophisticated than before (for example, refer to Emperor Huizong of Song), and reached its classical age maturity during the Ming Dynasty (1368 – 1644 AD). During the late 13th century and first half of the 14th century, Chinese under the Mongol-controlled Yuan Dynasty were not allowed to enter higher posts of government (reserved for Mongols or other ethnic groups from Central Asia), and the Imperial examination was ceased for the time being. Many Confucian-educated Chinese who now lacked profession turned to the arts of painting and theatre instead, as the Yuan period became one of the most vibrant and abundant eras for Chinese artwork. An example of such would be Qian Xuan (1235–1305 AD), who was an official of the Song dynasty, but out of patriotism, refused to serve the Yuan court and dedicated himself to painting. Examples of superb art from this period include the rich and detailed painted murals of the Yongle Palace , or \"Dachunyang Longevity Palace\", of 1262 AD, a UNESCO World Heritage site. Within the palace, paintings cover an area of more than 1000 square meters, and hold mostly Daoist themes. It was during the Song dynasty that painters would also gather in social clubs or meetings to discuss their art or others' artwork, the praising of which often led to persuasions to trade and sell precious works of art. However, there were also many harsh critics of others art as well, showing the difference in style and taste amongst different painters. In 1088 AD, the polymath scientist and statesman Shen Kuo once wrote of the artwork of one Li Cheng, who he criticized as follows:\n\nAlthough high level of stylization, mystical appeal, and surreal elegance were often preferred over realism (such as in shan shui style), beginning with the medieval Song dynasty there were many Chinese painters then and afterwards who depicted scenes of nature that were vividly real. Later Ming Dynasty artists would take after this Song dynasty emphasis for intricate detail and realism on objects in nature, especially in depictions of animals (such as ducks, swans, sparrows, tigers, etc.) amongst patches of brightly colored flowers and thickets of brush and wood (a good example would be the anonymous Ming Dynasty painting \"Birds and Plum Blossoms\", housed in the Freer Gallery of the Smithsonian Museum in Washington, D.C.). There were many renowned Ming Dynasty artists; Qiu Ying is an excellent example of a paramount Ming era painter (famous even in his own day), utilizing in his artwork domestic scenes, bustling palatial scenes, and nature scenes of river valleys and steeped mountains shrouded in mist and swirling clouds. During the Ming Dynasty there were also different and rivaling schools of art associated with painting, such as the Wu School and the Zhe School.\n\nClassical Chinese painting continued on into the early modern Qing Dynasty, with highly realistic portrait paintings like seen in the late Ming Dynasty of the early 17th century. The portraits of Kangxi Emperor, Yongzheng Emperor, and Qianlong Emperor are excellent examples of realistic Chinese portrait painting. During the Qianlong reign period and the continuing 19th century, European Baroque styles of painting had noticeable influence on Chinese portrait paintings, especially with painted visual effects of lighting and shading. Likewise, East Asian paintings and other works of art (such as porcelain and lacquerware) were highly prized in Europe since initial contact in the 16th century.\n\nJapanese painting (絵画) is one of the oldest and most highly refined of the Japanese arts, encompassing a wide variety of genres and styles. As with Japanese arts in general, Japanese painting developed through a long history of synthesis and competition between native Japanese aesthetics and adaptation of imported ideas. Ukiyo-e, or \"pictures of the floating world,\" is a genre of Japanese woodblock prints (or \"woodcuts\") and paintings produced between the 17th and 20th centuries, featuring motifs of landscapes, theater, and courtesan districts. It is the main artistic genre of Japanese woodblock printing. Japanese printmaking, especially from the Edo period, exerted enormous influence on French painting over the 19th century.\n\nKorean painting, as an independent form, began around 108 B.C., around the fall of Gojoseon, making it one of the oldest in the world. The artwork of that time period evolved into the various styles that characterized the Three Kingdoms of Korea period, most notably the paintings and frescoes that adorn the tombs of Goguryeo's royalty. During the Three Kingdoms period and through the Goryeo dynasty, Korean painting was characterized primarily by a combination of Korean-style landscapes, facial features, Buddhist-centered themes, and an emphasis on celestial observation that was facilitated by the rapid development of Korean astronomy. It wasn't until the Joseon dynasty that Confucian themes began to take root in Korean paintings, used in harmony with indigenous aspects.\n\nThe history of Korean painting has been characterized by the use monochromatic works of black brushwork, often on mulberry paper or silk. This style is evident in \"Min-Hwa\", or colorful folk art, tomb paintings, and ritual and festival arts, both of which incorporated an extensive use of colour.\n\nIndian paintings historically revolved around the religious deities and kings. Indian art is a collective term for several different schools of art that existed in the Indian subcontinent. The paintings varied from large frescoes of Ajanta to the intricate Mughal miniature paintings to the metal embellished works from the Tanjore school. The paintings from the Gandhar–Taxila are influenced by the Persian works in the west. The eastern style of painting was mostly developed around the Nalanda school of art. The works are mostly inspired by various scenes from Indian mythology.\n\nThe earliest Indian paintings were the rock paintings of prehistoric times, the petroglyphs as found in places like the Rock Shelters of Bhimbetka, and some of them are older than 5500 BC. Such works continued and after several millennia, in the 7th century, carved pillars of Ajanta, Maharashtra state present a fine example of Indian paintings, and the colors, mostly various shades of red and orange, were derived from minerals.\n\nAjanta Caves in Maharashtra, India are rock-cut cave monuments dating back to the 2nd century BCE and containing paintings and sculpture considered to be masterpieces of both Buddhist religious art and universal pictorial art.\n\nMadhubani painting is a style of Indian painting, practiced in the Mithila region of Bihar state, India. The origins of Madhubani painting are shrouded in antiquity.\nRajput painting, a style of Indian painting, evolved and flourished, during the 18th century, in the royal courts of Rajputana, India. Each Rajput kingdom evolved a distinct style, but with certain common features. Rajput paintings depict a number of themes, events of epics like the Ramayana and the Mahabharata, Krishna's life, beautiful landscapes, and humans. Miniatures were the preferred medium of Rajput painting, but several manuscripts also contain Rajput paintings, and paintings were even done on the walls of palaces, inner chambers of the forts, havelies, particularly, the havelis of Shekhawait.\n\nThe colors extracted from certain minerals, plant sources, conch shells, and were even derived by processing precious stones, gold and silver were used. The preparation of desired colors was a lengthy process, sometimes taking weeks. Brushes used were very fine.\n\nMughal painting is a particular style of Indian painting, generally confined to illustrations on the book and done in miniatures, and which emerged, developed and took shape during the period of the Mughal Empire 16th −19th centuries.\n\nTanjore painting is an important form of classical South Indian painting native to the town of Tanjore in Tamil Nadu. The art form dates back to the early 9th century, a period dominated by the Chola rulers, who encouraged art and literature. These paintings are known for their elegance, rich colors, and attention to detail. The themes for most of these paintings are Hindu Gods and Goddesses and scenes from Hindu mythology. In modern times, these paintings have become a much sought after souvenir during festive occasions in South India.\n\nThe process of making a Tanjore painting involves many stages. The first stage involves the making of the preliminary sketch of the image on the base. The base consists of a cloth pasted over a wooden base. Then chalk powder or zinc oxide is mixed with water-soluble adhesive and applied on the base. To make the base smoother, a mild abrasive is sometimes used. After the drawing is made, decoration of the jewellery and the apparels in the image is done with semi-precious stones. Laces or threads are also used to decorate the jewellery. On top of this, the gold foils are pasted. Finally, dyes are used to add colors to the figures in the paintings.\n\nDuring British rule in India, the crown found that Madras had some of the most talented and intellectual artistic minds in the world. As the British had also established a huge settlement in and around Madras, Georgetown was chosen to establish an institute that would cater to the artistic expectations of the royals in London. This has come to be known as the Madras School. At first traditional artists were employed to produce exquisite varieties of furniture, metal work, and curios and their work was sent to the royal palaces of the Queen.\n\nUnlike the Bengal School where 'copying' is the norm of teaching, the Madras School flourishes on 'creating' new styles, arguments and trends.\n\nThe Bengal school of art was an influential style of art that flourished in India during the British Raj in the early 20th century. It was associated with Indian nationalism, but was also promoted and supported by many British arts administrators.\n\nThe Bengal School arose as an avant garde and nationalist movement reacting against the academic art styles previously promoted in India, both by Indian artists such as Raja Ravi Varma and in British art schools. Following the widespread influence of Indian spiritual ideas in the West, the British art teacher Ernest Binfield Havel attempted to reform the teaching methods at the Calcutta School of Art by encouraging students to imitate Mughal miniatures. This caused immense controversy, leading to a strike by students and complaints from the local press, including from nationalists who considered it to be a retrogressive move. Havel was supported by the artist Abanindranath Tagore, a nephew of the poet Rabindranath Tagore. Tagore painted a number of works influenced by Mughal art, a style that he and Havel believed to be expressive of India's distinct spiritual qualities, as opposed to the \"materialism\" of the West. Tagore's best-known painting, \"Bharat Mata\" (Mother India), depicted a young woman, portrayed with four arms in the manner of Hindu deities, holding objects symbolic of India's national aspirations. Tagore later attempted to develop links with Japanese artists as part of an aspiration to construct a pan-Asianist model of art.\n\nThe Bengal School's influence in India declined with the spread of modernist ideas in the 1920s. In the post-independence period, Indian artists showed more adaptability as they borrowed freely from european styles and amalgamated them freely with the Indian motifs to new forms of art. While artists like Francis Newton Souza and Tyeb Mehta were more western in their approach, there were others like Ganesh Pyne and Maqbool Fida Husain who developed thoroughly indigenous styles of work. Today after the process of liberalization of market in India, the artists are experiencing more exposure to the international art-scene which is helping them in emerging with newer forms of art which were hitherto not seen in India. Jitish Kallat had shot to fame in the late 1990s with his paintings which were both modern and beyond the scope of generic definition. However, while artists in India in the new century are trying out new styles, themes and metaphors, it would not have been possible to get such quick recognition without the aid of the business houses which are now entering the art field like they had never before.\n\nAmrita Sher-Gil was an Indian painter, sometimes known as India's Frida Kahlo, and today considered an important woman painter of 20th-century India, whose legacy stands at par with that of the Masters of Bengal Renaissance; she is also the 'most expensive' woman painter of India.\n\nToday, she is amongst \"Nine Masters\", whose work was declared as \"art treasures\" by The Archaeological Survey of India, in 1976 and 1979, and over 100 of her paintings are now displayed at National Gallery of Modern Art, New Delhi.\n\nDuring the colonial era, Western influences started to make an impact on Indian art. Some artists developed a style that used Western ideas of composition, perspective and realism to illustrate Indian themes. Others, like Jamini Roy, consciously drew inspiration from folk art.\n\nBy the time of Independence in 1947, several schools of art in India provided access to modern techniques and ideas. Galleries were established to showcase these artists. Modern Indian art typically shows the influence of Western styles, but is often inspired by Indian themes and images. Major artists are beginning to gain international recognition, initially among the Indian diaspora, but also among non-Indian audiences.\n\nThe Progressive Artists' Group, established shortly after India became independent in 1947, was intended to establish new ways of expressing India in the post-colonial era. The founders were six eminent artists – K. H. Ara, S. K. Bakre, H. A. Gade, M.F. Husain, S.H. Raza and F. N. Souza, though the group was dissolved in 1956, it was profoundly influential in changing the idiom of Indian art. Almost all India's major artists in the 1950s were associated with the group. Some of those who are well-known today are Bal Chabda, Manishi Dey, Mukul Dey, V. S. Gaitonde, Ram Kumar, Tyeb Mehta, and Akbar Padamsee. Other famous painters like Jahar Dasgupta, Prokash Karmakar, John Wilkins, Narayanan Ramachandran, and Bijon Choudhuri enriched the art culture of India. They have become the icons of modern Indian art. Art historians like Prof. Rai Anand Krishna have also referred to those works of modern artistes that reflect Indian ethos. Geeta Vadhera has had acclaim in translating complex, Indian spiritual themes onto canvas like Sufi thought, the Upanishads and the Bhagwad Geeta.\n\nIndian art got a boost with the economic liberalization of the country since the early 1990s. Artists from various fields now started bringing in varied styles of work. In post-liberalization India, many artists have established themselves in the international art market like the abstract painter Natvar Bhavsar, figurative artist Devajyoti Ray and sculptor Anish Kapoor whose mammoth postminimalist artworks have acquired attention for their sheer size. Many art houses and galleries have also opened in USA and Europe to showcase Indian artworks.\n\nFilipino painting as a whole can be seen as an amalgamation of many cultural influences, though it tends to be more Western in its current form with Eastern roots.\n\nEarly Filipino painting can be found in red slip (clay mixed with water) designs embellished on the ritual pottery of the Philippines such as the acclaimed Manunggul Jar. Evidence of Philippine pottery-making dated as early as 6000BC has been found in Sanga-sanga Cave, Sulu and Laurente Cave, Cagayan. It has been proven that by 5000BC, the making of pottery was practiced throughout the country. Early Filipinos started making pottery before their Cambodian neighbors and at about the same time as the Thais as part of what appears to be a widespread Ice Age development of pottery technology. Further evidences of painting are manifested in the tattoo tradition of early Filipinos, whom the Portuguese explorer referred to as \"Pintados\" or the 'Painted People' of the Visayas. Various designs referencing flora and fauna with heavenly bodies decorate their bodies in various colored pigmentation. Perhaps, some of the most elaborate painting done by early Filipinos that survive to the present day can be manifested among the arts and architecture of the Maranao who are well known for the Nāga Dragons and the Sarimanok carved and painted in the beautiful Panolong of their Torogan or King's House.\n\nFilipinos began creating paintings in the European tradition during the 17th-century Spanish period. The earliest of these paintings were Church frescoes, religious imagery from Biblical sources, as well as engravings, sculptures and lithographs featuring Christian icons and European nobility. Most of the paintings and sculptures between the 19th, and 20th century produced a mixture of religious, political, and landscape art works, with qualities of sweetness, dark, and light. Early modernist painters such as Damián Domingo was associated with religious and secular paintings. The art of Juan Luna and Félix Hidalgo showed a trend for political statement. Artist such as Fernando Amorsolo used post-modernism to produce paintings that illustrated Philippine culture, nature, and harmony. While other artists such as Fernando Zóbel used realities and abstract on his work.\n\nAncient Egypt, a civilization with very strong traditions of architecture and sculpture (both originally painted in bright colours) also had many mural paintings in temples and buildings, and painted illustrations on papyrus manuscripts. Egyptian wall painting and decorative painting is often graphic, sometimes more symbolic than realistic. Egyptian painting depicts figures in bold outline and flat silhouette, in which symmetry is a constant characteristic. Egyptian painting has close connection with its written language – called Egyptian hieroglyphs. Painted symbols are found amongst the first forms of written language. The Egyptians also painted on linen, remnants of which survive today. Ancient Egyptian paintings survived due to the extremely dry climate. The ancient Egyptians created paintings to make the afterlife of the deceased a pleasant place. The themes included journey through the afterworld or their protective deities introducing the deceased to the gods of the underworld. Some examples of such paintings are paintings of the gods and goddesses Ra, Horus, Anubis, Nut, Osiris and Isis. Some tomb paintings show activities that the deceased were involved in when they were alive and wished to carry on doing for eternity. In the New Kingdom and later, the Book of the Dead was buried with the entombed person. It was considered important for an introduction to the afterlife.\n\nTo the north of Egypt was the Minoan civilization on the island of Crete. The wall paintings found in the palace of Knossos are similar to that of the Egyptians but much more free in style. Around 1100BC, tribes from the north of Greece conquered Greece and the Greek art took a new direction.\n\nAncient Greece had skilled painters, sculptors (though both endeavours were regarded as mere manual labour at the time), and architects. The Parthenon is an example of their architecture that has lasted to modern days. Greek marble sculpture is often described as the highest form of Classical art. Painting on pottery of Ancient Greece and ceramics gives a particularly informative glimpse into the way society in Ancient Greece functioned. Black-figure vase painting and Red-figure vase painting gives many surviving examples of what Greek painting was. Some famous Greek painters on wooden panels who are mentioned in texts are Apelles, Zeuxis and Parrhasius, however no examples of Ancient Greek panel painting survive, only written descriptions by their contemporaries or later Romans. Zeuxis lived in 5–6BC and was said to be the first to use sfumato. According to Pliny the Elder, the realism of his paintings was such that birds tried to eat the painted grapes. Apelles is described as the greatest painter of Antiquity for perfect technique in drawing, brilliant color and modeling.\n\nRoman art was influenced by Greece and can in part be taken as a descendant of ancient Greek painting. However, Roman painting does have important unique characteristics. The only surviving Roman paintings are wall paintings, many from villas in Campania, in Southern Italy. Such painting can be grouped into 4 main \"styles\" or periods and may contain the first examples of trompe-l'œil, pseudo-perspective, and pure landscape. Almost the only painted portraits surviving from the Ancient world are a large number of coffin-portraits of bust form found in the Late Antique cemetery of Al-Fayum. Although these were neither of the best period nor the highest quality, they are impressive in themselves, and give an idea of the quality that the finest ancient work must have had. A very small number of miniatures from Late Antique illustrated books also survive, and a rather larger number of copies of them from the Early Medieval period.\n\nThe rise of Christianity imparted a different spirit and aim to painting styles. Byzantine art, once its style was established by the 6th century, placed great emphasis on retaining traditional iconography and style, and has changed relatively little through the thousand years of the Byzantine Empire and the continuing traditions of Greek and Russian Orthodox icon-painting. Byzantine painting has a particularly hieratic feeling and icons were and still are seen as a reflection of the divine. There were also many wall-paintings in fresco, but fewer of these have survived than Byzantine mosaics. In general Byzantium art borders on abstraction, in its flatness and highly stylised depictions of figures and landscape. However, there are periods, especially in the so-called Macedonian art of around the 10th century, when Byzantine art became more flexible in approach.\n\nIn post-Antique Catholic Europe the first distinctive artistic style to emerge that included painting was the Insular art of the British Isles, where the only surviving examples (and quite likely the only medium in which painting was used) are miniatures in Illuminated manuscripts such as the Book of Kells. These are most famous for their abstract decoration, although figures, and sometimes scenes, were also depicted, especially in Evangelist portraits. Carolingian and Ottonian art also survives mostly in manuscripts, although some wall-painting remain, and more are documented. The art of this period combines Insular and \"barbarian\" influences with a strong Byzantine influence and an aspiration to recover classical monumentality and poise.\n\nWalls of Romanesque and Gothic churches were decorated with frescoes as well as sculpture and many of the few remaining murals have great intensity, and combine the decorative energy of Insular art with a new monumentality in the treatment of figures. Far more miniatures in Illuminated manuscripts survive from the period, showing the same characteristics, which continue into the Gothic period.\n\nPanel painting becomes more common during the Romanesque period, under the heavy influence of Byzantine icons. Towards the middle of the 13th century, Medieval art and Gothic painting became more realistic, with the beginnings of interest in the depiction of volume and perspective in Italy with Cimabue and then his pupil Giotto. From Giotto on, the treatment of composition by the best painters also became much more free and innovative. They are considered to be the two great medieval masters of painting in western culture. Cimabue, within the Byzantine tradition, used a more realistic and dramatic approach to his art. His pupil, Giotto, took these innovations to a higher level which in turn set the foundations for the western painting tradition. Both artists were pioneers in the move towards naturalism.\n\nChurches were built with more and more windows and the use of colorful stained glass become a staple in decoration. One of the most famous examples of this is found in the cathedral of Notre Dame de Paris. By the 14th century Western societies were both richer and more cultivated and painters found new patrons in the nobility and even the bourgeoisie. Illuminated manuscripts took on a new character and slim, fashionably dressed court women were shown in their landscapes. This style soon became known as International Gothic and tempera panel paintings and altarpieces gained importance.\n\nThe Renaissance is said by many to be the golden age of painting. Roughly spanning the 14th through the mid-17th century. In Italy artists like Paolo Uccello, Fra Angelico, Masaccio, Piero della Francesca, Andrea Mantegna, Filippo Lippi, Giorgione, Tintoretto, Sandro Botticelli, Leonardo da Vinci, Michelangelo Buonarroti, Raphael, Giovanni Bellini, and Titian took painting to a higher level through the use of perspective, the study of human anatomy and proportion, and through their development of an unprecedented refinement in drawing and painting techniques.\n\nFlemish, Dutch and German painters of the Renaissance such as Hans Holbein the Younger, Albrecht Dürer, Lucas Cranach, Matthias Grünewald, Hieronymous Bosch, and Pieter Brueghel represent a different approach from their Italian colleagues, one that is more realistic and less idealized. Genre painting became a popular idiom amongst Northern painters such as Pieter Brueghel. A new verisimilitude in depicting reality became possible with the adoption of oil painting, whose invention was traditionally, but erroneously, credited to Jan Van Eyck (an important transitional figure who bridges painting in the Middle Ages with painting of the early Renaissance). Unlike the Italians whose work drew heavily from the art of ancient Greece and Rome, the northerners retained a stylistic residue of the sculpture and illuminated manuscripts of the Middle Ages. These tendencies are also seen in the art of Tudor England, which was heavily influenced by Protestant refugees from the Low Countries.\n\nRenaissance painting reflects the revolution of ideas and science (astronomy, geography) that occur in this period, the Reformation, and the invention of the printing press. Dürer, considered one of the greatest of printmakers, states that painters are not mere artisans but thinkers as well. With the development of easel painting in the Renaissance, painting gained independence from architecture. Following centuries dominated by religious imagery, secular subject matter slowly returned to Western painting. Artists included visions of the world around them, or the products of their own imaginations in their paintings. Those who could afford the expense could become patrons and commission portraits of themselves or their family.\n\nIn the 15th and 16th centuries, panel paintings which could be hung on walls and moved around at will, became increasingly popular for both churches and private houses, rather than fresco wall-paintings or paintings incorporated into on permanent structures, such as altarpieces. The High Renaissance gave rise to a stylized art known as Mannerism. In place of the balanced compositions and rational approach to perspective that characterized art at the dawn of the 16th century, the Mannerists sought instability, artifice, and doubt. The unperturbed faces and gestures of Piero della Francesca and the calm Virgins of Raphael are replaced by the troubled expressions of Pontormo and the emotional intensity of El Greco. Some decades later Northern Mannerism dominated Netherlandish and German art until the arrival of the Baroque.\n\nBaroque painting is associated with the Baroque cultural movement, a movement often identified with Absolutism and the Counter Reformation or Catholic Revival; the existence of important Baroque painting in non-absolutist and Protestant states also, however, underscores its popularity, as the style spread throughout Western Europe.\n\nBaroque painting is characterized by great drama, rich, deep color, and intense light and dark shadows. Baroque art was meant to evoke emotion and passion instead of the calm rationality that had been prized during the Renaissance. During the period beginning around 1600 and continuing throughout the 17th century, painting is characterized as Baroque. Among the greatest painters of the Baroque are Caravaggio, Rembrandt, Frans Hals, Rubens, Velázquez, Poussin, and Jan Vermeer. Caravaggio is an heir of the humanist painting of the High Renaissance. His realistic approach to the human figure, painted directly from life and dramatically spotlit against a dark background, shocked his contemporaries and opened a new chapter in the history of painting.\nBaroque painting often dramatizes scenes using light effects; this can be seen in works by Rembrandt, Vermeer, Le Nain and La Tour.\n\nDuring the 18th century, Rococo followed as a lighter extension of Baroque, often frivolous and erotic. Rococo developed first in the decorative arts and interior design in France. Louis XV's succession brought a change in the court artists and general artistic fashion. The 1730s represented the height of Rococo development in France exemplified by the works of Antoine Watteau and François Boucher. Rococo still maintained the Baroque taste for complex forms and intricate patterns, but by this point, it had begun to integrate a variety of diverse characteristics, including a taste for Oriental designs and asymmetric compositions.\n\nThe Rococo style spread with French artists and engraved publications. It was readily received in the Catholic parts of Germany, Bohemia, and Austria, where it was merged with the lively German Baroque traditions. German Rococo was applied with enthusiasm to churches and palaces, particularly in the south, while Frederician Rococo developed in the Kingdom of Prussia.\n\nThe French masters Watteau, Boucher and Fragonard represent the style, as do Giovanni Battista Tiepolo and Jean-Baptiste-Siméon Chardin who was considered by some as the best French painter of the 18th century – the \"Anti-Rococo\". Portraiture was an important component of painting in all countries, but especially in England, where the leaders were William Hogarth, in a blunt realist style, and Francis Hayman, Angelica Kauffman (who was Swiss), Thomas Gainsborough and Joshua Reynolds in more flattering styles influenced by Anthony van Dyck. While in France during the Rococo era Jean-Baptiste Greuze (the favorite painter of Denis Diderot), Maurice Quentin de La Tour, and Élisabeth Vigée-Lebrun were highly accomplished Portrait painters and History painters.\n\nWilliam Hogarth helped develop a theoretical foundation for Rococo beauty. Though not intentionally referencing the movement, he argued in his \"Analysis of Beauty\" (1753) that the undulating lines and S-curves prominent in Rococo were the basis for grace and beauty in art or nature (unlike the straight line or the circle in Classicism). The beginning of the end for Rococo came in the early 1760s as figures like Voltaire and Jacques-François Blondel began to voice their criticism of the superficiality and degeneracy of the art. Blondel decried the \"ridiculous jumble of shells, dragons, reeds, palm-trees and plants\" in contemporary interiors. By 1785, Rococo had passed out of fashion in France, replaced by the order and seriousness of Neoclassical artists like Jacques-Louis David.\n\nAfter Rococo there arose in the late 18th century, in architecture, and then in painting severe neo-classicism, best represented by such artists as David and his heir Ingres. Ingres' work already contains much of the sensuality, but none of the spontaneity, that was to characterize Romanticism.\nThis movement turned its attention toward landscape and nature as well as the human figure and the supremacy of natural order above mankind's will. There is a pantheist philosophy (see Spinoza and Hegel) within this conception that opposes Enlightenment ideals by seeing mankind's destiny in a more tragic or pessimistic light. The idea that human beings are not above the forces of Nature is in contradiction to Ancient Greek and Renaissance ideals where mankind was above all things and owned his fate. This thinking led romantic artists to depict the sublime, ruined churches, shipwrecks, massacres and madness.\n\nBy the mid-19th-century painters became liberated from the demands of their patronage to only depict scenes from religion, mythology, portraiture or history. The idea \"art for art's sake\" began to find expression in the work of painters like Francisco de Goya, John Constable, and J.M.W. Turner. Romantic painters turned landscape painting into a major genre, considered until then as a minor genre or as a decorative background for figure compositions.\nSome of the major painters of this period are Eugène Delacroix, Théodore Géricault, J. M. W. Turner, Caspar David Friedrich and John Constable. Francisco de Goya's late work demonstrates the Romantic interest in the irrational, while the work of Arnold Böcklin evokes mystery and the paintings of Aesthetic movement artist James McNeill Whistler evoke both sophistication and decadence. In the United States the Romantic tradition of landscape painting was known as the Hudson River School: exponents include Thomas Cole, Frederic Edwin Church, Albert Bierstadt, Thomas Moran, and John Frederick Kensett. Luminism was a movement in American landscape painting related to the Hudson River School.\n\nThe leading Barbizon School painter Camille Corot painted in both a romantic and a realistic vein; his work prefigures Impressionism, as does the paintings of Eugène Boudin who was one of the first French landscape painters to paint outdoors. Boudin was also an important influence on the young Claude Monet, whom in 1857 he introduced to Plein air painting. A major force in the turn towards Realism at mid-century was Gustave Courbet. In the latter third of the century Impressionists like Édouard Manet, Claude Monet, Pierre-Auguste Renoir, Camille Pissarro, Alfred Sisley, Berthe Morisot, Mary Cassatt, and Edgar Degas worked in a more direct approach than had previously been exhibited publicly. They eschewed allegory and narrative in favor of individualized responses to the modern world, sometimes painted with little or no preparatory study, relying on deftness of drawing and a highly chromatic pallette. Manet, Degas, Renoir, Morisot, and Cassatt concentrated primarily on the human subject. Both Manet and Degas reinterpreted classical figurative canons within contemporary situations; in Manet's case the re-imaginings met with hostile public reception. Renoir, Morisot, and Cassatt turned to domestic life for inspiration, with Renoir focusing on the female nude. Monet, Pissarro, and Sisley used the landscape as their primary motif, the transience of light and weather playing a major role in their work. While Sisley most closely adhered to the original principals of the Impressionist perception of the landscape, Monet sought challenges in increasingly chromatic and changeable conditions, culminating in his series of monumental works of Water Lilies painted in Giverny.\n\nPissarro adopted some of the experiments of Post-Impressionism. Slightly younger Post-Impressionists like Vincent van Gogh, Paul Gauguin, and Georges Seurat, along with Paul Cézanne led art to the edge of modernism; for Gauguin Impressionism gave way to a personal symbolism; Seurat transformed Impressionism's broken color into a scientific optical study, structured on frieze-like\ncompositions; Van Gogh's turbulent method of paint application, coupled with a sonorous use of color, predicted Expressionism and Fauvism, and Cézanne, desiring to unite classical composition with a revolutionary abstraction of natural forms, would come to be seen as a precursor of 20th-century art.\nThe spell of Impressionism was felt throughout the world, including in the United States, where it became integral to the painting of American Impressionists such as Childe Hassam, John Twachtman, and Theodore Robinson; and in Australia where painters of the Heidelberg School such as Arthur Streeton, Frederick McCubbin and Charles Conder painted \"en plein air\" and were particularly interested in the Australian landscape and light. It also exerted influence on painters who were not primarily Impressionistic in theory, like the portrait and landscape painter John Singer Sargent. At the same time in America at the turn of the 20th century there existed a native and nearly insular realism, as richly embodied in the figurative work of Thomas Eakins, the Ashcan School, and the landscapes and seascapes of Winslow Homer, all of whose paintings were deeply invested in the solidity of natural forms. The visionary landscape, a motive largely dependent on the ambiguity of the nocturne, found its advocates in Albert Pinkham Ryder and Ralph Albert Blakelock.\n\nIn the late 19th century there also were several, rather dissimilar, groups of Symbolist painters whose works resonated with younger artists of the 20th century, especially with the Fauvists and the Surrealists. Among them were Gustave Moreau, Odilon Redon, Pierre Puvis de Chavannes, Henri Fantin-Latour, Arnold Böcklin, Edvard Munch, Félicien Rops, and Jan Toorop, and Gustave Klimt amongst others including the Russian Symbolists like Mikhail Vrubel.\n\nSymbolist painters mined mythology and dream imagery for a visual language of the soul, seeking evocative paintings that brought to mind a static world of silence. The symbols used in Symbolism are not the familiar emblems of mainstream iconography but intensely personal, private, obscure and ambiguous references. More a philosophy than an actual style of art, the Symbolist painters influenced the contemporary Art Nouveau movement and Les Nabis. In their exploration of dreamlike subjects, symbolist painters are found across centuries and cultures, as they are still today; Bernard Delvaille has described René Magritte's surrealism as \"Symbolism plus Freud\".\n\nThe heritage of painters like Van Gogh, Cézanne, Gauguin, and Seurat was essential for the development of modern art. At the beginning of the 20th century Henri Matisse and several other young artists revolutionized the Paris art world with \"wild\", multi-colored, expressive, landscapes and figure paintings that the critics called Fauvism. Pablo Picasso made his first cubist paintings based on Cézanne's idea that all depiction of nature can be reduced to three solids: cube, sphere and cone.\n\nThe heritage of painters like Van Gogh, Cézanne, Gauguin, and Seurat was essential for the development of modern art. At the beginning of the 20th century Henri Matisse and several other young artists including the pre-cubist Georges Braque, André Derain, Raoul Dufy and Maurice de Vlaminck revolutionized the Paris art world with \"wild\", multi-colored, expressive, landscapes and figure paintings that the critics called Fauvism – (as seen in the gallery above). Henri Matisse's second version of \"The Dance\" signifies a key point in his career and in the development of modern painting. It reflects Matisse's incipient fascination with primitive art: the intense warm colors against the cool blue-green background and the rhythmical succession of dancing nudes convey the feelings of emotional liberation and hedonism. Pablo Picasso made his first cubist paintings based on Cézanne's idea that all depiction of nature can be reduced to three solids: cube, sphere and cone. With the painting Les Demoiselles d'Avignon 1907, (see gallery) Picasso dramatically created a new and radical picture depicting a raw and primitive brothel scene with five prostitutes, violently painted women, reminiscent of African tribal masks and his own new Cubist inventions. analytic Cubism (see gallery) was jointly developed by Pablo Picasso and Georges Braque, exemplified by \"Violin and Candlestick, Paris\", (seen above) from about 1908 through 1912. Analytic cubism, the first clear manifestation of cubism, was followed by synthetic cubism, practised by Braque, Picasso, Fernand Léger, Juan Gris, Albert Gleizes, Marcel Duchamp and countless other artists into the 1920s. Synthetic cubism is characterized by the introduction of different textures, surfaces, collage elements, papier collé and a large variety of merged subject matter.\n\nLes Fauves (French for \"The Wild Beasts\") were early-20th-century painters, experimenting with freedom of expression through color. The name was given, humorously and not as a compliment, to the group by art critic Louis Vauxcelles. Fauvism was a short-lived and loose grouping of early-20th-century artists whose works emphasized painterly qualities, and the imaginative use of deep color over the representational values. Fauvists made the subject of the painting easy to read, exaggerated perspectives and an interesting prescient prediction of the Fauves was expressed in 1888 by Paul Gauguin to Paul Sérusier,\n\n\"\"How do you see these trees? They are yellow. So, put in yellow; this shadow, rather blue, paint it with pure ultramarine; these red leaves? Put in vermilion.\"\"\n\nThe leaders of the movement were Henri Matisse and André Derain — friendly rivals of a sort, each with his own followers. Ultimately Matisse became the \"yang\" to Picasso's \"yin\" in the 20th century. Fauvist painters included Albert Marquet, Charles Camoin, Maurice de Vlaminck, Raoul Dufy, Othon Friesz, the Dutch painter Kees van Dongen, and Picasso's partner in Cubism, Georges Braque amongst others.\nFauvism, as a movement, had no concrete theories, and was short lived, beginning in 1905 and ending in 1907, they only had three exhibitions. Matisse was seen as the leader of the movement, due to his seniority in age and prior self-establishment in the academic art world. His 1905 portrait of Mme. Matisse \"The Green Line\", (above), caused a sensation in Paris when it was first exhibited. He said he wanted to create art to delight; art as a decoration was his purpose and it can be said that his use of bright colors tries to maintain serenity of composition. In 1906 at the suggestion of his dealer Ambroise Vollard, André Derain went to London and produced a series of paintings like \"Charing Cross Bridge, London\" (above) in the Fauvist style, paraphrasing the famous series by the Impressionist painter Claude Monet. Masters like Henri Matisse and Pierre Bonnard continued developing their narrative styles independent of any movement throughout the 20th century.\n\nBy 1907 Fauvism no longer was a shocking new movement, soon it was replaced by Cubism on the critics' radar screen as the latest new development in Contemporary Art of the time.\nIn 1907 Appolinaire, commenting about Matisse in an article published in La Falange, said, \"We are not here in the presence of an extravagant or an extremist undertaking: Matisse's art is eminently reasonable.\"\nAnalytic cubism (see gallery) was jointly developed by Pablo Picasso and Georges Braque from about 1908 through 1912. Analytic cubism, the first clear manifestation of cubism, was followed by Synthetic cubism, practised by Braque, Picasso, Fernand Léger, Juan Gris, Albert Gleizes, Marcel Duchamp and countless other artists into the 1920s. Synthetic cubism is characterized by the introduction of different textures, surfaces, collage elements, papier collé and a large variety of merged subject matter.\n\nDuring the years between 1910 and the end of World War I and after the heyday of cubism, several movements emerged in Paris. Giorgio De Chirico moved to Paris in July 1911, where he joined his brother Andrea (the poet and painter known as Alberto Savinio). Through his brother he met Pierre Laprade a member of the jury at the Salon d'Automne, where he exhibited three of his dreamlike works: \"Enigma of the Oracle\", \"Enigma of an Afternoon\" and \"Self-Portrait\". During 1913 he exhibited his work at the Salon des Indépendants and Salon d'Automne, his work was noticed by Pablo Picasso and Guillaume Apollinaire and several others. His compelling and mysterious paintings are considered instrumental to the early beginnings of Surrealism. (see gallery) During the first half of the 20th century in Europe masters like Georges Braque, André Derain, and Giorgio De Chirico continued painting independent of any movement.\n\nIn the first two decades of the 20th century and after Cubism, several other important movements emerged; futurism (Balla), abstract art (Kandinsky), Der Blaue Reiter), Bauhaus, (Kandinsky) and (Klee), Orphism, (Robert Delaunay and František Kupka), Synchromism (Morgan Russell), De Stijl (Mondrian), Suprematism (Malevich), Constructivism (Tatlin), Dadaism (Duchamp, Picabia, Arp) and Surrealism (De Chirico, André Breton, Miró, Magritte, Dalí, Ernst). Modern painting influenced all the visual arts, from Modernist architecture and design, to avant-garde film, theatre and modern dance and became an experimental laboratory for the expression of visual experience, from photography and concrete poetry to advertising art and fashion. Van Gogh's painting exerted great influence upon 20th-century Expressionism, as can be seen in the work of the Fauves, Die Brücke (a group led by German painter Ernst Kirchner), and the Expressionism of Edvard Munch, Egon Schiele, Marc Chagall, Amedeo Modigliani, Chaim Soutine and others..\n\nWassily Kandinsky a Russian painter, printmaker and art theorist, one of the most famous 20th-century artists is generally considered the first important painter of modern abstract art. As an early Modernist, in search of new modes of visual expression, and spiritual expression, he theorized as did contemporary occultists and theosophists, that pure visual abstraction had corollary vibrations with sound and music. They posited that pure abstraction could express pure spirituality. His earliest abstractions were generally titled as the example in the (above gallery) \"Composition VII\", making connection to the work of the composers of music. Kandinsky included many of his theories about abstract art in his book \"Concerning the Spiritual in Art.\" Robert Delaunay was a French artist who is associated with Orphism, (reminiscent of a link between pure abstraction and cubism). His later works were more abstract, reminiscent of Paul Klee. His key contributions to abstract painting refer to his bold use of color, and a clear love of experimentation of both depth and tone. At the invitation of Wassily Kandinsky, Delaunay and his wife the artist Sonia Delaunay, joined The Blue Rider (Der Blaue Reiter), a Munich-based group of abstract artists, in 1911, and his art took a turn to the abstract.\n\nOther major pioneers of early abstraction include Russian painter Kasimir Malevich, who after the Russian Revolution in 1917, and after pressure from the Stalinist regime in 1924 returned to painting imagery and \"Peasants and Workers in the field\", and Swiss painter Paul Klee whose masterful color experiments made him an important pioneer of abstract painting at the Bauhaus. Still other important pioneers of abstract painting include the Swedish artist Hilma af Klint, Czech painter František Kupka as well as American artists Stanton MacDonald-Wright and Morgan Russell who, in 1912, founded Synchromism, an art movement that closely resembles Orphism.\n\n\"Expressionism\" and \"Symbolism\" are broad rubrics that involve several important and related movements in 20th-century painting that dominated much of the avant-garde art being made in Western, Eastern and Northern Europe. Expressionist works were painted largely between World War I and World War II, mostly in France, Germany, Norway, Russia, Belgium, and Austria. Expressionist artists are related to both Surrealism and Symbolism and are each uniquely and somewhat eccentrically personal. Fauvism, Die Brücke, and Der Blaue Reiter are three of the best known groups of Expressionist and Symbolist painters.\n\nArtists as interesting and diverse as Marc Chagall, whose painting \"I and the Village\", (above) tells an autobiographical story that examines the relationship between the artist and his origins, with a lexicon of artistic Symbolism. Gustav Klimt, Egon Schiele, Edvard Munch, Emil Nolde, Chaim Soutine, James Ensor, Oskar Kokoschka, Ernst Ludwig Kirchner, Max Beckmann, Franz Marc, Käthe Schmidt Kollwitz, Georges Rouault, Amedeo Modigliani and some of the Americans abroad like Marsden Hartley, and Stuart Davis, were considered influential expressionist painters. Although Alberto Giacometti is primarily thought of as an intense Surrealist sculptor, he made intense expressionist paintings as well.\n\nPiet Mondrian's art was also related to his spiritual and philosophical studies. In 1908 he became interested in the theosophical movement launched by Helena Petrovna Blavatsky in the late 19th century. Blavatsky believed that it was possible to attain a knowledge of nature more profound than that provided by empirical means, and much of Mondrian's work for the rest of his life was inspired by his search for that spiritual knowledge.\n\nDe Stijl also known as neoplasticism, was a Dutch artistic movement founded in 1917. The term \"De Stijl\" is used to refer to a body of work from 1917 to 1931 founded in the Netherlands.\n\n\"De Stijl\" is also the name of a journal that was published by the Dutch painter, designer, writer, and critic Theo van Doesburg propagating the group's theories. Next to van Doesburg, the group's principal members were the painters Piet Mondrian, Vilmos Huszár, and Bart van der Leck, and the architects Gerrit Rietveld, Robert van 't Hoff, and J.J.P. Oud. The artistic philosophy that formed a basis for the group's work is known as \"neoplasticism\" — the new plastic art (or \"Nieuwe Beelding\" in Dutch).\n\nProponents of De Stijl sought to express a new utopian ideal of spiritual harmony and order. They advocated pure abstraction and universality by a reduction to the essentials of form and colour; they simplified visual compositions to the vertical and horizontal directions, and used only primary colors along with black and white. Indeed, according to the Tate Gallery's online article on neoplasticism, Mondrian himself sets forth these delimitations in his essay 'Neo-Plasticism in Pictorial Art'. He writes, \"... this new plastic idea will ignore the particulars of appearance, that is to say, natural form and colour. On the contrary, it should find its expression in the abstraction of form and colour, that is to say, in the straight line and the clearly defined primary colour.\" The Tate article further summarizes that this art allows \"only primary colours and non-colours, only squares and rectangles, only straight and horizontal or vertical line.\" The Guggenheim Museum's online article on De Stijl summarizes these traits in similar terms: \"It [De Stijl] was posited on the fundamental principle of the geometry of the straight line, the square, and the rectangle, combined with a strong asymmetricality; the predominant use of pure primary colors with black and white; and the relationship between positive and negative elements in an arrangement of non-objective forms and lines.\"\n\nDe Stijl movement was influenced by Cubist painting as well as by the mysticism and the ideas about \"ideal\" geometric forms (such as the \"perfect straight line\") in the neoplatonic philosophy of mathematician M.H.J. Schoenmaekers. The works of De Stijl would influence the Bauhaus style and the international style of architecture as well as clothing and interior design. However, it did not follow the general guidelines of an \"ism\" (Cubism, Futurism, Surrealism), nor did it adhere to the principles of art schools like Bauhaus; it was a collective project, a joint enterprise.\n\nMarcel Duchamp, came to international prominence in the wake of his notorious success at the New York City Armory Show in 1913, (soon after he denounced artmaking for chess). After Duchamp's Nude Descending a Staircase became the international cause celebre at the 1913 Armory show in New York he created the \"The Bride Stripped Bare by Her Bachelors, Even, Large Glass\". The \"Large Glass\" pushed the art of painting to radical new limits being part painting, part collage, part construction. Duchamp became closely associated with the Dada movement that began in neutral Zurich, Switzerland, during World War I and peaked from 1916 to 1920. The movement primarily involved visual arts, literature (poetry, art manifestoes, art theory), theatre, and graphic design, and concentrated its anti war politic through a rejection of the prevailing standards in art through anti-art cultural works. Francis Picabia (see above), Man Ray, Kurt Schwitters, Tristan Tzara, Hans Richter, Jean Arp, Sophie Taeuber-Arp, along with Duchamp and many others are associated with the Dadaist movement. Duchamp and several Dadaists are also associated with Surrealism, the movement that dominated European painting in the 1920s and 1930s.\n\nIn 1924 André Breton published the \"Surrealist Manifesto.\" The Surrealist movement in painting became synonymous with the avant-garde and which featured artists whose works varied from the abstract to the super-realist. With works on paper like \"Machine Turn Quickly\", (above) Francis Picabia continued his involvement in the Dada movement through 1919 in Zurich and Paris, before breaking away from it after developing an interest in Surrealist art. Yves Tanguy, René Magritte and Salvador Dalí are particularly known for their realistic depictions of dream imagery and fantastic manifestations of the imagination. Joan Miró's \"The Tilled Field\" of 1923–1924 verges on abstraction, this early painting of a complex of objects and figures, and arrangements of sexually active characters; was Miró's first Surrealist masterpiece. Miró's \"The Tilled Field\" also contains several parallels to Bosch's \"Garden of Earthly Delights\": similar flocks of birds; pools from which living creatures emerge; and oversize disembodied ears all echo the Dutch master's work that Miró saw as a young painter in The Prado. The more abstract Joan Miró, Jean Arp, André Masson, and Max Ernst were very influential, especially in the United States during the 1940s.\nThroughout the 1930s, Surrealism continued to become more visible to the public at large. A Surrealist group developed in Britain and, according to Breton, their 1936 London International Surrealist Exhibition was a high water mark of the period and became the model for international exhibitions. Surrealist groups in Japan, and especially in Latin America, the Caribbean and in Mexico produced innovative and original works.\n\nDalí and Magritte created some of the most widely recognized images of the movement. The 1928/1929 painting \"This Is Not A Pipe\", by Magritte is the subject of a Michel Foucault 1973 book, \"This is not a Pipe\" (English edition, 1991), that discusses the painting and its paradox. Dalí joined the group in 1929, and participated in the rapid establishment of the visual style between 1930 and 1935.\n\nSurrealism as a visual movement had found a method: to expose psychological truth by stripping ordinary objects of their normal significance, in order to create a compelling image that was beyond ordinary formal organization, and perception, sometimes evoking empathy from the viewer, sometimes laughter and sometimes outrage and bewilderment.\n\n1931 marked a year when several Surrealist painters produced works which marked turning points in their stylistic evolution: in one example (see gallery above) liquid shapes become the trademark of Dalí, particularly in his \"The Persistence of Memory\", which features the image of watches that sag as if they are melting. Evocations of time and its compelling mystery and absurdity.\n\nThe characteristics of this style – a combination of the depictive, the abstract, and the psychological – came to stand for the alienation which many people felt in the modernist period, combined with the sense of reaching more deeply into the psyche, to be \"made whole with one's individuality.\"\n\nMax Ernst whose 1920 painting \"Murdering Airplane\", studied philosophy and psychology in Bonn and was interested in the alternative realities experienced by the insane. His paintings may have been inspired by the psychoanalyst Sigmund Freud's study of the delusions of a paranoiac, Daniel Paul Schreber. Freud identified Schreber's fantasy of becoming a woman as a \"castration complex.\" The central image of two pairs of legs refers to Schreber's hermaphroditic desires. Ernst's inscription on the back of the painting reads: \"The picture is curious because of its symmetry. The two sexes balance one another.\"\n\nDuring the 1920s André Masson's work was enormously influential in helping the newly arrived in Paris and young artist Joan Miró find his roots in the new Surrealist painting. Miró acknowledged in letters to his dealer Pierre Matisse the importance of Masson as an example to him in his early years in Paris.\n\nLong after personal, political and professional tensions have fragmented the Surrealist group into thin air and ether, Magritte, Miró, Dalí and the other Surrealists continue to define a visual program in the arts. Other prominent surrealist artists include Giorgio de Chirico, Méret Oppenheim, Toyen, Grégoire Michonze, Roberto Matta, Kay Sage, Leonora Carrington, Dorothea Tanning, and Leonor Fini among others.\n\nDer Blaue Reiter was a German movement lasting from 1911 to 1914, fundamental to Expressionism, along with Die Brücke which was founded the previous decade in 1905 and was a group of German expressionist artists formed in Dresden in 1905. Founding members of Die Brücke were Fritz Bleyl, Erich Heckel, Ernst Ludwig Kirchner and Karl Schmidt-Rottluff. Later members included Max Pechstein, Otto Mueller and others. The group was one of the seminal ones, which in due course had a major impact on the evolution of modern art in the 20th century and created the style of Expressionism.\n\nWassily Kandinsky, Franz Marc, August Macke, Alexej von Jawlensky, whose psychically expressive painting of the Russian dancer \"Portrait of Alexander Sakharoff\", 1909 is in the gallery above, Marianne von Werefkin, Lyonel Feininger and others founded the Der Blaue Reiter group in response to the rejection of Kandinsky's painting \"Last Judgement\" from an exhibition. Der Blaue Reiter lacked a central artistic manifesto, but was centered around Kandinsky and Marc. Artists Gabriele Münter and Paul Klee were also involved.\nThe name of the movement comes from a painting by Kandinsky created in 1903 (see illustration). It is also claimed that the name could have derived from Marc's enthusiasm for horses and Kandinsky's love of the colour blue. For Kandinsky, \"blue\" is the colour of spirituality: the darker the blue, the more it awakens human desire for the eternal.\n\nIn the USA during the period between World War I and World War II painters tended to go to Europe for recognition. Artists like Marsden Hartley, Patrick Henry Bruce, Gerald Murphy and Stuart Davis, created reputations abroad. In New York City, Albert Pinkham Ryder and Ralph Blakelock were influential and important figures in advanced American painting between 1900 and 1920. During the 1920s photographer Alfred Stieglitz exhibited Georgia O'Keeffe, Arthur Dove, Alfred Henry Maurer, Charles Demuth, John Marin and other artists including European Masters Henri Matisse, Auguste Rodin, Henri Rousseau, Paul Cézanne, and Pablo Picasso, at his gallery \"the 291.\"\n\nDuring the 1920s and the 1930s and the Great Depression, Surrealism, late Cubism, the Bauhaus, De Stijl, Dada, German Expressionism, Expressionism, and modernist and masterful color painters like Henri Matisse and Pierre Bonnard characterized the European art scene. In Germany Max Beckmann, Otto Dix, George Grosz and others politicized their paintings, foreshadowing the coming of World War II. While in America American Scene painting and the social realism and regionalism movements that contained both political and social commentary dominated the art world. Artists like Ben Shahn, Thomas Hart Benton, Grant Wood, George Tooker, John Steuart Curry, Reginald Marsh, and others became prominent. In Latin America besides the Uruguayan painter Joaquín Torres García and Rufino Tamayo from Mexico, the muralist movement with Diego Rivera, David Siqueiros, José Orozco, Pedro Nel Gómez and Santiago Martinez Delgado and the Symbolist paintings by Frida Kahlo began a renaissance of the arts for the region, with a use of color and historic, and political messages. Frida Kahlo's Symbolist works also relate strongly to Surrealism and to the Magic Realism movement in literature. The psychological drama in many of Kahlo's self portraits (above) underscore the vitality and relevance of her paintings to artists in the 21st century.\n\n\"American Gothic\" is a painting by Grant Wood from 1930. Portraying a pitchfork-holding farmer and a younger woman in front of a house of Carpenter Gothic style, it is one of the most familiar images in 20th-century American art. Art critics had favorable opinions about the painting, like Gertrude Stein and Christopher Morley, they assumed the painting was meant to be a satire of rural small-town life. It was thus seen as part of the trend towards increasingly critical depictions of rural America, along the lines of Sherwood Anderson's \"1919 Winesburg, Ohio\", Sinclair Lewis' 1920 \"Main Street\", and Carl Van Vechten's \"The Tattooed Countess\" in literature. However, with the onset of the Great Depression, the painting came to be seen as a depiction of steadfast American pioneer spirit.\n\nDiego Rivera is perhaps best known by the public world for his 1933 mural, \"Man at the Crossroads\", in the lobby of the RCA Building at Rockefeller Center. When his patron Nelson Rockefeller discovered that the mural included a portrait of Vladimir Lenin and other communist imagery, he fired Rivera, and the unfinished work was eventually destroyed by Rockefeller's staff. The film \"Cradle Will Rock\" includes a dramatization of the controversy. Frida Kahlo (Rivera's wife's) works are often characterized by their stark portrayals of pain. Of her 143 paintings 55 are self-portraits, which frequently incorporate symbolic portrayals of her physical and psychological wounds. Kahlo was deeply influenced by indigenous Mexican culture, which is apparent in her paintings' bright colors and dramatic symbolism. Christian and Jewish themes are often depicted in her work as well; she combined elements of the classic religious Mexican tradition—which were often bloody and violent—with surrealist renderings. While her paintings are not overtly Christian – she was, after all, an avowed communist – they certainly contain elements of the macabre Mexican Christian style of religious paintings.\n\nPolitical activism was an important piece of David Siqueiros' life, and frequently inspired him to set aside his artistic career. His art was deeply rooted in the Mexican Revolution, a violent and chaotic period in Mexican history in which various social and political factions fought for recognition and power. The period from the 1920s to the 1950s is known as the Mexican Renaissance, and Siqueiros was active in the attempt to create an art that was at once Mexican and universal. He briefly gave up painting to focus on organizing miners in Jalisco. He ran a political art workshop in New York City in preparation for the 1936 General Strike for Peace and May Day parade. The young Jackson Pollock attended the workshop and helped build floats for the parade. Between 1937 and 1938 he fought in the Spanish Civil War alongside the Spanish Republican forces, in opposition to Francisco Franco's military coup. He was exiled twice from Mexico, once in 1932 and again in 1940, following his assassination attempt on Leon Trotsky.\n\nDuring the 1930s radical leftist politics characterized many of the artists connected to Surrealism, including Pablo Picasso. On 26 April 1937, during the Spanish Civil War, the Basque town of Gernika was the scene of the \"Bombing of Gernika\" by the Condor Legion of Nazi Germany's Luftwaffe. The Germans were attacking to support the efforts of Francisco Franco to overthrow the Basque Government and the Spanish Republican government. The town was devastated, though the Biscayan assembly and the Oak of Gernika survived. Pablo Picasso painted his mural sized \"Guernica\" to commemorate the horrors of the bombing.\n\nIn its final form, \"Guernica\" is an immense black and white, tall and wide mural painted in oil. The mural presents a scene of death, violence, brutality, suffering, and helplessness without portraying their immediate causes. The choice to paint in black and white contrasts with the intensity of the scene depicted and invokes the immediacy of a newspaper photograph.\nPicasso painted the mural sized painting called \"Guernica\" in protest of the bombing. The painting was first exhibited in Paris in 1937, then Scandinavia, then London in 1938 and finally in 1939 at Picasso's request the painting was sent to the United States in an extended loan (for safekeeping) at MoMA. The painting went on a tour of museums throughout the USA until its final return to the Museum of Modern Art in New York City where it was exhibited for nearly thirty years. Finally in accord with Pablo Picasso's wish to give the painting to the people of Spain as a gift, it was sent to Spain in 1981.\n\nDuring the Great Depression of the 1930s, through the years of World War II American art was characterized by Social Realism and American Scene Painting (as seen above) in the work of Grant Wood, Edward Hopper, Ben Shahn, Thomas Hart Benton, and several others. \"Nighthawks\" (1942) is a painting by Edward Hopper that portrays people sitting in a downtown diner late at night. It is not only Hopper's most famous painting, but one of the most recognizable in American art. It is currently in the collection of the Art Institute of Chicago. The scene was inspired by a diner (since demolished) in Greenwich Village, Hopper's home neighborhood in Manhattan. Hopper began painting it immediately after the attack on Pearl Harbor. After this event there was a large feeling of gloominess over the country, a feeling that is portrayed in the painting. The urban street is empty outside the diner, and inside none of the three patrons is apparently looking or talking to the others but instead is lost in their own thoughts. This portrayal of modern urban life as empty or lonely is a common theme throughout Hopper's work.\n\nThe Dynamic for artists in Europe during the 1930s deteriorated rapidly as the Nazi's power in Germany and across Eastern Europe increased. The climate became so hostile for artists and art associated with Modernism and abstraction that many left for the Americas. \"Degenerate art\" was a term adopted by the Nazi regime in Germany for virtually all modern art. Such art was banned on the grounds that it was un-German or Jewish Bolshevist in nature, and those identified as degenerate artists were subjected to sanctions. These included being dismissed from teaching positions, being forbidden to exhibit or to sell their art, and in some cases being forbidden to produce art entirely.\n\n\"Degenerate Art\" was also the title of an exhibition, mounted by the Nazis in Munich in 1937, consisting of modernist artworks chaotically hung and accompanied by text labels deriding the art. Designed to inflame public opinion against modernism, the exhibition subsequently traveled to several other cities in Germany and Austria. German artist Max Beckmann and scores of others fled Europe for New York. In New York City a new generation of young and exciting Modernist painters led by Arshile Gorky, Willem de Kooning, and others were just beginning to come of age.\n\nArshile Gorky's portrait of someone who might be Willem de Kooning (above) is an example of the evolution of abstract expressionism from the context of figure painting, cubism and surrealism. Along with his friends de Kooning and John D. Graham Gorky created bio-morphically shaped and abstracted figurative compositions that by the 1940s evolved into totally abstract paintings. Gorky's work seems to be a careful analysis of memory, emotion and shape, using line and color to express feeling and nature.\n\nThe 1940s in New York City heralded the triumph of American abstract expressionism, a modernist movement that combined lessons learned from Henri Matisse, Pablo Picasso, Surrealism, Joan Miró, Cubism, Fauvism, and early Modernism via great teachers in America like Hans Hofmann and John D. Graham. American artists benefited from the presence of Piet Mondrian, Fernand Léger, Max Ernst and the André Breton group, Pierre Matisse's gallery, and Peggy Guggenheim's gallery \"The Art of This Century\", as well as other factors. The figurative work of Francis Bacon, Frida Kahlo, Edward Hopper, Lucian Freud, Andrew Wyeth and others served as a kind of alternative to abstract expressionism.\n\nPost-Second World War American painting called Abstract expressionism included artists like Jackson Pollock, Willem de Kooning, Arshile Gorky, Mark Rothko, Hans Hofmann, Clyfford Still, Franz Kline, Adolph Gottlieb, Mark Tobey, Barnett Newman, James Brooks, Philip Guston, Robert Motherwell, Conrad Marca-Relli, Jack Tworkov, William Baziotes, Richard Pousette-Dart, Ad Reinhardt, Hedda Sterne, Jimmy Ernst, Esteban Vicente, Bradley Walker Tomlin, and Theodoros Stamos, among others. American Abstract expressionism got its name in 1946 from the art critic Robert Coates. It is seen as combining the emotional intensity and self-denial of the German Expressionists with the anti-figurative aesthetic of the European abstract schools such as futurism, the Bauhaus and synthetic cubism. Abstract expressionism, action painting, and Color Field painting are synonymous with the New York School.\n\nTechnically Surrealism was an important predecessor for abstract expressionism with its emphasis on spontaneous, automatic or subconscious creation. Jackson Pollock's dripping paint onto a canvas laid on the floor is a technique that has its roots in the work of André Masson. Another important ear<nowiki>ly m</nowiki>anifestation of what came to be abstract expressionism is the work of American Northwest artist Mark Tobey, especially his \"white writing\" canvases, which, though generally not large in scale, anticipate the \"all over\" look of Pollock's drip paintings.\n\nAdditionally, Abstract expressionism has an image of being rebellious, anarchic, highly idiosyncratic and, some feel, rather nihilistic. In practice, the term is applied to any number of artists working (mostly) in New York who had quite different styles, and even applied to work which is not especially abstract nor expressionist. Pollock's energetic \"action paintings\", with their \"busy\" feel, are different both technically and aesthetically, to the violent and grotesque \"Women\" series of Willem de Kooning. As seen above in the gallery \"Woman V\" is one of a series of six paintings made by de Kooning between 1950 and 1953 that depict a three-quarter-length female figure. He began the first of these paintings, \"Woman I\" collection: The Museum of Modern Art, New York City, in June 1950, repeatedly changing and painting out the image until January or February 1952, when the painting was abandoned unfinished. The art historian Meyer Schapiro saw the painting in de Kooning's studio soon afterwards and encouraged the artist to persist. De Kooning's response was to begin three other paintings on the same theme; \"Woman II\" collection: The Museum of Modern Art, New York City, \"Woman III\", Tehran Museum of Contemporary Art, \"Woman IV\", Nelson-Atkins Museum of Art, Kansas City, Missouri. During the summer of 1952, spent at East Hampton, de Kooning further explored the theme through drawings and pastels. He may have finished work on \"Woman I\" by the end of June, or possibly as late as November 1952, and probably the other three women pictures were concluded at much the same time. The \"Woman series\" are decidedly figurative paintings. Another important artist is Franz Kline, as demonstrated by his painting \"High Street\", 1950 (see gallery) as with Jackson Pollock and other Abstract Expressionists, was labelled an \"action painter\" because of his seemingly spontaneous and intense style, focusing less, or not at all, on figures or imagery, but on the actual brush strokes and use of canvas.\n\nClyfford Still, Barnett Newman, (see above), Adolph Gottlieb, and the serenely shimmering blocks of color in Mark Rothko's work (which is not what would usually be called expressionist and which Rothko denied was abstract), are classified as abstract expressionists, albeit from what Clement Greenberg termed the Color Field direction of abstract expressionism. Both Hans Hofmann (see gallery) and Robert Motherwell (gallery) can be comfortably described as practitioners of action painting and Color Field painting.\n\nAbstract expressionism has many stylistic similarities to the Russian artists of the early 20th century such as Wassily Kandinsky. Although it is true that spontaneity or of the impression of spontaneity characterized many of the abstract expressionists works, most of these paintings involved careful planning, especially since their large size demanded it. An exception might be the drip paintings of Pollock.\n\nWhy this style gained mainstream acceptance in the 1950s is a matter of debate. American Social realism had been the mainstream in the 1930s. It had been influenced not only by the Great Depression but also by the Social Realists of Mexico such as David Alfaro Siqueiros and Diego Rivera. The political climate after World War II did not long tolerate the social protests of those painters. Abstract expressionism arose during World War II and began to be showcased during the early 1940s at galleries in New York like \"The Art of This Century Gallery\". The late 1940s through the mid-1950s ushered in the McCarthy era. It was after World War II and a time of political conservatism and extreme artistic censorship in the United States. Some people have conjectured that since the subject matter was often totally abstract, Abstract expressionism became a safe strategy for artists to pursue this style. Abstract art could be seen as apolitical. Or if the art was political, the message was largely for the insiders. However, those theorists are in the minority. As the first truly original school of painting in America, Abstract expressionism demonstrated the vitality and creativity of the country in the post-war years, as well as its ability (or need) to develop an aesthetic sense that was not constrained by the European standards of beauty.\n\nAlthough Abstract expressionism spread quickly throughout the United States, the major centers of this style were New York City and California, especially in the New York School, and the San Francisco Bay area. Abstract expressionist paintings share certain characteristics, including the use of large canvases, an \"all-over\" approach, in which the whole canvas is treated with equal importance (as opposed to the center being of more interest than the edges). The canvas as the \"arena\" became a credo of action painting, while the \"integrity of the picture plane\" became a credo of the Color Field painters. Many other artists began exhibiting their abstract expressionist related paintings during the 1950s including Alfred Leslie, Sam Francis, Joan Mitchell, Helen Frankenthaler, Cy Twombly, Milton Resnick, Michael Goldberg, Norman Bluhm, Ray Parker, Nicolas Carone, Grace Hartigan, Friedel Dzubas, and Robert Goodnough among others. \n\nDuring the 1950s Color Field painting initially referred to a particular type of abstract expressionism, especially the work of Mark Rothko, Clyfford Still, Barnett Newman, Robert Motherwell and Adolph Gottlieb. It essentially involved abstract paintings with large, flat expanses of color that expressed the sensual, and visual feelings and properties of large areas of nuanced surface. Art critic Clement Greenberg perceived Color Field painting as related to but different from Action painting. The overall expanse and gestalt of the work of the early color field painters speaks of an almost religious experience, awestruck in the face of an expanding universe of sensuality, color and surface. During the early-to-mid-1960s, \"Color Field painting\" came to refer to the styles of artists like Jules Olitski, Kenneth Noland, and Helen Frankenthaler, whose works were related to second-generation abstract expressionism, and to younger artists like Larry Zox, and Frank Stella, – all moving in a new direction. Artists like Clyfford Still, Mark Rothko, Hans Hofmann, Morris Louis, Jules Olitski, Kenneth Noland, Helen Frankenthaler, Larry Zox, and others often used greatly reduced references to nature, and they painted with a highly articulated and psychological use of color. In general these artists eliminated recognizable imagery. In \"Mountains and Sea\", from 1952, (see above) a seminal work of Color Field painting by Helen Frankenthaler the artist used the stain technique for the first time.\n\nIn Europe there was the continuation of Surrealism, Cubism, Dada and the works of Matisse. Also in Europe, Tachisme (the European equivalent to Abstract expressionism) took hold of the newest generation. Serge Poliakoff, Nicolas de Staël, Georges Mathieu, Vieira da Silva, Jean Dubuffet, Yves Klein and Pierre Soulages among others are considered important figures in post-war European painting.\n\nEventually abstract painting in America evolved into movements such as Neo-Dada, Color Field painting, Post painterly abstraction, Op art, hard-edge painting, Minimal art, shaped canvas painting, Lyrical Abstraction, Neo-expressionism and the continuation of Abstract expressionism. As a response to the tendency toward abstraction imagery emerged through various new movements, notably Pop art.\n\nEarlier in England in 1956 the term \"Pop Art\" was used by Lawrence Alloway for paintings that celebrated consumerism of the post World War II era. This movement rejected abstract expressionism and its focus on the hermeneutic and psychological interior, in favor of art which depicted, and often celebrated material consumer culture, advertising, and iconography of the mass production age. The early works of David Hockney and the works of Richard Hamilton Peter Blake and Eduardo Paolozzi were considered seminal examples in the movement.\n\nPop art in America was to a large degree initially inspired by the works of Jasper Johns, Larry Rivers, and Robert Rauschenberg. Although the paintings of Gerald Murphy, Stuart Davis and Charles Demuth during the 1920s and 1930s set the table for pop art in America. In New York City during the mid-1950s Robert Rauschenberg and Jasper Johns created works of art that at first seemed to be continuations of Abstract expressionist painting. Actually their works and the work of Larry Rivers, were radical departures from abstract expressionism especially in the use of banal and literal imagery and the inclusion and the combining of mundane materials into their work. The innovations of Johns' specific use of various images and objects like chairs, numbers, targets, beer cans and the American flag; Rivers paintings of subjects drawn from popular culture such as George Washington crossing the Delaware, and his inclusions of images from advertisements like the camel from Camel cigarettes, and Rauschenberg's surprising constructions using inclusions of objects and pictures taken from popular culture, hardware stores, junkyards, the city streets, and taxidermy gave rise to a radical new movement in American art. Eventually by 1963 the movement came to be known worldwide as pop art.\n\nAmerican pop art is exemplified by artists: Andy Warhol, Claes Oldenburg, Wayne Thiebaud, James Rosenquist, Jim Dine, Tom Wesselmann and Roy Lichtenstein among others. Lichtenstein's most important work is arguably \"Whaam!\" (1963, Tate Modern, London), one of the earliest known examples of pop art, adapted a comic-book panel from a 1962 issue of DC Comics' \"All-American Men of War\". The painting depicts a fighter aircraft firing a rocket into an enemy plane, with a red-and-yellow explosion. The cartoon style is heightened by the use of the onomatopoeic lettering \"\"Whaam!\"\" and the boxed caption \"\"I pressed the fire control... and ahead of me rockets blazed through the sky...\"\" Pop art merges popular and mass culture with fine art, while injecting humor, irony, and recognizable imagery and content into the mix. In October 1962 the Sidney Janis Gallery mounted \"The New Realists\" the first major pop art group exhibition in an uptown art gallery in New York City. Sidney Janis mounted the exhibition in a 57th Street storefront near his gallery at 15 E. 57th Street. The show sent shockwaves through the New York School and reverberated worldwide. Earlier in the fall of 1962 an historically important and ground-breaking \"New Painting of Common Objects\" exhibition of pop art, curated by Walter Hopps at the Pasadena Art Museum sent shock waves across the Western United States.\n\nWhile in the downtown scene in New York City's East Village 10th Street galleries artists were formulating an American version of Pop Art. Claes Oldenburg had his storefront and made painted objects, and the Green Gallery on 57th Street began to show Tom Wesselmann and James Rosenquist. Later Leo Castelli exhibited other American artists including the bulk of the careers of Andy Warhol and Roy Lichtenstein and his use of Benday dots, a technique used in commercial reproduction. There is a connection between the radical works of Duchamp, and Man Ray, the rebellious Dadaists – with a sense of humor; and pop artists like Alex Katz (who became known for his parody's of portrait photography and suburban life), Claes Oldenburg, Andy Warhol, Roy Lichtenstein and the others.\n\nWhile throughout the 20th century many painters continued to practice landscape and figurative painting with contemporary subjects and solid technique, like Milton Avery, John D. Graham, Fairfield Porter, Edward Hopper, Balthus, Francis Bacon, Nicolas de Staël, Andrew Wyeth, Lucian Freud, Frank Auerbach, Philip Pearlstein, David Park, Nathan Oliveira, David Hockney, Malcolm Morley, Richard Estes, Ralph Goings, Audrey Flack, Chuck Close, Susan Rothenberg, Eric Fischl, Vija Celmins and Richard Diebenkorn.\n\nDuring the 1930s through the 1960s abstract painting in America and Europe evolved into movements such as abstract expressionism, Color Field painting, Post painterly abstraction, Op art, hard-edge painting, Minimal art, shaped canvas painting, and Lyrical Abstraction. Other artists reacted as a response to the tendency toward abstraction, allowing figurative imagery to continue through various new contexts like the Bay Area Figurative Movement in the 1950s and new forms of expressionism from the 1940s through the 1960s. In Italy during this time, Giorgio Morandi was the foremost still life painter, exploring a wide variety of approaches to depicting everyday bottles and kitchen implements. Throughout the 20th century many painters practiced Realism and used expressive imagery; practicing landscape and figurative painting with contemporary subjects and solid technique, and unique expressivity like still-life painter Giorgio Morandi, Milton Avery, John D. Graham, Fairfield Porter, Edward Hopper, Andrew Wyeth, Balthus, Francis Bacon, Leon Kossoff, Frank Auerbach, Lucian Freud, Philip Pearlstein, Willem de Kooning, Arshile Gorky, Grace Hartigan, Robert De Niro, Sr., Elaine de Kooning and others. Along with Henri Matisse, Pablo Picasso, Pierre Bonnard, Georges Braque, and other 20th-century masters. In particular Milton Avery through his use of color and his interest in seascape and landscape paintings connected with the Color field aspect of Abstract expressionism as manifested by Adolph Gottlieb and Mark Rothko as well as the lessons American painters took from the work of Henri Matisse.\n\n\"Head VI\", 1949 (see above) is a painting by the Irish born artist Francis Bacon and is an example of Post World War II European Expressionism. The work shows a distorted version of the Portrait of Innocent X painted by the Spanish artist Diego Velázquez in 1650. The work is one of a series of variants of the Velázquez painting which Bacon executed throughout the 1950s and early 1960s, over a total of forty-five works. When asked why he was compelled to revisit the subject so often, Bacon replied that he had nothing against the Popes, that he merely \"wanted an excuse to use these colours, and you can't give ordinary clothes that purple colour without getting into a sort of false fauve manner.\" The Pope in this version seethes with anger and aggression, and the dark colors give the image a grotesque and nightmarish appearance. The pleated curtains of the backdrop are rendered transparent, and seem to fall through the Pope's face.\n\nItalian painter Giorgio Morandi was an important 20th-century early pioneer of Minimalism. Born in Bologna, Italy in 1890, throughout his career, Morandi concentrated almost exclusively on still lives and landscapes, except for a few self-portraits. With great sensitivity to tone, color, and compositional balance, he would depict the same familiar bottles and vases again and again in paintings notable for their simplicity of execution. Morandi executed 133 etchings, a significant body of work in its own right, and his drawings and watercolors often approach abstraction in their economy of means. Through his simple and repetitive motifs and economical use of color, value and surface, Morandi became a prescient and important forerunner of Minimalism. He died in Bologna in 1964.\n\nAfter World War II the term School of Paris often referred to Tachisme, the European equivalent of American Abstract expressionism and those artists are also related to Cobra. Important proponents being Jean Dubuffet, Pierre Soulages, Nicholas de Staël, Hans Hartung, Serge Poliakoff, and Georges Mathieu, among several others. During the early 1950s Dubuffet (who was always a figurative artist), and de Staël, abandoned abstraction, and returned to imagery via figuration and landscape. De Staël 's work was quickly recognised within the post-war art world, and he became one of the most influential artists of the 1950s. His return to representation (seascapes, footballers, jazz musicians, seagulls) during the early 1950s can be seen as an influential precedent for the American Bay Area Figurative Movement, as many of those abstract painters like Richard Diebenkorn, David Park, Elmer Bischoff, Wayne Thiebaud, Nathan Oliveira, Joan Brown and others made a similar move; returning to imagery during the mid-1950s. Much of de Staël 's late work – in particular his thinned, and diluted oil on canvas abstract landscapes of the mid-1950s predicts Color Field painting and Lyrical Abstraction of the 1960s and 1970s. Nicolas de Staël's bold and intensely vivid color in his last paintings predict the direction of much of contemporary painting that came after him including Pop art of the 1960s.\n\nDuring the 1950s and 1960s as abstract painting in America and Europe evolved into movements such as Color Field painting, post-painterly abstraction, op art, hard-edge painting, minimal art, shaped canvas painting, Lyrical Abstraction, and the continuation of Abstract expressionism. Other artists reacted as a response to the tendency toward abstraction with art brut, as seen in \"Court les rues,\" 1962, by Jean Dubuffet, fluxus, neo-Dada, New Realism, allowing imagery to re-emerge through various new contexts like pop art, the Bay Area Figurative Movement (a prime example is Diebenkorn's \"Cityscape I, (Landscape No. 1),\" 1963, Oil on canvas, 60 1/4 x 50 1/2 inches, collection: San Francisco Museum of Modern Art) and later in the 1970s Neo-expressionism. The Bay Area Figurative Movement of whom David Park, Elmer Bischoff, Nathan Oliveira and Richard Diebenkorn whose painting \"Cityscape 1\", 1963 is a typical example (see above) were influential members flourished during the 1950s and 1960s in California. Although throughout the 20th century painters continued to practice Realism and use imagery, practicing landscape and figurative painting with contemporary subjects and solid technique, and unique expressivity like Milton Avery, Edward Hopper, Jean Dubuffet, Francis Bacon, Frank Auerbach, Lucian Freud, Philip Pearlstein, and others. Younger painters practiced the use of imagery in new and radical ways. Yves Klein, Martial Raysse, Niki de Saint Phalle, Wolf Vostell, David Hockney, Alex Katz, Malcolm Morley, Ralph Goings, Audrey Flack, Richard Estes, Chuck Close, Susan Rothenberg, Eric Fischl, John Baeder and Vija Celmins were a few who became prominent between the 1960s and the 1980s. Fairfield Porter (see above) was largely self-taught, and produced representational work in the midst of the Abstract Expressionist movement. His subjects were primarily landscapes, domestic interiors and portraits of family, friends and fellow artists, many of them affiliated with the New York School of writers, including John Ashbery, Frank O'Hara, and James Schuyler. Many of his paintings were set in or around the family summer house on Great Spruce Head Island, Maine.\n\nAlso during the 1960s and 1970s, there was a reaction against painting. Critics like Douglas Crimp viewed the work of artists like Ad Reinhardt, and declared the 'death of painting'. Artists began to practice new ways of making art. New movements gained prominence some of which are: Postminimalism, Earth art, video art, installation art, arte povera, performance art, body art, fluxus, mail art, the situationists and conceptual art among others.\n\nNeo-Dada is also a movement that started 1n the 1950s and 1960s and was related to Abstract expressionism only with imagery. Featuring the emergence of combined manufactured items, with artist materials, moving away from previous conventions of painting. This trend in art is exemplified by the work of Jasper Johns and Robert Rauschenberg, whose \"combines\" in the 1950s were forerunners of Pop Art and Installation art, and made use of the assemblage of large physical objects, including stuffed animals, birds and commercial photography. Robert Rauschenberg, (see \"untitled combine\", 1963, above), Jasper Johns, Larry Rivers, John Chamberlain, Claes Oldenburg, George Segal, Jim Dine, and Edward Kienholz among others were important pioneers of both abstraction and Pop Art; creating new conventions of art-making; they made acceptable in serious contemporary art circles the radical inclusion of unlikely materials as parts of their works of art.\n\nColor Field painting clearly pointed toward a new direction in American painting, away from abstract expressionism. Color Field painting is related to post-painterly abstraction, suprematism, abstract expressionism, hard-edge painting and Lyrical Abstraction.\nDuring the 1960s and 1970s abstract painting continued to develop in America through varied styles. Geometric abstraction, Op art, hard-edge painting, Color Field painting and minimal painting, were some interrelated directions for advanced abstract painting as well as some other new movements. Morris Louis was an important pioneer in advanced Color Field painting, his work can serve as a bridge between abstract expressionism, Color Field painting, and minimal art. Two influential teachers Josef Albers and Hans Hofmann introduced a new generation of American artists to their advanced theories of color and space. Josef Albers is best remembered for his work as an Geometric abstractionist painter and theorist. Most famous of all are the hundreds of paintings and prints that make up the series \"Homage to the Square\", (see gallery). In this rigorous series, begun in 1949, Albers explored chromatic interactions with flat colored squares arranged concentrically on the canvas. Albers' theories on art and education were formative for the next generation of artists. His own paintings form the foundation of both hard-edge painting and Op art.\n\nJosef Albers, Hans Hofmann, Ilya Bolotowsky, Burgoyne Diller, Victor Vasarely, Bridget Riley, Richard Anuszkiewicz, Frank Stella, Morris Louis, Kenneth Noland, Ellsworth Kelly, Barnett Newman, Larry Poons, Ronald Davis, Larry Zox, Al Held and some others like Mino Argento, are artists closely associated with Geometric abstraction, Op art, Color Field painting, and in the case of Hofmann and Newman Abstract expressionism as well.\n\nIn 1965, an exhibition called \"The Responsive Eye\", curated by William C. Seitz, was held at the Museum of Modern Art, in New York City. The works shown were wide ranging, encompassing the Minimalism of Frank Stella, the Op art of Larry Poons, the work of Alexander Liberman, alongside the masters of the Op Art movement: Victor Vasarely, Richard Anuszkiewicz, Bridget Riley and others. The exhibition focused on the perceptual aspects of art, which result both from the illusion of movement and the interaction of color relationships. Op art, also known as optical art, is a style present in some paintings and other works of art that use optical illusions. Op art is also closely akin to geometric abstraction and hard-edge painting. Although sometimes the term used for it is perceptual abstraction.\n\nOp art is a method of painting concerning the interaction between illusion and picture plane, between understanding and seeing. Op art works are abstract, with many of the better known pieces made in only black and white. When the viewer looks at them, the impression is given of movement, hidden images, flashing and vibration, patterns, or alternatively, of swelling or warping.\n\nColor Field painting sought to rid art of superfluous rhetoric. Artists like Clyfford Still, Mark Rothko, Hans Hofmann, Morris Louis, Jules Olitski, Kenneth Noland, Helen Frankenthaler, John Hoyland, Larry Zox, and others often used greatly reduced references to nature, and they painted with a highly articulated and psychological use of color. In general these artists eliminated recognizable imagery. Certain artists quoted references to past or present art, but in general color field painting presents abstraction as an end in itself. In pursuing this direction of modern art, artists wanted to present each painting as one unified, cohesive, monolithic image.\n\nFrank Stella, Kenneth Noland, Ellsworth Kelly, Barnett Newman, Ronald Davis, Neil Williams, Robert Mangold, Charles Hinman, Richard Tuttle, David Novros, and Al Loving are examples of artists associated with the use of the shaped canvas during the period beginning in the early 1960s. Many Geometric abstract artists, minimalists, and Hard-edge painters elected to use the edges of the image to define the shape of the painting rather than accepting the rectangular format. In fact, the use of the shaped canvas is primarily associated with paintings of the 1960s and 1970s that are coolly abstract, formalistic, geometrical, objective, rationalistic, clean-lined, brashly sharp-edged, or minimalist in character. The Andre Emmerich Gallery, the Leo Castelli Gallery, the Richard Feigen Gallery, and the Park Place Gallery were important showcases for Color Field painting, shaped canvas painting and Lyrical Abstraction in New York City during the 1960s. There is a connection with post-painterly abstraction, which reacted against abstract expressionisms' mysticism, hyper-subjectivity, and emphasis on making the act of painting itself dramatically visible – as well as the solemn acceptance of the flat rectangle as an almost ritual prerequisite for serious painting. During the 1960s Color Field painting and Minimal art were often closely associated with each other. In actuality by the early 1970s both movements became decidedly diverse.\n\nAnother related movement of the late 1960s, Lyrical Abstraction (the term being coined by Larry Aldrich, the founder of the Aldrich Contemporary Art Museum, Ridgefield Connecticut), encompassed what Aldrich said he saw in the studios of many artists at that time. It is also the name of an exhibition that originated in the Aldrich Museum and traveled to the Whitney Museum of American Art and other museums throughout the United States between 1969 and 1971.\n\nLyrical Abstraction in the late 1960s is characterized by the paintings of Dan Christensen, Ronnie Landfield, Peter Young and others, and along with the fluxus movement and postminimalism (a term first coined by Robert Pincus-Witten in the pages of \"Artforum\" in 1969) sought to expand the boundaries of abstract painting and minimalism by focusing on process, new materials and new ways of expression. Postminimalism often incorporating industrial materials, raw materials, fabrications, found objects, installation, serial repetition, and often with references to Dada and Surrealism is best exemplified in the sculptures of Eva Hesse. Lyrical Abstraction, conceptual art, postminimalism, Earth art, video, performance art, installation art, along with the continuation of fluxus, abstract expressionism, Color Field painting, hard-edge painting, minimal art, op art, pop art, photorealism and New Realism extended the boundaries of contemporary art in the mid-1960s through the 1970s. Lyrical Abstraction is a type of freewheeling abstract painting that emerged in the mid-1960s when abstract painters returned to various forms of painterly, pictorial, expressionism with a predominate focus on process, gestalt and repetitive compositional strategies in general.\n\nLyrical Abstraction shares similarities with color field painting and abstract expressionism, Lyrical Abstraction as exemplified by the 1968 Ronnie Landfield painting \"For William Blake\", (above) especially in the freewheeling usage of paint – texture and surface. Direct drawing, calligraphic use of line, the effects of brushed, splattered, stained, squeegeed, poured, and splashed paint superficially resemble the effects seen in abstract expressionism and color field painting. However, the styles are markedly different. Setting it apart from abstract expressionism and action painting of the 1940s and 1950s is the approach to composition and drama. As seen in action painting there is an emphasis on brushstrokes, high compositional drama, dynamic compositional tension. While in Lyrical Abstraction there is a sense of compositional randomness, all over composition, low key and relaxed compositional drama and an emphasis on process, repetition, and an all over sensibility.,\n\nAgnes Martin, Robert Mangold (see above), Brice Marden, Jo Baer, Robert Ryman, Richard Tuttle, Neil Williams, David Novros, Paul Mogenson, Charles Hinman are examples of artists associated with Minimalism and (exceptions of Martin, Baer and Marden) the use of the shaped canvas also during the period beginning in the early 1960s. Many Geometric abstract artists, minimalists, and hard-edge painters elected to use the edges of the image to define the shape of the painting rather than accepting the rectangular format. In fact, the use of the shaped canvas is primarily associated with paintings of the 1960s and 1970s that are coolly abstract, formalistic, geometrical, objective, rationalistic, clean-lined, brashly sharp-edged, or minimalist in character. The Bykert Gallery, and the Park Place Gallery were important showcases for Minimalism and shaped canvas painting in New York City during the 1960s.\n\nDuring the 1960s and 1970s artists such as Robert Motherwell, Adolph Gottlieb, Phillip Guston, Lee Krasner, Cy Twombly, Robert Rauschenberg, Jasper Johns, Richard Diebenkorn, Josef Albers, Elmer Bischoff, Agnes Martin, Al Held, Sam Francis, Ellsworth Kelly, Morris Louis, Helen Frankenthaler, Gene Davis, Frank Stella, Kenneth Noland, Joan Mitchell, Friedel Dzubas, and younger artists like Brice Marden, Robert Mangold, Sam Gilliam, John Hoyland, Sean Scully, Pat Steir, Elizabeth Murray, Larry Poons, Walter Darby Bannard, Larry Zox, Ronnie Landfield, Ronald Davis, Dan Christensen, Joan Snyder, Ross Bleckner, Archie Rand, Susan Crile, and dozens of others produced a wide variety of paintings.\n\nDuring the 1960s and 1970s, there was a reaction against abstract painting. Some critics viewed the work of artists like Ad Reinhardt, and declared the 'death of painting'. Artists began to practice new ways of making art. New movements gained prominence some of which are: postminimalism, Earth art, video art, installation art, arte povera, performance art, body art, fluxus, happening, mail art, the situationists and conceptual art among others.\n\nHowever still other important innovations in abstract painting took place during the 1960s and the 1970s characterized by monochrome painting and hard-edge painting inspired by Ad Reinhardt, Barnett Newman, Milton Resnick, and Ellsworth Kelly. Artists as diverse as Agnes Martin, Al Held, Larry Zox, Frank Stella, Larry Poons, Brice Marden and others explored the power of simplification. The convergence of Color Field painting, minimal art, hard-edge painting, Lyrical Abstraction, and postminimalism blurred the distinction between movements that became more apparent in the 1980s and 1990s. The neo-expressionism movement is related to earlier developments in abstract expressionism, neo-Dada, Lyrical Abstraction and postminimal painting.\n\nIn the late 1960s the abstract expressionist painter Philip Guston helped to lead a transition from abstract expressionism to Neo-expressionism in painting, abandoning the so-called \"pure abstraction\" of abstract expressionism in favor of more cartoonish renderings of various personal symbols and objects. These works were inspirational to a new generation of painters interested in a revival of expressive imagery. His painting \"Painting, Smoking, Eating\" 1973, seen above in the gallery is an example of Guston's final and conclusive return to representation.\n\nIn the late 1970s and early 1980s, there was also a return to painting that occurred almost simultaneously in Italy, Germany, France and Britain. These movements were called Transavantguardia, Neue Wilde, Figuration Libre, Neo-expressionism, the school of London, and in the late 1980s the Stuckists respectively. These painting were characterized by large formats, free expressive mark making, figuration, myth and imagination. All work in this genre came to be labeled neo-expressionism. Critical reaction was divided. Some critics regarded it as driven by profit motivations by large commercial galleries. This type of art continues in popularity into the 21st century, even after the art crash of the late 1980s. Anselm Kiefer is a leading figure in European Neo-expressionism by the 1980s, (see \"To the Unknown Painter\" 1983, in the gallery above) Kiefer's themes widened from a focus on Germany's role in civilization to the fate of art and culture in general. His work became more sculptural and involves not only national identity and collective memory, but also occult symbolism, theology and mysticism. The theme of all the work is the trauma experienced by entire societies, and the continual rebirth and renewal in life.\n\nDuring the late 1970s in the United States painters who began working with invigorated surfaces and who returned to imagery like Susan Rothenberg gained in popularity, especially as seen above in paintings like \"Horse 2\", 1979. During the 1980s American artists like Eric Fischl, (see \"Bad Boy\", 1981, above), David Salle, Jean-Michel Basquiat (who began as a graffiti artist), Julian Schnabel, and Keith Haring, and Italian painters like Mimmo Paladino, Sandro Chia, and Enzo Cucchi, among others defined the idea of Neo-expressionism in America.\n\nNeo-expressionism was a style of modern painting that became popular in the late 1970s and dominated the art market until the mid-1980s. It developed in Europe as a reaction against the conceptual and minimalistic art of the 1960s and 1970s. Neo-expressionists returned to portraying recognizable objects, such as the human body (although sometimes in a virtually abstract manner), in a rough and violently emotional way using vivid colours and banal colour harmonies. The veteran painters Philip Guston, Frank Auerbach, Leon Kossoff, Gerhard Richter, A. R. Penck and Georg Baselitz, along with slightly younger artists like Anselm Kiefer, Eric Fischl, Susan Rothenberg, Francesco Clemente, Jean-Michel Basquiat, Julian Schnabel, Keith Haring, and many others became known for working in this intense expressionist vein of painting.\n\nPainting still holds a respected position in contemporary art. Art is an open field no longer divided by the objective versus non-objective dichotomy. Artists can achieve critical success whether their images are representational or abstract. What has currency is content, exploring the boundaries of the medium, and a refusal to recapitulate the works of the past as an end goal.\n\nAt the beginning of the 21st century Contemporary painting and Contemporary art in general continues in several contiguous modes, characterized by the idea of pluralism. The \"crisis\" in painting and current art and current art criticism today is brought about by pluralism. There is no consensus, nor need there be, as to a representative style of the age. There is an \"anything goes\" attitude that prevails; an \"everything going on\", and consequently \"nothing going on\" syndrome; this creates an aesthetic traffic jam with no firm and clear direction and with every lane on the artistic superhighway filled to capacity. Consequently magnificent and important works of art continue to be made albeit in a wide variety of styles and aesthetic temperaments, the marketplace being left to judge merit.\n\nHard-edge painting, geometric abstraction, appropriation, hyperrealism, photorealism, expressionism, minimalism, Lyrical Abstraction, pop art, op art, abstract expressionism, Color Field painting, monochrome painting, neo-expressionism, collage, intermedia painting, assemblage painting, digital painting, postmodern painting, neo-Dada painting, shaped canvas painting, environmental mural painting, traditional figure painting, landscape painting, portrait painting, are a few continuing and current directions in painting at the beginning of the 21st century.\n\nDuring the period before and after European exploration and settlement of the Americas, including North America, Central America, South America and the Islands of the Caribbean, the Antilles, the Lesser Antilles and other island groups, indigenous native cultures produced creative works including architecture, pottery, ceramics, weaving, , sculpture, painting and murals as well as other religious and utilitarian objects. Each continent of the Americas hosted societies that were unique and individually developed cultures; that produced totems, works of religious symbolism, and decorative and expressive painted works. African influence was especially strong in the art of the Caribbean and South America. The arts of the indigenous people of the Americas had an enormous impact and influence on European art and vice versa during and after the Age of Exploration. Spain, Portugal, France, The Netherlands, and England were all powerful and influential colonial powers in the Americas during and after the 15th century. By the 19th century cultural influence began to flow both ways across the Atlantic\n\nThe depiction of humans, animals or any other figurative subjects is forbidden within Islam to prevent believers from idolatry so there is no religiously motivated painting (or sculpture) tradition within Muslim culture. Pictorial activity was reduced to Arabesque, mainly abstract, with geometrical configuration or floral and plant-like patterns. Strongly connected to architecture and calligraphy, it can be widely seen as used for the painting of tiles in mosques or in illuminations around the text of the Koran and other books. In fact, abstract art is not an invention of modern art but it is present in pre-classical, barbarian and non-western cultures many centuries before it and is essentially a decorative or applied art. Notable illustrator M. C. Escher was influenced by this geometrical and pattern-based art. Art Nouveau (Aubrey Beardsley and the architect Antonio Gaudí) re-introduced abstract floral patterns into western art.\n\nNote that despite the taboo of figurative visualization, some Muslim countries did cultivate a rich tradition in painting, though not in its own right, but as a companion to the written word. Iranian or Persian art, widely known as Persian miniature, concentrates on the illustration of epic or romantic works of literature. Persian illustrators deliberately avoided the use of shading and perspective, though familiar with it in their pre-Islamic history, in order to abide by the rule of not creating any lifelike illusion of the real world. Their aim was not to depict the world as it is, but to create images of an ideal world of timeless beauty and perfect order.\n\nIn present days, painting by art students or professional artists in Arab and non-Arab Muslim countries follows the same tendencies of Western culture art.\n\nOriental historian Basil Gray believes \"Iran has offered a particularly unique art to the world which is excellent in its kind\". Caves in Iran's Lorestan province exhibit painted imagery of animals and hunting scenes. Some such as those in Fars Province and Sialk are at least 5,000 years old. Painting in Iran is thought to have reached a climax during the Tamerlane era, when outstanding masters such as Kamaleddin Behzad gave birth to a new style of painting.\n\nPaintings of the Qajar period are a combination of European influences and Safavid miniature schools of painting such as those introduced by Reza Abbasi and classical works by Mihr 'Ali. Masters such as Kamal-ol-molk further pushed forward the European influence in Iran. It was during the Qajar era when \"Coffee House painting\" emerged. Subjects of this style were often religious in nature depicting scenes from Shia epics and the like.\n\nAfrican traditional culture and tribes do not seem to have great interest in two-dimensional representations in favour of sculpture and relief. However, decorative painting in African culture is often abstract and geometrical. Another pictorial manifestation is body painting, and face painting present for example in Maasai and Kĩkũyũ culture in their ceremony rituals. Ceremonial cave painting in certain villages can be found to be still in use. Note that Pablo Picasso and other modern artists were influenced by African sculpture and masks in their varied styles.\nContemporary African artists follow western art movements and their paintings have little difference from occidental art works.\n\nAt the start of the 20th century, artists like Picasso, Matisse, Vincent van Gogh, Paul Gauguin and Modigliani became aware of, and were inspired by, African art. In a situation where the established avant garde was straining against the constraints imposed by serving the world of appearances, African Art demonstrated the power of supremely well organised forms; produced not only by responding to the faculty of sight, but also and often primarily, the faculty of imagination, emotion and mystical and religious experience. These artists saw in African art a formal perfection and sophistication unified with phenomenal expressive power.\n\n\n\n", "id": "13971", "title": "History of painting"}
{"url": "https://en.wikipedia.org/wiki?curid=13972", "text": "Hungarian language\n\nHungarian is the official language of Hungary and one of the 24 official languages of the European Union. Outside Hungary it is also spoken by communities of Hungarian people in neighbouring countries (especially in Romania, Slovakia, and Serbia) and by Hungarian diaspora communities worldwide. Like Finnish and Estonian, it belongs to the Uralic language family, its closest relatives being Mansi and Khanty. It is one of the several European languages not part of the Indo-European languages.\n\nThe Hungarian name for the language is \"magyar\" or \"magyar nyelv\" (). The word \"Magyar\" is also used as an English word to refer to Hungarian people as an ethnic group or its language.\n\nHungarian is a member of the Uralic language family. Linguistic connections between Hungarian and other Uralic languages were noticed in the 1670s, and the family itself (then called Finno-Ugric) was established in 1717, but the classification of Hungarian as a Uralic/Finno-Ugric rather than Turkic language continued to be a matter of impassioned political controversy throughout the 18th and into the 19th centuries. Hungarian has traditionally been assigned to a Ugric branch within Uralic/Finno-Ugric, along with the Mansi and Khanty languages of western Siberia (Khanty–Mansia region), but it is no longer clear that it is a valid group. When the Samoyed languages were determined to be part of the family, it was thought at first that Finnic and Ugric (Finno-Ugric) were closer to each other than to the Samoyed branch of the family, but that now is frequently questioned.\n\nThe name of Hungary could be a result of regular sound changes of \"Ungrian/Ugrian\", and the fact that the Eastern Slavs referred to Hungarians as \"Ǫgry/Ǫgrove\" (sg. \"Ǫgrinŭ\") seemed to confirm that. Current literature favors the hypothesis that it comes from the name of the Turkic tribe Onogur (which means \"ten arrows\" or \"ten tribes\").\n\nThere are numerous regular sound correspondences between Hungarian and the other Ugric languages. For example, Hungarian corresponds to Khanty in certain positions, and Hungarian corresponds to Khanty , while Hungarian final corresponds to Khanty final . For example, Hungarian \"ház\" \"house\" vs. Khanty \"xot\" \"house\", and Hungarian \"száz\" \"hundred\" vs. Khanty \"sot\" \"hundred\". The distance between the Ugric and Finnic languages is greater, but the correspondences are also regular.\n\nDuring the later half of the 19th century, a competing hypothesis proposed a Turkic affinity of Hungarian. Following an academic debate known as \"Az ugor-török háború\" (\"the Ugric-Turkic battle\"), the Finno-Ugric hypothesis was concluded the sounder of the two, foremost based on work by the German linguist .\n\nIt is thought that Hungarian separated from its Ugric relatives in the first half of the 1st millennium, in western Siberia, east of the southern Urals. The Hungarians gradually changed their lifestyle from settled hunters to nomadic pastoralists (cattle, sheep), probably as a result of early contacts with Iranian nomads (Scythians, Sarmatians). In Hungarian, Iranian loans date back to the time immediately following the breakup of Ugric and probably span well over a millennium. Among these include \"tehén\" ‘cow’ (cf. Avestan \"dhaénu\"), \"tíz\" ‘ten’ (cf. Avestan \"dasa\"), \"tej\" ‘milk’ (cf. Persian \"dáje\" ‘wet nurse’), and \"nád\" ‘reed’ (from late Middle Iranian; cf. Middle Persian \"nāy\").\n\nA small number of anthropologists disputed this theory, such as Hungarian historian and archaeologist Gyula László who claimed that geological data from pollen analysis seems to contradict placing the ancient homeland of the Hungarians near the Urals. However, increasing archaeological evidence from present-day southern Bashkortostan found in the previous decades confirms the existence of Hungarian settlements between the Volga River and Ural Mountains.\n\nThe Onogurs (and Bulgars) later had a great influence on the language, especially between the 5th-9th centuries. This layer of Turkic loans is large and varied (e.g. \"szó\" ‘word’, from Turkic, \"daru\" ‘crane’, from the related Permic languages), and includes words borrowed from Oghur Turkic, e.g. \"borjú\" ‘calf’ (cf. Chuvash \"păru\", \"părăv\" vs. Turkish \"buzağı\"), \"dél\" ‘noon; south’ (cf. Chuvash \"tĕl\" vs. Turkish dial. \"düš\"). Many words related to agriculture, to state administration or even to family relations have such backgrounds. Hungarian syntax and grammar were not influenced in a similarly dramatic way during these 300 years.\n\nAfter the arrival of the Hungarians into the Carpathian Basin the language came into contact with different speech communities (mainly Slavic, Turkic, German and Romanian). Turkic loans from this period come mainly from the Pechenegs and Cumanians who settled in Hungary during the 12th-13th centuries; e.g., \"koboz\" ‘cobza’ (cf. Turkish \"kopuz\" ‘lute’), \"komondor\" ‘mop dog’ (< *\"kumandur\" < \"Cuman\"). Hungarian borrowed many words from especially the neighbouring Slavic languages (e.g., \"tégla\" ‘brick’, \"mák\" ‘poppy’, and \"karácsony\" ‘Christmas’). In exchange, these languages also borrowed words from Hungarian, e.g. Serbo-Croatian \"ašov\" from Hung \"ásó\" ‘spade’. Approximately 1.6% of the Romanian lexicon is of Hungarian origin.\n\nThe first written accounts of Hungarian, mostly personal and place names, are dated back to the 10th century. Hungarians also had their own writing system, the Old Hungarian script, but no significant texts remain from that time, as the usual medium of writing, wooden sticks, is perishable.\n\nThe Kingdom of Hungary was founded in 1000, by Stephen I of Hungary. The country was a western-styled Christian (Roman Catholic) state, and Latin held an important position, as was usual in the Middle Ages. The Latin script was adopted to write the Hungarian language and Latin influenced the language. The earliest remaining fragments of the language are found in the establishing charter of the abbey of Tihany from 1055, mixed into Latin text. The first extant text fully written in Hungarian is the Funeral Sermon and Prayer, written in the 1190s. The orthography of these early texts was considerably different from the one used today, but when hearing a reconstructed spoken version, contemporary Hungarians can still understand a large part of it, though both vocabulary and grammar has changed to some extent since then. More extensive Hungarian literature arose after 1300. The earliest known example of Hungarian religious poetry is the 14th-century \"Lamentations of Mary\". The first Bible translation is the Hussite Bible from the 1430s.\n\nThe standard language lost its diphthongs, and several postpositions transformed into suffixes, such as \"reá\" \"onto\" (the phrase \"utu rea \"onto the way\" found in the 1055 text would later become \"útra). There were also changes in the system of vowel harmony. At one time, Hungarian used six verb tenses; today, only two are commonly used (present and past; future is formed with an auxiliary verb and is usually not counted as a separate tense).\n\nThe first printed Hungarian book was published in Kraków in 1533, by Benedek Komjáti. The work's title is \"Az Szent Pál levelei magyar nyelven\" (In original spelling: \"Az zenth Paal leueley magyar nyeluen\"), i.e. \"The letters of Saint Paul in the Hungarian language\". In the 17th century, the language was already very similar to its present-day form, although two of the past tenses were still used. German, Italian and French loans also appeared in the language by these years. Further Turkish words were borrowed during the Ottoman rule of part of Hungary between 1541 and 1699.\n\nIn the 18th century a group of writers, most notably Ferenc Kazinczy, began the process of language renewal (Hungarian: \"nyelvújítás\"). Some words were shortened (\"győzedelem\" > \"győzelem\", 'triumph' or 'victory'); a number of dialectal words spread nationally (e.g. \"cselleng\" 'dawdle'); extinct words were reintroduced (\"dísz\" 'décor'); a wide range of expressions were coined using the various derivative suffixes; and some other, less frequently used methods of expanding the language were utilized. This movement produced more than ten thousand words, most of which are used actively today.\n\nThe 19th and 20th centuries saw further standardization of the language, and differences between the mutually comprehensible dialects gradually lessened. In 1920, by signing the Treaty of Trianon, Hungary lost 71% of its territory, and along with these, 33% of the ethnic Hungarian population. Today, the language is official in Hungary, and regionally also in Romania, in Slovakia, in Serbia, in Austria and in Slovenia.\n\nHungarian has about 13 million native speakers, of whom more than 9.8 million live in Hungary. According to the 2011 Hungarian census 9,896,333 people (99.6% of the total population) speak Hungarian, of whom 9,827,875 people (98.9%) speak it as a first language, while 68,458 people (0.7%) speak it as a second language. About 2.2 million speakers live in areas that were part of the Kingdom of Hungary before the Treaty of Trianon (1920). Of these, the largest group lives in Transylvania, the western half of present-day Romania, where there are approximately 1.25 million Hungarians. There are large Hungarian communities also in Slovakia, Serbia and Ukraine, and Hungarians can also be found in Austria, Croatia, and Slovenia, as well as about a million additional people scattered in other parts of the world. For example, there are more than one hundred thousand Hungarian speakers in the Hungarian American community and 1.5 million with Hungarian ancestry in the United States.\n\nHungarian is the official language of Hungary, and thus an official language of the European Union. Hungarian is also one of the official languages of Vojvodina and an official language of three municipalities in Slovenia: Hodoš, Dobrovnik and Lendava, along with Slovene. Hungarian is officially recognized as a minority or regional language in Austria, Croatia, Romania, Zakarpattia in Ukraine, and Slovakia. In Romania it is a recognized minority language used at local level in communes, towns and municipalities with an ethnic Hungarian population of over 20%.\n\nThe dialects of Hungarian identified by Ethnologue are: Alföld, West Danube, Danube-Tisza, King's Pass Hungarian, Northeast Hungarian, Northwest Hungarian, Székely and West Hungarian. These dialects are, for the most part, mutually intelligible. The Hungarian Csángó dialect, which is mentioned but not listed separately by Ethnologue, is spoken primarily in Bacău County in eastern Romania. The Csángó Hungarian group has been largely isolated from other Hungarian people, and they therefore preserved features that closely resemble earlier forms of Hungarian.\n\nHungarian has 14 vowel phonemes and 25 consonant phonemes. The vowel phonemes can be grouped as pairs of short and long vowels, e.g. \"o\" and \"ó\". Most of these pairs have a similar pronunciation, only varying significantly in their duration. However, the pairs \"a\"/\"á\" and \"e\"/\"é\" differ both in closedness and length.\n\nConsonant length is also distinctive in Hungarian. Most of the consonant phonemes can occur as geminates.\n\nThe sound voiced palatal plosive , written , sounds similar to 'd' in British English 'duty' (in fact, more similar to the Macedonian phoneme 'ѓ' as in 'ѓакон'). It occurs in the name of the country, \"Magyarország\" (Hungary), pronounced .\n\nSingle /r/s are tapped (e.g. \"akkora\" 'of that size'), double /r/s are trilled (e.g. \"akkorra\" 'by that time'), similar to the Spanish \"pero\" and \"perro\".\n\nPrimary stress is always on the first syllable of a word, as in the related Finnish languages and in the neighbouring languages Slovak and Czech. There is secondary stress on other syllables in compounds, e.g. \"viszontlátásra\" (\"goodbye\") pronounced . Elongated vowels in non-initial syllables may seem to be stressed to the ear of an English speaker, since length and stress correlate in English.\n\nHungarian is an agglutinative language. It uses various affixes, mainly suffixes, but also some prefixes and a circumfix to change a word's meaning and grammatical function.\n\nHungarian uses vowel harmony when attaching suffixes to words. This means that most suffixes have two or three different forms and the choice between them depends on the vowels of the head word. There are some minor and unpredictable exceptions to this rule.\n\nNouns have a large number of cases (up to 18, depending on definition), but in general, they are formed regularly with suffixes. The nominative case is unmarked (\"az alma\" ‘the apple'), and for example, the accusative is marked with the suffix \"–t\" (\"az almát\" ‘[I eat] the apple'). Half of the 18 cases express a combination of the source-location-target and surface-inside-proximity ternary distinctions (three times three cases), e.g. there is a separate case ending –\"ból\"/\"–ből\" meaning a combination of source and insideness, i.e. 'from inside of'.\n\nPossession is expressed using a possessive suffix on the possessed object and not on the possessor (Peter's apple becomes \"Péter almája\", literally 'Peter apple-his'). Noun plurals are formed using the suffix \"–k\" (\"az almák\" ‘the apples’)—however, following a numeral, the singular is used (e.g. \"két alma\" ‘two apples’, literally ‘two apple’; not \"*két almák\").\n\nUnlike English, Hungarian has no prepositions; instead, it uses case suffixes and postpositions.\n\nThere are two types of articles in Hungarian, definite and indefinite, roughly corresponding to the English equivalents.\n\nAdjectives precede nouns (\"a piros alma\" ‘the red apple’). They have three degrees: positive (\"piros\" ‘red’), comparative (\"pirosabb\" ‘redder’), and superlative (\" a legpirosabb\" ‘the reddest’). If the noun takes the plural or a case, the adjective, used attributively, does not agree with it: \"a piros almák\" ‘the red apples’. However, when the adjective is used in a predicative sense, it must agree with the noun: \"az almák pirosak\" ‘the apples are red’. Adjectives in themselves can behave as nouns (e.g. take case suffixes): \"Melyik almát kéred? – A pirosat.\" 'Which apple would you like? – The red one.'\n\nVerbs are conjugated according to two tenses (past and present), to three moods (indicative, conditional and imperative-subjunctive), to two numbers (singular or plural), to three persons (first, second and third) and to whether the object (if any) is definite. This latter feature is the most characteristic: the definite conjugation is used with a transitive verb whose (direct) object is definite (\"Péter eszi az almát.\" \"Peter eats the apple.\") and the indefinite conjugation either for a verb with an indefinite direct object (\"Péter eszik egy almát.\" \"Peter eats an apple.\") or for a verb without an object. (\"Péter eszik.\" \"Peter eats.\")\nSince conjugation expresses the person and number, personal pronouns are usually omitted, unless they are emphasized.\n\nThe Present tense is unmarked, while the past is formed using the suffix –t or –tt: \"lát\" 'sees'; \"látott\" 'saw', past. Future may be expressed either with the present tense (usually with a word defining the time of the event, such as \"holnap\" 'tomorrow'), or using the auxiliary verb \"fog\" (similar to the English ‘will’) together with the verb’s infinitive.\n\nThe indicative mood and the conditional mood are used both in the present and the past tenses. Conditional past is expressed using the conjugated past form and the auxiliary word \"volna \"(\"látott volna\" 'would have seen')\".\" The imperative mood is used only with the present tense.\n\nVerbs have verbal prefixes, also known as coverbs. Most of them define direction of movement (as \"lemegy\" \"goes down\", \"felmegy\" \"goes up\"). Some verbal prefixes give an aspect to the verb, such as the prefix meg-, which generally marks telicity.\n\nThe neutral word order is subject–verb–object (SVO). However, Hungarian is a topic-prominent language, which means that word order does not only depend on syntax, but also on the topic-comment structure of the sentence (e.g. what aspect is assumed to be known and what is emphasized).\n\nA Hungarian sentence generally has the following order: topic, comment (or focus), verb, other parts.\n\nPutting something into the topic means that the proposition is only stated for that particular thing or aspect, and implies that the proposition is not true for some others. For example, in the sentence \"\"Az almát János látja.\" \"('John sees the apple', more exactly, 'It is John who sees the apple.', literally \"The apple John sees.\"), the apple is in the topic, implying that other objects may not be seen by him, but by other people (the pear may be seen by Peter). The topic part may be empty.\n\nPutting something in the focus means that it is the new information for the listener that they may have not known or where their knowledge must be corrected. For example, in the sentence \"\"Én vagyok az apád.\" \"('I am your father', more exactly, 'It is I who am your father.') from the movie , the pronoun I (\"én\") is in the focus, implying that this is new information, and the listener thought that another person is his father.\n\nNote that sometimes this is described as Hungarian having free word order, even though different word orders are generally not interchangeable and the neutral order is not always correct to use. Besides word order, intonation is also different with different topic-comment structures. The topic usually has a rising intonation and the focus has a falling intonation. In the following examples the topic is marked with italics, and the focus (comment) with boldface.\n\nHungarian has a four-tiered system for expressing levels of politeness.\n\n\nThe four-tiered system has somewhat been eroded due to the recent expansion of \"\"tegeződés\"\".\n\nSome anomalies emerged with the arrival of multinational companies who have addressed their customers in the \"te\" (least polite) form right from the beginning of their presence in Hungary. A typical example is the Swedish furniture shop IKEA, whose web site and other publications address the customers in \"te\" form. When a news site asked IKEA—using the \"te\" form—why they address their customers this way, IKEA's PR Manager explained in his answer—using the \"ön\" form—that their way of communication reflects IKEA's open-mindedness and the Swedish culture. However IKEA in France use the most polite (\"vous\") form. Another example is the communication of Telenor (a mobile network operator) towards its customers. Telenor chose to communicate towards business customers in the polite \"ön\" form while all other customers are addressed in the less polite \"te\" form.\n\nGiving an accurate estimate for the total word count is difficult, since it is hard to define what to call \"a word\" in agglutinating languages, due to the existence of affixed words and compound words. To have a meaningful definition of compound words, we have to exclude such compounds whose meaning is the mere sum of its elements. The largest dictionaries from Hungarian to another language contain 120,000 words and phrases (but this may include redundant phrases as well, because of translation issues). The new desk lexicon of the Hungarian language contains 75,000 words and the Comprehensive Dictionary of Hungarian Language (to be published in 18 volumes in the next twenty years) will contain 110,000 words. The default Hungarian lexicon is usually estimated to comprise 60,000 to 100,000 words. (Independently of specific languages, speakers actively use at most 10,000 to 20,000 words, with an average intellectual using 25–30 thousand words.) However, all the Hungarian lexemes collected from technical texts, dialects etc. would all together add up to 1,000,000 words.\n\nParts of the Lexicon can be organized using word-bushes. (See an example on the right.) The words in these bushes share a common root, are related through inflection, derivation and compounding, and are usually broadly related in meaning.\n\nThe basic vocabulary shares a couple of hundred word roots with other Uralic languages like Finnish, Estonian, Mansi and Khanty. Examples of such include the verb \"él\" 'live' (Finnish \"elää\"), the numbers \"kettő\" 'two', \"három\" 'three', \"négy\" 'four' (cf. Mansi китыг \"kitig\", хурум \"khurum\", нила \"nila\",\nFinnish \"kaksi, kolme, neljä\", Estonian \"kaks, kolm, neli\", ), as well as \"víz\" 'water', \"kéz\" 'hand', \"vér\" 'blood', \"fej\" 'head' (cf. Finnish and Estonian \"vesi, käsi, veri\", Finnish \"pää\", Estonian \"pea\" or 'pää\").\n\nWords for elementary kinship and nature are more Ugric, less r-Turkic and less Slavic. Agricultural words are about 50% r-Turkic and 50% Slavic; pastoral terms are more r-Turkic, less Ugric and less Slavic. Finally, Christian and state terminology is more Slavic and less r-Turkic. The Slavic is most probably proto-Slovakian and/or -Slovenian. This is easily understood in the Uralic paradigm, proto-Magyars were first similar to Ob-Ugors who were mainly hunters, fishers & gatherers, but with some horses, too. Then they accultured to Bulgarian r-Turks, so the older layer of agriculture words (wine, beer, wheat, barley &c.) are purely r-Turkic, and also lots of termini of statemanship & religion were, too.\n\nExcept for a few Latin and Greek loan-words, these differences are unnoticed even by native speakers; the words have been entirely adopted into the Hungarian lexicon. There are an increasing number of English loan-words, especially in technical fields.\n\nAnother source differs in that loanwords in Hungarian are held to constitute about 45% of bases in the language. Although the lexical percentage of native words in Hungarian is 55%, their use accounts for 88.4% of all words used (the percentage of loanwords used being just 11.6%). Therefore, the history of Hungarian has come, especially since the 19th century, to favor neologisms from original bases, whilst still having developed as many terms from neighboring languages in the lexicon.\n\nWords can be compounds or derived. Most derivation is with suffixes, but there is a small set of derivational prefixes as well.\n\nCompounds have been present in the language since the Proto-Uralic era. Numerous ancient compounds transformed to base words during the centuries. Today, compounds play an important role in vocabulary.\n\nA good example is the word \"arc\":\n\nCompounds are made up of two base words: the first is the prefix, the latter is the suffix. A compound can be \"subordinative\": the prefix is in logical connection with the suffix. If the prefix is the subject of the suffix, the compound is generally classified as a subjective one. There are objective, determinative, and adjunctive compounds as well. Some examples are given below:\n\nAccording to current orthographic rules, a subordinative compound word has to be written as a single word, without spaces; however, if the length of a compound of three or more words (not counting one-syllable verbal prefixes) is seven or more syllables long (not counting case suffixes), a hyphen must be inserted at the appropriate boundary to ease the determination of word boundaries for the reader.\n\nOther compound words are \"coordinatives\": there is no concrete relation between the prefix and the suffix. Subcategories include word duplications (to emphasise the meaning; \"olykor-olykor\"\n'really occasionally'), twin words (where a base word and a distorted form of it makes up a compound: \"gizgaz\", where the suffix 'gaz' means 'weed' and the prefix \"giz\" is the distorted form; the compound itself means 'inconsiderable weed'), and such compounds which have meanings, but neither their prefixes, nor their suffixes make sense (for example, \"hercehurca\" 'complex, obsolete procedures').\n\nA compound also can be made up by multiple (i.e., more than two) base words: in this case, at least one word element, or even both the prefix and the suffix is a compound. Some examples:\n\nHungarian words for the points of the compass are directly derived from the position of the Sun during the day in the Northern hemisphere.\n\n\nThere are two basic words for \"red\" in Hungarian: \"piros\" and \"vörös\" (variant: \"veres\"; compare with Estonian \"verev\" or Finnish \"punainen\"). (They are basic in the sense that one is not a sub-type of the other, as the English \"scarlet\" is of \"red\".) The word \"vörös\" is related to \"vér\", meaning \"blood\" (Finnish and Estonian \"veri\"). When they refer to an actual difference in colour (as on a colour chart), \"vörös\" usually refers to the deeper (darker and/or more red and less orange) hue of red. In English similar differences exist between \"scarlet\" and \"red\". While many languages have multiple names for this colour, often Hungarian scholars assume this is unique in recognizing two shades of red as separate and distinct \"folk colours\".\n\nHowever, the two words are also used independently of the above in collocations. \"Piros\" is learned by children first, as it is generally used to describe inanimate, artificial things, or things seen as cheerful or neutral, while \"vörös\" typically refers to animate or natural things (biological, geological, physical and astronomical objects), as well as serious or emotionally charged subjects.\n\nWhen the rules outlined above are in contradiction, typical collocations usually prevail. In some cases where a typical collocation does not exist, the use of either of the two words may be equally adequate.\n\nExamples:\n\nThe Hungarian words for brothers and sisters are differentiated based upon relative age. There is also a general word for sibling, testvér, from \"test\" = body and \"vér\" = blood—i.e. originating from the same body and blood.\n\nIn addition, there are separate prefixes for several ancestors and descendants:\n\nThe words for \"boy\" and \"girl\" are applied with possessive suffixes. Nevertheless, the terms are differentiated with different declension or lexemes:\n\n\"Fia\" is only used in this, irregular possessive form; it has no nominative on its own (see inalienable possession). However, the word \"fiú\" can also take the regular suffix, in which case the resulting word \"(fiúja)\" will refer to a lover or partner (boyfriend), rather than a male offspring.\n\nThe word \"fiú\" (boy) is also often noted as an extreme example of the ability of the language to add suffixes to a word, by forming \"fiaiéi\", adding vowel-form suffixes only, where the result is quite a frequently used word:\n\n\nThe above word is often considered to be the longest word in Hungarian, although there are longer words like:\n\n\nWords of such length are not used in practice, but when spoken they are easily understood by natives. They were invented to show, in a somewhat facetious way, the ability of the language to form long words (see agglutinative language). They are not compound words—they are formed by adding a series of one and two-syllable suffixes (and a few prefixes) to a simple root (\"szent\", saint or holy).\nThere is virtually no limit for the length of words, but when too many suffixes are added, the meaning of the word becomes less clear, and the word becomes hard to understand, and will work like a riddle even for native speakers.\n\nThe English word best known as being of Hungarian origin is probably \"paprika\", from Serbo-Croatian \"papar\" \"pepper\" and the Hungarian diminutive \"-ka\". The most common however is \"coach\", from \"kocsi\", originally \"kocsi szekér\" \"car from/in the style of Kocs\". Others are:\n\n\nThe Hungarian language was originally written in right-to-left Old Hungarian runes, superficially similar in appearance to the better-known futhark runes but unrelated. When Stephen I of Hungary established the Kingdom of Hungary in the year 1000, the old system was gradually discarded in favour of the Latin alphabet and left-to-right order. Although now not used at all in everyday life, the old script is still known and practiced by some enthusiasts.\n\nModern Hungarian is written using an expanded Latin alphabet, and has a phonemic orthography, i.e. pronunciation can generally be predicted from the written language. In addition to the standard letters of the Latin alphabet, Hungarian uses several modified Latin characters to represent the additional vowel sounds of the language. These include letters with acute accents \"(á,é,í,ó,ú)\" to represent long vowels, and umlauts (\"ö\" and \"ü\") and their long counterparts \"ő\" and \"ű\" to represent front vowels. Sometimes (usually as a result of a technical glitch on a computer) or is used for , and for . This is often due to the limitations of the Latin-1 / ISO-8859-1 code page. These letters are not part of the Hungarian language, and are considered misprints. Hungarian can be properly represented with the Latin-2 / ISO-8859-2 code page, but this code page is not always available. (Hungarian is the only language using both and .) Unicode includes them, and so they can be used on the Internet.\n\nAdditionally, the letter pairs , , and represent the palatal consonants , , and (a little like the \"d+y\" sounds in British \"\"du\"ke\" or American \"woul\"d y\"ou\") – a bit like saying \"d\" with the tongue pointing to the palate.\n\nHungarian uses for and for , which is the reverse of Polish usage. The letter is and is . These digraphs are considered single letters in the alphabet. The letter is also a \"single letter digraph\", but is pronounced like (English ), and appears mostly in old words. The letters and are exotic remnants and are hard to find even in longer texts. Some examples still in common use are \"madzag\" (\"string\"), \"edzeni\" (\"to train (athletically)\") and \"dzsungel\" (\"jungle\").\n\nSometimes additional information is required for partitioning words with digraphs: házszám (\"street number\") = \"ház\" (\"house\") + \"szám\" (\"number\"), not an unintelligible \"házs\" + \"zám\".\n\nHungarian distinguishes between long and short vowels, with long vowels written with acutes. It also distinguishes between long and short consonants, with long consonants being doubled. For example, \"lenni\" (\"to be\"), \"hozzászólás\" (\"comment\"). The digraphs, when doubled, become trigraphs: + = , e.g. \"művésszel\" (\"with an artist\"). But when the digraph occurs at the end of a line, all of the letters are written out. For example, (\"with a bus\"):\n\nWhen the first lexeme of a compound ends in a digraph and the second lexeme starts with the same digraph, both digraphs are written out: \"jegy\" + \"gyűrű\" = \"jegygyűrű\" (\"engagement/wedding ring\", \"jegy\" means \"sign\", \"mark\". The term \"jegyben lenni/járni\" means \"to be engaged\"; \"gyűrű\" means \"ring\").\n\nUsually a trigraph is a double digraph, but there are a few exceptions: \"tizennyolc\" (\"eighteen\") is a concatenation of \"tizen\" + \"nyolc\". There are doubling minimal pairs: \"tol\" (\"push\") vs. \"toll\" (\"feather\" or \"pen\").\n\nWhile to English speakers they may seem unusual at first, once the new orthography and pronunciation are learned, written Hungarian is almost completely phonemic (except for etymological spellings and \"ly, j\" representing ).\n\nThe word order is basically from general to specific. This is a typical analytical approach and is used generally in Hungarian.\n\nThe Hungarian language uses the so-called eastern name order, in which the surname (general, deriving from the family) comes first and the given name comes last. If a second given name is used, this follows the first given name.\n\nFor clarity, in foreign languages Hungarian names are usually represented in the western name order. Sometimes, however, especially in the neighbouring countries of Hungary – where there is a significant Hungarian population – the Hungarian name order is retained, as it causes less confusion there.\n\nFor an example of foreign use, the birth name of the Hungarian-born physicist, the \"father of the hydrogen bomb\" was Teller Ede, but he immigrated to the USA in the 1930s and thus became known as Edward Teller. Prior to the mid-20th century, given names were usually translated along with the name order; this is no longer as common. For example, the pianist uses \"András Schiff\" when abroad, not \"Andrew Schiff\" (in Hungarian \"Schiff András\"). If a second given name is present, it becomes a middle name and is usually written out in full, rather than truncated to an initial.\n\nIn modern usage, foreign names retain their order when used in Hungarian. Therefore:\n\n\nBefore the 20th century, not only was it common to reverse the order of foreign personalities, they were also \"Hungarianised\": \"Goethe János Farkas\" (originally Johann Wolfgang Goethe). This usage sounds odd today, when only a few well-known personalities are referred to using their Hungarianised names, including \"Verne Gyula\" (Jules Verne), \"Marx Károly\" (Karl Marx), \"Kolumbusz Kristóf\" (Christopher Columbus, note that it is also translated in English).\n\nSome native speakers disapprove of this usage; the names of certain historical religious personalities (including popes), however, are always Hungarianised by practically all speakers, such as \"Luther Márton\" (Martin Luther), \"Husz János\" (Jan Hus), \"Kálvin János\" (John Calvin); just like the names of monarchs, for example the king of Spain, Juan Carlos I is referred to as \"I. János Károly\" or the queen of the UK, Elizabeth II is referred to as \"II. Erzsébet\".\n\nJapanese names, which are usually written in western order in the rest of Europe, retain their original order in Hungarian, e. g. \"Kuroszava Akira\" instead of Akira Kurosawa.\n\nThe Hungarian convention for date and time is to go from the generic to the specific: 1. year, 2. month, 3. day, 4. hour, 5. minute, (6. second)\n\nThe year and day are always written in Arabic numerals, followed by a full stop. The month can be written by its full name or can be abbreviated, or even denoted by Roman or Arabic numerals. Except for the first case (month written by its full name), the month is followed by a full stop. Usually, when the month is written in letters, there is no leading zero before the day. On the other hand, when the month is written in Arabic numerals, a leading zero is common, but not obligatory. Except at the beginning of a sentence, the name of the month always begins with a lower-case letter.\n\nHours, minutes, and seconds are separated by a colon (H:m:s). Fractions of a second are separated by a full stop from the rest of the time. Hungary generally uses the 24-hour clock format, but in verbal (and written) communication 12-hour clock format can also be used. See below for usage examples.\n\nDate and time may be separated by a comma or simply written one after the other.\n\n\nDate separated by hyphen is also spreading, especially on datestamps. Here – just like the version separated by full stops – leading zeros are in use.\n\n\nWhen only hours and minutes are written in a sentence (so not only \"displaying\" time), these parts can be separated by a full stop (e.g. \"Találkozzunk 10.35-kor.\" – \"Let's meet at 10.35.\"), or it is also regular to write hours in normal size, and minutes put in superscript (and not necessarily) underlined (e.g. \"A találkozó 10-kor kezdődik.\" \"or\" \"A találkozó 10-kor kezdődik.\" – \"The meeting begins at 10.35.\").\n\nAlso, in verbal and written communication it is common to use \"délelőtt\" (literally \"before noon\") and \"délután\" (lit. \"after noon\") abbreviated as \"de.\" and \"du.\" respectively. Délelőtt and délután is said or written before the time, e.g. \"Délután 4 óra van.\" – \"It's 4 p.m.\". However e.g. \"délelőtt 5 óra\" (should mean \"5 a.m.\") or \"délután 10 óra\" (should mean \"10 p.m.\") are never used, because at these times the sun is not up, instead \"hajnal\" (\"dawn\"), \"reggel\" (\"morning\"), \"este\" (\"evening\") and \"éjjel\" (\"night\") is used, however there are no exact rules for the use of these, as everybody uses them according to their habits (e.g. somebody may have woken up at 5 a.m. so he/she says \"Reggel 6-kor ettem.\" – \"I had food at \"*morning\" 6.\", and somebody woke up at 11 a.m. so he/she says \"Hajnali 6-kor még aludtam.\" – \"I was still sleeping at \"*dawn\" 6.\"). Roughly, these expressions mean these times:\n\n\nAlthough address formatting is increasingly being influenced by standard European conventions, the traditional Hungarian style is:\n\nBudapest, Deák Ferenc tér 1. 1052\n\nSo the order is: 1) settlement (most general), 2) street/square/etc. (more specific), 3) house number (most specific) 4)(HU-)postcode. The house number may be followed by the storey and door numbers. The HU- part before the postcode is only for incoming postal traffic from foreign countries. Addresses on envelopes and postal parcels should be formatted and placed on the right side as follows:\n\nName of the recipient\nSettlement\nStreet address (up to door number if necessary)\n(HU-)postcode\n\n\"Note: The stress is always placed on the first syllable of each word. The remaining syllables all receive an equal, lesser stress. All syllables are pronounced clearly and evenly, even at the end of a sentence, unlike in English.\"\n\n\nToday the scientific consensus among linguists is that Hungarian is part of the Uralic family of languages. For many years (from 1869), it was a matter of dispute whether Hungarian was a Finno-Ugric/Uralic language, or was more closely related to the Turkic languages, a controversy known as the \"Ugric–Turkish war\", or whether indeed both the Uralic and the Turkic families formed part of a superfamily of \"Ural–Altaic languages\". Hungarians did absorb some Turkic influences during several centuries of cohabitation. For example, it appears that the Hungarians learned animal breeding techniques from the Turkic Chuvash, as a high proportion of words specific to agriculture and livestock are of Chuvash origin. There was also a strong Chuvash influence in burial customs. Furthermore, all Ugric languages, not just Hungarian, have Turkic loanwords related to horse riding.\n\nThere have been attempts, dismissed by mainstream linguists as pseudoscientific comparisons, to show that Hungarian is related to other languages including Hebrew, Hunnic, Sumerian, Egyptian, Etruscan, Basque, Persian, Pelasgian, Greek, Chinese, Sanskrit, English, Tibetan, Magar, Quechua, Armenian, Japanese and at least 40 other languages.\n\nWiktionary: Swadesh lists for Uralic languages\n\n\n\n\n\n\n\n\n\n", "id": "13972", "title": "Hungarian language"}
{"url": "https://en.wikipedia.org/wiki?curid=13974", "text": "Hymenoptera\n\nFemales typically have a special ovipositor for inserting eggs into hosts or otherwise inaccessible places. The ovipositor is often modified into a stinger. The young develop through holometabolism (complete metamorphosis)—that is, they have a worm-like larval stage and an inactive pupal stage before they mature.\n\nThe name Hymenoptera refers to the wings of the insects, but the original derivation is ambiguous. All references agree that the derivation involves the Ancient Greek πτερόν (\"pteron\") for wing. The Ancient Greek ὑμήν (\"hymen\") for membrane provides a plausible etymology for the term because this order like several others has membranous wings. However, a key characteristic of this order is that the hind wings are connected to the fore wings by a series of hooks. Thus, another plausible etymology involves Hymen, the Ancient Greek god of marriage, as these insects have \"married wings\" in flight.\n\nHymenoptera originated in the Triassic, with the oldest fossils belonging to the family Xyelidae. Social hymenopterans appeared during the Cretaceous. The evolution of this group has been intensively studied by Alex Rasnitsyn, Michael S. Engel, G. Dlussky, and others.\n\nThis clade has been studied by examining the mitochondrial DNA. Although this study was unable to resolve all the ambiguities in this clade, some relationships could be established. The \"Aculeata\", \"Ichneumonomorpha\", and \"Proctotrupomorpha\" were monophyletic. The \"Megalyroidea\" and \"Trigonalyoidea\" are sister clades as are the \"Chalcidoidea\"+\"Diaprioidea\". The \"Cynipoidea\" was generally recovered as the sister group to \"Chalcidoidea\" and \"Diaprioidea\" which are each other's closest relations.\n\nThe cladogram is based on Schulmeister 2003.\n\nHymenopterans range in size from very small to large insects, and usually have two pairs of wings. Their mouthparts are adapted for chewing, with well-developed mandibles (ectognathous mouthparts). Many species have further developed the mouthparts into a lengthy proboscis, with which they can drink liquids, such as nectar. They have large compound eyes, and typically three simple eyes, (ocelli).\n\nThe forward margin of the hind wing bears a number of hooked bristles, or \"hamuli\", which lock onto the fore wing, keeping them held together. The smaller species may have only two or three hamuli on each side, but the largest wasps may have a considerable number, keeping the wings gripped together especially tightly. Hymenopteran wings have relatively few veins compared with many other insects, especially in the smaller species.\n\nIn the more ancestral hymenopterans, the ovipositor is blade-like, and has evolved for slicing plant tissues. In the majority, however, it is modified for piercing, and, in some cases, is several times the length of the body. In some species, the ovipositor has become modified as a stinger, and the eggs are laid from the base of the structure, rather than from the tip, which is used only to inject venom. The sting is typically used to immobilise prey, but in some wasps and bees may be used in defense.\n\nThe larvae of the more ancestral hymenopterans resemble caterpillars in appearance, and like them, typically feed on leaves. They have large chewing mandibles, three pairs of thoracic limbs, and, in most cases, a number of abdominal prolegs. Unlike caterpillars, however, the prolegs have no grasping spines, and the antennae are reduced to mere stubs.\n\nThe larvae of other hymenopterans, however, more closely resemble maggots, and are adapted to life in a protected environment. This may be the body of a host organism, or a cell in a nest, where the adults will care for the larva. Such larvae have soft bodies with no limbs. They are also unable to defecate until they reach adulthood due to having an incomplete digestive tract, presumably to avoid contaminating their environment.\n\nAmong most or all hymenopterans, sex is determined by the number of chromosomes an individual possesses. Fertilized eggs get two sets of chromosomes (one from each parent's respective gametes), so develop into diploid females, while unfertilized eggs only contain one set (from the mother), so develop into haploid males; the act of fertilization is under the voluntary control of the egg-laying female. This phenomenon is called haplodiploidy.\n\nHowever, the actual genetic mechanisms of haplodiploid sex determination may be more complex than simple chromosome number. In many Hymenoptera, sex is actually determined by a single gene locus with many alleles. In these species, haploids are male and diploids heterozygous at the sex locus are female, but occasionally a diploid will be homozygous at the sex locus and develop as a male, instead. This is especially likely to occur in an individual whose parents were siblings or other close relatives. Diploid males are known to be produced by inbreeding in many ant, bee, and wasp species. Diploid biparental males are usually sterile but a few species that have fertile diploid males are known.\n\nOne consequence of haplodiploidy is that females on average actually have more genes in common with their sisters than they do with their own daughters. Because of this, cooperation among kindred females may be unusually advantageous, and has been hypothesized to contribute to the multiple origins of eusociality within this order. In many colonies of bees, ants, and wasps, worker females will remove eggs laid by other workers due to increased relatedness to direct siblings, a phenomenon known as worker policing.\n\nSome hymenopterans take advantage of parthenogenesis, the creation of embryos without fertilization. Thelytoky is a particular form of parthenogenesis in which female embryos are created (without fertilisation). The form of thelytoky in hymenopterans is a kind of automixis in which two haploid products (proto-eggs) from the same meiosis fuse to form a diploid zygote. This process tends to maintain heterozygosity in the passage of the genome from mother to daughter. It is found in several ant species including the desert ant \"Cataglyphis cursor\", the clonal raider ant \"Cerapachys biroi\", the predaceous ant \"Platythyrea punctata\", and the electric ant (little fire ant) \"Wasmannia auropunctata\". It also occurs in the Cape honey bee \"Apis mellifera capensis\".\n\nOocytes that undergo automixis with central fusion often have a reduced rate of crossover recombination, which helps to maintain heterozygosity and avoid inbreeding depression. Species that display central fusion with reduced recombination include the ants \"Platythyrea punctata\" and \"Wasmannia auropunctata\" and the honey bee \"Apis mellifera capensis\". In \"A. m. capensis\", the recombination rate during meiosis is reduced more than 10-fold. In \"W. auropunctata\" the reduction is 45-fold.\n\nSingle queen colonies of the narrow headed ant \"Formica exsecta\" illustrate the possible deleterious effects of increased homozygosity. In this ant, colonies with more homozygous queens age more rapidly. The result is reduced colony survival.\n\nDifferent species of Hymenoptera show a wide range of feeding habits. The most primitive forms are typically herbivorous, feeding on leaves or pine needles. Stinging wasps are predators, and will provision their larvae with immobilised prey, while bees feed on nectar and pollen.\n\nA number of species are parasitoid as larvae. The adults inject the eggs into a paralysed host, which they begin to consume after hatching. Some species are even hyperparasitoid, with the host itself being another parasitoid insect. Habits intermediate between those of the herbivorous and parasitoid forms are shown in some hymenopterans, which inhabit the galls or nests of other insects, stealing their food, and eventually killing and eating the occupant.\n\nThe Hymenoptera are divided into two groups, the Symphyta which have no waist, and the Apocrita which have a narrow waist.\n\nThe suborder Symphyta includes the sawflies, horntails, and parasitic wood wasps. The group may be paraphyletic, as it has been suggested that the family Orussidae may be the group from which the Apocrita arose. They have an unconstricted junction between the thorax and abdomen. The larvae are herbivorous, free-living eruciforms, with three pairs of true legs, prolegs (on every segment, unlike Lepidoptera) and ocelli. The prolegs do not have crochet hooks at the ends unlike the larvae of the Lepidoptera.\n\nThe wasps, bees, and ants together make up the suborder Apocrita, characterized by a constriction between the first and second abdominal segments called a wasp-waist (petiole), also involving the fusion of the first abdominal segment to the thorax. Also, the larvae of all Apocrita lack legs, prolegs, or ocelli. The hindgut of the larvae also remains closed during development, with feces being stored inside the body, with the exception of some bee larvae where the larval anus has reappeared through developmental reversion. In general, the anus only opens at the completion of larval growth.\n\n\n\n\n\n\n", "id": "13974", "title": "Hymenoptera"}
{"url": "https://en.wikipedia.org/wiki?curid=13976", "text": "Hannibal Hamlin\n\nHannibal Hamlin (August 27, 1809July 4, 1891) was an American attorney and politician from the state of Maine. In a public service career that spanned over 50 years, he is most notable for having served as the 15th Vice President of the United States. The first Republican to hold the office, Hamlin served from 1861 to 1865, during the first term of President Abraham Lincoln.\n\nA native of Paris, Maine, Hamlin was a descendant of an English family that had originally settled in New England in the 1600s. After his early education was complete, Hamlin managed his father's farm before becoming a newspaper editor. He studied law, was admitted to the bar in 1833, and began to practice in Hampden, Maine.\n\nOriginally a Democrat, Hamlin began his political career with election to the Maine House of Representatives in 1835 and an appointment to the military staff of the Governor of Maine. As an officer in the militia, he took part in the 1839 negotiations that helped end the Aroostook War. In the 1840s Hamlin was elected and served in the United States House of Representatives. In 1848 the state house elected him to the United States Senate, where he served until January 1857. He served temporarily as governor for six weeks in the beginning of 1857, after which he returned to the Senate.\n\nHamlin was an active opponent of slavery; he supported the Wilmot Proviso and opposed the Compromise Measures of 1850. In 1854, he strongly opposed passage of the Kansas–Nebraska Act. Hamlin's increasingly anti-slavery views caused him to leave the Democratic Party for the newly formed Republican Party in 1856.\n\nIn 1860, Hamlin was the Republican nominee for Vice President; selected to run with Abraham Lincoln, who was from Illinois, Hamlin was chosen in part to bring geographic balance to the ticket and in part because as a former Democrat, he could work to convince other anti-slavery Democrats that their future lay with the Republican Party. The Lincoln and Hamlin ticket was successful, and Hamlin served as Vice President from 1861 to 1865, which included the majority of the American Civil War. The first Republican Vice President, Hamlin held the office in an era when the incumbent was considered more a part of the legislative branch than the executive; he was not personally close to Lincoln and did not play a major role in his administration. Even so, Hamlin supported the administration's legislative program in his role as presiding officer of the Senate, and he looked for other ways to demonstrate his support for the Union, including a term of service in a Maine militia unit during the war.\n\nFor the 1864 election, Hamlin was replaced as Vice Presidential nominee by Andrew Johnson, a southern Democrat chosen for his appeal to pro-Union southerners. The Republicans considered their support to be necessary during the reconciliation and rebuilding anticipated once the Civil War was over. After leaving the vice presidency, Hamlin served as Collector of the Port of Boston, a lucrative post to which he was appointed by Johnson after the latter succeeded to the presidency following Lincoln's assassination. Hamlin resigned as Collector because of disagreement with Johnson over Reconstruction of the former Confederacy.\n\nIn 1869, Hamlin was elected again to the U.S. Senate, and he served two terms. After leaving the Senate in 1881, he served briefly as United States Ambassador to Spain before returning to Maine in late 1882. In retirement, Hamlin was a resident of Bangor, Maine, where he died in 1891. He was buried at Mount Hope Cemetery in Bangor.\n\nHamlin was born to Cyrus Hamlin and his wife Anna, née Livermore, in Paris, Maine. He was a descendant in the sixth generation of English colonist James Hamlin, who had settled in the Massachusetts Bay Colony in 1639. He was a grandnephew of U.S. Senator Samuel Livermore II of New Hampshire. \n\nHamlin attended the district schools and Hebron Academy and later managed his father's farm. From 1827 to 1830 he published the \"Oxford Jeffersonian\" newspaper in partnership with Horatio King.\n\nHe studied law with the firm headed by Samuel Fessenden, was admitted to the bar in 1833, and began practicing in Hampden, Maine, where he lived until 1848.\n\nHamlin married Sarah Jane Emery of Paris Hill in 1833. Her father was Stephen Emery, who was appointed as Maine's Attorney General in 1839–1840. Hamlin and Sarah had four children together: George, Charles, Cyrus and Sarah.\n\nSarah died in 1855. The next year, Hamlin married her half-sister, Ellen Vesta Emery in 1856. They had two children together: Hannibal E. and Frank. Ellen Hamlin died in 1925.\n\nHamlin's political career began in 1835, when he was elected to the Maine House of Representatives. Appointed a Major on the staff of Governor John Fairfield, he served with the militia in the bloodless Aroostook War of 1839. He facilitated negotiations between Fairfield and Lieutenant Governor John Harvey of New Brunswick, which helped reduce tensions and make possible the Webster-Ashburton Treaty, which ended the war.\n\nHamlin unsuccessfully ran for the United States House of Representatives in 1840 and left the State House in 1841. He later was elected to two terms in the United States House of Representatives, serving from 1843 to 1847. He was elected by the state legislature to fill a U.S. Senate vacancy in 1848, and to a full term in 1851. A Democrat at the beginning of his career, Hamlin supported the candidacy of Franklin Pierce in 1852.\n\nFrom the very beginning of his service in Congress, Hamlin was prominent as an opponent of the extension of slavery. He was a conspicuous supporter of the Wilmot Proviso and spoke against the Compromise Measures of 1850. In 1854, Hamlin strongly opposed the passage of the Kansas–Nebraska Act, which repealed the Missouri Compromise. After the Democratic Party endorsed that repeal at the 1856 Democratic National Convention, on June 12, 1856, he withdrew from the Democratic Party and joined the newly organized Republican Party, causing a national sensation.\n\nThe Republicans nominated Hamlin for Governor of Maine in the same year. He carried the election by a large majority and was inaugurated on January 8, 1857. In the latter part of February 1857, however, he resigned the governorship. He returned to the United States Senate, serving from 1857 to January 1861.\n\nHamlin was nominated by the Republican Party to serve as Vice President of the United States in the 1860 presidential election on a ticket with former Representative Abraham Lincoln. Given that Lincoln was from Illinois, a vice presidential nominee from Maine made sense in terms of regional balance. As a former Democrat, Hamlin could also be expected to try to persuade other anti-slavery Democrats that joining the Republican Party was the only way to ensure slavery's demise.\n\nHamlin and Lincoln were not close personally, but had a good working relationship. At the time, the Vice President was considered part of the legislative branch in his role as President of the Senate, and so did not attend cabinet meetings; thus, Hamlin did not regularly visit the White House. It was said that Mary Todd Lincoln and Hamlin disliked each other. For his part, Hamlin complained, \"I am only a fifth wheel of a coach and can do little for my friends.\"\n\nHe had little influence in the Lincoln Administration, although he urged both the Emancipation Proclamation and the arming of Black Americans. He strongly supported Joseph Hooker's appointment as commander of the Army of the Potomac, which ended in failure at the Battle of Chancellorsville.\n\nBeginning in 1860, Hamlin was a member of Company A of the Maine Coast Guard, a militia unit. When the company was called up in the summer of 1864, Hamlin was told that because of his position as Vice President, he did not have to take part in the muster. He opted to serve, arguing that he could set an example by doing the duty expected of any citizen, and the only concession made because of his office was that he was quartered with the officers. He reported to Fort McClary in July, initially taking part in routine assignments including guard duty, and later taking over as the company cook. He was promoted to corporal during his service, and mustered out with the rest of his unit in mid-September.\n\nIn June 1864, the Republicans and War Democrats joined to form the National Union Party. Although Lincoln was renominated, War Democrat Andrew Johnson of Tennessee was named to replace Hamlin as Lincoln's running mate. Lincoln was seeking to broaden his base of support and was also looking ahead to Southern Reconstruction, at which Johnson had proven himself adept as military governor of occupied Tennessee. Hamlin, by contrast, was an ally of Northern radicals (who would later impeach Johnson). Lincoln and Johnson were elected in November 1864, and Hamlin's term expired on March 4, 1865.\n\nAfter leaving the vice presidency Hamlin served briefly as Collector of the Port of Boston. Appointed to the post by Johnson, Hamlin resigned in protest over Johnson's Reconstruction policy and accompanying efforts to build a political following loyal to him after he had been repudiated by the Republicans. Republicans had supported Johnson as part of the National Union ticket during the war, but opposed him after he became President and his position on Reconstruction deviated from theirs.\n\nAlthough Hamlin narrowly missed becoming President, his vice presidency would usher in a half-century of sustained national influence for the Maine Republican Party. In the period 1861–1911, Maine Republicans occupied the offices of Vice President, Secretary of the Treasury (twice), Secretary of State, President pro tempore of the United States Senate, Speaker of the United States House of Representatives (twice), and would field a presidential nominee in James G. Blaine, a level of influence in national politics unmatched by subsequent Maine political delegations.\n\nNot content with private life, Hamlin returned to the U.S. Senate in 1869 to serve two more 6-year terms before declining to run for re-election in 1880 because of an ailing heart. His last duty as a public servant came in 1881 when Secretary of State James G. Blaine convinced President James A. Garfield to name Hamlin as United States Ambassador to Spain. Hamlin received the appointment on June 30, 1881, and held the post until October 17, 1882.\n\nUpon returning from Spain, Hamlin retired from public life to his home in Bangor, Maine, which he had purchased in 1851. The Hannibal Hamlin House – as it is known today – is located in central Bangor at 15 5th Street; incorporating Victorian, Italianate, and Mansard-style architecture, the mansion was posted to the National Register of Historic Places in 1979.\n\nHamlin was elected as a Third Class Companion of the Military Order of the Loyal Legion of the United States. Third Class was the MOLLUS division created to recognize civilians who had contributed outstanding service to the Union during the war.\n\nOn Independence Day, July 4, 1891, Hamlin collapsed and fell unconscious while playing cards at the Tarratine Club he founded in downtown Bangor. He was then placed on one of the club's couches and died a few hours later. He was 81. Hannibal Hamlin was buried in the Hamlin family plot at Mount Hope Cemetery in Bangor, Maine.\n\nHe had survived six of his successors as Vice President: Andrew Johnson, Schuyler Colfax, Henry Wilson, William A. Wheeler, Chester A. Arthur and Thomas A. Hendricks. From June 4, 1887 to March 4, 1889, he was the only living Vice President (current or former) and afterwards he continued to be the only living former Vice President until his own death in 1891.\n\nHamlin had four sons who grew to adulthood: Charles Hamlin, Cyrus Hamlin, Hannibal Emery and Frank Hamlin. Charles and Cyrus served in the Union forces during the Civil War, both becoming generals, Charles by brevet. Cyrus was among the first Union officers to argue for the enlistment of black troops, and himself commanded a brigade of freedmen in the Mississippi River campaign. Charles and sister Sarah were present at Ford's Theater the night of Lincoln's assassination. Hannibal Emery Hamlin was Maine Attorney General from 1905 to 1908. Hannibal Hamlin's great-granddaughter Sally Hamlin was a child actor who made many spoken word recordings for the Victor Talking Machine Company in the early years of the 20th century.\n\nHannibal's older brother, Elijah Livermore Hamlin, was president of the Mutual Fire Insurance Co. of Bangor, and the Bangor Institution for Savings. He was twice an unsuccessful candidate for Governor of Maine in the late 1840s, though he did serve as Mayor of Bangor in 1851–52. The brothers were members of different political parties (Hannibal a Democrat, and Elijah a Whig) before both becoming Republican in the later 1850s.\n\nHannibal's nephew (Elijah's son) Augustus Choate Hamlin was a physician, artist, mineralogist, author, and historian. He was also Mayor of Bangor in 1877–78, and a founding member of the Bangor Historical Society.\n\nAugustus served as surgeon in the 2nd Maine Volunteer Infantry Regiment during the Civil War, eventually becoming a U.S. Army Medical Inspector, and later the Surgeon General of Maine. He wrote books about Andersonville Prison and the Battle of Chancellorsville. Hannibal's grand-nephew (Elijah's grandson) Isaiah K. Stetson was Speaker of the Maine House of Representatives in 1899–1900, and owned a large company in Bangor which manufactured and shipped lumber and ice and ran a shipyard and marine railway.\n\nHannibal's first cousin Cyrus Hamlin, who was a graduate of the Bangor Theological Seminary, became a missionary in Turkey, where he founded Robert College. He later became president of Middlebury College in Vermont. His son, A. D. F. Hamlin, Hannibal's first cousin once removed, became a professor of architecture at Columbia University and a noted architectural historian. There are biographies of Hamlin by his grandson Charles E. Hamlin (published 1899, reprinted 1971) and by H. Draper Hunt (published 1969).\n\nHamlin County, South Dakota is named in his honor, as are Hamlin, Kansas; Hamlin, New York; Hamlin, West Virginia; Hamlin Township; Hamlin Lake in Mason County, Michigan; and, Hamlin, a small Maine village that is a U.S.-Canada border crossing with Grand Falls, New Brunswick. There are statues in Hamlin's likeness in the United States Capitol and in a public park (Norumbega Mall) in Bangor, Maine.\n\nThere is also a building on the University of Maine Campus, in Orono, named Hannibal Hamlin Hall. This burned down in 1945, in a fire that killed two students, but was subsequently rebuilt. Hannibal Hamlin Memorial Library is next to his birthplace in Paris, Maine.\n\nHamlin's house in Bangor subsequently housed the Presidents of the adjacent Bangor Theological Seminary. It is listed on the National Register of Historic Places, as is Hamlin's birthplace in Paris, Maine (as part of the Paris Hill Historic District).\n\nHamlin appears briefly in three alternate history writings by Harry Turtledove: \"The Guns of the South\", \"Must and Shall\", and \"How Few Remain\".\n\n\n\n \n", "id": "13976", "title": "Hannibal Hamlin"}
{"url": "https://en.wikipedia.org/wiki?curid=13978", "text": "Hopwood Award\n\nThe Hopwood Awards are a major scholarship program at the University of Michigan, founded by Avery Hopwood.\n\nUnder the terms of the will of Avery Hopwood, a prominent American dramatist and member of the Class of 1905 of The University of Michigan, one-fifth of Mr. Hopwood's estate was given to the Regents of the University for the encouragement of creative work in writing. The first awards were made in 1931, and today the Hopwood Program offers approximately $120,000 in prizes every year to aspiring writers at the University of Michigan. According to Nicholas Delbanco, UM English Professor and Director of the Hopwood Awards Program, \"This is the oldest and best known series of writing prizes in the country and it is a very good indicator of future success.\"\n\nAwards are offered in the following genres: drama/screenplay, essay, the novel, short fiction, Nonfiction, and poetry. These awards are classified under two categories, Graduate or Undergraduate, except the novel and drama/screenplay, which are combined categories. Award amounts for this contest vary, but usually fall in the range of $1000 to $6000.\n\nThis contest is open only to students who take writing courses during spring and summer terms. Awards are given in the categories of Drama or Screenplay, Nonfiction, Short Fiction, and Poetry. Novels are not eligible for the Summer Hopwood Contest.\n\nThis contest is open only to freshmen and sophomores who are enrolled in writing courses. Awards are given in the categories of Nonfiction, Fiction, and Poetry.\n\nThe Hopwood Program administers the Hopwood Award, as well as several other awards in writing. It is located in the Hopwood Room at the University of Michigan and serves the needs and interests of Hopwood contestants. The Room was established by Professor Roy W. Cowden, Director of the Hopwood Awards from 1933 to 1952, who generously contributed a part of his library, which has grown through the addition of many volumes of contemporary literature. In addition to housing the winning manuscripts from the past years of the contests, the Hopwood Room has a lending library of twentieth -century literature, a generous supply of non-circulating current periodicals, some reference \nbooks on how to get published, information on graduate and summer writing \nprograms, and a collection of screen plays donated by former Hopwood winner \nLawrence Kasdan.\n\nThe Hopwood Program also administers the following writing contests: \n\n\n\n", "id": "13978", "title": "Hopwood Award"}
{"url": "https://en.wikipedia.org/wiki?curid=13980", "text": "Homeostasis\n\nHomeostasis or homoeostasis is the property of a system in which a variable (for example, the concentration of a substance in solution, or its temperature) is actively regulated to remain very nearly constant. This regulation occurs inside a defined environment (mostly within a living organism's body). Examples of homeostasis include the regulation of the body temperature of an animal, the pH of its extracellular fluids, or the concentrations of sodium (Na) and calcium (Ca) ions or of glucose in the blood plasma, despite changes in the animal’s environment, or what it has eaten, or what it is doing (for example, resting or exercising). Each of these variables is controlled by a separate “homeostat” (or regulator), which, together, maintain life. Homeostats are energy-consuming physiological mechanisms.\n\nThe concept was described by French physiologist Claude Bernard in 1865 and the word was coined by Walter Bradford Cannon in 1926.\n\nAlthough the term was originally used to refer to processes within living organisms, it is frequently applied to technological control systems such as thermostats. A homeostat has an absolute requirement for a sensor to detect changes in the controlled entity's value, as well as an effector mechanism that reverses any detected deviation from the desired value (or “setpoint”) of the regulated entity. Since the correction of any error detected by the sensor is always in the opposite direction to the error, a homeostat relies on what is known as a negative feedback connection between the sensor and effector. The effector's corrective effects are monitored by the sensor, which turns the corrective measures off when setpoint conditions have been restored. Negative feedback systems are therefore referred to as \"closed loop\", or \"negative feedback loops\", to distinguish them from \"open loop\" systems where a stimulus (acting on a sensor) results in an, often, all-or-none response that is not subject to modification once it has been set in motion.\n\nThe metabolic processes of all living organisms can only take place in very specific physical and chemical environments. The conditions vary with each organism, and with whether the chemical processes take place inside the cell or in the fluids bathing the cells in multicellular creatures. The best known homeostats in human and other mammalian bodies are regulators that keep the composition of the extracellular fluids (or the ”internal environment”) constant, especially with regard to the temperature, pH, osmolality, and the concentrations of Na, K, Ca, glucose and CO and O. However, a great many other homeostats, encompassing many aspects of human physiology, control other entities in the body.\n\nIf an entity is homeostatically controlled it does not imply that its value is necessarily absolutely steady in health. Core body temperature is, for instance, regulated by a homeostat with temperature sensors in, amongst others, the hypothalamus of the brain. However the set point of the regulator is regularly reset. For instance, core body temperature in humans varies during the course of the day (i.e. has a circadian rhythm), with the lowest temperatures occurring at night, and the highest in the afternoons (see diagram on the right). The temperature regulator's set point is also readjusted in adult women at the start of the luteal phase of the menstrual cycle (see the diagram on the right, below). The temperature regulator's set point is also reset during infections to produce a fever.\n\nHomeostasis doesn't govern every activity in the body. For instance the signal (be it via neurons or hormones) from the sensor to the effector is, of necessity, highly variable in order to convey information about the direction and magnitude of the error detected by the sensor. Similarly the effector’s response needs to be highly adjustable to reverse the error – in fact it should be very nearly in proportion (but in the opposite direction) to the error that is threatening the internal environment. For instance, the arterial blood pressure in mammals is homeostatically controlled, and measured by sensors in the aorta and carotid arteries. The sensors send messages via sensory nerves to the medulla oblongata of the brain indicating whether the blood pressure has fallen or risen, and by how much. The medulla oblongata then distributes messages along motor or efferent nerves belonging to the autonomic nervous system to a wide variety of effector organs, whose activity is consequently changed to reverse the error in the blood pressure. One of the effector organs is the heart whose rate is stimulated to rise (tachycardia) when the arterial blood pressure falls, or to slow down (bradycardia) when the pressure rises above set point. Thus the heart rate (for which there is no sensor in the body) is not homeostatically controlled, but is one of effector responses to errors in the arterial blood pressure. Another example is the rate of sweating. This is one of the effectors in the homeostatic control of body temperature, and therefore highly variable in rough proportion to the heat load that threatens to destabilize the body’s core temperature, for which there is a sensor in the hypothalamus of the brain.\n\nApart from the entities that are homeostatically controlled in the internal environment of the body, and the mechanisms that are responsible for this regulation, there are variables that are neither homeostatically controlled nor involved in the operation of homeostats. The blood urea concentration is an example. Mammals do not have “urea sensors”. Instead the concentration of urea is determined by a dynamic equilibrium, in much the same way that the water level in a river at any particular point along its course is determined. The level of a river is simply dependent on the rate at which water flows into a particular section and how fast it flows away from there. It therefore varies with the rainfall in the catchment area and obstructions or otherwise to the flow down stream – there is no energy consuming “regulation”. The blood urea concentration is comparable to the water level in a natural river. It is manufactured by the liver from the amino groups of the amino acids of proteins that are being degraded in this organ. It is then excreted by the kidneys which simply pass most of the urea in the glomerular filtrate on into the urine without active resorption or excretion by the renal tubules (a relatively small proportion of the urea in the tubules diffuses passively back into the blood as its concentration in the tubules rises when water, without urea, is removed from the tubular fluid). A high protein diet therefore produces high blood urea concentrations, and a protein-poor diet produced low blood plasma urea concentrations, without any physiological attempt to correct or mitigate these fluctuations in the level of urea in the extracellular fluids.\n\nMammals regulate their core temperatures, using hypothalamic temperature sensors in their brains, but also elsewhere in their bodies. When core body temperature falls behavioral changes are set in motion, which, in humans, include the donning of warmer clothes, the seeking out of wind-free, warmer environments, and, eventually, the curling up in the “fetal position” to reduce the surface area (skin) exposed to the cold.\n\nThe blood flow to the limbs is reduced to a minimum via sympathetic nerves which constrict the limb arteries. The blood returns from the limb through the deep veins which surround the artery like a coarsely knitted stocking. These deep veins are called venae comitantes (see diagram on the right). This counter-current flow of blood into and out of a cold limb ensures that the arterial blood is cooled on its way into the limb, and is then re-warmed before it returns to the torso. The body heat is thus short-circuited before entering the inevitably cold limb, and relatively little heat is lost by the blood flow into that limb. The superficial subcutaneous veins of the limbs (which are very prominently visible in warm weather) are tightly constricted, as is the capillary blood flow to the skin in general, thus separating the blood as far as possible from the cold surroundings.\n\nThe metabolic rate is increased, initially by non-shivering thermogenesis, followed by shivering thermogenesis if the earlier reactions are insufficient to correct the hypothermia.\n\nWhen body temperature rises, or skin heat sensors detect a threatening rise in body temperature, behavioral changes cause the animal to seek shade, and, in humans, the sweat glands in the skin are stimulated via cholinergic sympathetic nerves to secrete a dilute watery fluid called sweat onto the skin, which, when it evaporates, cools the skin and the blood flowing through it. Panting is an alternative effector in many vertebrates, which cools the body also by the evaporation of water, but this time from the mucous membranes of the throat and mouth.\n\nAll animals regulate the glucose concentration in their extracellular fluids. In mammals the primary sensor is situated in the beta cells of the pancreatic islets. The beta cells respond to a rise in the blood sugar level by secreting insulin into the blood, and simultaneously inhibiting their neighboring alpha cells from secreting glucagon into the blood. This combination (high blood insulin levels and low glucagon levels) act on effector tissues, chief of which are the liver, fat cells and muscle cells. The liver is inhibited from producing glucose, taking it up instead, and converting it to glycogen and triglycerides. The glycogen is stored in the liver, but the triglycerides are secreted into the blood as very low-density lipoprotein (VLDL) particles which are taken up by adipose tissue, there to be stored as fats. The fat cells take up glucose through special glucose transporters (GLUT4), whose numbers in the cell wall are increased as a direct effect of insulin acting on these cells. The glucose that enters the fat cells in this manner is converted into triglycerides (via the same metabolic pathways as are used by the liver) and then stored in those fat cells together with the VLDL-derived triglycerides that were made in the liver. Muscle cells also take glucose up through insulin-sensitive GLUT4 glucose channels, and convert it into muscle glycogen.\n\nWhen the beta cells in the pancreatic islets detect lower than normal blood glucose levels, insulin secretion into the blood ceases and the alpha cells are stimulated to secrete glucagon into the blood. This inhibits the uptake of glucose from the blood by the liver, fats cells and muscle. Instead the liver is strongly stimulated to manufacture glucose from glycogen (through glycogenolysis) and from non-carbohydrate sources (such as lactate and de-aminated amino acids) using a process known as gluconeogenesis. The glucose thus produced is discharged into the blood correcting the detected error (hypoglycemia). The glycogen stored in muscles remains in the muscles, and is only broken down, during exercise, to glucose-6-phosphate and thence to pyruvate to be fed into the citric acid cycle or turned into lactate. It is only the lactate and the waste products of the citric acid cycle that are returned to the blood. The liver can take up only the lactate, and by the process of energy consuming gluconeogenesis convert it back to glucose.\n\nThe plasma ionized calcium (Ca) concentration is very tightly controlled by a pair of homeostats. The sensor for the one is situated in the parathyroid glands, where the chief cells sense the Ca level by means of specialized calcium receptors in their membranes. The sensors for the second homeostat are the parafollicular cells in the thyroid gland. The parathyroid chief cells secrete parathyroid hormone (PTH) in response to a fall in the plasma ionized calcium level; the parafollicular cells of the thyroid gland secrete calcitonin in response to a rise in the plasma ionized calcium level.\n\nThe effector organs of the first homeostat are the skeleton, the kidney, and, via a hormone released into the blood by the kidney in response to high PTH levels in the blood, the duodenum and jejunum. Parathyroid hormone (in high concentrations in the blood) causes bone resorption, releasing calcium into the plasma. This is a very rapid action which can correct a threatening hypocalcemia within minutes. High PTH concentrations cause the excretion of phosphate ions via the urine. Since phosphates combine with calcium ions to form insoluble salts, a decrease in the level of phosphates in the blood, releases free calcium ions into the plasma ionized calcium pool. PTH has a second action on the kidneys. It stimulates the manufacture and release, by the kidneys, of calcitriol (or 1,25 dihydroxycholecalciferol, or 1,25 dihydroxyvitamin D) into the blood. This steroid hormone acts on the epithelial cells of the upper small intestine, increasing their capacity to absorb calcium from the gut contents into the blood.\n\nThe second homeostat, with its sensors in the thyroid gland, releases calcitonin into the blood when the blood ionized calcium rises. This hormone acts primarily on bone, causing the rapid removal of calcium from the blood and depositing it, in insoluble form, in the skeleton.\n\nThe two homeostats working through PTH on the one hand, and calcitonin on the other, can very rapidly correct any impending error in the plasma ionized calcium level by either removing calcium from the blood and depositing it in the skeleton, or by removing calcium from it. The skeleton acts as an extremely large calcium store (about 1 kg) compared with the plasma calcium store (about 180 mg). Longer term regulation occurs through calcium absorption or loss from the gut (see Regulation of calcium metabolism in the Calcium metabolism article).\n\nThe partial pressure of oxygen (formula_1) in the arterial blood is measured in the aortic and carotid bodies, near the splitting of the common carotid artery into the internal and external carotid arteries. The partial pressure of carbon dioxide (formula_1) is measured on the surface of medulla oblongata of the brain. Information from these sets of sensors is sent to the respiratory center in the medulla oblongata of the brain which activates the effector organs, which, in this case, are the skeletal muscles of respiration (particularly the diaphragm). An increase in the formula_1 of the blood, or a decrease in the formula_1, causes deeper and more rapid breathing (hyperventilation) thus increasing the ventilation rate of the lung alveoli, which blows CO off, out of the blood, and into the outside air, while increasing the uptake of O from the alveolar air into the blood.\n\nToo little CO, and, to a lesser extent, too much O, in the blood can temporarily halt breathing, which breath-holding divers use to prolong the time they can stay underwater.\n\nThe formula_1 homeostat is an important component of the pH of the extracellular fluid homeostat. At sea level it receives priority over the formula_1 homeostat. But above elevations of about 2500 m (or approximately 8000 ft) the rate of breathing is determined by the arterial formula_1 rather than the formula_1. (At 2500 m the atmospheric pressure and partial pressure of oxygen are 75% of what they are at sea level.) At higher elevations than this, the formula_1 is allowed to fall, while hyperventilation now keeps the formula_1 constant. To keep the plasma pH at 7.4, despite the very low formula_1 levels as climbers ascend to these high elevations, the kidneys are stimulated secrete hydrogen ions (H) into the blood while excreting of bicarbonate ions (HCO) into the urine. This is an important contribution to the acclimatization to high altitude.\n\nThe kidneys measure the oxygen content (rather than the formula_1) of the arterial blood. When the oxygen content of the blood is chronically low, these oxygen-sensitive cells secrete erythropoietin (EPO) into the blood. The effector tissue in this case is the red bone marrow which produces red blood cells (or erythrocytes). This tissue is stimulated by high levels of erythropoietin to increase the rate of red cell production, which leads to an increase in the hematocrit of the blood, and a consequent increase in its oxygen carrying capacity (due to the now high hemoglobin content of the blood). This is the mechanism whereby high altitude dwellers have higher hematocrits than sea-level residents, and also why persons with pulmonary insufficiency or right-to-left shunts in the heart (through which venous blood by-passes the lungs and goes directly into the systemic circulation) have similarly high hematocrits.\n\nThe distinction between the formula_1 of the arterial blood and its oxygen content (or oxygen concentration) is important. The formula_1is the pressure with which the oxygen has been forced into the blood in the alveoli of the lungs. The amount of oxygen that is consequently carried in the blood (at a given formula_1) depends on the hemoglobin concentration in the blood. The greater the hemoglobin concentration the greater the amount of oxygen that can be carried per liter of blood at that formula_1. Thus, in anemia the formula_1 of the arterial blood is normal but the oxygen content is below normal. The oxygen content sensors in the kidneys detect this lower than normal oxygen concentration in the arterial blood, and increase their secretion of erythropoietin into the blood. This stimulates a greater rate of red blood cell production in the red bone marrow. This will correct the anemia, and therefore the oxygen concentration in the blood, if there are enough raw materials and co-factors (e.g. iron, vitamin B and folic acid) to manufacture the extra red cells.\n\nStretch receptors in the walls of the aortic arch and carotid sinus (at the beginning of the internal carotid artery) act as arterial blood pressure sensors. As the pressure rises the arteries balloon out, stretching their walls. This information is then conveyed, via sensory nerves, to the medulla oblongata of the brain stem. From here motor nerves belonging to the autonomic nervous system are stimulated to influence the activity of chiefly the heart and the smallest diameter arteries, called arterioles. The arterioles are the main resistance vessels in the arterial tree, and small changes in diameter cause large changes in the resistance to flow through them. When the arterial blood pressure rises the arterioles are stimulated to dilate making it easier for blood to leave the arteries, thus deflating them, and bringing the blood pressure down, back to normal. At the same time the heart is stimulated via cholinergic parasympathetic nerves to beat more slowly (called bradycardia), ensuring that the inflow of blood into the arteries is reduced, thus adding to the reduction in pressure, and correction of the original error.\n\nIf the pressure in the arteries falls, the opposite reflex is elicited: constriction of the arterioles, and a speeding up of the heart rate (called tachycardia). If the drop in blood pressure is very rapid or excessive, the medulla oblongata stimulates the adrenal medulla, via \"preganglionic\" sympathetic nerves, to secrete epinephrine (adrenaline) into the blood. This hormone enhances the tachycardia and causes severe vasoconstriction of the arterioles to all but the essential organ in the body (especially the heart, lungs and brain). These reactions usually correct the low arterial blood pressure (hypotension) very effectively.\n\nThe sodium concentration homeostat is rather more complex than most of the other homeostats described on this page.\nThe sensor is situated in the juxtaglomerular apparatus of kidneys, which senses the plasma sodium concentration in a surprisingly indirect manner. Instead of measuring it directly in the blood flowing past the juxtaglomerular cells, these cells respond to the sodium concentration in the renal tubular fluid after it has already undergone a certain amount of modification in the proximal convoluted tubule and loop of Henle. These cells also respond to rate of blood flow through the juxtaglomerular apparatus, which, under normal circumstances, is directly proportional to the arterial blood pressure, making this tissue an ancillary arterial blood pressure sensor.\n\nIn response to a lowering of the plasma sodium concentration, or to a fall in the arterial blood pressure, the juxtaglomerular cells release renin into the blood. Renin is an enzyme which cleaves a decapeptide (a short protein chain, 10 amino acids long) from a plasma α-2-globulin called angiotensinogen. This decapeptide is known as angiotensin I. It has no known biological activity. However, when the blood circulates through the lungs a pulmonary capillary endothelial enzyme called angiotensin-converting enzyme (ACE) cleaves a further two amino acids from angiotensin I to form an octapeptide known as angiotensin II. Angiotensin II is a hormone which acts on the adrenal cortex, causing the release into the blood of the steroid hormone, aldosterone. Angiotensin II also acts on the smooth muscle in the walls of the arterioles causing these small diameter vessels to constrict, thereby restricting the outflow of blood from the arterial tree, causing the arterial blood pressure to rise. This therefore reinforces the measures described above (under the heading of \"The arterial blood pressure homeostat\"), which defend the arterial blood pressure against changes, especially hypotension.\n\nThe angiotensin II-stimulated aldosterone released from the zona glomerulosa of the adrenal glands has an effect on particularly the epithelial cells of the distal convoluted tubules and collecting ducts of the kidneys. Here it causes the reabsorption of sodium ions from the renal tubular fluid, in exchange for potassium ions which are secreted from the blood plasma into the tubular fluid to exit the body via the urine. The reabsorption of sodium ions from the renal tubular fluid halts further sodium ion losses from the body, and therefore preventing the worsening of hyponatremia. The hyponatremia can only be \"corrected\" by the consumption of salt in the diet. However, it is not certain whether a “salt hunger” can be initiated by hyponatremia, or by what mechanism this might come about.\n\nWhen the plasma sodium ion concentration is higher than normal (hypernatremia), the release of renin from the juxtaglomerular apparatus is halted, ceasing the production of angiotensin II, and its consequent aldosterone-release into the blood. The kidneys respond by excreting sodium ions into the urine, thereby normalizing the plasma sodium ion concentration. The low angiotensin II levels in the blood lower the arterial blood pressure as an inevitable concomitant response.\n\nThe reabsorption of sodium ions from the tubular fluid as a result of high aldosterone levels in the blood does not, of itself, cause renal tubular water to be returned to the blood from the distal convoluted tubules or collecting ducts. This is because sodium is reabsorbed in exchange for potassium and therefore causes only a modest change in the osmotic gradient between the blood and the tubular fluid. Furthermore, the epithelium of the distal convoluted tubules and collecting ducts is impermeable to water in the absence of antidiuretic hormone (ADH) in the blood. ADH is part of the body water homeostat. Its levels in the blood vary with the osmolality of the plasma, which is measured in the hypothalamus of the brain. Aldosterone's action on the kidney tubules does not \"add\" sodium to the extracellular fluids (ECF) - it simply prevents further loss. So there is no change in the osmolality of the ECF, and therefore no change in the ADH concentration of the plasma. However, low aldosterone levels cause a loss of sodium ions from the ECF, which could potentially cause a change in extracellular osmolality and therefore of ADH levels in the blood.\n\nThe extracellular potassium ion (K) concentration is sensed by the zona glomerulosa cells of the outer layer of the adrenal cortex, as well as, probably, by sensors in the carotid arteries. High potassium concentrations in the plasma cause depolarization of the zona glomerulosa cells’ membranes. This causes the release of aldosterone into the blood.\n\nAldosterone acts primarily on the distal convoluted tubules and collecting ducts of the kidneys, stimulating them to excrete potassium ions into the tubular fluid, and thus into the urine. It does so, however, by activating the basolateral Na/K pumps of the tubular epithelial cells. These sodium/potassium exchangers pump three sodium ions out of the cell, into the interstitial fluid and two potassium ions into the cell from the interstitial fluid. This creates concentration gradients which result in the reabsorption of sodium (Na) ions from the tubular fluid into the blood, and secreting potassium (K) ions from the blood into the urine (lumen of collecting duct).\n\nThis obviously implies that excess potassium in the plasma can only be excreted at the expense of sodium retention by the body. The fact that the sodium and potassium homeostats seem to rely entirely on the same effector, but in opposite directions, implies that the body can only excrete potassium while retaining sodium, or vice versa. It cannot simultaneously excrete sodium and potassium ions in higher than modest quantities (when aldosterone is at an intermediate concentration in the plasma), and has no way of retaining both of them if there is a shortage in the body of the two cations. How these two conflicting homeostats (using the same effector) are disentangled to allow the plasma sodium and potassium ion levels to be regulated independently is currently not clear.\n\nThe volume of water in the body is measured by stretch receptors in the heart atria, and, somewhat indirectly, by the measurement of the osmolality of the plasma by the hypothalamus. Measurement of the plasma osmolality to give an indication of the water content of the body, relies on the fact that water losses from the body, through sweat, gut fluids (normal fecal water losses, and through vomiting and diarrhea), and the exhaled air, are all hypotonic, meaning that they are less salty than the body fluids (compare, for instance, the taste of saliva with that of tears. The latter have almost the same salt content as the extracellular fluids, whereas the former is hypotonic with respect to plasma. Saliva does not taste salty, whereas tears are decidedly salty). Nearly all normal and abnormal losses of body water therefore cause the extracellular fluids to become hyperosmolar. Conversely excessive water intake (in the form of most regular beverages) dilutes the extracellular fluids causing the hypothalamus to register hypo-osmolar conditions.\n\nWhen the hypothalamus detects a hyperosmolar extracellular environment, it causes the secretion from the posterior pituitary gland of a peptide hormone called antidiuretic hormone (ADH), which acts on the effector organ, which in this case is the kidney. The effect of ADH on the kidney tubules is to reabsorb water from the distal convoluted tubules and collecting ducts, thus preventing aggravation of the water loss via the urine. The hypothalamus simultaneously stimulates the nearby thirst center causing an almost irresistible (if the hyperosmolarity is severe enough) urge to drink water. The cessation of urine flow prevents the hypovolemia and hypertonicity from getting worse; the drinking of water corrects the defect.\n\nHypo-osmolality results in very low plasma ADH levels. This results in the inhibition of water reabsorption from the kidney tubules, causing high volumes of very dilute urine to be excreted, thus getting rid of the excess water in the body.\n\nNote that urinary water loss, when the body water homeostat is intact, is a \"compensatory\" water loss, \"correcting\" any water excess in the body. However, since the kidneys cannot generate water, the thirst reflex is the all important second effector mechanism of the body water homeostat, \"correcting\" any water deficit in the body.\n\nStretching of the right atrium of the heart, usually a sign of an excessive blood volume, causes stretch receptors to secrete a hormone known as atrial natriuretic peptide (ANP) into the blood. This also acts on the kidneys causing sodium, and accompanying water loss into the urine, thereby reducing the volume of circulating blood.\n\nThe pH of the extracellular fluids (which includes the blood plasma) is regulated by adjusting the ratio of the concentration of carbonic acid (HCO) to that of the bicarbonate ions (HCO) to equal 1:20. This ratio and its relationship to the pH is described by the Henderson–Hasselbalch equation, which, when applied to the bicarbonate buffering system in the extracellular fluids, states that:\n\nwhere:\n\nHowever, since the carbonic acid concentration is directly proportional to the formula_1 in the extracellular fluid, the Henderson–Hasselbalch equation can be rewritten as follows:\n\nwhere:\n\nThere are therefore at least two homeostats responsible for the regulation of the plasma pH. The first is the formula_1 homeostat described above which keeps the arterial blood formula_1 at 5.3 kPa (or 40 mm Hg). The sensor is on the surface of the medulla oblongata of the brain stem, which is also sensitive to the pH of the cerebrospinal fluid. The effector organs are the muscles of respiration, which are stimulated via motor nerves to breathe faster and more deeply (hyperventilation) when the formula_1 rises and the plasma pH falls, or more slowly and less deeply (hypoventilation) when the formula_1 falls and the pH rises. Changes in the rate and depth of breathing can change the pH of the arterial plasma within a few seconds.\n\nThe sensor for the plasma HCO concentration is not known for certain. It is very probable that the renal tubular cells of the distal convoluted tubules are themselves sensitive to the pH of the plasma. The metabolism of these cells produces CO, which is rapidly converted to H and HCO through the action of carbonic anhydrase. When the extracellular fluids tend towards acidity, the renal tubular cells secrete the H ions into the tubular fluid from where they exit the body via the urine. The HCO ions are simultaneously secreted into the blood plasma, thus raising the bicarbonate ion concentration in the plasma, increasing the [formula_25:formula_1 ratio, and consequently the pH of the plasma. The converse happens when the plasma pH rises above normal: bicarbonate ions are excreted into the urine, and hydrogen ions into the plasma.\n\nMany diseases are the result of the failure of one or more homeostat(s). Almost any functional component of any homeostat can malfunction, either as a result of an inherited defect, or an acquired disease. Some of the homeostats have inbuilt redundancies, which insures that life is not immediately threatened if a component malfunctions; but in other cases malfunction of a homeostat causes severe disease, which can be fatal if not treated. Here only a few well known examples of homeostat dysfunction are described.\n\nType 1 diabetes mellitus is probably the best known example. Here the blood glucose homeostat ceases to function because the beta cells of the pancreatic islets are destroyed. This means that the glucose sensor is absent, and its effector pathway (the insulin level in the blood) remains unchanged at zero. The blood glucose concentration therefore rises to very high levels, while the body’s proteins are degraded into amino acids which are turned at a very high rate into glucose, via gluconeogenesis, by the liver. The condition is fatal if not treated.\n\nThe plasma ionized calcium homeostat can be disrupted by the constant, unchanging, over-production of parathyroid hormone by a parathyroid adenoma resulting in the typically features of hyperparathyroidism, namely high plasma ionized Ca levels and the resorption of bone, which can lead to spontaneous fractures. The abnormally high plasma ionized calcium concentrations cause conformational changes in many cell-surface proteins (especially ion channels and hormone or neurotransmitter receptors) giving rise to lethargy, muscle weakness, anorexia, constipation and labile emotions.\n\nThe body water homeostat can be compromised by the inability to secrete ADH in response to even the normal daily water losses via the exhaled air, the feces, and insensible sweating. On receiving a zero blood ADH signal, the kidneys produce huge unchanging volumes of very dilute urine, causing dehydration and death if not treated.\n\nAs organisms age, the efficiency of their control systems becomes reduced. The inefficiencies gradually result in an unstable internal environment that increases the risk of illness, and leads to the physical changes associated with aging.\n\nVarious chronic diseases are kept under control by homeostatic compensation, which masks a problem by compensating for it (making up for it) in another way. However, the compensating mechanisms eventually wear out or are disrupted by a new complicating factor (such as the advent of a concurrent acute viral infection), which sends the body reeling through a new cascade of events. Such decompensation unmasks the underlying disease, worsening its symptoms. Common examples include decompensated heart failure, kidney failure, and liver failure.\n\nThe following are all examples of familiar technological homeostatic mechanisms:\n\nIn the Gaia hypothesis, James Lovelock stated that the entire mass of living matter on Earth (or any planet with life) functions as a vast homeostatic superorganism that actively modifies its planetary environment to produce the environmental conditions necessary for its own survival. In this view, the entire planet maintains several homeostats (the primary one being temperature homeostasis). Whether this sort of system is present on Earth is open to debate. However, some relatively simple homeostatic mechanisms are generally accepted. For example, it is sometimes claimed that when atmospheric carbon dioxide levels rise, certain plants may be able to grow better and thus act to remove more carbon dioxide from the atmosphere. However, warming has exacerbated droughts, making water the actual limiting factor on land. When sunlight is plentiful and atmospheric temperature climbs, it has been claimed that the phytoplankton of the ocean surface waters, acting as global sunshine, and therefore heat sensors, may thrive and produce more dimethyl sulfide (DMS). The DMS molecules act as cloud condensation nuclei, which produce more clouds, and thus increase the atmospheric albedo, and this feeds back to lower the temperature of the atmosphere. However, rising sea temperature has stratified the oceans, separating warm, sunlit waters from cool, nutrient-rich waters. Thus, nutrients have become the limiting factor, and plankton levels have actually fallen over the past 50 years, not risen. As scientists discover more about Earth, vast numbers of positive and negative feedback loops are being discovered, that, together, maintain a metastable condition, sometimes within very broad range of environmental conditions.\n\nPredictive homeostasis is an anticipatory response to an expected challenge in the future, such as the stimulation of insulin secretion by gut hormones which enter the blood in response to a meal. This insulin secretion occurs before the blood sugar level rises, lowering the blood sugar level in anticipation of a large influx into the blood of glucose resulting from the digestion of carbohydrates in the gut. Such anticipatory reactions are open loop systems which are based, essentially, on “guess work”, and are not self-correcting. Anticipatory responses always require a closed loop negative feedback system to correct the over- and undershoots to which the anticipatory systems are prone.\n\nThe term has come to be used in other fields, for example:\n\nAn actuary may refer to \"risk homeostasis\", where (for example) people who have anti-lock brakes have no better safety record than those without anti-lock brakes, because the former unconsciously compensate for the safer vehicle via less-safe driving habits. Previous to the innovation of anti-lock brakes, certain maneuvers involved minor skids, evoking fear and avoidance: Now the anti-lock system moves the boundary for such feedback, and behavior patterns expand into the no-longer punitive area. It has also been suggested that ecological crises are an instance of risk homeostasis in which a particular behavior continues until proven dangerous or dramatic consequences actually occur.\n\nSociologists and psychologists may refer to \"stress homeostasis\", the tendency of a population or an individual to stay at a certain level of stress, often generating artificial stresses if the \"natural\" level of stress is not enough. \n\nJean-François Lyotard, a postmodern theorist, has applied this term to societal 'power centers' that he describes as being 'governed by a principle of homeostasis,' for example, the scientific hierarchy, which will sometimes ignore a radical new discovery for years because it destabilises previously accepted norms. (See \"\" by Jean-François Lyotard)\n\nThe conceptual origins of homeostasis reach back to Greek concepts such as balance, harmony, equilibrium, and steady-state; all believed to be fundamental attributes of life and health. Thus, the philosopher Empedocles (495-435 BC) postulated that all matter consisted of elements and qualities that were in dynamic opposition or alliance to one another, and that balance or harmony was a necessary condition for the survival of living organisms. Following these hypotheses, Hippocrates (460-375 BC) compared health to the harmonious balance of the elements, and illness and disease to the systematic disharmony of these elements.\n\nNearly 150 years ago, Claude Bernard published his seminal work, stating that the maintenance of the internal environment, or milieu intérieur, surrounding the body's cells, was essential for the life of the organism. In 1929, Walter B. Cannon published an extrapolation from Bernard's 1865 work naming his theory \"homeostasis\". Cannon postulated that homeostasis was a process of synchronized adjustments in the internal environment resulting in the maintenance of specific physiological variables within defined parameters; and that these precise parameters included blood pressure, temperature, pH, and others; all with clearly defined \"normal\" ranges. Cannon further posited that threats to homeostasis might originate from the external environment (e.g., temperature extremes, traumatic injury) or the internal environment (e.g., pain, infection), and could be physical or psychological, as in emotional distress. Cannon's work outlined that maintenance of this internal physical and psychological balance, homeostasis, demands an internal network of communication, with sensors capable of identifying deviations from the acceptable ranges and effectors to return those deviations back within acceptable limits. Cannon identified these negative feedback systems and emphasized that, regardless of the nature of the threat to homeostasis, the response he mapped within the body would be the same.\n\nThe word \"homeostasis\" () uses combining forms of \"homeo-\" and \"-stasis\", New Latin from Greek: ὅμοιος \"homoios\", \"similar\" and στάσις \"stasis\", \"standing still\", yielding the idea of \"staying the same\".\n\n\n", "id": "13980", "title": "Homeostasis"}
{"url": "https://en.wikipedia.org/wiki?curid=13981", "text": "Hockey\n\nHockey is a family of sports in which two teams play against each other by trying to maneuver a ball or a puck into the opponent's goal using a hockey stick. In many areas, one sport (typically field hockey or ice hockey) is generally referred to simply as hockey.\n\nThe first recorded use of the word \"hockey\" is in the 1773 book \"Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education\" by Richard Johnson (Pseud. Master Michel Angelo), whose chapter XI was titled \"New Improvements on the Game of Hockey\". The belief that hockey was mentioned in a 1363 proclamation by King Edward III of England is based on modern translations of the proclamation, which was originally in Latin and explicitly forbade the games \"Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam\". The English historian and biographer John Strype did not use the word \"hockey\" when he translated the proclamation in 1720.\n\nThe word \"hockey\" itself is of unknown origin. One supposition is that it is a derivative of \"hoquet\", a Middle French word for a shepherd's stave. The curved, or \"hooked\" ends of the sticks used for hockey would indeed have resembled these staves. Another supposition derives from the known use of cork bungs, (stoppers) in place of wooden balls to play the game. The stoppers came from barrels containing \"hock\" ale, also called \"hocky\".\n\nGames played with curved sticks and a ball can be found in the histories of many cultures. In Egypt, 4000-year-old carvings feature teams with sticks and a projectile, hurling dates to before 1272 BC in Ireland, and there is a depiction from approximately 600 BC in Ancient Greece, where the game may have been called \"kerētízein\" or (κερητίζειν) because it was played with a horn or horn-like stick (\"kéras\", κέρας). In Inner Mongolia, the Daur people have been playing \"beikou\", a game similar to modern field hockey, for about 1,000 years.\n\nMost evidence of hockey-like games during the Middle Ages is found in legislation concerning sports and games. The Galway Statute enacted in Ireland in 1527 banned certain types of ball games, including games using \"hooked\" (written \"hockie\", similar to \"hooky\") sticks.\n\nBy the 19th century, the various forms and divisions of historic games began to differentiate and coalesce into the individual sports defined today. Organizations dedicated to the codification of rules and regulations began to form, and national and international bodies sprang up to manage domestic and international competition.\n\nBandy is played with a ball on a football field-sized ice arena (bandy rink), typically outdoors, and with many rules similar to association football. It is played professionally in Russia and Sweden and is considered a national sport in Russia. The sport is recognised by the IOC; its international governing body is the Federation of International Bandy.\n\nBandy has its roots in England in the 19th century, was originally called \"hockey on the ice\", and spread from England to other European countries around 1900; a similar Russian sport can also be seen as a predecessor and in Russia, bandy is sometimes called \"Russian hockey\". Bandy World Championships have been played since 1957 and Women's Bandy World Championships since 2004. There are national club championships in many countries and the top clubs in the world play in the Bandy World Cup every year.\n\nField hockey is played on gravel, natural grass, or sand-based or water-based artificial turf, with a small, hard ball approximately 73 mm (2.9 in) in diameter. The game is popular among both males and females in many parts of the world, particularly in Europe, Asia, Australia, New Zealand, South Africa, and Argentina. In most countries, the game is played between single-sex sides, although they can be mixed-sex.\n\nThe governing body is the 126-member International Hockey Federation (FIH). Men's field hockey has been played at each Summer Olympic Games since 1908 except for 1912 and 1924, while women's field hockey has been played at the Summer Olympic Games since 1980.\n\nModern field hockey sticks are constructed of a composite of wood, glass fibre or carbon fibre (sometimes both) and are J-shaped, with a curved hook at the playing end, a flat surface on the playing side and a curved surface on the rear side. All sticks are right-handed – left-handed sticks are not permitted.\n\nWhile field hockey in its current form appeared in mid-18th century England, primarily in schools, it was not until the first half of the 19th century that it became firmly established. The first club was created in 1849 at Blackheath in south-east London. Field hockey is the national sport of Pakistan. It was the national sport of India until the Ministry of Youth Affairs and Sports declared in August 2012 that India has no national sport.\n\nIce hockey is played between two teams of skaters on a large flat area of ice, using a three-inch-diameter (76.2 mm) vulcanized rubber disc called a puck. This puck is often frozen before high-level games to decrease the amount of bouncing and friction on the ice. The game is played all over North America, Europe and to varying extents in many other countries around the world. It is the most popular sport in Canada, Finland, Latvia, the Czech Republic, and Slovakia. Ice hockey is the national sport of Latvia and the national winter sport of Canada. Ice hockey is played at a number of levels, by all ages. \n\nThe governing body of international play is the 77-member International Ice Hockey Federation (IIHF). Men's ice hockey has been played at the Winter Olympics since 1924, and was in the 1920 Summer Olympics. Women's ice hockey was added to the Winter Olympics in 1998. North America's National Hockey League (NHL) is the strongest professional ice hockey league, drawing top ice hockey players from around the globe. The NHL rules are slightly different from those used in Olympic ice hockey over many categories. International ice hockey rules were adopted from Canadian rules in the early 1900s.\n\nThe contemporary sport developed in Canada from European and native influences. These included various stick and ball games similar to field hockey, bandy and other games where two teams push a ball or object back and forth with sticks. These were played outdoors on ice under the name \"hockey\" in England throughout the 19th century, and even earlier under various other names. In Canada, there are 24 reports of hockey-like games in the 19th century before 1875 (five of them using the name \"hockey\"). The first organized and recorded game of ice hockey was played indoors in Montreal, Quebec, Canada, on March 3, 1875, and featured several McGill University students.\n\nIce hockey sticks are long L-shaped sticks made of wood, graphite, or composites with a blade at the bottom that can lie flat on the playing surface when the stick is held upright and can legally curve either way, for left- or right-handed players.\n\nInline hockey is a variation of roller hockey very similar to ice hockey, from which it is derived. Inline hockey is played by two teams, consisting of four skaters and one goalie, on a dry rink divided into two halves by a center line, with one net at each end of the rink. The game is played in three 15-minute periods with a variation of the ice hockey off-side rule. Icings are also called, but are usually referred to as illegal clearing. The governing body is the IIHF, as for ice hockey, but some leagues and competitions do not follow the IIHF regulations, in particular USA Inline and Canada Inline.\n\nRoller hockey, also known as quad hockey, international-style ball hockey, and Hoquei em Patins, is an overarching name for a roller sport that has existed since long before inline skates were invented. This sport is played in over sixty countries and has a worldwide following. Roller hockey was a demonstration sport at the 1992 Barcelona Summer Olympics.\n\nSledge hockey is a form of ice hockey designed for players with physical disabilities affecting their lower bodies. Players sit on double-bladed sledges and use two sticks; each stick has a blade at one end and small picks at the other. Players use the sticks to pass, stickhandle and shoot the puck, and to propel their sledges. The rules are very similar to IIHF ice hockey rules.\n\nCanada is a recognized international leader in the development of sledge hockey, and much of the equipment for the sport was first developed there, such as sledge hockey sticks laminated with fiberglass, as well as aluminum shafts with hand-carved insert blades and special aluminum sledges with regulation skate blades.\n\nBased on ice sledge hockey, inline sledge hockey is played to the same rules as inline puck hockey (essentially ice hockey played off-ice using inline skates). There is no classification point system dictating who can play inline sledge hockey, unlike the situation with other team sports such as wheelchair basketball and wheelchair rugby. Inline sledge hockey is being developed to allow everyone, regardless of whether they have a disability or not, to complete up to world championship level based solely on talent and ability. The first game of inline sledge hockey was played at Bisley, England, on 19 December 2009 between the Hull Stingrays and the Grimsby Redwings. Matt Lloyd is credited with inventing inline sledge hockey, and Great Britain is seen as the international leader in the game's development.\n\nAlso known as road hockey, this is a dry-land variant of ice and roller hockey played year-round on a hard surface (usually asphalt). A ball is usually used instead of a puck, and protective equipment is not usually worn.\n\nOther games derived from hockey or its predecessors include the following:\n\n\n\n", "id": "13981", "title": "Hockey"}
{"url": "https://en.wikipedia.org/wiki?curid=13983", "text": "Hawick\n\nHawick ( ; , ) is a town in the Scottish Borders council area and historic county of Roxburghshire in the east Southern Uplands of Scotland. It is south-west of Jedburgh and south-southeast of Selkirk. It is one of the farthest towns from the sea in Scotland, in the heart of Teviotdale, and the biggest town in the former county of Roxburghshire. Hawick's architecture is distinctive in that it has many sandstone buildings with slate roofs. The town is at the\nconfluence of the Slitrig Water with the River Teviot. Hawick is known for its yearly Common Riding, for its rugby team Hawick Rugby Football Club and for its knitwear industry.\n\nAt the 2001 census Hawick had a resident population of 14,801. By 2011, this had reduced to 14,294.\n\nThe west end of the town contains \"the Mote\", the remains of a Norman motte-and-bailey. In the centre of the High Street is the Scots baronial style town hall, built in 1886, and the east end has an equestrian statue, known as \"the Horse\", erected in 1914. Drumlanrig's Tower, now a museum, dates largely from the mid-16th century.\nIn 2009 another monument the \"Turning of the Bull\" (artist, Angela Hunter, Innerleithen, Scotland) was unveiled in Hawick. This monument depicts William Rule turning the wild bull as it was charging King Robert the Bruce, thus saving the king's life and beginning the Scottish Clan of Turnbull. A poem written by John Leyden commemorates this historical event. \"His arms robust the hardy hunter flung around his bending horns, and upward wrung, with writhing force his neck retorted round, and rolled the panting monster to the ground, crushed, with enormous strength, his bony skull; and courtiers hailed the man who turned the bull.\"\n\nCompanies such as Hawick Cashmere, Hawick Knitwear, Johnstons of Elgin, Lyle & Scott, Peter Scott, Pringle of Scotland, and Scott and Charters, all have had and in many cases still have manufacturing plants in Hawick, producing some of the most luxurious cashmere and merino wool knitwear in the world today. The first knitting machine was brought to Hawick in 1771 by John Hardie, building on an existing carpet manufacturing trade. Originally based on linen, this quickly moved to wool and factories multiplied, driving the growth of the town. Engineering firm Turnbull and Scott previously had their headquarters in an Elizabethan-style listed building on Commercial Road before moving to Burnfoot.\n\nHawick lies in the centre of the valley of the Teviot. The A7 Edinburgh to Carlisle road passes through the town, with main roads also leading to Berwick-upon-Tweed (the A698) and Newcastle upon Tyne (the A6088, which joins the A68 at the Carter Bar, south-east of Hawick).\n\nThe town lost its rail service in 1969, when as part of the Beeching Axe the 'Waverley Line' from Carlisle to Edinburgh via Hawick was closed. It was said to be the farthest large town from a railway station in the United Kingdom, but this changed as a result of the partial reopening of the Waverley Line to Tweedbank, near Galashiels. Regular buses serve the railway station at Carlisle, away.\n\nThe nearest major airports are at Edinburgh, away, and Newcastle, away.\n\nThe town hosts the annual Common Riding, which combines the annual riding of the boundaries of the town's common land with the commemoration of a victory of local youths over an English raiding party in 1514. In March 2007, this was described by the \"Rough Guide\" publication \"World Party\" as one of the best parties in the world.\n\nPeople from Hawick call themselves \"Teries\", after a traditional song which includes the line \"Teribus ye teri odin\".\n\nMany Hawick residents speak the local dialect of Border Scots which is informally known as \"Teri Talk\". It is similar (but not identical by any means) to the dialects spoken in surrounding towns, especially Jedburgh, Langholm and Selkirk. The speech of this general area was described in \"Dialect of the Southern Counties of Scotland\" (1873) by James Murray, considered the first systematic study of any dialect. The Hawick tongue retains many elements of Old English, together with particular vocabulary, grammar and pronunciation. Its distinctiveness arose from the relative isolation of the town.\n\nThe town is the home of Hawick Rugby Football Club and a senior football team, Hawick Royal Albert, who currently play in the East of Scotland Football League.\n\nRivalry between the small Border towns is generally played out on the rugby union field. The historical competition continues to this day, as Hawick's main rival is the similarly-sized town of Galashiels.\n\nThe Hawick Baw game was once played here by the 'uppies' and the 'doonies' on the first Monday after the new moon in the month of February. The river of the town formed an important part of the pitch. Although no longer played at Hawick, it is still played at nearby Jedburgh.\n\nThe Borders Abbeys Way passes through Hawick.\n\n\n\n\n\n\n\n\n\nHawick's villages:\n\n\n", "id": "13983", "title": "Hawick"}
{"url": "https://en.wikipedia.org/wiki?curid=13985", "text": "Hatfield, Hertfordshire\n\nHatfield is a town and civil parish in Hertfordshire, England, in the borough of Welwyn Hatfield. It had a population of 29,616 in 2001, increasing to 39,201 at the 2011 Census and is of Saxon origin. Hatfield House, the home of the Marquess of Salisbury, is the nucleus of the old town. From the 1930s when de Havilland opened a factory until the 1990s when British Aerospace closed, Hatfield was associated with aircraft design and manufacture, which employed more people than any other industry. Hatfield was one of the post-war New Towns built around London and has much modernist architecture from the period. The University of Hertfordshire is based there. Hatfield is north of London and is connected to the capital via the A1(M) and direct trains to London King's Cross, Finsbury Park and Moorgate. As a result, the town has seen a recent increase in commuters who work in London moving to the area.\n\nIn the Saxon period Hatfield was known as Hetfelle, but by the year 970, when King Edgar gave to the monastery of Ely, it had become known as Haethfeld. Hatfield is mentioned in the Domesday Book as the property of the Abbey of Ely, and unusually, the original census data which compilers of Domesday used still survives, giving us slightly more information than in the final Domesday record. No other records remain until 1226, when Henry III granted the Bishops of Ely rights to an annual four-day fair and a weekly market. The town was then called Bishop's Hatfield.\n\nHatfield House is the seat of the Cecil family, the Marquesses of Salisbury. Elizabeth Tudor was confined there for three years in what is now known as \"The Old Palace\" in Hatfield Park. Legend has it that it was here in 1558, while sitting under an oak tree in the Park, that she learned that she had become Queen following the death of her half-sister, Queen Mary I. She held her first Council in the Great Hall (The Old Palace) of Hatfield. In 1851, the route of the Great North Road (now the A1000) was altered to avoid cutting through the grounds of Hatfield House.\n\nThe town grew up around the gates of Hatfield House. Old Hatfield retains many historic buildings, notably the Old Palace, St Etheldreda's Church and Hatfield House. The Old Palace was built by the Bishop of Ely, Cardinal Morton, in 1497, during the reign of Henry VII, and the only surviving wing is still used today for Elizabethan-style banquets. St Etheldreda's Church was founded by the monks from Ely, and the first wooden church, built in 1285, was probably sited where the existing building stands overlooking the old town.\n\nIn 1930 the de Havilland airfield and aircraft factory was opened at Hatfield and by 1949 it had become the largest employer in the town, with almost 4,000 staff. It was taken over by Hawker Siddeley in 1960 and merged into British Aerospace in 1978. In the 1930s it produced a range of small biplanes. During the Second World War it produced the Mosquito fighter bomber and developed the Vampire, the second British production jet aircraft after the Gloster Meteor. After the war, facilities were expanded and it developed the Comet airliner (the world's first production jet liner), the Trident airliner, and an early bizjet, the DH125.\n\nBritish Aerospace closed the Hatfield site in 1993 having moved the BAe 146 production line to Woodford Aerodrome. The land was used as a film set for Steven Spielberg's movie \"Saving Private Ryan\" and most of the BBC/HBO television drama \"Band of Brothers\". It was later developed for housing, higher education, commerce and retail. Part of the former British Aerospace site was intended to be the site of a £500 million new hospital to replace the Queen Elizabeth II Hospital in Welwyn GC and a new campus for Oaklands College, but both projects were cancelled.\n\nToday, Hatfield's aviation history is remembered by the names of certain local streets and pubs (e.g. Comet Way, The Airfield, Dragon Road) as well as The Comet Hotel (now owned by Ramada) built in the 1930s. (The Harrier Pub (formerly The Hilltop) is actually named after the Harrier Bird, not the aircraft, hence the original pub sign of a Harrier Bird.) The de Havilland Aircraft Heritage Centre, at Salisbury Hall in nearby London Colney, preserves and displays many historic de Havilland aeroplanes and related archives.\n\nThe Abercrombie Plan for London in 1944 proposed a New Town in Hatfield and it was designated in the New Towns Act 1946, forming part of the initial Hertfordshire group with nearby Stevenage, Welwyn Garden City and Letchworth. The Government allocated for Hatfield New Town, with a population target of 25,000. (By 2001 the population had reached 27,833.) The Hatfield Development Corporation, tasked with creating the New Town, chose to build a new town centre, rejecting Old Hatfield because it was on the wrong side of the railway, without space for expansion and \"with its intimate village character, out of scale with the town it would have to serve.\" They chose instead St Albans Road on the town's east-west bus route. A road pattern was planned that offered no temptation to through traffic to take short cuts through the town and which enabled local traffic to move rapidly.\n\nHatfield retains New Town characteristics, including much modernist architecture of the 1950s and the trees and open spaces that were outlined in the original design. The redevelopment of the town centre is being planned, involving the construction of 275 flats and retail units. Planning permission has been granted and compulsory purchase orders have been approved.\n\nHatfield is part of Welwyn Hatfield borough council in the county of Hertfordshire. It is a civil parish and has a town council. It is twinned with the Dutch port town of Zierikzee. Hatfield is part of the Welwyn Hatfield constituency, which includes Welwyn Garden City. The MP for Welwyn Hatfield is Grant Shapps, (Conservative).\n\nHatfield experiences an oceanic climate (Köppen climate classification \"Cfb\") similar to almost all of the United Kingdom.\n\nHatfield has a nine-screen Odeon cinema, a stately home (Hatfield House), a museum (Mill Green Museum), a contemporary art gallery (Art and Design Gallery), a theatre (The Weston Auditorium) and a music venue (The Forum Hertfordshire). There are shopping centres in the new town: the Galleria (indoor shopping centre), The Stable Yard (Hatfield House), and at two supermarkets (ASDA and Tesco).\n\nHatfield Town F.C. in Non-League football plays at Gosling Sports Park.\n\nThe town also has one public swimming pool, and four sports/leisure centres (two with indoor swimming pools).\n\nHatfield contains numerous primary and secondary schools, including The Ryde School, St. Philip Howard Catholic Primary School, Onslow St Audrey's School and Bishops Hatfield Girls School and the independent day and boarding girls' school Queenswood School.\n\nThe University of Hertfordshire is based in Hatfield. A large section of the airfield site was purchased by the University and the £120 million de Havilland Campus, incorporating a £15 million Sports Village, was opened in September 2003. The university has closed its sites at Watford and Hertford; faculties situated there have been moved to the de Havilland Campus.\n\n\n\nHatfield is to the north of London. It is from London Luton Airport and also near to Stansted airport The A1(M) runs through the town and it is close to the M25.\n\nThe East Coast railway line from London to York runs through the town and separates the old and new parts of Hatfield. A commuter service connects Hatfield railway station to London Kings Cross and a new railway station and car park opened in late 2015. The frequent train service runs directly from Hatfield Station to London King's Cross via Finsbury Park (Victoria Underground Line), taking approximately 16 minutes to Finsbury Park and 21 minutes to London King's Cross on the fast trains, which run two to three times an hour. An additional train service calls at all stations to Moorgate in the City of London.\n\nThere was a fatal rail crash at Hatfield in 2000, which brought track maintenance deficiencies to public attention. A garden beside the East Coast Main Line was built as a memorial for the crash victims.\n\n\n\n\nAmong several celebrities who frequent Hatfield as an escape from London life is Martin Freeman, best known for Bilbo Baggins in The Hobbit films, who exercises at a centre in Hatfield Business Park. One Direction’s Niall Horan is often spotted enjoying a meal in the town's main shopping centre, The Gallerias.\n\n", "id": "13985", "title": "Hatfield, Hertfordshire"}
{"url": "https://en.wikipedia.org/wiki?curid=13986", "text": "Hertfordshire\n\nHertfordshire (; often abbreviated Herts) is a county in southern England, bordered by Bedfordshire to the north, Cambridgeshire to the north-east, Essex to the east, Buckinghamshire to the west and Greater London to the south. For government statistical purposes, it is placed in the East of England region.\n\nIn 2013, the county had a population of 1,140,700 living in an area of . Four towns have between 50,000 and 100,000 residents: Hemel Hempstead, Stevenage, Watford and St Albans. The county town, Hertford, once the main market town for the medieval agricultural county ranks 13th in population today deriving its name from a hart (stag) and a ford used as the components of the county's coat of arms and flag. Elevations are high for the region in the north and west. These reach over 240m in the western projection around Tring which is in the Chilterns. The county's borders are approximately the watersheds of the Colne and Lea; both flowing to the south; each accompanied by a canal. Hertfordshire's undeveloped land is mainly agricultural and much is protected by green belt.\n\nThe county's landmarks span many centuries, ranging from the Six Hills in the new town of Stevenage built by local inhabitants during the Roman period, to Leavesden Film Studios. Leavesden filmed much of the UK-based $7.7 Bn box office \"Harry Potter\" film series and has the country's studio tour. The volume of intact medieval and Tudor buildings surpasses London, in places in well-preserved conservation areas, especially in St Albans which includes some remains of Verulamium, the town where in the 3rd century an early recorded British martyrdom took place. Saint Alban, a Romano-British soldier, took the place of a Christian priest and was beheaded on Holywell Hill. His martyr's cross of a yellow saltire on a blue background is reflected in the flag and coat of arms of Hertfordshire.\n\nHertfordshire is well-served with motorways and railways, providing good access to London. The largest sector of the economy of the county is in services.\n\nHertfordshire was the area assigned to a fortress constructed at Hertford under the rule of Edward the Elder in 913. Hertford is derived from the Anglo-Saxon \"heort ford,\" meaning deer crossing (of a watercourse). The name Hertfordshire is first recorded in the \"Anglo-Saxon Chronicle\" in 1011. Deer feature in many county emblems.\n\nThere is evidence of humans living in Hertfordshire from the Mesolithic period. It was first farmed during the Neolithic period and permanent habitation appeared at the beginning of the Bronze Age. This was followed by tribes settling in the area during the Iron Age.\n\nFollowing the Roman conquest of Britain in AD 43, the aboriginal Catuvellauni quickly submitted and adapted to the Roman life; resulting in the development of several new towns, including Verulamium (St Albans) where in c. 293 the first recorded British martyrdom is traditionally believed to have taken place. Saint Alban, a Romano-British soldier, took the place of a Christian priest and was beheaded on Holywell Hill. His martyr's cross of a yellow saltire on a blue background is reflected in the flag and coat of arms of Hertfordshire as the yellow background to the stag or Hart representing the county. He is the Patron Saint of Hertfordshire.\n\nWith the departure of the Roman Legions in the early 5th century, the now unprotected territory was invaded and colonised by the Anglo-Saxons. By the 6th century the majority of the modern county was part of the East Saxon kingdom. This relatively short lived kingdom collapsed in the 9th century, ceding the territory of Hertfordshire to the control of the West Anglians of Mercia. The region finally became an English shire in the 10th century, on the merger of the West Saxon and Mercian kingdoms.\n\nA century later the victorious William of Normandy received the surrender of the surviving senior English Lords and Clergy, at Berkhamsted, resulting in a new Anglicised title of William the Conqueror. He then embarked on an uncontested entry into London and coronation at Westminster.\n\nAfter the Norman conquest, Hertfordshire was used for some of the new Norman castles at Bishop's Stortford and at the royal residence of Berkhamsted and at King's Langley, a staging post between London and the royal residence of Berkhamsted.\n\nThe Domesday Book recorded the county as having nine hundreds. Tring and Danais became oneDacorumfrom Danis Corum or Danish rule harking back to a Viking not Saxon past. The other seven were Braughing, Broadwater, Cashio, Edwinstree, Hertford, Hitchin and Odsey.\n\nAs London grew, Hertfordshire became conveniently close to the English capital; much of the area was owned by the nobility and aristocracy, this patronage helped to boost the local economy. However, the greatest boost to Hertfordshire came during the Industrial Revolution, after which the population rose dramatically. In 1903, Letchworth became the world's first garden city and Stevenage became the first town to redevelop under the New Towns Act 1946.\n\nFrom the 1920s until the late 1980s, the town of Borehamwood was home to one of the major British film studio complexes, including the MGM-British Studios. Many well-known films were made here including the first three Star Wars movies (IV, V, & VI). The studios generally used the name of Elstree (the adjoining village). American director Stanley Kubrick not only used to shoot in those studios but also lived in the area until his death. In more recent times, Elstree has had the likes of Big Brother UK and Who Wants To Be A Millionaire? filmed there, whilst EastEnders is also filmed at the studios. Also Hertfordshire has seen development in other film studio complexes, Leavesden Film Studios were developed on the Leavesden Aerodome site, north of Watford. The Harry Potter series was filmed at the studios, whilst the 1995 James Bond film GoldenEye was also filmed there.\n\nOn 17 October 2000, the Hatfield rail crash killed four people with over 70 injured. The crash exposed the shortcomings of Railtrack, which consequently saw speed restrictions and major track replacement. On 10 May 2002, the second of the Potters Bar rail accidents occurred killing seven people; the train was at high speed when it derailed and flipped into the air when one of the carriages slid along the platform where it came to rest.\n\nIn early December 2005, the 2005 Hemel Hempstead fuel depot explosions occurred at the Hertfordshire Oil Storage Terminal.\n\nIn 2012, the canoe and kayak slalom events of the 2012 Summer Olympic Games took place in the town of Waltham Cross, within the borough of Broxbourne.\n\nFollowing a proposal put forward by The Welwyn Garden Heritage Trust, town-planner Andrés Duany has suggested that designated \"Garden Villages\" could be built within Hertfordshire to relieve some of the pressure for new homes, with perhaps a third Garden City to follow.\n\nHertfordshire is the county immediately north of London and is part of the East of England region, a mainly statistical unit. A significant minority of the population across all districts are City of London commuters. To the east is Essex, to the west is Buckinghamshire and to the north are Bedfordshire and Cambridgeshire.\n\nThe county's boundaries were roughly fixed by the Counties (Detached Parts) Act 1844 which eliminated exclaves; amended when, in 1965 under the London Government Act 1963, East Barnet Urban District and Barnet Urban District were abolished, their area was transferred to form part of the present-day London Borough of Barnet and the Potters Bar Urban District of Middlesex was transferred to Hertfordshire.\n\nThe highest point in the county is at (AOD) on the Ridgeway long distance national path, on the border of Hastoe near Tring with Drayton Beauchamp, Buckinghamshire.\n\nAs at the 2011 census of the ten Districts, East Hertfordshire had the minimal, 290 people per km², whereas Watford had the maximal 4210 people per km²\n\nAn unofficial status, the purple star-shaped flower with yellow stamens, the Pasqueflower is among endemic county flowers.\n\nThe rocks of Hertfordshire belong to the great shallow syncline known as the London Basin. The beds dip in a south-easterly direction towards the syncline's lowest point roughly under the River Thames. The most important formations are the Cretaceous Chalk, exposed as the high ground in the north and west of the county, forming the Chiltern Hills and the younger Palaeocene, Reading Beds and Eocene, London Clay which occupy the remaining southern part. The eastern half of the county was covered by glaciers during the Ice Age and has a superficial layer of glacial boulder clays.\n\nDespite the spread of built areas, much of the county is given over to agriculture. One product, now largely defunct, was water-cress, based in Hemel Hempstead and Berkhamsted supported by reliable, clean chalk rivers. \n\nSome quarrying of sand and gravel occurs in the St Albans area. In the past, clay has supplied local brick-making and still does in Bovingdon, just south-west of Hemel Hempstead. The chalk that is the bedrock of much of the county provides an aquifer that feeds streams and is also exploited to provide water supplies for much of the county and beyond. Chalk has also been used as a building material and, once fired, the resultant lime was spread on agricultural land to improve fertility. The mining of chalk since the early 18th century has left unrecorded underground galleries that occasionally collapse unexpectedly and endanger buildings.\n\nFresh water is supplied to London from Ware, using the New River built by Hugh Myddleton and opened in 1613. Local rivers, although small, supported developing industries such as paper production at Nash Mills. \n\nHertfordshire affords habitat for a variety of flora and fauna. One bird common in the shire is the Royston crow, which is the eponymous name of the regional newspaper, the \"Royston Crow\" published in Royston.\n\nIn November 2013, the uSwitch Quality of Life Index listed Hertfordshire as the third-best place to live in the UK.\n\nThis is a table of trends of regional gross value added of Hertfordshire at current basic prices with figures in millions of British Pounds Sterling.\n\nHertfordshire has headquarters of many large well-known UK companies. Hemel Hempstead is home to DSG International. Welwyn Garden City hosts Tesco, as well as Roche UK's headquarters (subsidiary of the Swiss pharmaceutical firm Hoffman-La Roche) and Cereal Partners production facilities, Pure the DAB radio maker is based in Kings Langley. JD Wetherspoon is in Watford. Comet and Skanska are in Rickmansworth, GlaxoSmithKline has plants in Ware and Stevenage. Hatfield used to be connected with the aircraft industry, as it was where de Havilland developed the world's first commercial jet liner, the Comet. Now the site is a business park and new campus for the University of Hertfordshire. This major new employment site is home to, among others, EE, Computacenter and Ocado. A subsidiary of BAE Systems, EADS and Finmeccanica in Stevenage, MBDA, develops missiles. In the same town EADS Astrium produces satellites. The National Pharmacy Association (NPA), the trade association for all of the UK's community pharmacies, is based in St. Albans. Warner Bros. also owns and runs Warner Studios in Leavesden.\n\nBelow is a list of notable visitor attractions in Hertfordshire:\n\n\n\nHertfordshire lies across major road and rail routes connecting London to the Midlands, Northern England and Scotland. As one of the home counties, many towns in the county form part of the London commuter belt.\n\nThe county has some of the principal roads in England: A1, A1(M), A5, A6, A41, M1, M11, and M25.\n\nFour principal national railway lines pass through the county:\n\nA number of other local rail routes also cross Hertfordshire:\n\nThree commuter lines operated by Transport for London enter the county:\n\nStansted and Luton are within of the county's borders. A commercial airfield is at Elstree for light aircraft.\n\nThe Grand Union Canal passes through much of the far west of Hertfordshire: Rickmansworth, Watford, Hemel Hempstead, Berkhamsted and Tring.\n\nLocal bus services are run by a number of private operators. Intalink is an organisation run by the county council that manages transport and funds bus services in rural areas.\n\nHertfordshire has 26 independent schools and 73 state secondary schools.\nThe state secondary schools are entirely comprehensive, although 7 schools in the south and southwest of the county are partially selective (see Education in Watford).\nAll state schools have sixth forms, and there are no sixth form colleges.\nThe tertiary colleges, each with multiple campuses, are Hertford Regional College, North Hertfordshire College, Oaklands College and West Herts College.\nThe University of Hertfordshire is a modern university based largely in Hatfield. It has more than 23,000 students.\n\nHertfordshire is the location of Jack Worthing's country house in Oscar Wilde's play \"The Importance of Being Earnest\".\n\nJane Austen's novel \"Pride and Prejudice\" is primarily set in Hertfordshire. Topographical scholars place the town of Meryton either as Hertford or Hemel Hempstead, based on how far Mr Collins travels on the post from Watford, in either an easterly or westerly direction. The former location places the Bennet family home Longbourn as the town of Ware.\n\nThe location of Mr Jarndyce's Bleak House in Charles Dickens's Bleak House is near St. Albans in Hertfordshire.\n\nThe eponymous residence in E. M. Forster's novel, \"Howards End\" was based on Rooks Nest House just outside Stevenage. In the novel, Forster describes Hertfordshire as \"England at its quietest\".\n\nGeorge Orwell based his book \"Animal Farm\" on the village of Wallington, Hertfordshire where he lived between 1936 and 1940. Manor Farm and The Great Barn both feature in the novel.\n\n\n", "id": "13986", "title": "Hertfordshire"}
{"url": "https://en.wikipedia.org/wiki?curid=13987", "text": "Helene Kröller-Müller\n\nHelene Kröller-Müller (11 February 1869 – 14 December 1939) was one of the first European women to put together a major art collection and credited as being one of the first collectors to recognise the genius of Vincent van Gogh. She donated her entire collection to the Dutch people, along with her and her husband, Anton Müller's, large forested country estate. Today it is the Kröller-Müller Museum and sculpture garden and Hoge Veluwe National Park, the largest national park in the Netherlands.\n\nShe was born Helene Emma Laura Juliane Müller at , Essen, Germany, into a wealthy industrialist family. Her father, Wilhelm Müller, owned Wm. H. Müller & Co., a prosperous supplier of raw materials to the mining and steel industries. She married Dutch shipping and mining tycoon Anton Kröller in 1888 and used both surnames in accordance with Dutch tradition.\n\nShe studied under Henk Bremmer in 1906-1907. As she was one of the wealthiest women in the Netherlands at the time, Bremmer recommended that she form an art collection. In 1907, she began her collection with the painting \"Train in a Landscape\" by Paul Gabriël. Subsequently, Helene Kröller-Müller became an avid art collector, and one of the first people to recognise the genius of Vincent van Gogh. She eventually amassed more than 90 van Gogh paintings and 185 drawings, one of the world's largest collections of the artist's work, second only to the Van Gogh Museum in Amsterdam. She also bought more than 400 works by Dutch artist Bart van der Leck, but his popularity did not take off like van Gogh's.\nKröller-Müller also collected works by modern artists, such as Picasso, Georges Braque, Jean Metzinger, Albert Gleizes, Fernand Léger, Diego Rivera, Juan Gris, Piet Mondrian, Gino Severini, Joseph Csaky, Auguste Herbin, Georges Valmier, María Blanchard, Léopold Survage and Tobeen. However, Bremmer advised her not to buy \"A Sunday Afternoon on the Island of La Grande Jatte\" by Georges Seurat, which turned out to be an important icon of 20th-century art. She did purchase however \"Le Chahut\" by Seurat, another icon in the history of modern art. Also, she steered away from artists of her native Germany, whose work she found \"insufficiently authoritative.\"\n\nOn a trip to Florence in June 1910, she conceived the idea of creating a museum-house. From 1913 onwards parts of her collection were open to the public; until the mid-1930s her exhibition hall in The Hague was one of the very rare places where one could see more than a few works of modern art. In 1928, Anton and Helene created the Kröller-Müller Foundation to protect the collection and the estates. In 1935, they donated to the Dutch people their entire collection totaling approximately 12,000 objects, on condition that a large museum be built in the gardens of her park. Held in the care of the Dutch government, the Kröller-Müller Museum was opened in 1938.\n\nThe Kröller-Müller Museum is nestled in their 75-acre (300,000 m) forested country estate, today the largest national park in the Netherlands, the Hoge Veluwe National Park near the town of Otterlo and Arnhem. A lavish art gallery was planned near their iconic lakeside Jachthuis Sint Hubertus hunting lodge and landscape statue of their close personal friend, the South African Boer General Christian de Wet on the estate. Due to threat of war the plans were never implemented in their lifetime but once the war was over a large forest sculpture garden and understated open exhibition extension was opened, housing statues by Rodin and the second largest collection of van Gogh paintings in the world, including the famous Sunflowers.\n\n\n", "id": "13987", "title": "Helene Kröller-Müller"}
{"url": "https://en.wikipedia.org/wiki?curid=13988", "text": "Hans-Georg Gadamer\n\nHans-Georg Gadamer (; February 11, 1900 – March 13, 2002) was a German philosopher of the continental tradition, best known for his 1960 magnum opus \"Truth and Method\" (\"Wahrheit und Methode\") on hermeneutics.\n\nGadamer was born in Marburg, Germany, the son of Johannes Gadamer (1867–1928) a pharmaceutical chemistry professor who later also served as the rector of the University of Marburg. He resisted his father's urging to take up the natural sciences and became more and more interested in the humanities. His mother, Emma Karoline Johanna Geiese (1869–1904) died of diabetes while Hans-Georg was four years old, and he later noted that this may have had an effect on his decision to not pursue scientific studies. Jean Grondin describes Gadamer as finding in his mother \"a poetic and almost religious counterpart to the iron fist of his father\". Gadamer did not serve during World War I for reasons of ill health and similarly was exempted from serving during World War II due to polio.\n\nHe grew up and studied classics and philosophy in the University of Breslau under Richard Hönigswald, but soon moved back to the University of Marburg to study with the Neo-Kantian philosophers Paul Natorp (his doctoral thesis advisor) and Nicolai Hartmann. He defended his dissertation—\"The Essence of Pleasure according to Plato's Dialogues\" ()—in 1922.\n\nShortly thereafter, Gadamer moved to Freiburg University and began studying with Martin Heidegger, who was then a promising young scholar who had not yet received a professorship. He and Heidegger became close, and when Heidegger received a position at Marburg, Gadamer followed him there, where he became one of a group of students such as Leo Strauss, Karl Löwith, and Hannah Arendt. It was Heidegger's influence that gave Gadamer's thought its distinctive cast and led him away from the earlier neo-Kantian influences of Natorp and Hartmann. Gadamer studied Aristotle both under Edmund Husserl and under Heidegger.\n\nGadamer habilitated in 1929 and spent most of the early 1930s lecturing in Marburg. Unlike Heidegger, who joined the Nazi Party in May 1933 and continued as a member until the party was dissolved following World War II, Gadamer was silent on Nazism, and he was not politically active during the Third Reich. Gadamer did not join the Nazis, and he did not serve in the army because of the polio he had contracted in 1922. He joined the National Socialist Teachers League in August 1933.\n\nIn 1933 Gadamer signed the \"Loyalty Oath of German Professors to Adolf Hitler and the National Socialist State\".\n\nIn April 1937 he became a temporary professor at Marburg, then in 1938 he received a professorship at Leipzig University. From an SS-point of view Gadamer was classified as neither supportive nor disapproving in the \"\"SD-Dossiers über Philosophie-Professoren\"\" (i.e. SD-files concerning philosophy professors) that were set up by the SS-Security-Service (SD). In 1946, he was found by the American occupation forces to be untainted by Nazism and named rector of the university.\n\nThe level of Gadamer's involvement with the Nazis has been disputed in the works of Richard Wolin and Teresa Orozco. Orozco alleges, with reference to Gadamer's published works, that Gadamer had supported the Nazis more than scholars had supposed. Gadamer scholars have rejected these assertions: Jean Grondin has said that Orozco is engaged in a \"witch-hunt\" while Donatella Di Cesare said that \"the archival material on which Orozco bases her argument is actually quite negligible\". Cesare and Grondin have argued that there is no trace of antisemitism in Gadamer's work, and that Gadamer maintained friendships with Jews and provided shelter for nearly two years for the philosopher Jacob Klein in 1933 and 1934. Gadamer also reduced his contact with Heidegger during the Nazi era.\n\nThe communist DDR was no more to Gadamer's liking than the Third Reich, and he left for West Germany, accepting first a position in Goethe University Frankfurt and then the succession of Karl Jaspers in the University of Heidelberg in 1949. He remained in this position, as emeritus, until his death in 2002 at the age of 102. He was also an Editorial Advisor of the journal Dionysius. It was during this time that he completed his \"magnum opus\", \"Truth and Method\" (1960), and engaged in his famous debate with Jürgen Habermas over the possibility of transcending history and culture in order to find a truly objective position from which to critique society. The debate was inconclusive, but marked the beginning of warm relations between the two men. It was Gadamer who secured Habermas's first professorship in the University of Heidelberg.\n\nIn 1968, Gadamer invited Tomonobu Imamichi for lectures at Heidelberg, but their relationship became very cool after Imamichi alleged that Heidegger had taken his concept of \"Dasein\" out of Okakura Kakuzo's concept of \"das in-der-Welt-sein\" (to be in the being in the world) expressed in \"The Book of Tea\", which Imamichi's teacher had offered to Heidegger in 1919, after having followed lessons with him the year before. Imamichi and Gadamer renewed contact four years later during an international congress.\n\nIn 1981, Gadamer attempted to engage with Jacques Derrida at a conference in Paris but it proved less enlightening because the two thinkers had little in common. A last meeting between Gadamer and Derrida was held at the Stift of Heidelberg in July 2001, coordinated by Derrida's students, Joseph Cohen and Raphael Zagury-Orly. This meeting marked, in many ways, a turn in their philosophical encounter. After Gadamer's death, Derrida called their failure to find common ground one of the worst debacles of his life and expressed, in the main obituary for Gadamer, his great personal and philosophical respect. Richard J. Bernstein said that \"[a] genuine dialogue between Gadamer and Derrida has never taken place. This is a shame because there are crucial and consequential issues that arise between hermeneutics and deconstruction\".\n\nGadamer received honorary doctorates from the University of Bamberg, the University of Breslau, Boston College, Charles University in Prague, Hamilton College, the University of Leipzig, the University of Marburg (1999) the University of Ottawa, Saint Petersburg State University (2001), the University of Tübingen and University of Washington.\n\nOn February 11, 2000, the University of Heidelberg celebrated Gadamer's one hundredth birthday with a ceremony and conference. Gadamer's last academic engagement was in the summer of 2001 at an annual symposium on hermeneutics that two of Gadamer's American students had organised. On March 13, 2002, Gadamer died at Heidelberg's University Clinic. He is buried in the Köpfel cemetery in Ziegelhausen.\n\nGadamer's philosophical project, as explained in \"Truth and Method\", was to elaborate on the concept of \"philosophical hermeneutics\", which Heidegger initiated but never dealt with at length. Gadamer's goal was to uncover the nature of human understanding. In \"Truth and Method\", Gadamer argued that \"truth\" and \"method\" were at odds with one another. For Gadamer, \"the experience of art is exemplary in its provision of truths that are inaccessible by scientific methods, and this experience is projected to the whole domain of human sciences.\" He was critical of two approaches to the human sciences (\"Geisteswissenschaften\"). On the one hand, he was critical of modern approaches to humanities that modeled themselves on the natural sciences, which simply sought to “objectively” observe and analyze texts and art. On the other hand, he took issue with the traditional German approaches to the humanities, represented for instance by Friedrich Schleiermacher and Wilhelm Dilthey, who believed that meaning, as an object, could be found within a text through a particular process that allowed for a connection with the author’s thoughts that led to the creation of a text (Schleiermacher), or the situation that led to an expression of human inner life (Dilthey).\n\nHowever, Gadamer argued meaning and understanding are not objects to be found through certain methods, but are inevitable phenomena. Hermeneutics is not a process in which an interpreter finds a particular meaning, but “a philosophical effort to account for understanding as an ontological—the ontological—process of man.” Thus, Gadamer is not giving a prescriptive method on how to understand, but rather he is working to examine how understanding, whether of texts, artwork, or experience, is possible at all. Gadamer intended \"Truth and Method\" to be a description of what we always do when we interpret things (even if we do not know it): \"My real concern was and is philosophic: not what we do or what we ought to do, but what happens to us over and above our wanting and doing\".\n\nAs a result of Martin Heidegger’s temporal analysis of human existence, Gadamer argued that people have a \"historically-effected\" consciousness (\"wirkungsgeschichtliches Bewußtsein\"), and that they are embedded in the particular history and culture that shaped them. However the historical consciousness is not an object over and against our existence, but “a stream in which we move and participate, in every act of understanding.” Therefore, people do not come to any given thing without some form of preunderstanding established by this historical stream. The tradition in which an interpreter stands establishes \"prejudices\" that affect how he or she will make interpretations. For Gadamer, these prejudices are not something that hinders our ability to make interpretations, but are both integral to the reality of being, and “are the basis of our being able to understand history at all.” Gadamer criticized Enlightenment thinkers for harboring a \"prejudice against prejudices\".\n\nFor Gadamer, interpreting a text involves a fusion of horizons (\"Horizontverschmelzung\"). Both the text and the interpreter find themselves within a particular historical tradition, or “horizon.” Each horizon is expressed through the medium of language, and both text and interpreter belong to and participate in history and language. This “belongingness” to language is the common ground between interpreter and text that makes understanding possible. As an interpreter seeks to understand a text, a common horizon emerges. This fusion of horizons does not mean the interpreter now fully understands some kind of objective meaning, but is “an event in which a world opens itself to him.” The result is a deeper understanding of the subject matter.\n\nGadamer further explains the hermeneutical experience as a dialogue. To justify this, he uses Plato’s dialogues as a model for how we are to engage with written texts. To be in conversation, one must take seriously “the truth claim of the person with whom one is conversing.” Further, each participant in the conversation relates to one another insofar as they belong to the common goal of understanding one another. Ultimately, for Gadamer, the most important dynamic of conversation as a model for the interpretation of a text is “the give-and-take of question and answer.” In other words, the interpretation of a\ngiven text will change depending on the questions the interpreter asks of the\ntext. The \"meaning\" emerges not as an object that lies in the text or in the interpreter, but rather an event that results from the interaction of the two.\n\n\"Truth and Method\" was published twice in English, and the revised edition is now considered authoritative. The German-language edition of Gadamer's Collected Works includes a volume in which Gadamer elaborates his argument and discusses the critical response to the book. Finally, Gadamer's essay on Celan (entitled \"Who Am I and Who Are You?\") has been considered by many—including Heidegger and Gadamer himself—as a \"second volume\" or continuation of the argument in \"Truth and Method\".\n\nGadamer's \"Truth and Method\" has become an authoritative work in the communication ethics field, spawning several prominent ethics theories and guidelines. The most profound of these is the formulation of the dialogic coordinates, a standard set of prerequisite communication elements necessary for inciting dialogue. Adhering to Gadamer's theories regarding bias, communicators can better initiate dialogic transaction, allowing biases to merge and promote mutual understanding and learning.\n\nGadamer also added philosophical substance to the notion of human health. In \"The Enigma of Health\", Gadamer explored what it means to heal, as a patient and a provider. In this work the practice and art of medicine are thoroughly examined, as is the inevitability of any cure.\n\nIn addition to his work in hermeneutics, Gadamer is also well known for a long list of publications on Greek philosophy. Indeed, while \"Truth and Method\" became central to his later career, much of Gadamer's early life centered around studying Greek thinkers, Plato and Aristotle specifically. In the Italian introduction to \"Truth and Method\", Gadamer said that his work on Greek philosophy was \"the best and most original part\" of his career. His book \"Plato's Dialectical Ethics\" looks at the \"Philebus\" dialogue through the lens of phenomenology and the philosophy of Martin Heidegger.\n\n\n\n\n\n\n", "id": "13988", "title": "Hans-Georg Gadamer"}
{"url": "https://en.wikipedia.org/wiki?curid=13991", "text": "Honeymoon\n\nA honeymoon is the traditional holiday taken by newlyweds to celebrate their marriage in intimacy and seclusion. Today, honeymoons are often celebrated in destinations considered exotic or romantic.\n\nThis is the period when newly wed couples take a break to share some private and intimate moments that helps establish love in their relationship. This privacy in turn is believed to ease the comfort zone towards a physical relationship, which is one of the primary means of bonding during the initial days of marriage. The earliest term for this in English was \"hony moone\", which was recorded as early as 1546.\n\nIn Western culture, the custom of a newlywed couple going on a holiday together originated in early 19th century Great Britain. Upper-class couples would take a \"bridal tour\", sometimes accompanied by friends or family, to visit relatives who had not been able to attend the wedding. The practice soon spread to the European continent and was known as \"voyage à la façon anglaise\" (English-style voyage) in France from the 1820s onwards.\n\nHoneymoons in the modern sense (i.e. a pure holiday voyage undertaken by the married couple) became widespread during the Belle Époque, as one of the first instances of modern mass tourism. This came about in spite of initial disapproval by contemporary medical opinion (which worried about women's frail health) and by \"savoir vivre\" guidebooks (which referred the public attention drawn to what was assumed to be the wife's sexual initiation). The most popular honeymoon destinations at the time were the French Riviera and Italy, particularly its seaside resorts and romantic cities such as Rome, Verona or Venice. Typically honeymoons would start on the night they were married, with the couple leaving midway through the reception to catch a late train or ship. However, in the 21st century, many couples will not leave until 1–3 days after the ceremony and reception in order to tie up loose ends with the reception venue or simply enjoy the reception to its fullest and have a relaxing night afterwards to recover, before undertaking a long journey. In Jewish traditions, honeymoons are often put off seven days to allow for the seven nights of feasting if the visits to friends and family can't be incorporated into the trip.\n\nThe \"Oxford English Dictionary\" offers no etymology, but gives examples dating back to the 16th century. The Merriam-Webster dictionary reports the etymology as from \"the idea that the first month of marriage is the sweetest.\" (1546)\n\nIn ancient times honeymoon referred to the time of year when bee honey was ripe and cured to be harvested from hives or from the wild which made it the sweetest time of the year. This was usually around the Summer solstice by end June.\n\nA honeymoon can also be the first, \"sweetest\" moments a newly-wed couple spend together, or the first holiday they spend together to celebrate their marriage.\n\nOne of the more recent citations in the \"Oxford English Dictionary\" indicates that, while today \"honeymoon\" has a positive meaning, the word was originally a reference to the inevitable waning of love like a phase of the moon. This, the first known literary reference to the honeymoon, was penned in 1552, in Richard Huloet's \"Abecedarium Anglico Latinum\". Huloet writes:\n\nA widely disputed explanation of the term claims that it comes from a tradition in any of a number of cultures (e.g. Welsh, German or Scandinavian or Babylonian) where mead was drunk in great quantities at weddings and then after the ceremony nuptial couples were given a month’s supply of mead. It was believed that by faithfully drinking mead for that first month, the woman would “bear fruit” and a child would be born within the year.\n\nThere are many words of similar meaning in other languages. The Sinhalese form translates as \"Madhu Samaya\" (\"මධු සමය\"). The French form translates as \"moon of honey\" (\"lune de miel\"), as do the Spanish (\"luna de miel\"), Romanian (\"luna de miere\"), Nepali (\"Madhumas\") Portuguese (\"lua de mel\") and Italian (\"luna di miele\") equivalents. The Welsh word for honeymoon is \"mis mêl\", which means \"honey month\", and similarly the Ukrainian (\"медовий місяць\"), Polish (\"miesiąc miodowy\"), Russian (\"медовый месяц\"), Arabic ( \"shahr el 'assal\"), Greek (\"μήνας του μέλιτος\") and Hebrew (ירח דבש \"yerach d'vash\") versions. (\"Yerach\" is used for month, rather than the more common \"Chodesh\". \"Yerach\" is related to the word \"Yare'ach\" for moon and the two words are spelled alike: ירח.) The Persian word is ماه عسل \"māh-e asal\" which means both \"honey moon\" and \"honey month\" (\"māh\" in Persian means both \"moon\" and \"month\"). The same applies to the word \"ay\" in the Turkish equivalent, \"balayı\". In Hungarian language it is called \"honey weeks\" (mézeshetek). Likewise, the Tamil word for honeymoon is \"தேனிலவு\" (thaen nilavu), with \"thaen\" 'honey' and \"nilavu\" 'moon', and the Marathi word for honeymoon is \"मधुचंद्र\" (madhuchandra) with \"Madhu\" 'honey' and \"chandra\" 'moon', whereas in Bangla ('Bengali') language, it is referred to as মধুচন্দ্রিমা (modhuchondrima) with \"modhu\" 'honey' and \"chondrima\" 'moon'.\n\nBride kidnapping\n", "id": "13991", "title": "Honeymoon"}
{"url": "https://en.wikipedia.org/wiki?curid=13992", "text": "Harold Kushner\n\nRabbi Harold Samuel Kushner is a prominent American rabbi aligned with the progressive wing of Conservative Judaism, and a popular author.\n\nBorn in Brooklyn, Kushner was educated at Columbia University and later obtained his rabbinical ordination from the Jewish Theological Seminary (JTS) in 1960. The same institution awarded him a doctoral degree in Bible in 1972. Kushner has also studied at the Hebrew University of Jerusalem, taught at Clark University and the Rabbinical School of the JTS, and received six honorary doctorates.\n\nHe served as the congregational rabbi of Temple Israel of Natick, in Natick, Massachusetts for 24 years and belongs to the Rabbinical Assembly.\n\nHe is the author of a best selling book on the problem of evil, \"When Bad Things Happen to Good People.\" Written following the death of his son, Aaron, from the premature aging disease progeria, the book deals with questions about human suffering, God, omnipotence and theodicy. Aaron was born in 1963 and died in 1977; the book was published in 1981. \n\nKushner has written a number of other popular theological books, such as \"How Good Do We Have to Be?\" (Dedicated to his grandson, Carl), \"To Life!\" and many others. In collaboration with the late Chaim Potok, Kushner co-edited \"Etz Hayim: A Torah Commentary\", the new official Torah commentary of the Conservative movement, which was jointly published in 2001 by the Rabbinical Assembly and the Jewish Publication Society. His \"Living a Life That Matters\" became a best seller in the fall of 2001. Kushner's book, \"The Lord Is My Shepherd\", was a meditation on the Twenty-Third Psalm released in 2003. Kushner also wrote a response to Simon Wiesenthal's question of forgiveness in the book \".\"\n\n\n\n", "id": "13992", "title": "Harold Kushner"}
{"url": "https://en.wikipedia.org/wiki?curid=13994", "text": "Hotspot\n\nHotspot or Hot spot may refer to:\n\n\n\n\n\n\n\n\n", "id": "13994", "title": "Hotspot"}
{"url": "https://en.wikipedia.org/wiki?curid=13995", "text": "Heapsort\n\nIn computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.\n\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(\"n\" log \"n\") runtime. Heapsort is an in-place algorithm, but it is not a stable sort.\n\nHeapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.\n\nThe heapsort algorithm can be divided into two parts.\n\nIn the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based array, the root node is stored at index 0; if codice_1 is the index of the current node, then\nIn the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap. Once all objects have been removed from the heap, the result is a sorted array.\n\nHeapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here. The heap's invariant is preserved after each extraction, so the only cost is that of extraction.\n\nThe heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\n\nThe steps are:\nThe buildMaxHeap() operation is run once, and is O(n) in performance. The siftDown() function is O(log(n)), and is called n times. Therefore, the performance of this algorithm is O(n + n * log(n)) which evaluates to O(\"n\" log \"n\").\n\nThe following is a simple way to implement the algorithm in pseudocode. Arrays are zero-based and codice_2 is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at codice_3, while at the end of the sort, the largest element is in codice_4.\n\nThe sorting routine uses two subroutines, codice_5 and codice_6. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing codice_5.\n\nThe codice_5 procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This codice_9 version can be visualized as starting with an empty heap and successively inserting elements, whereas the codice_6 version given above treats the entire input array as a full but \"broken\" heap and \"repairs\" it starting from the last non-trivial sub-heap (that is, the last parent node).\nAlso, the codice_6 version of heapify has time complexity, while the codice_9 version given below has time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.\nThis may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never affects asymptotic analysis.\n\nTo grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call \"increases\" with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more \"deep\" nodes than there are \"shallow\" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the \"bottom\" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call \"decreases\" as the depth of the node on which the call is made increases. Thus, when the codice_6 codice_5 begins and is calling codice_6 on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the \"height\" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.\n\nThe heapsort algorithm itself has time complexity using either version of heapify.\n\n\nBottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000. This version of heapsort keeps the linear-time heap-building phase, but changes the second phase, as follows.\n\nOrdinary heapsort extracts the top of the heap, , and fills the gap it leaves with , then sifts this latter element down the heap; but this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. Each step of the sift-down requires two comparisons, to find the minimum of the new node and its two children.\n\nBottom-up heapsort instead finds the path of largest children to the leaf level of the tree (as if it were inserting −∞) using only one comparison per level. Put another way, it find the leaf which has the property that it and all of its ancestors are greater than their siblings. (In the absence of equal keys, this leaf is unique.) Then, from this leaf, it searches \"upward\" (using one comparison per level) for the correct position in that path to insert . This is the same location as ordinary heapsort finds, and requires the same number of exchanges to perform the insert, but fewer comparisons are required to find that location.\n\nThe return value of the codice_16 is used in the modified codice_6 routine:\n\nBottom-up heapsort requires only comparisons in the worst case and on average. For comparison, ordinary heapsort requires comparisons worst-case and on average.\n\nA 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort for integer keys, though, presumably because modern branch prediction nullifies the cost of the predictable comparisons which bottom-up heapsort manages to avoid. (It still has an advantage if comparisons are expensive.)\n\nA further refinement does a binary search in the path to the selected leaf, and sorts in a worst case of comparisons, approaching the information-theoretic lower bound of comparisons.\n\nA variant which uses two extra bits per internal node (\"n\"−1 bits total for an \"n\"-element heap) to cache information about which child is greater (two bits are required to store three cases: left, right, and unknown) uses less than compares.\n\nHeapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm.\n\nQuicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is O(\"n\"), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See quicksort for a detailed discussion of this problem and possible solutions.\n\nThus, because of the O(\"n\" log \"n\") upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort, such as the Linux kernel. \n\nHeapsort also competes with merge sort, which has the same time bounds. Merge sort requires Ω(n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches, and does not require as much external memory. On the other hand, merge sort has several advantages over heapsort:\n\nIntrosort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.\n\nLet { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)\n\n1. Build the heap\n2. Sorting.\n\n", "id": "13995", "title": "Heapsort"}
{"url": "https://en.wikipedia.org/wiki?curid=13996", "text": "Heap (data structure)\n\nIn computer science, a heap is a specialized tree-based data structure that satisfies the \"heap property:\" If A is a parent node of B then the \"key\" (the \"value\") of node A is ordered with respect to the key of node B with the same ordering applying across the heap. A heap can be classified further as either a \"max heap\" or a \"min heap\". In a max heap, the keys of parent nodes are always greater than or equal to those of the children and the highest key is in the root node. In a min heap, the keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node.\n\nThe heap is one maximally efficient implementation of an abstract data type called a priority queue, and in fact priority queues are often referred to as \"heaps\", regardless of how they may be implemented. A common implementation of a heap is the binary heap, in which the tree is a complete binary tree (see figure). The heap data structure, specifically the binary heap, was introduced by J. W. J. Williams in 1964, as a data structure for the heapsort sorting algorithm. Heaps are also crucial in several efficient graph algorithms such as Dijkstra's algorithm.\nIn a heap, the highest (or lowest) priority element is always stored at the root. A heap is not a sorted structure and can be regarded as partially ordered. As visible from the heap-diagram, there is no particular relationship among nodes on any given level, even among the siblings. When a heap is a complete binary tree, it has a smallest possible height—a heap with N nodes always has log N height. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority.\n\nNote that, as shown in the graphic, there is no implied ordering between siblings or cousins and no implied sequence for an in-order traversal (as there would be in, e.g., a binary search tree). The heap relation mentioned above applies only between nodes and their parents, grandparents, etc. The maximum number of children each node can have depends on the type of heap, but in many types it is at most two, which is known as a binary heap.\n\nThe common operations involving heaps are:\n\n\n\n\nHeaps are usually implemented in an array (fixed size or dynamic array), and do not require pointers between elements. After an element is inserted into or deleted from a heap, the heap property may be violated and the heap must be balanced by internal operations.\n\nFull and almost full binary heaps may be represented in a very space-efficient way (as an implicit data structure) using an array alone. The first (or last) element will contain the root. The next two elements of the array contain its children. The next four contain the four children of the two child nodes, etc. Thus the children of the node at position \"n\" would be at positions 2n and 2n + 1 in a one-based array, or 2n + 1 and 2n + 2 in a zero-based array. This allows moving up or down the tree by doing simple index computations. Balancing a heap is done by sift-up or sift-down operations (swapping elements which are out of order). As we can build a heap from an array without requiring extra memory (for the nodes, for example), heapsort can be used to sort an array in-place.\n\nDifferent types of heaps implement the operations in different ways, but notably, insertion is often done by adding the new element at the end of the heap in the first available free space. This will generally violate the heap property, and so the elements are then sifted up until the heap property has been reestablished. Similarly, deleting the root is done by removing the root and then putting the last element in the root and sifting down to rebalance. Thus replacing is done by deleting the root and putting the \"new\" element in the root and sifting down, avoiding a sifting up step compared to pop (sift down of last element) followed by push (sift up of new element).\n\nConstruction of a binary (or \"d\"-ary) heap out of a given array of elements may be performed in linear time using the classic Floyd algorithm, with the worst-case number of comparisons equal to 2\"N\" − 2\"s\"(\"N\") − \"e\"(\"N\") (for a binary heap), where \"s\"(\"N\") is the sum of all digits of the binary representation of \"N\" and \"e\"(\"N\") is the exponent of 2 in the prime factorization of \"N\". This is faster than a sequence of consecutive insertions into an originally empty heap, which is log-linear (or linearithmic).\n\nThe heap data structure has many applications.\n\n\n\n\n", "id": "13996", "title": "Heap (data structure)"}
{"url": "https://en.wikipedia.org/wiki?curid=13998", "text": "Hierarchy\n\nA hierarchy (from the Greek \"hierarchia\", \"rule of a high priest\", from \"hierarkhes\", \"leader of sacred rites\") is an arrangement of items (objects, names, values, categories, etc.) in which the items are represented as being \"above,\" \"below,\" or \"at the same level as\" one another.\n\nA hierarchy can link entities either directly or indirectly, and either vertically or diagonally. The only direct links in a hierarchy, insofar as they are hierarchical, are to one's immediate superior or to one of one's subordinates, although a system that is largely hierarchical can also incorporate alternative hierarchies. Indirect hierarchical links can extend \"vertically\" upwards or downwards via multiple links in the same direction, following a path. All parts of the hierarchy which are not linked vertically to one another nevertheless can be \"horizontally\" linked through a path by traveling up the hierarchy to find a common direct or indirect superior, and then down again. This is akin to two co-workers or colleagues; each reports to a common superior, but they have the same relative amount of authority. Organizational forms exist that are both alternative and complementary to hierarchy. Heterarchy is one such form.\n\nA hierarchy is a system or organization in which people or groups are ranked one above the other according to status or authority.\n\nHierarchies have their own special vocabulary. These terms are easiest to understand when a hierarchy is diagrammed (see below).\n\nIn an organizational context, the following terms are often used related to hierarchies:\n\nIn a mathematical context (in graph theory), the general terminology used is different.\n\nMost hierarchies use a more specific vocabulary pertaining to their subject, but the idea behind them is the same. For example, with data structures, objects are known as nodes, superiors are called parents and subordinates are called children. In a business setting, a superior is a supervisor/boss and a peer is a colleague.\n\nDegree of branching refers to the number of direct subordinates or children an object has (in graph theory, equivalent to the number of other vertices connected to via outgoing arcs, in a directed graph) a node has). Hierarchies can be categorized based on the \"maximum degree\", the highest degree present in the system as a whole. Categorization in this way yields two broad classes: \"linear\" and \"branching\".\n\nIn a linear hierarchy, the maximum degree is 1. In other words, all of the objects can be visualized in a lineup, and each object (excluding the top and bottom ones) has exactly one direct subordinate and one direct superior. Note that this is referring to the \"objects\" and not the \"levels\"; every hierarchy has this property with respect to levels, but normally each level can have an infinite number of objects. An example of a linear hierarchy is the hierarchy of life.\n\nIn a 'branching hierarchy', one or more objects has a degree of 2 or more (and therefore the minimum degree is 2 or higher). For many people, the word \"hierarchy\" automatically evokes an image of a branching hierarchy. Branching hierarchies are present within numerous systems, including organizations and classification schemes. The broad category of branching hierarchies can be further subdivided based on the degree.\n\nA 'flat hierarchy' is a branching hierarchy in which the maximum degree approaches infinity, i.e., that has a wide span. Most often, systems intuitively regarded as hierarchical have at most a moderate span. Therefore, a flat hierarchy is often not viewed as a hierarchy at all. For example, diamonds and graphite are flat hierarchies of numerous carbon atoms which can be further decomposed into subatomic particles.\n\nAn 'overlapping hierarchy' is a branching hierarchy in which at least one object has two parent objects. For example, a graduate student can have two co-supervisors to whom the student reports directly and equally, and who have the same level of authority within the university hierarchy (i.e., they have the same position or tenure status.\n\nPossibly the first use of the English word \"hierarchy\" cited by the Oxford English Dictionary was in 1880, when it was used in reference to the three orders of three angels as depicted by Pseudo-Dionysius the Areopagite (5th–6th centuries). Pseudo-Dionysius used the related Greek word (\"hierarchia\") both in reference to the celestial hierarchy and the ecclesiastical hierarchy. The Greek term \"ἱεραρχία\" means \"rule by priests\" (from \"ἱεράρχης\" – \"ierarches\", meaning \"president of sacred rites, high-priest\" and that from \"ἱερεύς\" – \"iereus\", \"priest\" + \"ἀρχή\" – \"arche\", amongst others \"first place or power, rule\"), and Dionysius is credited with first use of it as an abstract noun. Since hierarchical churches, such as the Roman Catholic (see Catholic Church hierarchy) and Eastern Orthodox churches, had tables of organization that were \"hierarchical\" in the modern sense of the word (traditionally with God as the pinnacle or head of the hierarchy), the term came to refer to similar organizational methods in secular settings.\n\nA hierarchy is typically depicted as a pyramid, where the height of a level represents that level's status and width of a level represents the quantity of items at that level relative to the whole. For example, the few Directors of a company could be at the apex, and the base could be thousands of people who have no subordinates.\n\nThese pyramids are typically diagrammed with a tree or triangle diagram (but note that not all triangle/pyramid diagrams are hierarchical, for example, the 1992 USDA food guide pyramid), both of which serve to emphasize the size differences between the levels. An example of a triangle diagram appears to the right. An organizational chart is the diagram of a hierarchy within an organization, and is depicted in tree form below.\n\nMore recently, as computers have allowed the storage and navigation of ever larger data sets, various methods have been developed to represent hierarchies in a manner that makes more efficient use of the available space on a computer's screen. Examples include fractal maps, TreeMaps and Radial Trees.\n\nIn the design field, mainly graphic design, successful layouts and formatting of the content on documents are heavily dependent on the rules of visual hierarchy. Visual hierarchy is also important for proper organization of files on computers.\n\nAn example of visually representing hierarchy is through the Nested clusters. The Nested clusters represents hierarchical relationships by using layers of information. The child element is within the parent element, such as in a Venn diagram. This structure of representing hierarchy is most effective in representing simple relationships. For example, when directing someone to open a file on a computer desktop, one may first direct them towards the main folder, then the subfolders within the main folder. They will keep opening files within the folders until the designated file is located.\n\nFor more complicated hierarchies, the stair structure represents hierarchical relationships through the use of visual stacking. Visually imagine the top of a downward staircase beginning at the left and descending on the right. The child elements are towards the bottom of the stairs and the parent elements are at the top. This structure is effective when representing more complicated hierarchies where steps are not placed in obvious sequences. Further steps are concealed unless all of the steps are revealed in sequence. In the computer desktop example, a file that is being sought after can only be found once another file is opened. The link for the desired file is within another document. All the steps must be completed until the final destination is reached.\n\nIn plain English, a hierarchy can be thought of as a set in which:\nThe first requirement is also interpreted to mean that a hierarchy can have no circular relationships; the association between two objects is always transitive.\nThe second requirement asserts that a hierarchy must have a leader or root that is common to all of the objects.\n\nMathematically, in its most general form, a hierarchy is a partially ordered set or \"poset\". The system in this case is the entire poset, which is constituted of elements. Within this system, each element shares a particular unambiguous property. Objects with the same property value are grouped together, and each of those resulting levels is referred to as a class.\n\n\"Hierarchy\" is particularly used to refer to a poset in which the classes are organized in terms of increasing complexity. \nOperations such as addition, subtraction, multiplication and division are often performed in a certain sequence or order. Usually, addition and subtraction are performed after multiplication and division has already been applied to a problem. The use of parenthesis is also a representation of hierarchy, for they show which operation is to be done prior to the following ones. For example:\n(2 + 5) × (7 - 4).\nIn this problem, typically one would multiply 5 by 7 first, based on the rules of mathematical hierarchy. But when the parentheses are placed, one will know to do the operations within the parentheses first before continuing on with the problem. These rules are largely dominant in algebraic problems, ones that include several steps in order to solve. The use of hierarchy in mathematics is beneficial in order to quickly and efficiently solve a problem without having to go through the process of slowly dissecting the problem. Most of these rules are now known as the proper way into solving certain equations.\n\nA nested hierarchy or \"inclusion hierarchy\" is a hierarchical ordering of nested sets. The concept of nesting is exemplified in Russian matryoshka dolls. Each doll is encompassed by another doll, all the way to the outer doll. The outer doll holds all of the inner dolls, the next outer doll holds all the remaining inner dolls, and so on. Matryoshkas represent a nested hierarchy where each level contains only one object, i.e., there is only one of each size of doll; a generalized nested hierarchy allows for multiple objects within levels but with each object having only one parent at each level. The general concept is both demonstrated and mathematically formulated in the following example:\n\nA square can always also be referred to as a quadrilateral, polygon or shape. In this way, it is a hierarchy. However, consider the set of polygons using this classification. A square can \"only\" be a quadrilateral; it can never be a triangle, hexagon, etc.\n\nNested hierarchies are the organizational schemes behind taxonomies and systematic classifications. For example, using the original Linnaean taxonomy (the version he laid out in the 10th edition of \"Systema Naturae\"), a human can be formulated as:\n\nTaxonomies may change frequently (as seen in biological taxonomy), but the underlying concept of nested hierarchies is always the same.\n\nIn many programming taxonomies and syntax models (as well as fractals in mathematics), nested hierarchies, including Russian dolls, are also used to illustrate the properties of Self-similarity and Recursion. Recursion itself is included as a subset of hierarchical programming, and recursive thinking can be synonymous with a form of hierarchical thinking and logic.\n\nA containment hierarchy is a direct extrapolation of the nested hierarchy concept. All of the ordered sets are still nested, but every set must be \"strict\"—no two sets can be identical. The shapes example above can be modified to demonstrate this:\n\nThe notation formula_4 means \"x\" is a subset of \"y\" but is not equal to \"y\".\n\nA general example of a containment hierarchy is demonstrated in class inheritance in object-oriented programming.\n\nTwo types of containment hierarchies are the \"subsumptive\" containment hierarchy and the \"compositional\" containment hierarchy. A subsumptive hierarchy \"subsumes\" its children, and a compositional hierarchy is \"composed\" of its children. A hierarchy can also be both subsumptive \"and\" compositional.\n\nA \"subsumptive\" containment hierarchy is a classification of object classes from the general to the specific. Other names for this type of hierarchy are \"taxonomic hierarchy\" and \"IS-A hierarchy\". The last term describes the relationship between each level—a lower-level object \"is a\" member of the higher class. The taxonomical structure outlined above is a subsumptive containment hierarchy. Using again the example of Linnaean taxonomy, it can be seen that an object that is part of the level \"Mammalia\" \"is a\" member of the level \"Animalia\"; more specifically, a human \"is a\" primate, a primate \"is a\" mammal, and so on. A subsumptive hierarchy can also be defined abstractly as a hierarchy of \"concepts\". For example, with the Linnaean hierarchy outlined above, an entity name like \"Animalia\" is a way to group all the species that fit the conceptualization of an animal.\n\nA \"compositional\" containment hierarchy is an ordering of the parts that make up a system—the system is \"composed\" of these parts. Most engineered structures, whether natural or artificial, can be broken down in this manner.\n\nThe compositional hierarchy that every person encounters at every moment is the hierarchy of life. Every person can be reduced to organ systems, which are composed of organs, which are composed of tissues, which are composed of cells, which are composed of molecules, which are composed of atoms. In fact, the last two levels apply to all matter, at least at the macroscopic scale. Moreover, each of these levels inherit all the properties of their children.\n\nIn this particular example, there are also \"emergent properties\"—functions that are not seen at the lower level (e.g., cognition is not a property of neurons but is of the brain)—and a scalar quality (molecules are bigger than atoms, cells are bigger than molecules, etc.). Both of these concepts commonly exist in compositional hierarchies, but they are not a required general property. These \"level hierarchies\" are characterized by bi-directional causation. \"Upward causation\" involves lower-level entities causing some property of a higher level entity; children entities may interact to yield parent entities, and parents are composed at least partly by their children. \"Downward causation\" refers to the effect that the incorporation of entity \"x\" into a higher-level entity can have on \"x\"'s properties and interactions. Furthermore, the entities found at each level are \"autonomous\".\n\nAlmost every system of organization applied to the world is arranged hierarchically. By their common definitions, every nation has a government and every government is hierarchical. Socioeconomic systems are stratified into a social hierarchy (the social stratification of societies), and all systematic classification schemes (taxonomies) are hierarchical. Most organized religions, regardless of their internal governance structures, operate as a hierarchy under God. Many Christian denominations have an autocephalous ecclesiastical hierarchy of leadership. Families are viewed as a hierarchical structure in terms of cousinship (e.g., first cousin once removed, second cousin, etc.), ancestry (as depicted in a family tree) and inheritance (succession and heirship). All the requisites of a well-rounded life and lifestyle can be organized using Maslow's hierarchy of human needs. Learning must often follow a hierarchical scheme—to learn differential equations one must first learn calculus; to learn calculus one must first learn elementary algebra; and so on. Even nature itself has its own hierarchies, as numerous schemes such as Linnaean taxonomy, the organization of life, and biomass pyramids attempt to document. Hierarchies are so infused into daily life that they are viewed as trivial.\n\nWhile the above examples are often clearly depicted in a hierarchical form and are classic examples, hierarchies exist in numerous systems where this branching structure is not immediately apparent. For example, most postal code systems are hierarchical. Using the Canadian postal code system, the top level's binding concept is the \"postal district\", and consists of 18 objects (letters). The next level down is the \"zone\", where the objects are the digits 0–9. This is an example of an overlapping hierarchy, because each of these 10 objects has 18 parents. The hierarchy continues downward to generate, in theory, 7,200,000 unique codes of the format \"A0A 0A0\" (the second and third letter position allow 20 objects each). Most library classification systems are also hierarchical. The Dewey Decimal System is regarded as infinitely hierarchical because there is no finite bound on the number of digits can be used after the decimal point.\n\nOrganizations can be structured as a dominance hierarchy. In an organizational hierarchy, there is a single person or group with the most power and authority, and each subsequent level represents a lesser authority. Most organizations are structured in this manner, including governments, companies, militia and organized religions. The units or persons within an organization are depicted hierarchically in an organizational chart.\n\nIn a reverse hierarchy, the conceptual pyramid of authority is turned upside-down, so that the apex is at the bottom and the base is at the top. This model represents the idea that members of the higher rankings are responsible for the members of the lower rankings.\n\nWithin most CGI and computer animation programs is the use of hierarchies. On a 3D model of a human, the chest is a parent of the upper left arm, which is a parent of the lower left arm, which is a parent of the hand. This is used in modeling and animation of almost everything built as a 3D digital model.\n\nMany grammatical theories, such as phrase-structure grammar, involve hierarchy.\n\nDirect–inverse languages such as Cree and Mapudungun distinguish subject and object on verbs not by different subject and object markers, but via a hierarchy of persons.\n\nIn this system, the three (or four with Algonquian languages) persons are placed in a hierarchy of salience. To distinguish which is subject and which object, \"inverse markers\" are used if the object outranks the subject.\n\nThe structure of a musical composition is often understood hierarchically (for example by Heinrich Schenker (1768–1835, see Schenkerian analysis), and in the (1985) Generative Theory of Tonal Music, by composer Fred Lerdahl and linguist Ray Jackendoff). The sum of all notes in a piece is understood to be an all-inclusive surface, which can be reduced to successively more sparse and more fundamental types of motion. The levels of structure that operate in Schenker's theory are the foreground, which is seen in all the details of the musical score; the middle ground, which is roughly a summary of an essential contrapuntal progression and voice-leading; and the background or Ursatz, which is one of only a few basic \"long-range counterpoint\" structures that are shared in the gamut of tonal music literature.\n\nThe pitches and form of tonal music are organized hierarchically, all pitches deriving their importance from their relationship to a tonic key, and secondary themes in other keys are brought back to the tonic in a recapitulation of the primary theme. Susan McClary connects this specifically in the sonata-allegro form to the feminist hierarchy of gender (see above) in her book \"Feminine Endings\", even pointing out that primary themes were often previously called \"masculine\" and secondary themes \"feminine.\"\n\nIn ethics, various virtues are enumerated and sometimes organized hierarchically according to certain brands of virtue theory.\n\nIn some of these random examples, there is an asymmetry of 'compositional' significance between levels of structure, so that small parts of the whole hierarchical array depend, for their meaning, on their membership in larger parts.There is a hierarchy of activities in human life: productive activity serves or is guided by the moral life; the moral life is guided by practical reason; practical reason (used in moral and political life) serves contemplative reason (whereby we contemplate God). Practical reason sets aside time and resources for contemplative reason.\n\nIn the work of diverse theorists such as William James (1842–1910), Michel Foucault (1926–1984) and Hayden White, important critiques of hierarchical epistemology are advanced. James famously asserts in his work \"Radical Empiricism\" that clear distinctions of type and category are a constant but unwritten goal of scientific reasoning, so that when they are discovered, success is declared. But if aspects of the world are organized differently, involving inherent and intractable ambiguities, then scientific questions are often considered unresolved.\n\nHierarchy in ethics emerged in Western Europe, West Asia and North Africa around the 1600s. In this aspect, the term hierarchy refers to how distinguishable they are from real to unreal. Feminists, Marxists, anarchists, communists, critical theorists and others, all of whom have multiple interpretations, criticize the hierarchies commonly found within human society, especially in social relationships. Hierarchies are present in all parts of society: in businesses, schools, families, etc. These relationships are often viewed as necessary. Entities that stand in hierarchical arrangements are animals, humans, plants, etc. In some cultures, God can also be an addition to this hierarchy. However, feminists, Marxists, critical theorists and others analyze hierarchy in terms of the values and power that it arbitrarily assigns to one group over another. Hierarchical ethics offers a way of logical reasoning that is compatible with religious commitments. In some cultures, there is hierarchy within humanity. The dominant man in a family is above women, and children are after. In social classes, they are arranged as follows: king, civic officials, craftsmen, unskilled workers.\n\n\n\n\n\n\n\n\n\n\n", "id": "13998", "title": "Hierarchy"}
{"url": "https://en.wikipedia.org/wiki?curid=14002", "text": "Outline of health sciences\n\nThe following outline is provided as an overview of and topical guide to health sciences:\n\nHealth sciences – are applied sciences that address the use of science, technology, engineering or mathematics in the delivery of healthcare.\n\n\n\n\n\n\n\n\n\n", "id": "14002", "title": "Outline of health sciences"}
{"url": "https://en.wikipedia.org/wiki?curid=14004", "text": "Hour\n\nThe hour (common symbol: h or hr, h being the international form of the symbol) is a unit of measurement of time. In modern usage, an hour comprises 60 minutes, or 3,600 seconds. It is approximately of a mean solar day.\n\nAn hour in the Universal Coordinated Time (UTC) time standard can include a negative or positive leap second, and may therefore have a duration of 3,599 or 3,601 seconds for adjustment purposes.\n\nAlthough it is not a standard defined by the International System of Units (SI), the hour is a unit accepted for use with SI, represented by the symbol h.\n\nThe Middle English word \"ure\" first appears in the 13th century, as a loanword from Old French \"ure\", \"ore\", from Latin \"hōra\". \"Hora\", in turn, derives from Greek (\"season, time of day, hour\"). In terms of the Proto-Indo-European language, is a cognate of English \"year\" and is derived from the Proto-Indo-European word \"\" (\"year, summer\").\n\nThe \"ure\" of Middle English and the Anglo-French \"houre\" gradually supplanted the Old English nouns \"tīd\" (which survives in Modern English as \"tide\") and \"stund\". \"Stund\" is the progenitor of \"\", which remains an archaic synonym for \"hour\". \"Stund\" is related to the Old High German \"stunta\" (whence German \"Stunde\", \"hour, lesson\"), from Germanic \"*stundō\" (\"time, interval, while\").\n\nAncient Egyptians used sundials that \"divided a sunlit day into 10 parts plus two 'twilight hours' in the morning and evening\". The Greek astronomer Andronicus of Cyrrhus oversaw the construction of a horologion called the Tower of the Winds in Athens during the first century BCE. This structure tracked a 24-hour day using both sundials and mechanical hour indicators.\n\nAncient Sumer and India also divided days into either 1/12 of the time between sunrise and sunset or 1/24 of a full day. In either case the division reflected the widespread use of a duodecimal numbering system. The importance of 12 has been attributed to the number of lunar cycles in a year. In China, the whole day was divided into 12 parts.\n\nAstronomers in Egypt's Middle Kingdom (9th and 10th dynasties) observed a set of 36 decan stars throughout the year. These star tables have been found on the lids of coffins of the period. The heliacal rising of the next decan star marked the start of a new civil week, which was then 10 days. The period from sunset to sunrise was marked by 18 decan stars. Three of these were assigned to each of the two twilight periods, so the period of total darkness was marked by the remaining 12 decan stars, resulting in the 12 divisions of the night. The time between the appearance of each of these decan stars over the horizon during the night would have been about 40 modern minutes. During the New Kingdom, the system was simplified, using a set of 24 stars, 12 of which marked the passage of the night.\n\nAncient Sinhalese in Sri Lanka divided a solar day into 60 \"peya\" (now called Sinhala peya). One peya was divided into 24 \"vinadi\". Since 60 (peya) × 24 (vinadi) = 24 (hours) × 60 (minutes), one \"vinadi\" is equal to one minute.\n\nEarlier definitions of the hour varied within these parameters:\n\nMany different ways of counting the hours have been used. Because sunrise, sunset, and, to a lesser extent, noon, are the conspicuous points in the day, starting to count at these times was, for most people in most early societies, much easier than starting at midnight. However, with accurate clocks and modern astronomical equipment (and the telegraph or similar means to transfer a time signal in a split-second), this issue is much less relevant.\n\nAstrolabes, sundials, and astronomical clocks sometimes show the hour length and count using some of these older definitions and counting methods.\n\nIn ancient and medieval cultures, the counting of hours generally started with sunrise. Before the widespread use of artificial light, societies were more concerned with the division between night and day, and daily routines often began when light was sufficient.\n\nSunrise marked the beginning of the first hour (the \"zero\" hour), the middle of the day was at the end of the sixth hour and sunset at the end of the twelfth hour. This meant that the duration of hours varied with the season. In the Northern hemisphere, particularly in the more northerly latitudes, summer daytime hours were longer than winter daytime hours, each being one twelfth of the time between sunrise and sunset. These variable-length hours were variously known as temporal, unequal, or seasonal hours and were in use until the appearance of the mechanical clock, which furthered the adoption of equal length hours.\n\nThis is also the system used in Jewish law and frequently called \"Talmudic hour\" (\"Sha'a Zemanit\") in a variety of texts. The talmudic hour is one twelfth of time elapsed from sunrise to sunset, day hours therefore being longer than night hours in the summer; in winter they reverse.\n\nThe Indic day began at sunrise. The term \"hora\" was used to indicate an hour. The time was measured based on the length of the shadow at day time. A \"hora\" translated to 2.5 \"pe\". There are 60 \"pe\" per day, 60 minutes per \"pe\" and 60 \"kshana\" (snap of a finger or instant) per minute. \"Pe\" was measured with a bowl with a hole placed in still water. Time taken for this graduated bowl was one \"pe\". Kings usually had an officer in charge of this clock.\n\nBabylonian hours divide the day and night into 24 equal hours, reckoned from the time of sunrise.\n\nIn so-called \"Italian time\", \"Italian hours\", or \"old Czech time\", the first hour started with the sunset Angelus bell (or at the end of dusk, i.e., half an hour after sunset, depending on local custom and geographical latitude). The hours were numbered from 1 to 24. For example, in Lugano, the sun rose in December during the 14th hour and noon was during the 19th hour; in June the Sun rose during the 7th hour and noon was in the 15th hour. Sunset was always at the end of the 24th hour. The clocks in church towers struck only from 1 to 12, thus only during night or early morning hours.\n\nThis manner of counting hours had the advantage that everyone could easily know how much time they had to finish their day's work without artificial light. It was already widely used in Italy by the 14th century and lasted until the mid-18th century; it was officially abolished in 1755, or in some regions, customary, until the mid-19th century.\n\nThe system of Italian hours can be seen on a number of clocks in Europe, where the dial is numbered from 1 to 24 in either Roman or Arabic numerals. The St Mark's Clock in Venice, and the Orloj in Prague are famous examples. It was also used in Poland and Bohemia until the 17th century.\n\nThe Islamic day begins at sunset. The first prayer of the day (maghrib) is to be performed between just after sunset and the end of twilight.\n\nFor many centuries, up to 1925, astronomers counted the hours and days from noon, because it was the easiest solar event to measure accurately. An advantage of this method (used in the Julian Date system, in which a new Julian Day begins at noon) is that the date doesn't change during a single night's observing.\n\nIn the modern 12-hour clock, counting the hours starts at midnight and restarts at noon. Hours are numbered 12, 1, 2, ..., 11. Solar noon is always close to 12 noon (ignoring artificial adjustments due to time zones and daylight saving time), differing according to the equation of time by as much as fifteen minutes either way. At the equinoxes sunrise is around 6 a.m. (, before noon), and sunset around 6 p.m. (, after noon).\n\nIn the modern 24-hour clock, counting the hours starts at midnight, and hours are numbered from 0 to 23. Solar noon is always close to 12:00, again differing according to the equation of time. At the equinoxes sunrise is around 06:00, and sunset around 18:00.\n\nAlthough the SI unit for speed is metre per second, in everyday usage kilometre per hour or, in the UK and US, mile per hour are more practical. The knot, defined as one nautical mile per hour, is a unit of speed commonly used in maritime and air navigation. Occasionally the metre per hour is used for slow-moving objects like snails.\n\nWorker compensation is commonly based on working time in terms of number of hours worked, referred to as an hourly wage. Worker schedules are categorized by number of work hours per day or number of work hours per week; these are regulated and distinguish part-time from full-time jobs. The man-hour describes the amount of work that a person can complete in one hour, or alternately the length of time required to complete a job or time worked since a lost-time accident. Many professionals such as lawyers and therapists charge a fee per hour.\n\nThe credit hour measures the time commitment of an academic course (typically university level) in terms of \"contact hours\" between students and staff per week. For example, a class that meets for one hour three times a week is said to be a 3-hour class.\n\nThe kilowatt-hour, the energy expended by a 1000-watt device in one hour, is commonly used as a billing unit for energy delivered to consumers by electric utilities. Conversely, the Btu-hour is a unit of power used in the power industry and heating/cooling applications. In the railroad industry, when sharing locomotives, the horsepower-hour may be used as a measure of energy.\n\nThe ampere-hour is a unit of electric charge used in measurements of electrochemical systems such as electroplating and batteries.\n\nIn indoor air quality, air changes per hour measures how many times the air within a defined space, such as a room or house, is replaced in an hour.\n\nPassengers per hour per direction is used to describe the capacity of public transport systems.\n\nPound per hour is a mass flow unit used for fuel flow in engines.\n\n\n\n", "id": "14004", "title": "Hour"}
{"url": "https://en.wikipedia.org/wiki?curid=14005", "text": "Hezekiah\n\nHezekiah (; Hebrew: ; ; , \"Ezekias\", in the Septuagint; ; also transliterated as \"Ḥizkiyyahu\" or \"Ḥizkiyyah\") was, according to the Hebrew Bible, the son of Ahaz and the 13th king of Judah. Archaeologist Edwin Thiele has concluded that his reign was between c. 715 and 686 BC. He is considered by the author of the book of kings a very righteous king, he is also one of the most prominent kings of Judah mentioned in the Hebrew Bible and is one of the kings mentioned in the genealogy of Jesus in the Gospel of Matthew.\n\nAccording to the Hebrew Bible, Hezekiah witnessed the destruction of the northern Kingdom of Israel by Sargon's Assyrians in c. 720 BC and was king of Judah during the invasion and siege of Jerusalem by Sennacherib in 701 BC. Hezekiah enacted sweeping religious reforms, including a strict mandate for the sole worship of Yahweh and a prohibition on venerating other deities within the Temple in Jerusalem. Isaiah and Micah prophesied during his reign.\n\nHezekiah, more properly transliterated as Ḥizkiyyahu (and sometimes as Ezekias) (Hebrew: Ḥizqiyyāhu, Khizkiyahu; or Yəḥizqiyyāhu, Y'khizkiyahu); ; or Ḥizkiyyah (Hebrew: Ḥizqiyyāh). The root of the name Ḥizkiyyahu is חזק, a verb stem that can mean\n\nIt also spawns a number of nouns, including\n\nas well as the adjectives\n\nAccordingly, Ḥizkiyyahu can be said to mean something like \"\"Strengthened by Yahweh\"\"., or \"Yah is my strength\"/\"My strength is Yah\" (and see Psalms 18:2).\n\nThe main account of Hezekiah's reign is found in , , and of the Hebrew Bible. mentions that it is a collection of King Solomon's proverbs that were \"copied\" \"by the officials of King Hezekiah of Judah\". His reign is also referred to in the books of the prophets Jeremiah, Hosea, and Micah. The books of Hosea and Micah record that their prophecies were made during Hezekiah’s reign.\n\nHezekiah was the son of King Ahaz and Abijah. His mother, Abijah (also called Abi), was a daughter of the high priest Zechariah. Based on Thiele's dating, Hezekiah was born in c. 741 BC. He was married to Hephzi-bah. () He died from natural causes at the age of 54 in c. 687 BC, and was succeeded by his son Manasseh ().\n\nAccording to the Hebrew Bible, Hezekiah assumed the throne of Judah at the age of 25 and reigned for 29 years (). Some writers have proposed that Hezekiah served as coregent with his father Ahaz for about 14 years. His sole reign is dated by William F. Albright as 715–687 BC, and by Edwin R. Thiele as 716–687 BC (the last ten years being a co-regency with his son Manasseh).\n\nHezekiah purified and repaired the Temple, purged its idols, and reformed the priesthood. In an effort to abolish what he considered idolatry from his kingdom, he destroyed the high places (or bamot) and \"bronze serpent\" (or \"Nehushtan\"), recorded as being made by Moses, which became objects of idolatrous worship. In place of this, he centralized the worship of God at the Jerusalem Temple. Hezekiah also resumed the Passover pilgrimage and the tradition of inviting the scattered tribes of Israel to take part in a Passover festival. He sent messengers to Ephraim and Manasseh inviting them to Jerusalem for the celebration of the Passover. The messengers, however, were not only not listened to, but were even laughed at; only a few men of Asher, Manasseh, and Zebulun came to Jerusalem. Nevertheless, the Passover was celebrated with great solemnity and such rejoicing as had not been in Jerusalem since the days of Solomon. Hezekiah is portrayed by the Hebrew Bible as a great and good king.\n\nWhen Sargon II, the king of Assyria, died in 705 BC, states, including Judah, that were subject to Assyria saw an opportunity to throw off their subservience to the Assyrian kings. Hezekiah ceased to pay the tribute imposed on his father, and entered into a league with Egypt. In 703 BC Sennacherib, Sargon's son and successor, began a series of major campaigns to quash opposition to Assyrian rule. After dealing with rebels in the eastern part of the realm, in 701 BC the king turned toward those in the west. Though Hezekiah expected the Egyptians to come to his aid, they did not, and Hezekiah had to face the invasion of Judah by Sennacherib.\n\nThe Assyrians recorded that Sennacherib lifted his siege of Jerusalem after Hezekiah acknowledged Sennacherib as his overlord and paid him tribute. The Hebrew Bible records that Hezekiah tried to pay off Sennacherib with three hundred talents of silver and thirty of gold as tribute, even despoiling the doors of the Temple to produce the promised amount, but, after the payment was made, Sennacherib renewed his assault on Jerusalem. Sennacherib besieged Jerusalem and sent his Rabshakeh to the walls as a messenger. The Rabshakeh addressed the soldiers manning the city wall in Hebrew (\"Yĕhuwdiyth\"), asking them to distrust Yahweh and Hezekiah, pointing to Hezekiah's righteous reforms (destroying the High Places) as a sign that the people should not trust their god to be favorably disposed (). records that Hezekiah went to the Temple and there he prayed.\n\nKnowing that Jerusalem would eventually be subject to siege, Hezekiah had been preparing for war for some time by fortifying the walls of Jerusalem, building towers, and constructing a tunnel to bring fresh water to the city from a spring outside its walls. He made at least two major preparations that would help Jerusalem to resist conquest: the construction of the Siloam Tunnel, and construction of the Broad Wall.\n\n\"When Sennacherib had come, intent on making war against Jerusalem, Hezekiah consulted with his officers and warriors about stopping the flow of the springs outside the city … for otherwise, they thought, the King of Assyria would come and find water in abundance\" (). The narratives of the Hebrew Testament state that Sennacherib besieged Jerusalem (; ; ; ).\n\nSennacherib failed to conquer Judah in full before his death.\n\n\"It came about as he was worshiping in the house of Nisroch his god, that Adrammelech and Sharezer killed him [Sennacherib] with the sword; and they escaped into the land of Ararat. And Esarhaddon his son became king in his place.\"\n\nAssyrian records say that Sennacherib was assassinated in 681 BC (20 years after the 701 BC invasion of Judah). A Neo-Babylonian letter corroborates with the biblical account a sentiment from Sennacherib’s sons to assassinate him, an event Assyriologists have reconstructed as historical. The son Ardi-Mulishi, who is mentioned in the letter as killing anyone who would reveal his conspiracy, successfully murders his father in c. 681 BC, and was most likely the Adrammelech in 2 Kings, though Sharezer is not known elsewhere. Assyryologists posit the murder was motivated because Esarhaddon was chosen as heir to the throne instead of Ardi-Mulishi, the next eldest son. Assyrian and Hebrew biblical history corroborate that Esarhaddon ultimately did succeed the throne. Other Assyriologists assert that Sennacherib was murdered in revenge for his destruction of Babylon, a city sacred to all Mesopotamians, including the Assyrians.\n\nThe narrative of Hezekiah's sickness and miraculous recovery is found in , , . Various ambassadors came to congratulate him on his recovery, among them from Merodach-baladan, the king of Babylon (; 2 Kings 20:12). Hezekiah is also remembered for giving too much information to Baladan, king of Babylon (or perhaps for boasting about his wealth), for which he was confronted by Isaiah the prophet ().\n\nAccording to the Talmud, the disease came about because of a dispute between him and Isaiah over who should pay whom a visit and over Hezekiah's refusal to marry and have children. According to , Hezekiah lived another 15 more years of life after praying to God. Some Talmudists also considered that it might have come about as a way for Hezekiah to purge his sins or due to his arrogance in assuming his righteousness.\n\nExtra-Biblical sources do much more for us than give us a pan-Mid Eastern picture into which we contextualize Hezekiah: there are extra-Biblical sources that specify Hezekiah by name, along with his reign and influence. \"Historiographically, his reign is noteworthy for the convergence of a variety of biblical sources and diverse extrabiblical evidence often bearing on the same events. Significant data concerning Hezekiah appear in the Deuteronomistic History, the Chronicler, Isaiah, Assyrian annals and reliefs, Israelite epigraphy, and, increasingly, stratigraphy\". Archaeologist Amihai Mazar calls the tensions between Assyria and Judah \"one of the best-documented events of the Iron Age\" (172). Hezekiah's story is one of the best to cross-reference with the rest of the Mid Eastern world's historical documents.\n\nA seal impression dating back to 727–698 BCE, reading \"לחזקיהו [בן] אחז מלך יהדה\" \"Belonging to Hezekiah [son of] Ahaz king of Judah\" was uncovered in a dig at the Ophel in Jerusalem. the impression on this inscription was set in ancient Hebrew script.\n\nA lintel inscription, found over the doorway of a tomb, has been ascribed to his secretary, Shebnah ().\nLMLK store jars along the border with Assyria \"demonstrate careful preparations to counter Sennacherib's likely route of invasion\" and show \"a notable degree of royal control of towns and cities which would facilitate Hezekiah's destruction of rural sacrificial sites and his centralization of worship in Jerusalem\". Evidence suggests they were used throughout his 29-year reign (Grena, 2004, p. 338). There are some Bullae from sealed documents that may have belonged to Hezekiah himself (Grena, 2004, p. 26, Figs. 9 and 10). There are also some that name his servants (\"ah-vah-deem\" in Hebrew, ayin-bet-dalet-yod-mem). However, they are all from the antiquities market and subject to authentication disputes (see Biblical archaeology).\n\nAccording to the work of archaeologists and philologists, the reign of Hezekiah saw a notable increase in the power of the Judean state. At this time Judah was the strongest nation on the Assyrian-Egyptian frontier. There were increases in literacy and in the production of literary works. The massive construction of the Broad Wall was made during his reign, the city was enlarged to accommodate a large influx, and population increased in Jerusalem up to 25,000, \"five times the population under Solomon.\" Archaeologist Amihai Mazar explains, \"Jerusalem was a virtual city-state where the majority of the state's population was concentrated,\" in comparison to the rest of Judah's cities (167). Archaeologist Israel Finkelstein says, \"The key phenomenon—which cannot be explained solely against the background of economic prosperity—was the sudden growth of the population of Jerusalem in particular, and of Judah in general\" (153). He says the cause of this growth must be a large influx of Israelites fleeing from the Assyrian destruction of the northern state. It is \"[t]he only reasonable way to explain this unprecedented demographic development\" (154). This, according to Finkelstein, set the stage for motivations to compile and reconcile Hebrew history into a text at that time (157). Mazar questions this explanation, since, he argues, it is \"no more than an educated guess\" (167).\n\nThe Siloam Tunnel was chiseled through 533 meters (1,750 feet) of solid rock in order to provide Jerusalem underground access to the waters of the Gihon Spring or Siloam Pool, which lay outside the city.\n\nThe Siloam Inscription from the Siloam Tunnel is now in the Istanbul Archeological Museum. It \"commemorates the dramatic moment when the two original teams of tunnelers, digging with picks from opposite ends of the tunnel, met each other\" (564). It is \"[o]ne of the most important ancient Hebrew inscriptions ever discovered.\" Finkelstein and Mazar cite this tunnel as an example of Jerusalem's impressive state-level power at the time.\n\nArcheologists like William G. Dever have pointed at archaeological evidence for the iconoclasm during the period of Hezekiah's reign. The central cult room of the temple at Arad (a royal Judean fortress) was deliberately and carefully dismantled, \"with the altars and massebot\" concealed \"beneath a Str. 8 plaster floor\". This stratum correlates with the late 8th century; Dever concludes that \"the deliberate dismantling of the temple and its replacement by another structure in the days of Hezekiah is an archeological fact. I see no reason for skepticism here.\"\n\nUnder Rehoboam, Lachish became the second most important city of the kingdom of Judah. During the revolt of king Hezekiah against Assyria, it was captured by Sennacherib despite determined resistance (see Siege of Lachish).\n\nAs the Lachish relief attests, Sennacherib began his siege of the city of Lachish in 701 BC. The Lachish Relief graphically depicts the battle, and the defeat of the city, including Assyrian archers marching up a ramp and Judahites pierced through on mounted stakes. \"The reliefs on these slabs\" discovered in the Assyrian palace at Nineveh \"originally formed a single, continuous work, measuring 8 feet ... tall by 80 feet ... long, which wrapped around the room\" (559). Visitors \"would have been impressed not only by the magnitude of the artwork itself but also by the magnificent strength of the Assyrian war machine.\"\n\nSennacherib's Prism was found buried in the foundations of the Nineveh palace. It was written in cuneiform, the Mesopotamian form of writing of the day. The prism records the conquest of 46 strong towns and \"uncountable smaller places,\" along with the siege of Jerusalem where Sennacherib says he just \"shut him up...like a bird in a cage,\" subsequently enforcing a larger tribute upon him.\n\nThe Hebrew Bible states that during the night, the angel of Jehovah (YHWH Hebrew) brought death to 185,000 Assyrians troops (), forcing the army to abandon the siege, yet it also records a tribute paid to Sennacherib of 300 silver talents following the siege. There is no account of the supernatural event in the prism. Sennacherib's account records his levying of a tribute from Hezekiah, the king of Judea, who was within Jerusalem, leaving the city as the only one intact following the exile of the northern ten-tribe kingdom of Israel due to idolatry. (2 Kings 17:22,23; 2 Kings 18:1-8) Sennacherib recorded a payment of 800 silver talents, which suggests a capitulation to end the siege. However, Inscriptions have been discovered describing Sennacherib’s defeat of the Ethiopian forces. These say: “As to Hezekiah, the Jew, he did not submit to my yoke, I laid siege to 46 of his strong cities . . . and conquered (them) . . . Himself I made a prisoner in Jerusalem, his royal residence, like a bird in a cage.” (Ancient Near Eastern Texts, p. 288) He does not claim to have captured the city. This supports the Bible account of Hezekiah’s revolt against Assyria and Sennacherib’s failure to take Jerusalem. In the custom of the inscriptions of the pagan kings, to exalt themselves, Sennacherib in this inscription exaggerates the amount of silver paid by Hezekiah, as 800 talents, in contrast with the Bible’s 300. Furthermore, the annals record a list of booty sent from Jerusalem to Nineveh. Some theorize Hezekiah remained on his throne as a vassal ruler. (The campaign is recorded with differences in the Assyrian records and in the biblical Books of Kings; there is agreement that the Assyrian have a propensity for exaggeration).\n\nOne theory that takes the biblical view posits that a defeat was caused by \"possibly an outbreak of the bubonic plague\" (303). Another that this is a composite text which makes use of a 'legendary motif' analogous to that of the exodus story.\n\n\nThe Talmud (Bava Batra 15a) credits Hezekiah with overseeing the compilation of the biblical books of Isaiah, Proverbs, Song of Songs and Ecclesiastes.\n\nAccording to Jewish tradition, the victory over the Assyrians and Hezekiah's return to health happened at the same time, the first night of Passover.\n\nThe Greek historian, Herodotus (c. 484 BC – c. 425 BC), wrote of the invasion and acknowledges many Assyrian deaths, which he claims were the result of a plague of mice. The Jewish historian, Josephus, followed the writings of Herodotus. These historians record Sennacherib's failure to take Jerusalem is \"uncontested\".\n\nUnderstanding the biblically recorded sequence of events in Hezekiah's life as chronological or not is critical to the contextual interpretation of his reign. According to scholar Stephen L. Harris, chapter 20 of 2 Kings does not follow the events of chapters 18 and 19 (161). Rather, the Babylonian envoys precede the Assyrian invasion and siege. Chapter 20 would have been added during the exile, and Harris says it \"evidently took place before Sennacherib's invasion' when Hezekiah was \"trying to recruit Babylon as an ally against Assyria.' Consequently, \"Hezekiah ends his long reign impoverished and ruling over only a tiny scrap of his former domain.' Likewise, \"The Archaeological Study Bible\" says, \"The presence of these riches' that Hezekiah shows to the Babylonians \"indicates that this event took place before Hezekiah's payment of tribute to Sennacherib in 701 BC\" (564). Again, \"Though the king's illness and the subsequent Babylonian mission are described at the end of the accounts of his reign, they must have occurred before the war with Assyria. Thus, Isaiah's chastening of Hezekiah is due to his alliances made with other countries during the Assyrian conflict for insurance. To a reader who interprets the chapters chronologically, it would appear that Hezekiah ended his reign at a climax, but with a scholarly analysis, his end would contrarily be interpreted as a long fall from where he began.\n\nThere has been considerable academic debate about the actual dates of reigns of the Israelite kings. Scholars have endeavored to synchronize the chronology of events referred to in the Hebrew Bible with those derived from other external sources. In the case of Hezekiah, scholars have noted that the apparent inconsistencies are resolved by accepting the evidence that Hezekiah, like his predecessors for four generations in the kings of Judah, had a coregency with his father, and this coregency began in 729 BC.\n\nAs an example of the reasoning that finds inconsistencies in calculations when coregencies are \"a priori\" ruled out, dates the fall of Samaria (the Northern Kingdom) to the 6th year of Hezekiah's reign. William F. Albright has dated the fall of the Kingdom of Israel to 721 BC, while E. R. Thiele calculates the date as 723 BC. If Abright's or Thiele's dating are correct, then Hezekiah's reign would begin in either 729 or 727 BC. On the other hand, states that Sennacherib invaded Judah in the 14th year of Hezekiah's reign. Dating based on Assyrian records date this invasion to 701 BC, and Hezekiah's reign would therefore begin in 716/715 BC. This dating would be confirmed by the account of Hezekiah's illness in chapter 20, which immediately follows Sennacherib's departure (). This would date his illness to Hezekiah's 14th year, which is confirmed by Isaiah's statement () that he will live fifteen more years (29 − 15 = 14). As shown below, these problems are all addressed by scholars who make reference to the ancient Near Eastern practice of coregency.\n\nFollowing the approach of Wellhausen, another set of calculations shows it is probable that Hezekiah did not ascend the throne before 722 BC. By Albright's calculations, Jehu's initial year is 842 BC; and between it and Samaria's destruction the \"Books of Kings\" give the total number of the years the kings of Israel ruled as 143 7/12, while for the kings of Judah the number is 165. This discrepancy, amounting in the case of Judah to 45 years (165–120), has been accounted for in various ways; but every one of those theories must allow that Hezekiah's first six years fell before 722 BC. (That Hezekiah began to reign before 722 BC, however, is entirely consistent with the principle that the Ahaz/Hezekiah coregency began in 729 BC.) Nor is it clearly known how old Hezekiah was when called to the throne, although states he was twenty-five years of age. His father died at the age of thirty-six (); it is not likely that Ahaz at the age of eleven should have had a son. Hezekiah's own son Manasseh ascended the throne twenty-nine years later, at the age of twelve. This places his birth in the seventeenth year of his father's reign, or gives Hezekiah's age as forty-two, if he was twenty-five at his ascension. It is more probable that Ahaz was twenty-one or twenty-five when Hezekiah was born (and suggesting an error in the text), and that the latter was thirty-two at the birth of his son and successor, Manasseh.\nSince Albright and Friedman, several scholars have explained these dating problems on the basis of a coregency between Hezekiah and his father Ahaz between 729 and 716/715 BC. Assyriologists and Egyptologists recognize that coregency was a practice both in Assyria and Egypt. After noting that coregencies were only used sporadically in the northern kingdom (Israel), Nadav Na'aman writes,\nIn the kingdom of Judah, on the other hand, the nomination of a co-regent was the common procedure, beginning from David who, before his death, elevated his son Solomon to the throneWhen taking into account the permanent nature of the co-regency in Judah from the time of Joash, one may dare to conclude that dating the co-regencies accurately is indeed the key for solving the problems of biblical chronology in the eighth century BC.\"\n\nAmong the numerous scholars who have recognized the coregency between Ahaz and Hezekiah are Kenneth Kitchen in his various writings, Leslie McFall, and Jack Finegan. McFall, in his 1991 article, argues that if 729 BC (that is, the Judean regnal year beginning in Tishri of 729) is taken as the start of the Ahaz/Hezekiah coregency, and 716/715 BC as the date of the death of Ahaz, then all the extensive chronological data for Hezekiah and his contemporaries in the late eighth century BC are in harmony. Further, McFall found that no textual emendations are required among the numerous dates, reign lengths, and synchronisms given in the Hebrew Testament for this period. In contrast, those who do not accept the Ancient Near Eastern principle of coregencies require multiple emendations of the Scriptural text, and there is no general agreement on which texts should be emended, nor is there any consensus among these scholars on the resultant chronology for the eighth century BC. This is in contrast with the general consensus among those who accept the Biblical and near Eastern practice of coregencies that Hezekiah was installed as coregent with his father Ahaz in 729 BC, and the synchronisms of 2 Kings 18 must be measured from that date, whereas the synchronisms to Sennacherib are measured from the sole reign starting in 716/715 BC. The two synchronisms to Hoshea of Israel in 2 Kings 18 are then in exact agreement with the dates of Hoshea's reign that can be determined from Assyrian sources, as is the date of Samaria's fall as stated in 2 Kings 18:10. An analogous situation of two ways of measurement, both equally valid, is encountered in the dates given for Jehoram of Israel, whose first year is synchronized to the 18th year of the sole reign of Jehoshaphat of Judah in 2 Kings 3:1 (853/852 BC), but his reign is also reckoned according to another method as starting in the second year of the coregency of Jehoshaphat and his son Jehoram of Judah (2 Kings 1:17); both methods refer to the same calendrical year.\n\nScholars who accept the principle of coregencies note that abundant evidence for their use is found in the biblical material itself. The agreement of scholarship built on these principles with both biblical and secular texts was such that the Thiele/McFall chronology was accepted as the best chronology for the kingdom period in Jack Finegan's encyclopedic \"Handbook of Biblical Chronology\".\n\n\n", "id": "14005", "title": "Hezekiah"}
{"url": "https://en.wikipedia.org/wiki?curid=14006", "text": "Haemophilia\n\nHaemophilia, also spelled hemophilia, is a mostly inherited genetic disorder that impairs the body's ability to make blood clots, a process needed to stop bleeding. This results in people bleeding longer after an injury, easy bruising, and an increased risk of bleeding inside joints or the brain. Those with mild disease may only have symptoms after an accident or during surgery. Bleeding into a joint can result in permanent damage while bleeding in the brain can result in long term headaches, seizures, or a decreased level of consciousness.\nThere are two main types of haemophilia: haemophilia A, which occurs due to not enough clotting factor VIII, and haemophilia B, which occurs due to not enough clotting factor IX. They are typically inherited from one's parents through an X chromosome with a nonfunctional gene. Rarely a new mutation may occur during early development or haemophilia may develop later in life due to antibodies forming against a clotting factor. Other types include haemophilia C, which occurs due to not enough factor XI, and parahaemophilia, which occurs due to not enough factor V. Acquired haemophilia is associated with cancers, autoimmune disorders, and pregnancy. Diagnosis is by testing the blood for its ability to clot and its levels of clotting factors.\nPrevention may occur by removing an egg, fertilizing it, and testing the embryo before transferring it to the uterus. Treatment is by replacing the missing blood clotting factors. This may be done on a regular basis or during bleeding episodes. Replacement may take place at home or in hospital. The clotting factors are made either from human blood or by recombinant methods. Up to 20% of people develop antibodies to the clotting factors which makes treatment more difficult. The medication desmopressin may be used in those with mild haemophilia A. Studies of gene therapy are in early human trials.\nHaemophilia A affects about 1 in 5,000–10,000, while haemophilia B affects about 1 in 40,000, males at birth. As haemophilia A and B are X-linked recessive disorders females are very rarely severely affected. Some females with a nonfunctional gene on one of the X chromosomes may be mildly symptomatic. Haemophilia C occurs equally in both sexes and is mostly found in Ashkenazi Jews. In the 1800s haemophilia was common within the royal families of Europe. The difference between haemophilia A and B was determined in 1952. The word is from the Greek \"haima\" αἷμα meaning blood and \"philia\" φιλία meaning love.\nCharacteristic symptoms vary with severity. In general symptoms are internal or external bleeding episodes, which are called \"bleeds\". People with more severe haemophilia suffer more severe and more frequent bleeds, while people with mild haemophilia usually suffer more minor symptoms except after surgery or serious trauma. In cases of moderate haemophilia symptoms are variable which manifest along a spectrum between severe and mild forms.\n\nIn both haemophilia A and B, there is spontaneous bleeding but a normal bleeding time, normal prothrombin time, normal thrombin time, but prolonged partial thromboplastin time. Internal bleeding is common in people with severe haemophilia and some individuals with moderate haemophilia. The most characteristic type of internal bleed is a joint bleed where blood enters into the joint spaces. This is most common with severe haemophiliacs and can occur spontaneously (without evident trauma). If not treated promptly, joint bleeds can lead to permanent joint damage and disfigurement. Bleeding into soft tissues such as muscles and subcutaneous tissues is less severe but can lead to damage and requires treatment.\n\nChildren with mild to moderate haemophilia may not have any signs or symptoms at birth especially if they do not undergo circumcision. Their first symptoms are often frequent and large bruises and haematomas from frequent bumps and falls as they learn to walk. Swelling and bruising from bleeding in the joints, soft tissue, and muscles may also occur. Children with mild haemophilia may not have noticeable symptoms for many years. Often, the first sign in very mild haemophiliacs is heavy bleeding from a dental procedure, an accident, or surgery. Females who are carriers usually have enough clotting factors from their one normal gene to prevent serious bleeding problems, though some may present as mild haemophiliacs.\n\nSevere complications are much more common in cases of severe and moderate haemophilia. Complications may arise from the disease itself or from its treatment:\nHaemophilic arthropathy is characterized by chronic proliferative synovitis and cartilage destruction. If an intra-articular bleed is not drained early, it may cause apoptosis of chondrocytes and affect the synthesis of proteoglycans. The hypertrophied and fragile synovial lining while attempting to eliminate excessive blood may be more likely to easily rebleed, leading to a vicious cycle of hemarthrosis-synovitis-hemarthrosis. In addition, iron deposition in the synovium may induce an inflammatory response activating the immune system and stimulating angiogenesis, resulting in cartilage and bone destruction.\n\nFemales possess two X-chromosomes, males have one X and one Y-chromosome. Since the mutations causing the disease are X-linked recessive, a female carrying the defect on one of her X-chromosomes may not be affected by it, as the equivalent allele on her other chromosome should express itself to produce the necessary clotting factors, due to X inactivation. However, the Y-chromosome in the male has no gene for factors VIII or IX. If the genes responsible for production of factor VIII or factor IX present on a male's X-chromosome are deficient there is no equivalent on the Y-chromosome to cancel it out, so the deficient gene is not masked and the disorder will develop.\n\nSince a male receives his single X-chromosome from his mother, the son of a healthy female silently carrying the deficient gene will have a 50% chance of inheriting that gene from her and with it the disease; and if his mother is affected with haemophilia, he will have a 100% chance of being a haemophiliac. In contrast, for a female to inherit the disease, she must receive two deficient X-chromosomes, one from her mother and the other from her father (who must therefore be a haemophiliac himself). Hence haemophilia is far more common among males than females. However, it is possible for female carriers to become mild haemophiliacs due to lyonisation (inactivation) of the X-chromosomes. Haemophiliac daughters are more common than they once were, as improved treatments for the disease have allowed more haemophiliac males to survive to adulthood and become parents. Adult females may experience menorrhagia (heavy periods) due to the bleeding tendency. The pattern of inheritance is criss-cross type. This type of pattern is also seen in colour blindness.\n\nA mother who is a carrier has a 50% chance of passing the faulty X-chromosome to her daughter, while an affected father will always pass on the affected gene to his daughters. A son cannot inherit the defective gene from his father. This is a recessive trait and can be passed on if cases are more severe with carrier.Genetic testing and genetic counselling is recommended for families with haemophilia. Prenatal testing, such as amniocentesis, is available to pregnant women who may be carriers of the condition.\n\nAs with all genetic disorders, it is of course also possible for a human to acquire it spontaneously through mutation, rather than inheriting it, because of a new mutation in one of their parents' gametes. Spontaneous mutations account for about 33% of all cases of haemophilia A. About 30% of cases of haemophilia B are the result of a spontaneous gene mutation.\n\nIf a female gives birth to a haemophiliac son, either the female is a carrier for the blood disorder or the haemophilia was the result of a spontaneous mutation. Until modern direct DNA testing, however, it was impossible to determine if a female with only healthy children was a carrier or not. Generally, the more healthy sons she bore, the higher the probability that she was not a carrier.\n\nIf a male is afflicted with the disease and has children with a female who is not even a carrier, his daughters will be carriers of haemophilia. His sons, however, will not be affected with the disease. The disease is X-linked and the father cannot pass haemophilia through the Y-chromosome. Males with the disorder are then no more likely to pass on the gene to their children than carrier females, though all daughters they sire will be carriers and all sons they father will not have haemophilia (unless the mother is a carrier).\n\nThere are numerous different mutations which cause each type of haemophilia. Due to differences in changes to the genes involved, people with haemophilia often have some level of active clotting factor. Individuals with less than 1% active factor are classified as having severe haemophilia, those with 1-5% active factor have moderate haemophilia, and those with mild haemophilia have between 5-40% of normal levels of active clotting factor.\n\nHaemophilia can be diagnosed before, during or after birth if there is a family history of the condition. Several options are available to parents. If there is no family history of haemophilia, it is usually only diagnosed when a child begins to walk or crawl. They may experience joint bleeds or easy bruising.\n\nMild haemophilia may only be discovered later, usually after an injury or a dental or surgical procedure.\n\nGenetic testing and counselling are available to help determine the risk of passing the condition onto a child. This may involve testing a sample of your tissue or blood to look for signs of the genetic mutation that causes haemophilia.\n\nIf you become pregnant and have a history of haemophilia in your family, tests for the haemophilia gene can be carried out. These include:\nThere's a small risk of these procedures causing problems such as miscarriage or premature labour, so you may want to discuss this with the doctor in charge of your care.\n\nIf haemophilia is suspected after your child has been born, a blood test can usually confirm the diagnosis. Blood from the umbilical cord can be tested at birth if there's a family history of haemophilia. A blood test will also be able to identify whether your child has haemophilia A or B, and how severe it is.\n\nThere are several types of haemophilia: haemophilia A, haemophilia B, haemophilia C, \"parahaemophilia\", and \"acquired haemophilia A\".\n\nHaemophilia A, is a recessive X-linked genetic disorder resulting in a deficiency of functional clotting Factor VIII. Haemophilia B, is also a recessive X-linked genetic disorder involving a lack of functional clotting Factor IX. Haemophilia C, is an autosomal genetic disorder involving a lack of functional clotting Factor XI. Haemophilia C is not completely recessive, as heterozygous individuals also show increased bleeding.\n\nThe type of haemophilia known as \"parahaemophilia\" is a mild and rare form and is due to a deficiency in factor V. This type can be inherited or acquired.\n\nA non-genetic form of haemophilia is caused by autoantibodies against factor VIII and so is known as \"acquired haemophilia A\". Acquired haemophilia can be associated with cancers, autoimmune disorders and following childbirth.\n\nWhile there is no cure for haemophilia, treatment improves outcomes.\n\nClotting factors are usually not needed in mild haemophilia. In moderate haemophilia clotting factors are typically only needed when bleeding occurs or to prevent bleeding with certain events. In severe haemophilia preventive use is often recommended two or three times a week and may continue for life. Rapid treatment of bleeding episodes decreases damage to the body.\n\nFactor VIII is used in haemophilia A and factor IX in haemophilia B. Factor replacement can be either isolated from human blood serum, recombinant, or a combination of the two. Some people develop antibodies (inhibitors) against the replacement factors given to them, so the amount of the factor has to be increased or non-human replacement products must be given, such as porcine factor VIII.\n\nIf a person becomes refractory to replacement coagulation factor as a result of circulating inhibitors, this may be partially overcome with recombinant human factor VII.\n\nIn early 2008, the US Food and Drug Administration (FDA) approved anti-haemophilic factor, genetically engineered from the genes of Chinese hamster ovary cells. Since 1993 recombinant factor products (which are typically cultured in Chinese hamster ovary (CHO) tissue culture cells and involve little, if any human plasma products) have been available and have been widely used in wealthier western countries. While recombinant clotting factor products offer higher purity and safety, they are, like concentrate, extremely expensive, and not generally available in the developing world. In many cases, factor products of any sort are difficult to obtain in developing countries.\n\nClotting factors are either given preventively or on-demand. Preventive use involves the infusion of clotting factor on a regular schedule in order to keep clotting levels sufficiently high to prevent spontaneous bleeding episodes. On-demand (or episodic) treatment involves treating bleeding episodes once they arise. In 2007, a trial comparing on-demand treatment of boys (< 30 months) with haemophilia A with prophylactic treatment (infusions of 25 IU/kg body weight of Factor VIII every other day) in respect to its effect on the prevention of joint-diseases. When the boys reached 6 years of age, 93% of those in the prophylaxis group and 55% of those in the episodic-therapy group had a normal index joint-structure on MRI. Prophylactic treatment, however, resulted in average costs of $300,000 per year. The author of an editorial published in the same issue of the \"NEJM\" supports the idea that prophylactic treatment not only is more effective than on demand treatment but also suggests that starting after the first serious joint-related haemorrhage may be more cost effective than waiting until the fixed age to begin.\n\nDesmopressin (DDAVP) may be used in those with mild haemophilia A. Tranexamic acid or epsilon aminocaproic acid may be given along with clotting factors to prevent breakdown of clots.\n\nPain medicines, steroids, and physical therapy may be used to reduce pain and swelling in an affected joint.\n\nAnticoagulants such as heparin and warfarin are contraindicated for people with haemophilia as these can aggravate clotting difficulties. Also contraindicated are those drugs which have \"blood thinning\" side effects. For instance, medicines which contain aspirin, ibuprofen, or naproxen sodium should not be taken because they are well known to have the side effect of prolonged bleeding.\n\nAlso contraindicated are activities with a high likelihood of trauma, such as motorcycling and skateboarding. Popular sports with very high rates of physical contact and injuries such as American football, hockey, boxing, wrestling, and rugby should be avoided by people with haemophilia. Other active sports like soccer, baseball, and basketball also have a high rate of injuries, but have overall less contact and should be undertaken cautiously and only in consultation with a doctor.\n\nLike most aspects of the disorder, life expectancy varies with severity and adequate treatment. People with severe haemophilia who don't receive adequate, modern treatment have greatly shortened lifespans and often do not reach maturity. Prior to the 1960s when effective treatment became available, average life expectancy was only 11 years. By the 1980s the life span of the average haemophiliac receiving appropriate treatment was 50–60 years. Today with appropriate treatment, males with haemophilia typically have a near normal quality of life with an average lifespan approximately 10 years shorter than an unaffected male.\n\nSince the 1980s the primary leading cause of death of people with severe haemophilia has shifted from haemorrhage to HIV/AIDS acquired through treatment with contaminated blood products. The second leading cause of death related to severe haemophilia complications is intracranial haemorrhage which today accounts for one third of all deaths of people with haemophilia. Two other major causes of death include hepatitis infections causing cirrhosis and obstruction of air or blood flow due to soft tissue haemorrhage.\n\nHaemophilia is rare, with only about 1 instance in every 10,000 births (or 1 in 5,000 male births) for haemophilia A and 1 in 50,000 births for haemophilia B. About 18,000 people in the United States have haemophilia. Each year in the US, about 400 babies are born with the disorder. Haemophilia usually occurs in males and less often in females. It is estimated that about 2500 Canadians have haemophilia A, and about 500 Canadians have haemophilia B.\n\nThe first medical professional to describe the disease was Abulcasis. In the tenth century he described families whose males died of bleeding after only minor traumas. While many other such descriptive and practical references to the disease appear throughout historical writings, scientific analysis did not begin until the start of the nineteenth century.\n\nIn 1803, John Conrad Otto, a Philadelphian physician, wrote an account about \"a hemorrhagic disposition existing in certain families\" in which he called the affected males \"bleeders\". He recognised that the disorder was hereditary and that it affected mostly males and was passed down by healthy females. His paper was the second paper to describe important characteristics of an X-linked genetic disorder (the first paper being a description of colour blindness by John Dalton who studied his own family). Otto was able to trace the disease back to a woman who settled near Plymouth, NH in 1720. The idea that affected males could pass the trait onto their unaffected daughters was not described until 1813 when John F. Hay, published an account in The New England Journal of Medicine.\n\nIn 1924, a Finnish doctor discovered a hereditary bleeding disorder similar to haemophilia localised in the Åland Islands, southwest of Finland. This bleeding disorder is called \"Von Willebrand Disease\".\n\nThe term \"haemophilia\" is derived from the term \"haemorrhaphilia\" which was used in a description of the condition written by Friedrich Hopff in 1828, while he was a student at the University of Zurich. In 1937, Patek and Taylor, two doctors from Harvard, discovered anti-haemophilic globulin. In 1947, Pavlosky, a doctor from Buenos Aires, found haemophilia A and haemophilia B to be separate diseases by doing a lab test. This test was done by transferring the blood of one haemophiliac to another haemophiliac. The fact that this corrected the clotting problem showed that there was more than one form of haemophilia.\n\nHaemophilia has featured prominently in European royalty and thus is sometimes known as 'the royal disease'. Queen Victoria passed the mutation for haemophilia B to her son Leopold and, through two of her daughters, Alice and Beatrice, to various royals across the continent, including the royal families of Spain, Germany, and Russia. In Russia, Tsarevich Alexei Nikolaevich, son of Nicholas II, was a descendant of Queen Victoria through his mother Empress Alexandra and suffered from haemophilia.\n\nIt was claimed that Rasputin was successful at treating Tsarevich Alexei's haemophilia. At the time, a common treatment administered by professional doctors was to use aspirin, which worsened rather than lessened the problem. It is believed that, by simply advising against the medical treatment, Rasputin could bring visible and significant improvement to the condition of Tsarevich Alexei.\n\nIn Spain, Queen Victoria's youngest daughter, Princess Beatrice, had a daughter Victoria Eugenie of Battenberg, who later became Queen of Spain. Two of her sons were haemophiliacs and both died from minor car accidents. Her eldest son, Prince Alfonso of Spain, Prince of Asturias, died at the age of 31 from internal bleeding after his car hit a telephone booth. Her youngest son, Infante Gonzalo, died at age 19 from abdominal bleeding following a minor car accident in which he and his sister hit a wall while avoiding a cyclist. Neither appeared injured or sought immediate medical care and Gonzalo died two days later from internal bleeding.\n\nPrior to 1985, there were no laws enacted within the U.S. to screen blood. As a result, many people with haemophilia who received untested and unscreened clotting factor prior to 1992 were at an extreme risk for contracting HIV and hepatitis C via these blood products. It is estimated that more than 50% of the haemophilia population, i.e. over 10,000 people, contracted HIV from the tainted blood supply in the United States alone.\n\nAs a direct result of the contamination of the blood supply in the late 1970s and early/mid-1980s with viruses such as hepatitis and HIV, new methods were developed in the production of clotting factor products. The initial response was to heat-treat (pasteurise) plasma-derived factor concentrate, followed by the development of monoclonal factor concentrates, which use a combination of heat treatment and affinity chromatography to inactivate any viral agents in the pooled plasma from which the factor concentrate is derived. The Lindsay Tribunal in Ireland investigated, among other things, the slow adoption of the new methods.\n\nIn those with severe haemophilia, gene therapy may reduce symptoms to those that a mild or moderate person with haemophilia might have. The best results have been found in haemophilia B. As of 2016 early stage human research is ongoing with a few sites recruiting participants. It is not currently an accepted treatment for haemophilia.\n\n", "id": "14006", "title": "Haemophilia"}
{"url": "https://en.wikipedia.org/wiki?curid=14008", "text": "Hickory (disambiguation)\n\nHickory is a type of tree (\"Carya\" species) found in North America and East Asia.\n\nHickory may also refer to:\n\nIn the United States:\n\n\n", "id": "14008", "title": "Hickory (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=14009", "text": "Hemicellulose\n\nA hemicellulose (also known as polyose) is any of several heteropolymers (matrix polysaccharides), such as arabinoxylans, present along with cellulose in almost all plant cell walls. While cellulose is crystalline, strong, and resistant to hydrolysis, hemicellulose has a random, amorphous structure with little strength. It is easily hydrolyzed by dilute acid or base as well as myriad hemicellulase enzymes.\n\nHemicelluloses include xylan, glucuronoxylan, arabinoxylan, glucomannan, and xyloglucan.\n\nThese polysaccharides contain many different sugar monomers. In contrast, cellulose contains only anhydrous glucose. For instance, besides glucose, sugar monomers in hemicellulose can include xylose, mannose, galactose, rhamnose, and arabinose. Hemicelluloses contain most of the D-pentose sugars, and occasionally small amounts of L-sugars as well. Xylose is in most cases the sugar monomer present in the largest amount, although in softwoods mannose can be the most abundant sugar. Not only regular sugars can be found in hemicellulose, but also their acidified form, for instance glucuronic acid and galacturonic acid can be present. Hemicellulos is often assosiated with cellulose, but it has different composition.\n\nUnlike cellulose, hemicellulose (also a polysaccharide) consists of shorter chains – 500–3,000 sugar units as opposed to 7,000–15,000 glucose molecules per polymer, as seen in cellulose. In addition, hemicellulose is a branched polymer, while cellulose is unbranched.\n\nHemicelluloses are embedded in the cell walls of plants, sometimes in chains that form a 'ground' - they bind with pectin to cellulose to form a network of cross-linked fibres.\n\nHemicelluloses are synthesised from sugar nucleotides in the cell's Golgi apparatus. Two models explain their synthesis: 1) a ‘2 component model' where modification occurs at two transmembrane proteins, and 2) a '1 component model' where modification occurs only at one transmembrane protein. After synthesis, hemicelluloses are transported to the plasma membrane via Golgi vesicles.\n\nAs percent content of hemicellulose increases in animal feed, the voluntary feed intake decreases.\n\nHemicellulose is represented by the difference between neutral detergent fiber (NDF) and acid detergent fiber (ADF).\n\nMicrofibrils are cross-linked together by hemicellulose homopolymers. Lignins assist and strengthen the attachment of hemicelluloses to microfibrils.\n\nHemicellulose found in hardwood trees is predominantly xylan with some glucomannan, while in softwoods it is mainly rich in galactoglucomannan and contains only a small amount of xylan. The average molecular weight is lower than that of cellulose at less than 30,000, as opposed to the 100,000 average molecular weight reported for cellulose.\n\n\n", "id": "14009", "title": "Hemicellulose"}
{"url": "https://en.wikipedia.org/wiki?curid=14011", "text": "Hillbilly\n\nHillbilly is a term (often derogatory) for people who dwell in rural, mountainous areas in the United States, primarily in Appalachia and the Ozarks, Uwharrie Mountains, and Caraway Mountains. Due to its strongly stereotypical connotations, the term can be offensive to those Americans of Appalachian or Ozark heritage. \"Hillbilly\" first appeared in print in a 1900 \"New York Journal\" article, with the definition: \"a Hill-Billie is a free and untrammeled white citizen of Tennessee, who lives in the hills, has no means to speak of, dresses as he can, talks as he pleases, drinks whiskey when he gets it, and fires off his revolver as the fancy takes him\". The stereotype is twofold in that it incorporates both positive and negative traits: \"Hillbillies\" are often considered independent and self-reliant individuals who resist the modernization of society, but at the same time they are also defined as backward and violent. Scholars argue this duality is reflective of the split ethnic identities in white America.\n\nThe Appalachian Mountains were settled in the 18th century by settlers primarily from the Province of Ulster in Ireland. The settlers from Ulster were mainly Protestants who migrated to Ireland during the Plantation of Ulster in the 17th century from Scotland and Northern England. Many further migrated to the American colonies beginning in the 1730s, and in America became known as the Scotch-Irish. Scholars argue that the term \"hillbilly\" originated from Scottish dialect. The term \"hill-folk\" referred to people that preferred isolation from the greater society and \"billy\" meant \"comrade\" or \"companion.\" It is suggested that \"hill-folk\" and \"billie\" were combined when the Cameronians fled to the Highlands.\n\nOthers have suggested the term originated in 17th century Ireland, during the Williamite War, when Protestant supporters of King William III (\"\"King Billy\"\") were often referred to as \"Billy's Boys.\" Some scholars disagree with this theory. Michael Montgomery's \"From Ulster to America: The Scotch-Irish Heritage of American English\" states, \"In Ulster in recent years it has sometimes been supposed that it was coined to refer to followers of King William III and brought to America by early Ulster emigrants, but this derivation is almost certainly incorrect. … In America \"hillbilly\" was first attested only in 1898, which suggests a later, independent development.\"\n\nThe term \"hillbilly\" spread in the years following the American Civil War. At this time, the country was developing both technologically and socially, but the Appalachian region was falling behind. Before the war, Appalachia was not distinctively different from other rural areas of the country. Post-war, although the frontier pushed farther west, the region maintained frontier characteristics. Appalachians themselves were perceived as backward, quick to violence and inbred in their isolation. Fueled by news stories of mountain feuds such as that in the 1880s between the Hatfields and McCoys, the hillbilly stereotype developed in the late 19th to early 20th century.\n\nThe \"classic\" hillbilly stereotype reached its current characterization during the years of the Great Depression when many mountaineers left their homes to find work in other areas of the country. The period of Appalachian out-migration, roughly from the 1930s through the 1950s, saw many mountain residents moving North to the Midwestern industrial cities of Chicago, Cleveland, Akron and Detroit. This movement North became known as the \"Hillbilly Highway.\" The movement brought these previously isolated communities into mainstream United States culture. Poor white mountaineers became central characters in newspapers, pamphlets and eventually, motion pictures. Authors at this time were inspired by historical figures such as Davy Crockett and Daniel Boone. The mountaineer image transferred over to the 20th century where the \"hillbilly\" stereotype emerged.\n\nPop culture has perpetuated the \"hillbilly\" stereotype. The misrepresentation of Appalachian people in the media has led to great cultural distortion of Appalachian beliefs, practices, and lifestyle. Scholarly works suggest that the media has exploited both the Appalachian region and people by classifying them as \"hillbillies.\" These generalizations do not match the cultural experiences of Appalachians. Appalachians, like many other groups, do not subscribe to a single identity. One of the issues associated with stereotyping is that it is profitable. When \"hillbilly\" became a widely used term, entrepreneurs saw a window for potential revenue. They \"recycled\" the image and brought it to life through various forms of media.\n\nTelevision and film have portrayed \"hillbillies\" in both derogatory and sympathetic terms. Films such as \"Sergeant York\" or the Ma and Pa Kettle series portrayed the \"hillbilly\" as wild but good-natured. Television programs of the 1960s such as \"The Real McCoys\", \"The Andy Griffith Show\", and especially \"The Beverly Hillbillies\", portrayed the \"hillbilly\" as backwards but with enough wisdom to outwit more sophisticated city folk. \"Gunsmoke\"s Festus Haggen was portrayed as intelligent and quick-witted (but lacking \"education\"). The popular 1970s television variety show \"Hee Haw\" regularly lampooned the stereotypical \"hillbilly\" lifestyle. A darker image of the hillbilly is found in the film \"Deliverance\" (1972), based on a novel by James Dickey, which depicted some \"hillbillies\" as genetically deficient, inbred and murderous, while depicting others as helpful, friendly, and smart.\n\n\"Hillbillies\" were at the center of reality television in the 21st century. Network television shows such as \"New Beverly Hillbillies\", \"High Life\", and \"The Simple Life\" displayed the \"hillbilly\" lifestyle for viewers in the United States. This sparked protests across the country with rural-minded individuals gathering to fight the stereotype. The Center for Rural Strategies started a nationwide campaign stating the stereotype was \"politically incorrect.\" The Kentucky-based organization engaged political figures in the movement such as Robert Byrd and Mike Huckabee. Both protestors argued that the discrimination of any other group in United States would not be tolerated, so neither should the discrimination against rural U.S. citizens. A 2003 piece published by \"The Cincinnati Enquirer\" read, \"In this day of hypersensitivity to diversity and political correctness, Appalachians have been a group that it is still socially acceptable to demean and joke about. … But rural folks have spoken up and said \"enough\" to the Hollywood mockers.\"\n\n\"\" is a memoir by J. D. Vance about the Appalachian values of his upbringing and their relation to the social problems of his hometown of Middletown, Ohio. The book topped \"The New York Times\" Best Seller list in August 2016.\n\n\"Hillbilly music\" was at one time considered an acceptable label for what is now known as country music. The label, coined in 1925 by country pianist Al Hopkins, persisted until the 1950s.\n\nThe \"hillbilly music\" categorization covers a wide variety of musical genres including bluegrass, country western, and gospel. Appalachian folk song existed long before the \"hillbilly\" label. When the commercial industry was combined with \"traditional Appalachian folksong,\" \"hillbilly music\" was formed. Some argue this is a \"High Culture\" issue where sophisticated individuals may see something considered \"unsophisticated\" as \"trash.\"\n\nIn the early 20th century, artists began to utilize the \"hillbilly\" label. The York Brothers entitled one of their songs \"Hillbilly Rose\" and the Delmore Brothers followed with their song \"Hillbilly Boogie.\" In 1927, the Gennett studios in Richmond, Indiana, made a recording of black fiddler Jim Booker. The recordings were labeled \"made for Hillbilly\" in the Gennett files and were marketed to a white audience. Columbia Records had much success with the \"Hill Billies\" featuring Al Hopkins and Fiddlin' Charlie Bowman.\n\nBy the late 1940s, radio stations started to use the \"hillbilly music\" label. Originally, \"hillbilly\" was used to describe fiddlers and string bands, but now it was used to describe traditional Appalachian music. Appalachians had never used this term to describe their own music. Popular songs whose style bore characteristics of both hillbilly and African American music were referred to as hillbilly boogie and \"rockabilly\". Elvis Presley was a prominent player of rockabilly and was known early in his career as the \"Hillbilly Cat.\"\n\nWhen the Country Music Association was founded in 1958, the term \"hillbilly music\" gradually fell out of use. The music industry merged hillbilly music, Western swing, and Cowboy music, to form the current category C&W, Country and Western.\n\nSome artists and fans (notably Hank Williams Sr.) were offended by the \"hillbilly music\" label. While the term is not used as frequently today, it is still used on occasion to refer to old-time music or bluegrass. For example, WHRB broadcasts a popular weekly radio show entitled \"Hillbilly at Harvard.\" The show is devoted to playing a mix of old-time music, bluegrass, and traditional country and western.\n\nThe hillbilly stereotype is considered to have had a traumatizing effect on some in the Appalachian region. Feelings of shame, self-hatred, and detachment are cited as a result of \"culturally transmitted traumatic stress syndrome.\" Appalachian scholars say that the large-scale stereotyping has rewritten Appalachian history, making Appalachians feel particularly vulnerable. \"Hillbilly\" has now become part of Appalachian identity and some Appalachians feel they are constantly defending themselves against this image.\n\nThe stereotyping also has political implications for the region. There is a sense of \"perceived history\" that prevents many political issues from receiving adequate attention. Appalachians are often blamed for economic struggles. \"Moonshiners, welfare cheats, and coal miners\" are stereotypes stemming from the greater hillbilly stereotype in the region. This prejudice has been said to serve as a barrier for addressing some serious issues such as the economy and the environment.\n\nDespite the political and social difficulties associated with stereotyping, Appalachians have organized to enact change. The War on Poverty is sometimes considered to be an example of one effort that allowed for Appalachian community organization. Grassroots movements, protests, and strikes are common in the area, though not always successful.\n\nThe Springfield, Missouri Chamber of Commerce once presented dignitaries visiting the city with an \"Ozark Hillbilly Medallion\" and a certificate proclaiming the honoree a \"hillbilly of the Ozarks.\" On June 7, 1952, President Harry S. Truman received the medallion after a breakfast speech at the Shrine Mosque for the 35th Division Association. Other recipients included US Army generals Omar Bradley and Matthew Ridgeway, J. C. Penney, Johnny Olsen and Ralph Story.\n\nHillbilly Days is an annual festival held in mid-April in Pikeville, Kentucky celebrating the best of Appalachian culture. The event began by local Shriners as a fundraiser to support the Shriners Children's Hospital. It has grown since its beginning in 1976 and now is the second largest festival held in the state of Kentucky. Artists and craftspeople showcase their talents and sell their works on display. Nationally renowned musicians as well as the best of the regional mountain musicians share six different stages located throughout the downtown area of Pikeville. Want-to-be hillbillies from across the nation compete to come up with the wildest Hillbilly outfit. The event has earned its name as the Mardi Gras of the Mountains. Fans of \"mountain music\" come from around the United States to hear this annual concentrated gathering of talent. Some refer to this event as the equivalent of a \"Woodstock\" for mountain music.\n\n", "id": "14011", "title": "Hillbilly"}
{"url": "https://en.wikipedia.org/wiki?curid=14012", "text": "Host\n\nHost (masculine) and hostess (feminine) most often refer to a person responsible for guests at an event or providing hospitality during it, or to an event's presenter or master or mistress of ceremonies. Host or hosts may also refer to:\n\n\n\n\n\n\n\n", "id": "14012", "title": "Host"}
{"url": "https://en.wikipedia.org/wiki?curid=14013", "text": "Hernán Cortés\n\nHernán Cortés de Monroy y Pizarro Altamirano, Marquis of the Valley of Oaxaca (; 1485 – December 2, 1547) was a Spanish \"Conquistador\" who led an expedition that caused the fall of the Aztec Empire and brought large portions of mainland Mexico under the rule of the King of Castile in the early 16th century. Cortés was part of the generation of Spanish colonizers who began the first phase of the Spanish colonization of the Americas.\n\nBorn in Medellín, Spain, to a family of lesser nobility, Cortés chose to pursue a livelihood in the New World. He went to Hispaniola and later to Cuba, where he received an \"encomienda\" and, for a short time, became alcalde (magistrate) of the second Spanish town founded on the island. In 1519, he was elected captain of the third expedition to the mainland, an expedition which he partly funded. His enmity with the Governor of Cuba, Diego Velázquez de Cuéllar, resulted in the recall of the expedition at the last moment, an order which Cortés ignored.\n\nArriving on the continent, Cortés executed a successful strategy of allying with some indigenous people against others. He also used a native woman, Doña Marina, as an interpreter; she would later bear Cortés a son. When the Governor of Cuba sent emissaries to arrest Cortés, he fought them and won, using the extra troops as reinforcements. Cortés wrote letters directly to the king asking to be acknowledged for his successes instead of punished for mutiny. After he overthrew the Aztec Empire, Cortés was awarded the title of \"Marqués del Valle de Oaxaca\", while the more prestigious title of Viceroy was given to a high-ranking nobleman, Antonio de Mendoza. In 1541 Cortés returned to Spain, where he died peacefully but embittered, six years later.\n\nBecause of the controversial undertakings of Cortés and the scarcity of reliable sources of information about him, it has become difficult to assert anything definitive about his personality and motivations. Early lionizing of the conquistadors did not encourage deep examination of Cortés. Later reconsideration of the conquistadors' character in the context of modern anti-colonial sentiment also did little to expand understanding of Cortés as an individual. As a result of these historical trends, descriptions of Cortés tend to be simplistic, and either damning or idealizing.\n\nWhile now generally called \"Hernán\", Cortés himself used the form \"Hernando\" or \"Fernando\".\n\nCortés was born in 1485 in the town of Medellín, in modern-day Extremadura, Spain. His father, Martín Cortés de Monroy, born in 1449 to Rodrigo or Ruy Fernández de Monroy and his wife María Cortés, was an infantry captain of distinguished ancestry but slender means. Hernán's mother was Catalina Pizarro Altamirano.\n\nThrough his mother, Hernán was the second cousin once removed of Francisco Pizarro, who later conquered the Inca Empire of modern-day Peru (not to be confused with another Francisco Pizarro who joined Cortés to conquer the Aztecs), through her parents Diego Altamirano and wife and cousin Leonor Sánchez Pizarro Altamirano, first cousin of Pizarro's father. Through his father, Hernán was a twice distant relative of Nicolás de Ovando, the third Governor of Hispaniola. His paternal grandfather was a son of Rodrigo de Monroy y Almaraz, 5th Lord of Monroy, and wife Mencía de Orellana y Carvajal.\n\nAs a child Cortés was described as a pale, sickly child by his biographer, chaplain, and friend Francisco López de Gómara. At the age of 14, Cortés was sent to study Latin under an uncle-in-law in Salamanca.\n\nAfter two years, Cortés, tired of schooling, returned home to Medellín, much to the irritation of his parents, who had hoped to see him equipped for a profitable legal career. However, those two years at Salamanca, plus his long period of training and experience as a notary, first in Seville and later in Hispaniola, would give him a close acquaintance with the legal codes of Castile that helped him to justify his unauthorized conquest of Mexico.\n\nAt this point in his life, Cortés was described by Gómara as restless, haughty and mischievous. This was probably a fair description of a 16-year-old boy who had returned home only to find himself frustrated by life in his small provincial town. By this time, news of the exciting discoveries of Christopher Columbus in the New World was streaming back to Spain.\n\nPlans were made for Cortés to sail to the Americas with a family acquaintance and distant relative, Nicolás de Ovando, the newly appointed Governor of Hispaniola (currently Haiti and the Dominican Republic), but an injury he sustained while hurriedly escaping from the bedroom of a married woman from Medellín prevented him from making the journey. Instead, he spent the next year wandering the country, probably spending most of his time in Spain's southern ports of Cadiz, Palos, Sanlucar, and Seville, listening to the tales of those returning from the Indies, who told of discovery and conquest, gold, Indians, and strange unknown lands. He finally left for Hispaniola in 1504 where he became a colonist.\n\nCortés reached Hispaniola in a ship commanded by Alonso Quintero, who tried to deceive his superiors and reach the New World before them in order to secure personal advantages. Quintero's mutinous conduct may have served as a model for Cortés in his subsequent career. The history of the conquistadores is rife with accounts of rivalry, jockeying for positions, mutiny, and betrayal.\n\nUpon his arrival in 1504 in Santo Domingo, the capital of Hispaniola, the 18-year-old Cortés registered as a citizen, which entitled him to a building plot and land to farm. Soon afterwards, Nicolás de Ovando, still the governor, gave him an \"encomienda\" and made him a notary of the town of Azua de Compostela. His next five years seemed to help establish him in the colony; in 1506, Cortés took part in the conquest of Hispaniola and Cuba, receiving a large estate of land and Indian slaves for his efforts from the leader of the expedition.\n\nIn 1511, Cortés accompanied Diego Velázquez de Cuéllar, an aide of the Governor of Hispaniola, in his expedition to conquer Cuba. Velázquez was appointed Governor of New Spain. At the age of 26, Cortés was made clerk to the treasurer with the responsibility of ensuring that the Crown received the \"quinto\", or customary one fifth of the profits from the expedition.\n\nThe Governor of Cuba, Diego Velázquez, was so impressed with Cortés that he secured a high political position for him in the colony. He became secretary for Governor Velázquez. Cortés was twice appointed municipal magistrate (\"alcalde\") of Santiago. In Cuba, Cortés became a man of substance with an \"encomienda\" to provide Indian labor for his mines and cattle. This new position of power also made him the new source of leadership, which opposing forces in the colony could then turn to. In 1514, Cortés led a group which demanded that more Indians be assigned to the settlers.\n\nAs time went on, relations between Cortés and Governor Velázquez became strained. This began once news of Juan de Grijalva, establishing a colony on the mainland where there was a bonanza of silver and gold, reached Velázquez; it was decided to send him help. Cortés was appointed Captain-General of this new expedition in October 1518, but was advised to move fast before Velázquez changed his mind.\n\nWith Cortés's experience as an administrator, knowledge gained from many failed expeditions, and his impeccable rhetoric he was able to gather six ships and 300 men, within a month. Predictably, Velázquez's jealousy exploded and decided to place the leadership of the expedition in other hands. However, Cortés quickly gathered more men and ships in other Cuban ports.\n\nCortés also found time to become romantically involved with Catalina Xuárez (or Juárez), the sister-in-law of Governor Velázquez. Part of Velázquez's displeasure seems to have been based on a belief that Cortés was trifling with Catalina's affections. Cortés was temporarily distracted by one of Catalina's sisters but finally married Catalina, reluctantly, under pressure from Governor Velázquez. However, by doing so, he hoped to secure the good will of both her family and that of Velázquez.\n\nIt was not until he had been almost 15 years in the Indies, that Cortés began to look beyond his substantial status as mayor of the capital of Cuba and as a man of affairs in the thriving colony. He missed the first two expeditions, under the orders of Francisco Hernández de Córdoba and then Juan de Grijalva, sent by Diego Velázquez to Mexico in 1518.\n\nIn 1518, Velázquez put Cortés in command of an expedition to explore and secure the interior of Mexico for colonization. At the last minute, due to the old argument between the two, Velázquez changed his mind and revoked Cortés's charter. He ignored the orders and, in an act of open mutiny, went anyway in February 1519. He stopped in Trinidad, Cuba, to hire more soldiers and obtain more horses. Accompanied by about 11 ships, 500 men, 13 horses, and a small number of cannon, Cortés landed on the Yucatan Peninsula in Mayan territory. There he encountered Geronimo de Aguilar, a Spanish Franciscan priest who had survived a shipwreck followed by a period in captivity with the Maya, before escaping. Aguilar had learned the Chontal Maya language and was able to translate for Cortés.\n\nIn March 1519, Cortés formally claimed the land for the Spanish crown. Then he proceeded to Tabasco, where he met with resistance and won a battle against the natives. He received twenty young indigenous women from the vanquished natives, and he converted them all to Christianity.\n\nAmong these women was La Malinche, his future mistress and mother of his son Martín. Malinche knew both the Nahuatl language and the Chontal Maya, thus enabling Cortés to communicate with the Aztecs through Aguilar. At San Juan de Ulúa on Easter Sunday 1519, Cortés met with Moctezuma II's Aztec Empire governors Tendile and Pitalpitoque.\nIn July 1519, his men took over Veracruz. By this act, Cortés dismissed the authority of the Governor of Cuba to place himself directly under the orders of King Charles. In order to eliminate any ideas of retreat, Cortés scuttled his ships.\n\nIn Veracruz, he met some of the tributaries of the Aztecs and asked them to arrange a meeting with Moctezuma II, the \"tlatoani\" (ruler) of the Aztec Empire. Moctezuma repeatedly turned down the meeting, but Cortés was determined. Leaving a hundred men in Veracruz, Cortés marched on Tenochtitlan in mid-August 1519, along with 600 soldiers, 15 horsemen, 15 cannons, and hundreds of indigenous carriers and warriors.\n\nOn the way to Tenochtitlan, Cortés made alliances with indigenous peoples such as the Totonacs of Cempoala and the Nahuas of Tlaxcala. The Otomis initially, and then the Tlaxcalans fought the Spanish a series of three battles from 2 Sept. to 5 Sept. 1519, and at one point Diaz remarked, \"they surrounded us on every side\". After Cortés continued to release prisoners with messages of peace, and realizing the Spanish were enemies of Montezuma, Xicotencatl the Elder, and Maxixcatzin, persuaded the Tlaxcalan warleader, Xicotencatl the Younger, that it would be better to ally with the newcomers than to kill them.\n\nIn October 1519, Cortés and his men, accompanied by about 1,000 Tlaxcalteca, marched to Cholula, the second largest city in central Mexico. Cortés, either in a pre-meditated effort to instill fear upon the Aztecs waiting for him at Tenochtitlan or (as he later claimed, when he was being investigated) wishing to make an example when he feared native treachery, massacred thousands of unarmed members of the nobility gathered at the central plaza, then partially burned the city.\nBy the time he arrived in Tenochtitlan the Spaniards had a large army. On November 8, 1519, they were peacefully received by Moctezuma II. Moctezuma deliberately let Cortés enter the Aztec capital, the island city of Tenochtitlan, hoping to get to know their weaknesses better and to crush them later.\n\nMoctezuma gave lavish gifts of gold to the Spaniards which, rather than placating them, excited their ambitions for plunder. In his letters to King Charles, Cortés claimed to have learned at this point that he was considered by the Aztecs to be either an emissary of the feathered serpent god Quetzalcoatl or Quetzalcoatl himself – a belief which has been contested by a few modern historians. But quickly Cortés learned that several Spaniards on the coast had been killed by Aztecs while supporting the Totonacs, and decided to take Moctezuma as a hostage in his own palace, indirectly ruling Tenochtitlán through him.\n\nMeanwhile, Velázquez sent another expedition, led by Pánfilo de Narváez, to oppose Cortés, arriving in Mexico in April 1520 with 1,100 men. Cortés left 200 men in Tenochtitlan and took the rest to confront Narváez. He overcame Narváez, despite his numerical inferiority, and convinced the rest of Narváez's men to join him. In Mexico, one of Cortés's lieutenants Pedro de Alvarado, committed the \"massacre in the Great Temple\", triggering a local rebellion.\n\nCortés speedily returned to Tenochtitlán. On July 1, 1520 Moctezuma was killed (the Spaniards claimed he was stoned to death by his own people; others claim he was murdered by the Spanish once they realized his inability to placate the locals). Faced with a hostile population, Cortés decided to flee for Tlaxcala. During the \"Noche Triste\" (June 30 – July 1, 1520), the Spaniards managed a narrow escape from Tenochtitlan across the Tlacopan causeway, while their rearguard was being massacred. Much of the treasure looted by Cortés was lost (as well as his artillery) during this panicked escape from Tenochtitlán.\n\nAfter a battle in Otumba, they managed to reach Tlaxcala, having lost 870 men. With the assistance of their allies, Cortés's men finally prevailed with reinforcements arriving from Cuba. Cortés began a policy of attrition towards Tenochtitlan, cutting off supplies and subduing the Aztecs' allied cities. The siege of Tenochtitlán ended with Spanish victory and the destruction of the city.\n\nIn January 1521, Cortés countered a conspiracy against him, headed by Antonio de Villafana, who was hanged for the offense. Finally, with the capture of Cuauhtémoc, the \"tlatoani\" (ruler) of Tenochtitlán, on August 13, 1521, the Aztec Empire was captured, and Cortés was able to claim it for Spain, thus renaming the city Mexico City. From 1521 to 1524, Cortés personally governed Mexico.\n\nMany historical sources have conveyed an impression that Cortés was unjustly treated by the Spanish Crown, and that he received nothing but ingratitude for his role in establishing New Spain. This picture is the one Cortés presents in his letters and in the later biography written by Francisco López de Gómara. However, there may be more to the picture than this. Cortés's own sense of accomplishment, entitlement, and vanity may have played a part in his deteriorating position with the king:\n\nKing Charles appointed Cortés as governor, captain general and chief justice of the newly conquered territory, dubbed \"New Spain of the Ocean Sea\". But also, much to the dismay of Cortés, four royal officials were appointed at the same time to assist him in his governing – in effect, submitting him to close observation and administration. Cortés initiated the construction of Mexico City, destroying Aztec temples and buildings and then rebuilding on the Aztec ruins what soon became the most important European city in the Americas.\n\nCortés managed the founding of new cities and appointed men to extend Spanish rule to all of New Spain, imposing the \"encomienda\" system in 1524. He reserved many encomiendas for himself and for his retinue, which they considered just rewards for their accomplishment in conquering central Mexico. However, later arrivals and members of factions antipathetic to Cortés complained of the favoritism that excluded them.\nIn 1523, the Crown (possibly influenced by Cortés's enemy, Bishop Fonseca), sent a military force under the command of Francisco de Garay to conquer and settle the northern part of Mexico, the region of Pánuco. This was another setback for Cortés who mentioned this in his fourth letter to the King in which he describes himself as the victim of a conspiracy by his archenemies Diego Velázquez de Cuéllar, Diego Columbus and Bishop Fonseca as well as Francisco Garay. The influence of Garay was effectively stopped by this appeal to the King who sent out a decree forbidding Garay to interfere in the politics of New Spain, causing him to give up without a fight.\n\nAlthough Cortés had flouted the authority of Diego Velázquez in sailing to the mainland and then leading an expedition of conquest, Cortés's spectacular success was rewarded by the crown with a coat of arms, a mark of high honor, following the conqueror's request. The document granting the coat of arms summarizes Cortés's accomplishments in the conquest of Mexico. The proclamation of the king says in part We, respecting the many labors, dangers, and adventures which you underwent as stated above, and so that there might remain a perpetual memorial of you and your services and that you and your descendants might be more fully honored...it is our will that besides your coat of arms of your lineage, which you have, you may have and bear as your coat of arms, known and recognized, a shield... The grant specifies the iconography of the coat of arms, the central portion divided into quadrants. In the upper portion, there is a \"black eagle with two heads on a white field, which are the arms of the empire.\" Below that is a \"golden lion on a red field, in memory of the fact that you, the said Hernando Cortés, by your industry and effort brought matters to the state described above\" (i.e., the conquest). The specificity of the other two quadrants is linked directly to Mexico, with one quadrant showing three crowns representing the three Aztec emperors of the conquest era, Moctezuma, Cuitlahuac, and Cuauhtemoc and the other showing the Aztec capital of Tenochtitlan. Encircling the central shield are symbols of the seven city-states around the lake and their lords that Cortés defeated, with the lords \"to be shown as prisoners bound with a chain which shall be closed with a lock beneath the shield.\"\n\nCortés's wife Catalina Súarez arrived in New Spain from sometime around summer 1522, along with her sister and brother. His marriage to Catalina was at this point extremely awkward, since she was a kinswoman of governor of Cuba Diego Velázquez, whose authority Cortés had thrown off and was now his enemy. Catalina lacked the noble title of \"doña,\" so at this point his alliance with her no longer raised his status. Their marriage had been childless. Since Cortés had sired children with a variety of indigenous women, including a son ca. 1522 by his cultural translator, Doña Marina, Cortés knew he was capable of fathering children. Cortés's only male heir at this point was illegitimate, but nonetheless named after Cortés's father, Martín Cortés. This natural son Martín Cortés was sometimes called \"El Mestizo.\" Cortés's wife, Catalina Suárez, died under mysterious circumstances the night of November 1–2, 1522. There were accusations at the time that Cortés had murdered his wife. There was an investigation into her death, interviewing a variety of household residents and others. The documentation of the investigation published in the nineteenth century in Mexico and archival documents uncovered in the twentieth century. The death of Catalina Suárez had produced a scandal and a major investigation, but weathering that Cortés was now free to marry someone of high status more appropriate to his wealth and power. In 1526, he had built an imposing residence for himself, the Palace of Cortés in Cuernavaca, in a region close to the capital where he had extensive encomienda holdings. In 1529 he had been accorded the noble designation of \"don\", but more importantly was given the noble title of Marquis of the Valley of Oaxaca and married the Spanish noblewoman Doña Juana de Zúñiga. The marriage produced three children, including another son, who was also named Martín. As the first-born legitimate son, Don Martín Cortés y Zúñiga was now Cortés's heir and succeeded his father as holder of the title and estate of the Marquisate of the Valley of Oaxaca. Cortés's legitimate daughters were Doña Maria, Doña Catalina, and Doña Juana.\n\nSince the conversion to Christianity of indigenous peoples was an essential and integral part of the extension of Spanish power, making formal provisions for that conversion once the military conquest was completed was an important task for Cortés. During the Age of Discovery, the Catholic Church had seen early attempts at conversion in the Caribbean islands by Spanish friars, particularly mendicant orders. Cortés made a request to the Spanish monarch to send Franciscan and Dominican friars to Mexico to begin the daunting work of converting vast populations indigenous to Christianity. In his fourth letter to the king, Cortés pleaded for friars rather than diocesan or secular priests because those clerics were in his view a serious danger to the Indians' conversion. If these people [Indians] were now to see the affairs of the Church and the service of God in the hands of canons or other dignitaries, and saw them indulge in the vices and profanities now common in Spain, knowing that such men were the ministers of God, it would bring our Faith into much harm that I believe any further preaching would be of no avail. He wished the mendicants to be the main evangelists. Mendicant friars did not usually have full priestly powers to perform all the sacraments needed for conversion of the Indians and growth of the neophytes in the Christian faith, so Cortés laid out a solution to this to the king. Your Majesty should likewise beseech His Holiness [the pope] to grant these powers to the two principal persons in the religious orders that are to come here, and that they should be his delegates, one from the Order of St. Francis and the other from the Order of St. Dominic. They should bring the most extensive powers Your Majesty is able to obtain, for, because these lands are so far from the Church of Rome, and we, the Christians who now reside here and shall do so in the future, are so far from the proper remedies of our consciences and, as we are human, so subject to sin, it is essential that His Holiness should be generous with us and grant to these persons most extensive powers, to be handed down to persons actually in residence here whether it be given to the general of each order or to his provincials.\nThe Franciscans arrived in May 1524, a symbolically powerful group of twelve known as the Twelve Apostles of Mexico, led by Fray Martín de Valencia. Franciscan Geronimo de Mendieta claimed that Cortés's most important deed was the way he met this first group of Franciscans. The conqueror himself was said to have met the friars as they approached the capital, kneeling at the feet of the friars who had walked from the coast. This story was used by Franciscans as a demonstration of Cortés's piety and humility was a powerful message to all, including the Indians, that Cortés's earthly power was subordinate to the spiritual power of the friars. However, one of the first twelve Franciscans, Fray Toribio de Benavente Motolinia does not mention it in his history. Cortés and the Franciscans had a particularly strong alliance in Mexico, with Franciscans seeing him as \"the new Moses\" for conquering Mexico and opening it to Christian evangelization. In Motolinia's 1555 response to Dominican Bartolomé de Las Casas, he praises Cortés. And as to those who murmur against the Marqués del Valle [Cortés], God rest him, and who try to blacken and obscure his deeds, I believe that before God their deeds are not as acceptable as those of the Marqués. Although as a human he was a sinner, he had faith and works of a good Christian, and a great desire to employ his life and property in widening and augmenting the fair of Jesus Christ, and dying for the conversion of these gentiles... Who has loved and defended the Indians of this new world like Cortés?... Through this captain, God opened the door for us to preach his holy gospel and it was he who caused the Indians to revere the holy sacraments and respect the ministers of the church.\nIn Fray Bernardino de Sahagún's 1585 revision of the conquest narrative first codified as Book XII of the Florentine Codex, there are laudatory references to Cortés that do not appear in the earlier text from the indigenous perspective. Whereas Book XII of the Florentine Codex concludes with an account of Spaniards' search for gold, in Sahagún's 1585 revised account, he ends with praise of Cortés for requesting the Franciscans be sent to Mexico to convert the Indians.\n\nFrom 1524 to 1526, Cortés headed an expedition to Honduras where he defeated Cristóbal de Olid, who had claimed Honduras as his own under the influence of the Governor of Cuba Diego Velázquez. Fearing that Cuauhtémoc might head an insurrection in Mexico, he brought him with him to Honduras. In a controversial move, Cuauhtémoc was executed during the journey. Raging over Olid's treason, Cortés issued a decree to arrest Velázquez, whom he was sure was behind Olid's treason. This, however, only served to further estrange the Crown of Castile and the Council of Indies, both of which were already beginning to feel anxious about Cortés's rising power.\nCortés's fifth letter to King Charles attempts to justify his conduct, concludes with a bitter attack on \"various and powerful rivals and enemies\" who have \"obscured the eyes of your Majesty.\" Charles, who was also Holy Roman Emperor, had little time for distant colonies (much of Charles's reign was taken up with wars with France, the German Protestants and the expanding Ottoman Empire), except insofar as they contributed to finance his wars. In 1521, year of the Conquest, Charles was attending to matters in his German domains and Bishop Adrian of Utrecht functioned as regent in Spain.\n\nVelázquez and Fonseca persuaded the regent to appoint a commissioner with powers, (a \"Juez de residencia\", Luis Ponce de León), to investigate Cortés's conduct and even arrest him. Cortés was once quoted as saying that it was \"more difficult to contend against (his) own countrymen than against the Aztecs.\" Governor Diego Velázquez continued to be a thorn in his side, teaming up with Bishop Juan Rodríguez de Fonseca, chief of the Spanish colonial department, to undermine him in the Council of the Indies.\n\nA few days after Cortés's return from his expedition, Ponce de León suspended Cortés from his office of governor of New Spain. The Licentiate then fell ill and died shortly after his arrival, appointing Marcos de Aguilar as \"alcalde mayor\". The aged Aguilar also became sick and appointed Alonso de Estrada governor, who was confirmed in his functions by a royal decree in August 1527. Cortés, suspected of poisoning them, refrained from taking over the government.\n\nEstrada sent Diego de Figueroa to the south. De Figueroa raided graveyards and extorted contributions, meeting his end when the ship carrying these treasures sank. Albornoz persuaded Alonso de Estrada to release Salazar and Chirinos. When Cortés complained angrily after one of his adherents' hands was cut off, Estrada ordered him exiled. Cortés sailed for Spain in 1528 to appeal to King Charles.\n\nIn 1528, Cortés returned to Spain to appeal to the justice of his master, Charles V. Juan Altamirano and Alonso Valiente stayed in Mexico and acted as Cortés' representatives during his absence. Cortés presented himself with great splendor before Charles V's court. By this time Charles had returned and Cortés forthrightly responded to his enemy's charges. Denying he had held back on gold due the crown, he showed that he had contributed more than the quinto (one-fifth) required. Indeed, he had spent lavishly to build the new capital of Mexico City on the ruins of the Aztec capital of Tenochtitlán, leveled during the siege that brought down the Aztec empire.\n\nHe was received by Charles with every distinction, and decorated with the order of Santiago. In return for his efforts in expanding the still young Spanish Empire, Cortés was rewarded in 1529 by being accorded the noble title of \"don\" but more importantly named the \"\"Marqués del Valle de Oaxaca\"\" Marquisate of the Valley of Oaxaca and married the Spanish noblewoman, Doña Juana Zúñiga, after the 1522 death of his much less distinguished first wife, Catalina Suárez. The noble title and senorial estate of the Marquesado was passed down to his descendants until 1811. The Oaxaca Valley was one of the wealthiest region of New Spain, and Cortés had 23,000 vassals in 23 named encomiendas in perpetuity.\n\nAlthough confirmed in his land holdings and vassals, he was not reinstated as governor and was never again given any important office in the administration of New Spain. During his travel to Spain, his property was mismanaged by abusive colonial administrators. He sided with local natives in a lawsuit. The natives documented the abuses in the Huexotzinco Codex.\n\nThe entailed estate and title passed to his legitimate son Don Martín Cortés upon Cortés's death in 1547, who became the Second Marquis. Don Martín's association with the so-called Encomenderos' Conspiracy endangered the entailed holdings, but they were restored and remained the continuing reward for Hernán Cortés's family through the generations.\n\nCortés returned to Mexico in 1530 with new titles and honors, but with diminished power. Although Cortés still retained military authority and permission to continue his conquests, viceroy Don Antonio de Mendoza was appointed in 1535 to administer New Spain's civil affairs. This division of power led to continual dissension, and caused the failure of several enterprises in which Cortés was engaged.\n\nOn returning to Mexico, Cortés found the country in a state of anarchy. There was a strong suspicion in court circles of an intended rebellion by Cortés, and a charge was brought against him that cast a fatal blight upon his character and plans. He was accused of murdering his first wife. The proceedings of the investigation were kept secret.\n\nNo report, either exonerating or condemning Cortés, was published. Had the Government declared him innocent, it would have greatly increased his popularity. Had it declared him a criminal, a crisis would have been precipitated by the accused and his party. Silence was the only safe policy, but that silence is suggestive that grave danger was feared from his influence.\n\nAfter reasserting his position and reestablishing some sort of order, Cortés retired to his estates at Cuernavaca, about 30 miles (48 km) south of Mexico City. There he concentrated on the building of his palace and on Pacific exploration. Remaining in Mexico between 1530 and 1541, Cortés quarreled with Nuño Beltrán de Guzmán and disputed the right to explore the territory that is today California with Antonio de Mendoza, the first viceroy.\n\nCortes acquired several silver mines in Zumpango del Rio in 1534. By the early 1540s, he owned 20 silver mines in Sultepec, 12 in Taxco, and 3 in Zacualpan. Earlier, Cortes had claimed the silver in the Tamazula area.\n\nIn 1536, Cortés explored the northwestern part of Mexico and discovered the Baja California Peninsula. Cortés also spent time exploring the Pacific coast of Mexico. The Gulf of California was originally named the \"Sea of Cortes\" by its discoverer Francisco de Ulloa in 1539. This was the last major expedition by Cortés.\n\nAfter his exploration of Baja California, Cortés returned to Spain in 1541, hoping to confound his angry civilians, who had brought many lawsuits against him (for debts, abuse of power, etc.).\n\nOn his return he was utterly neglected, and could scarcely obtain an audience. On one occasion he forced his way through a crowd that surrounded the emperor's carriage, and mounted on the footstep. The emperor, astounded at such audacity, demanded of him who he was. \"I am a man,\" replied Cortés proudly, \"who has given you more provinces than your ancestors left you cities.\"\n\nThe emperor finally permitted Cortés to join him and his fleet commanded by Andrea Doria at the great expedition against Algiers in the Barbary Coast in 1541, which was then part of the Ottoman Empire and was used as a base by Hayreddin Barbarossa, a famous Turkish corsair and Admiral-in-Chief of the Ottoman Fleet. During this unfortunate campaign, which was his last, Cortés was almost drowned in a storm that hit his fleet while he was pursuing Barbarossa.\n\nHaving spent a great deal of his own money to finance expeditions, he was now heavily in debt. In February 1544 he made a claim on the royal treasury, but was ignored for the next three years. Disgusted, he decided to return to Mexico in 1547. When he reached Seville, he was stricken with dysentery. He died in Castilleja de la Cuesta, Seville province, on December 2, 1547, from a case of pleurisy at the age of 62.\n\nLike Columbus, he died a wealthy but embittered man. He left his many mestizo and white children well cared for in his will, along with every one of their mothers. He requested in his will that his remains eventually be buried in Mexico. Before he died he had the Pope remove the \"natural\" status of four of his children (legitimizing them in the eyes of the church), including Martin, the son he had with Doña Marina (also known as La Malinche), said to be his favourite.. His daughter, Doña Catalina, however, died shortly after her father's death.\n\nAfter his death his body has been moved more than eight times for several reasons. On December 4, 1547 he was buried in the mausoleum of the Duke of Medina in the church of San Isidoro del Campo, Sevilla. Three years later (1550) due to the space being required by the duke, his body was moved to the altar of Santa Catarina in the same church. In his testament, Cortés asked for his body to be buried in the monastery he had ordered to be built in Coyoacan in México, ten years after his death, but the monastery was never built. So in 1566, his body was sent to New Spain and buried in the church of \"San Francisco de Texcoco\", where his mother and one of his sisters were buried.\n\nIn 1629, \"Don Pedro Cortés fourth \"Marquez del Valle\", his last male descendant, died, so the viceroy decided to move the bones of Cortés along with those of his descendant to the Franciscan church in México. This was delayed for nine years, while his body stayed in the main room of the palace of the viceroy. Eventually it was moved to the Sagrario of Franciscan church, where it stayed for 87 years. In 1716, it was moved to another place in the same church. In 1794, his bones were moved to the \"Hospital de Jesus\" (founded by Cortés), where a statue by Tolsá and a mausoleum were made. There was a public ceremony and all the churches in the city rang their bells.\nIn 1823, after the independence of México, it seemed imminent that his body would be desecrated, so the mausoleum was removed, the statue and the coat of arms were sent to Palermo, Sicily, to be protected by the Duke of Terranova. The bones were hidden, and everyone thought that they had been sent out of México. In 1836, his bones were moved to another place in the same building.\n\nIt was not until November 24, 1946 that they were rediscovered, thanks to the discovery of a secret document by Lucas Alamán. His bones were put in charge of the Instituto Nacional de Antropología e Historia (INAH). The remains were authenticated by INAH. They were then restored to the same place, this time with a bronze inscription and his coat of arms. When the bones were first rediscovered, the supporters of the Hispanic tradition in Mexico were excited, but one supporter of an indigenist vision of Mexico \"proposed that the remains be publicly burned in front of the statue of Cuauhtemoc, and the ashes flung into the air.\" Following the discovery and authentication of Cortés's remains, there was a discovery of what were described as the bones of Cuauhtémoc occurred, resulting in the so-called \"battle of the bones\" In 1981, when a copy of the bust by Tolsa was put in the church, there was a failed attempt to destroy his bones.\n\nThere are relatively few sources to the early life of Cortés; his fame arose from his participation in the conquest of Mexico and it was only after this that people became interested in reading and writing about him.\n\nProbably the best source is his letters to the king which he wrote during the campaign in Mexico, but they are written with the specific purpose of putting his efforts in a favourable light and so must be read critically. Another main source is the biography written by Cortés's private chaplain Lopez de Gómara, which was written in Spain several years after the conquest. Gómara never set foot in the Americas and knew only what Cortés had told him, and he had an affinity for knightly romantic stories which he incorporated richly in the biography. The third major source is written as a reaction to what its author calls \"the lies of Gomara\", the eyewitness account written by the Conquistador Bernal Díaz del Castillo does not paint Cortés as a romantic hero but rather tries to emphasize that Cortés's men should also be remembered as important participants in the undertakings in Mexico.\n\nIn the years following the conquest more critical accounts of the Spanish arrival in Mexico were written. The Dominican friar Bartolomé de Las Casas wrote his \"A Short Account of the Destruction of the Indies\" which raises strong accusations of brutality and heinous violence towards the Indians; accusations against both the conquistadors in general and Cortés in particular. The accounts of the conquest given in the Florentine Codex by the Franciscan Bernardino de Sahagún and his native informants are also less than flattering towards Cortés. The scarcity of these sources has led to a sharp division in the description of Cortés's personality and a tendency to describe him as either a vicious and ruthless person or a noble and honorable cavalier.\n\nIn México there are few representations of Cortés. However, many landmarks still bear his name, from the castle Palacio de Cortés in the city of Cuernavaca to some street names throughout the republic.\n\nThe only authentic monuments are in Mexico City at the pass between the volcanoes Iztaccíhuatl and Popocatépetl where Cortés took his soldiers on their march to Mexico City. It is known as the Paso de Cortés.\n\nThe muralist Diego Rivera painted several representation of him but the most famous, depicts him as a powerful and ominous figure along with Malinche in a mural in the National Palace in Mexico City.\nIn 1981, President Lopez Portillo tried to bring Cortés to public recognition. First, he made public a copy of the bust of Cortés made by Manuel Tolsá in the Hospital de Jesús Nazareno with an official ceremony, but soon a nationalist group tried to destroy it, so it had to be taken out of the public. Today the copy of bust is in the \"Hospital de Jesús Nazareno\" while the original is in Naples, Italy, in the Villa Pignatelli.\n\nLater, another monument, known as \"Monumento al Mestizaje\" by Julián Martínez y M. Maldonado (1982) was commissioned by Mexican president José López Portillo to be put in the \"Zócalo\" (Main square) of Coyoacan, near the place of his country house, but it had to be removed to a little known park, the Jardín Xicoténcatl, Barrio de San Diego Churubusco, to quell protests. The statue depicts Cortés, Malinche and their son Martín.\n\nThere is another statue by Sebastián Aparicio, in Cuernavaca, was in a hotel \"El casino de la selva\". Cortés is barely recognizable, so it sparked little interest. The hotel was closed to make a commercial center, and the statue was put out of public display by Costco the builder of the commercial center.\n\nHernán Cortés is a character in the opera \"La Conquista\" (2005) by Italian composer Lorenzo Ferrero, which depicts the major episodes of the Spanish conquest of the Aztec Empire in 1521.\n\nCortés' personal account of the conquest of Mexico is narrated in his five letters addressed to Charles V. These five letters, the \"cartas de relación\", are Cortés' only surviving writings. See \"Letters and Dispatches of Cortés\", translated by George Folsom (New York, 1843); Prescott's \"Conquest of Mexico\" (Boston, 1843); and Sir Arthur Helps's \"Life of Hernando Cortes\" (London, 1871).\n\nHis first letter is lost, and the one from the municipality of Veracruz has to take its place. It was published for the first time in volume IV of \"Documentos para la Historia de España\", and subsequently reprinted. The first \"carta de relación\" is available online at the University of Wisconsin.\n\nThe \"Segunda Carta de Relacion\", bearing the date of October 30, 1520, appeared in print at Seville in 1522. The third letter, dated May 15, 1522, appeared at Seville in 1523. The fourth, October 20, 1524, was printed at Toledo in 1525. The fifth, on the Honduras expedition, is contained in volume IV of the \"Documentos para la Historia de España\". The important letter mentioned in the text has been published under the heading of \"Carta inédita de Cortés\" by Ycazbalceta. A great number of minor documents, either by Cortés or others, for or against him, are dispersed through the voluminous collection above cited and through the \"Colección de Documentos de Indias\", as well as in the \"Documentos para la Historia de México\" of Ycazbalceta. There are a number of reprints and translations of Cortés's writings into various languages.\n\nNatural children of Don Hernán Cortés\n\nHe married twice: firstly in Cuba to Catalina Suárez Marcaida, who died at Coyoacán in 1522 without issue, and secondly in 1529 to \"doña\" Juana Ramírez de Arellano de Zúñiga, daughter of \"don\" Carlos Ramírez de Arellano, 2nd Count of Aguilar and wife the Countess \"doña\" Juana de Zúñiga, and had:\n\n\n\n\n", "id": "14013", "title": "Hernán Cortés"}
{"url": "https://en.wikipedia.org/wiki?curid=14015", "text": "Herstory\n\nHerstory is history written from a feminist perspective, emphasizing the role of women, or told from a woman's point of view. It is a neologism coined as a pun with the word \"history\", as part of a feminist critique of conventional historiography, which in their opinion is traditionally written as \"his story\", i.e., from the masculine point of view. (The word \"history\"—from the Ancient Greek ἱστορία, or historia, meaning \"knowledge obtained by inquiry\"—is etymologically unrelated to the possessive pronoun \"his\".)\n\nThe herstory movement has spawned women-centered presses, such as Virago Press in 1973, which publishes fiction and non-fiction by noted women authors like Janet Frame and Sarah Dunant.\n\nRobin Morgan, in a book of her selected writings states that the debut of the word \"herstory\" was in the byline of her article \"Goodbye to All That\", in early 1970, in the first issue of the \"underground\" New Left newspaper \"Rat\" after it was overtaken by women to clean it of sexism. She writes that she identified herself as a member of W.I.T.C.H., decoding the acronym as \"\"Women Inspired to Commit Herstory\".\n\nIn 1976, Casey Miller and Kate Swift wrote in \"Words & Women,\"\n\nDuring the 1970s and 1980s, second-wave feminists saw the study of history as a male-dominated intellectual enterprise and presented \"herstory\" as a means of compensation. The term, intended to be both serious and comic, became a rallying cry used on T-shirts and buttons as well as in academia.\n\nChristina Hoff Sommers has been a vocal critic of the concept of herstory, and presented her argument against the movement in her 1994 book, \"Who Stole Feminism?\" Sommers defined herstory as an attempt to infuse education with ideology, at the expense of knowledge. The \"gender feminists\", as she termed them, were the band of feminists responsible for the movement, which she felt amounted to negationism. She regarded most attempts to make historical studies more female-inclusive as being artificial in nature, and an impediment to progress.\n\nProfessor and author Devoney Looser has criticized the concept of herstory for overlooking the contributions that some women made as historians before the twentieth century.\n\nThe Global Language Monitor, a nonprofit group that analyzes and tracks trends in language, named \"herstory\" the third most \"politically incorrect\" word of 2006—rivaled only by \"\"macaca\"\" and \"\"Global Warming Denier\".\"\n\nBooks published on the topic include:\n\n", "id": "14015", "title": "Herstory"}
{"url": "https://en.wikipedia.org/wiki?curid=14017", "text": "House of Cards (UK TV series)\n\nHouse of Cards is a 1990 British political television drama serial in four episodes, set after the end of Margaret Thatcher's tenure as Prime Minister of the United Kingdom. It was televised by the BBC from 18 November to 9 December 1990, to critical and popular acclaim.\n\nAndrew Davies adapted the story from a novel written by Michael Dobbs, a former Chief of Staff at Conservative Party headquarters. Neville Teller also dramatised Dobbs's novel for BBC World Service in 1996, and it had two television sequels (\"To Play the King\" and \"The Final Cut\"). The opening and closing theme music for those TV series is entitled \"Francis Urquhart's March.\"\n\n\"House of Cards\" was ranked 84th in the British Film Institute list of the 100 Greatest British Television Programmes in 2000. In 2013, the serial and the Dobbs novel were the basis for a US adaptation set in Washington, D.C., commissioned and released by Netflix.\n\nThe antihero of \"House of Cards\" is Francis Urquhart, a fictional Chief Whip of the Conservative Party, played by Ian Richardson. The plot follows his amoral and manipulative scheme to become leader of the governing party and, thus, Prime Minister of the United Kingdom.\n\nMichael Dobbs did not envisage writing the second and third books, as Urquhart dies at the end of the first novel. The screenplay of the BBC's dramatisation of \"House of Cards\" differs from the book, and hence allows future series. Dobbs wrote two following books, \"To Play the King\" and \"The Final Cut\", which were televised in 1993 and 1995, respectively.\n\n\"House of Cards\" was said to draw from Shakespeare's plays \"Macbeth\" and \"Richard III\", both of which feature main characters who are corrupted by power and ambition. Richardson has a Shakespearean background and said he based his characterisation of Urquhart on Shakespeare's portrayal of Richard III.\n\nUrquhart frequently talks through the camera to the audience, breaking the fourth wall using the aside.\n\nAfter Margaret Thatcher's resignation, the ruling Conservative Party is about to elect a new leader. Francis Urquhart (Ian Richardson), an MP and the Government Chief Whip in the House of Commons, introduces viewers to the contestants, from which Henry \"Hal\" Collingridge (David Lyon) emerges victorious. Urquhart is secretly contemptuous of the well-meaning but weak Collingridge, but expects a promotion to a senior position in the Cabinet. After the general election, which the party wins by a reduced majority, Urquhart submits his suggestions for a cabinet reshuffle that includes his desired promotion. However, Collingridge – citing Harold Macmillan's political demise after the 1962 Night of the Long Knives – effects no changes at all. Urquhart resolves to oust Collingridge, with encouragement from his wife, Elizabeth (Diane Fletcher).\n\nAt the same time, with Elizabeth's blessing, Urquhart begins an affair with Mattie Storin (Susannah Harker), a junior political reporter at a Conservative-leaning tabloid newspaper called \"The Chronicle\". The affair allows Urquhart to manipulate Mattie and indirectly skew her coverage of the Conservative leadership contest in his favour. Mattie has an apparent Electra complex; she finds appeal in Urquhart's much older age and later refers to him as \"Daddy.\" Another unwitting pawn is Roger O'Neill (Miles Anderson), the party's cocaine-addicted public relations consultant.\n\nUrquhart blackmails O'Neill into leaking information on budget cuts that humiliates Collingridge during the Prime Minister's Questions. Later, he blames party chairman Lord \"Teddy\" Billsborough (Nicholas Selby) for leaking an internal poll showing a drop in Tory numbers, leading Collingridge to sack him. As Collingridge's image suffers, Urquhart encourages ultraconservative Foreign Secretary Patrick Woolton (Malcolm Tierney) and \"Chronicle\" owner Benjamin Landless to support his removal. Urquhart also poses as Collingridge's alcoholic brother, Charles, to trade shares in a chemical company about to benefit from advance information confidential to the government. Consequently, Collingridge becomes falsely accused of insider trading and is forced to resign.\n\nIn the ensuing leadership race, Urquhart initially feigns unwillingness to stand before announcing his candidacy. With the help of his underling, Tim Stamper (Colin Jeavons), Urquhart goes about making sure his competitors drop out of the race: Health Secretary Peter MacKenzie (Christopher Owen) accidentally runs his car over a disabled protester at a demonstration staged by Urquhart and is forced by the public outcry to withdraw, while Education Secretary Harold Earle (Kenneth Gilbert) is blackmailed into withdrawing when Urquhart anonymously sends pictures of him in the company of a rent boy whom Earle had paid for sex.\n\nThe first ballot leaves Urquhart to face Woolton and Michael Samuels, the moderate Environment Secretary supported by Billsborough. Urquhart eliminates Woolton by a prolonged scheme: at the party conference, he pressures O'Neill into persuading his personal assistant and lover, Penny Guy (Alphonsia Emmanuel), to have a one-night stand with Woolton in his suite, which Urquhart records via a bugged ministerial red box. When the tape is sent to Woolton, he is led to assume that Samuels is behind the scheme and backs Urquhart in the contest. Urquhart also receives support from Collingridge, who is unaware of Urquhart's role in his own downfall. Samuels is forced out of the running when the tabloids reveal that he backed leftist causes as a student at University of Cambridge.\n\nStumbling across contradictions in the allegations against Collingridge and his brother, Mattie begins to dig deeper. On Urquhart's orders, O'Neill arranges for her car and flat to be vandalised in a show of intimidation. However, O'Neill becomes increasingly uneasy with what he is being asked to do, and his cocaine addiction adds to his instability. Urquhart mixes O'Neill's cocaine with rat poison, causing him to kill himself when taking the cocaine in a motorway lavatory. Though initially blind to the truth of matters thanks to her relations with Urquhart, Mattie eventually deduces that Urquhart is responsible for O'Neill's death and is behind the unfortunate downfalls of Collingridge and all of Urquhart's rivals.\n\nMattie looks for Urquhart at the point when it seems his victory is certain. She eventually finds him on the roof garden of the Houses of Parliament, where she confronts him. He admits to O'Neill's murder and everything else he has done. He then asks whether he can trust Mattie, and, though she answers in the affirmative, he does not believe her and throws her off the roof onto a van parked below. An unseen person picks up Mattie's tape recorder, which she had been using to secretly record her conversations with Urquhart. The series ends with Urquhart defeating Samuels in the second leadership ballot and being driven to Buckingham Palace to be invited to form a government by Elizabeth II.\n\nIn the first novel, but not in the television series:\n\nWhen the series was reissued in 2013, to coincide with the release of the US version of \"House of Cards\", Dobbs rewrote portions of the novel to bring the series in line with the television mini-series and restore continuity among the three novels. In the 2013 version:\n\n\nThe first installment of the TV series coincidentally aired two days before the Conservative Party leadership election. Author Dobbs said that John Major's leadership headquarters \"came to a halt\" to view the show. During a time of \"disillusionment with politics\", the series \"caught the nation's mood\".\n\nIan Richardson won a Best Actor BAFTA in 1991 for his role as Urquhart, and Andrew Davies won an Emmy for outstanding writing in a miniseries.\n\nThe series ranked 84th in the British Film Institute list of the 100 Greatest British Television Programmes.\n\nThe Urquhart trilogy has been adapted in the United States as \"House of Cards\". The show stars Kevin Spacey as Francis \"Frank\" Underwood, the Majority Whip of the Democratic Party, who schemes and murders his way to becoming President of the United States. It is produced by David Fincher and Spacey's Trigger Street Productions, with the initial episodes directed by Fincher.\n\nThe series, produced and financed by independent studio Media Rights Capital, is one of Netflix's first forays into original programming. Season one was made available online on 1 February 2013. The series is filmed in Baltimore, Maryland. The first season was critically acclaimed and earned four Golden Globe Nominations, including Best Drama, actor, actress and supporting actor, with Robin Wright winning best actress. It also earned nine Primetime Emmy Award nominations, winning three, and was the first show to earn nominations that was broadcast solely via an internet streaming service.\n\nThe drama introduced and popularised the phrase: \"You might very well think that; I couldn't possibly comment\". It was a non-confirmation confirmative statement, used by Urquhart whenever he could not be seen to agree with a leading statement, with the emphasis on either the \"I\" or the \"possibly\", depending on the situation. The phrase was even used in the House of Commons following the series.\n\nA variation on the phrase was written into the TV adaptation of Terry Pratchett's \"Hogfather\" for Death, as an in-joke on the fact that he was voiced by Richardson.\n\nA further variation was used by Nicola Murray, a fictional government minister, in the third series finale of \"The Thick of It\".\n\nIn the U.S. adaptation, the phrase is used by Frank Underwood in the first episode during his initial meeting with Zoe Barnes.\n\n\n", "id": "14017", "title": "House of Cards (UK TV series)"}
{"url": "https://en.wikipedia.org/wiki?curid=14018", "text": "Helen Gandy\n\nHelen W. Gandy (April 8, 1897 – July 7, 1988) was an American civil servant. For 54 years, she was the secretary to Federal Bureau of Investigation director J. Edgar Hoover, who called her \"indispensable\". She exercised great behind-the-scenes influence on Hoover and the workings of the Bureau. Following Hoover's death in 1972, she spent weeks destroying his \"Personal File\", thought to be where the most incriminating material he used to manipulate and control the most powerful figures in Washington was kept.\n\nGandy was born in Rockville, New Jersey, one of three children (two daughters and a son) born to Franklin Dallas and Annie (née Williams) Gandy. She grew up in New Jersey in Fairton or the Port Norris section of Commercial Township (sources differ) and graduated from Bridgeton High School in Bridgeton New Jersey. In 1918, aged 21, she moved to Washington, D.C., where she later took classes at Strayer Business College and George Washington University Law School.\n\nGandy briefly worked in a department store in Washington, D.C. before finding a job as a file clerk at the Justice Department in 1918. Within weeks, she went to work as a typist for Hoover, effective March 25, 1918, having told Hoover in her interview she had \"no immediate plans to marry.\" She, like Hoover, would never marry; both were completely devoted to the Bureau.\n\nWhen Hoover went to the Bureau of Investigation (as it was then known) as its assistant director on August 22, 1921, he specifically requested Gandy return from vacation to help him in the new post. Hoover became director of the Bureau in 1924, and Gandy continued in his service. She was promoted to \"office assistant\" on August 23, 1937 and \"executive assistant\" on October 1, 1939. Though she would receive promotions in her civil service grade subsequently, she would retain her title as executive assistant until her retirement on May 2, 1972, the day Hoover died. Hoover said of her: \"if there is anyone in this Bureau whose services are indispensable I consider Miss Gandy to be that person.\" Despite this, Curt Gentry wrote:\n\nTheirs was a rigidly formal relationship. He'd always called her 'Miss Gandy' (when angry, barking it out as one word). In all those fifty-four years he had never once called her by her first name.\n\nHoover biographers Theoharis and Cox would say \"her stern face recalled Cerberus at the gate,\" a view echoed by Anthony Summers in his life of Hoover, who also pictured Gandy as Hoover's first line of defense against the outside world. When Attorney General Robert F. Kennedy, Hoover's superior, had a direct telephone line installed between their offices, Hoover refused to answer the phone. \"Put that damn thing on Miss Gandy's desk where it belongs,\" Hoover would declare.\n\nGentry described Gandy's influence:\n\nHer genteel manner and pleasant voice contrasted sharply with this domineering presence. Yet behind the politeness was a resolute firmness not unlike his, and no small amount of influence. Many a career in the Bureau had been quietly manipulated by her. Even those who disliked him, praised her, most often commenting on her remarkable ability to get along with all kinds of people. That she had held her position for fifty-four years was the best evidence of this, for it was a Bureau tradition that the closer you were to him, the more demanding he was.\n\nWilliam C. Sullivan, an agent with the Bureau for three decades, reported in his memoir when he worked in the public relations section answering mail from the public, he gave a correspondent the wrong measurements for Hoover's personal popover recipe, relying on memory rather than the files. Gandy, ever protective of her boss, caught the error and brought it to Hoover's attention. The director then placed an official letter of reprimand in Sullivan's file for the lapse. Mark Felt, deputy associate director of the Bureau, wrote in his memoir that Gandy \"was bright and alert and quick-tempered—and completely dedicated to her boss.\"\n\nHoover died during the night of May 1–2, 1972. According to Curt Gentry, who wrote the book \"J Edgar Hoover: The Man and the Secrets\", Hoover's body was not discovered by his live-in cook and general housekeeper, Annie Fields. Rather, it was discovered by James Crawford, who had been Hoover's chauffeur for 37 years. Crawford then yelled out to Fields and Tom Moton (Hoover's new chauffeur after Crawford had retired in January, 1972). Ms. Fields first called Hoover's personal physician, Dr. Robert Choisser, then used another phone to call Clyde Tolson's private number. Tolson then called Helen Gandy's private number with the news of Hoover's death along with orders to begin destroying the files. Within an hour, the \"D List\" (\"d\" standing for destruction) was being distributed, and the destruction of files began. However, \"The New York Times\" quoted an anonymous F.B.I. source in spring 1975, who said: \"Gandy had begun almost a year before Mr. Hoover's death and was instructed to purge the files that were then in his office.\"\nAnthony Summers reported that G. Gordon Liddy had said of his sources in the F.B.I.: \"by the time Gray went in to get the files, Miss Gandy had already got rid of them.\" The day after Hoover died, Gray, who had been named acting director by President Richard Nixon upon Tolson's resignation from that position, went to Hoover's office. Gandy paused from her work to give Gray a tour. He found file cabinets open and packing boxes being filled with papers. She informed him the boxes contained personal papers of Hoover's. Gandy stated Gray flipped through a few files and approved her work, but Gray was to deny he looked at any papers. Gandy also told Gray it would be a week before she could clear Hoover's effects out so Gray could move into the suite.\n\nGray reported to Nixon that he had secured Hoover's office and its contents. However, he had sealed only Hoover's personal inner office, where no files were stored, not the entire suite of offices. Since 1957, Hoover's \"Official/Confidential\" files, containing material too sensitive to include in the Bureau's central files, had been kept in the outer office, where Gandy sat. Gentry reported that Gray would not have known where to look in Gandy's office for the files, as her office was lined floor to ceiling with filing cabinets; moreover, without her index to the files, he would not have been able to locate incriminating material, for files were deliberately mislabeled, e.g., President Nixon's file was labeled \"Obscene Matters\".\n\nOn May 4, Gandy turned over 12 boxes of labelled \"Official/Confidential\", containing 167 files and 17,750 pages, to Mark Felt. Many of them contained derogatory information. Gray told the press that afternoon that \"there are no dossiers or secret files. There are just general files and I took steps to preserve their integrity.\" Gandy retained the \"Personal File\".\n\nGandy worked on going through Hoover's \"Personal File\" in the office until May 12. She then transferred at least 32 file drawers of material to the basement rec room of Hoover's Washington home at 4936 Thirtieth Place, Northwest, where she would continue her work from May 13 to July 17. Gandy later testified nothing official had been removed from the Bureau's offices, \"not even his badge.\" At Hoover's residence the destruction was overseen by John P. Mohr, the number three man in the Bureau after Hoover and Tolson. They were aided by James Jesus Angleton, the Central Intelligence Agency's counterintelligence chief, whom Hoover's neighbors saw removing boxes from Hoover's home. Mohr would claim the boxes Angleton removed were cases of spoiled wine.\n\nWhen the House Committee on Government Oversight investigated the F.B.I.'s spying on and harassment of Martin Luther King, Jr. and others in 1975, Gandy was called to testify. \"I tore them up, put them in boxes, and they were taken away to be shredded,\" she told the congressmen about the papers. The Bureau's Washington field office had F.B.I. drivers transport the material to Hoover's home, then once Gandy had gone through the material, the drivers transported it back to the field office in the Old Post Office Building on Pennsylvania Avenue, where it was shredded and burned.\n\nGandy stated that Hoover had left standing instructions to destroy his personal papers upon his death, and that this instruction was confirmed by Tolson and Gray. Gandy stated that she destroyed no official papers, that everything was personal papers of Hoover's. The staff of the subcommittee did not believe her, but she told the committee: \"I have no reason to lie.\" Representative Andrew Maguire (D-New Jersey), a freshman member of the 94th Congress, said \"I find your testimony very difficult to believe.\" Gandy held her ground: \"That is your privilege.\"\n\n\"I can give you my word. I know what there was—letters to and from friends, personal friends, a lot of letters,\" she testified. Gandy also said the files she took to his home also included his financial papers, such as tax returns and investment statements, the deed to his home, and papers relating to his dogs' pedigrees.\n\nCurt Gentry wrote:\n\nIn \"J. Edgar Hoover: The Man and His Secrets\", Gentry describes the nature of the files: \"... their contents included blackmail material on the patriarch of an American political dynasty, his sons, their wives, and other women; allegations of two homosexual arrests which Hoover leaked to help defeat a witty, urbane Democratic presidential candidate; the surveillance reports on one of America's best-known first ladies and her alleged lovers, both male and female, white and black; the child molestation documentation the director used to control and manipulate one of the Red-baiting proteges; a list of the Bureau's spies in the White House during the eight administrations when Hoover was FBI director; the forbidden fruit of hundreds of illegal wiretaps and bugs, containing, for example, evidence that an attorney general, Tom C. Clark, who later became Supreme Court justice, had received payoffs from the Chicago syndicate; as well as celebrity files, with all the unsavory gossip Hoover could amass on some of the biggest names in show business.\"\n\nWhile she officially retired the day Hoover died, Gandy spent the next few weeks destroying his papers (as described and referenced above). Hoover left her $5,000 in his will. \n\nIn 1961, Gandy and her sister, Lucy G. Rodman, donated a portrait of their mother by Thomas Eakins to the Smithsonian American Art Museum. Gandy lived in Washington, D.C., until 1986, when she moved to DeLand, Florida, in Volusia County, where a niece lived. Gandy was an avid trout fisherman. \nGandy died of a heart attack on July 7, 1988, either in DeLand (says her \"New York Times\" obituary) or in nearby Orange City, Florida (says her \"Post\" obituary).\n\nGandy was portrayed by actresses Lee Kessler in the television film \"J. Edgar Hoover\" (1987) and Naomi Watts in the cinematic release \"J. Edgar\" (2011).\n\n\n", "id": "14018", "title": "Helen Gandy"}
{"url": "https://en.wikipedia.org/wiki?curid=14019", "text": "Horsepower\n\nHorsepower (hp) is a unit of measurement of power (the rate at which work is done). There are many different standards and types of horsepower. Two common definitions in use today are the mechanical horsepower (or imperial horsepower), which is approximately 746 watts, and the metric horsepower, which is approximately 735.5 watts.\n\nThe term was adopted in the late 18th century by Scottish engineer James Watt to compare the output of steam engines with the power of draft horses. It was later expanded to include the output power of other types of piston engines, as well as turbines, electric motors and other machinery. The definition of the unit varied among geographical regions. Most countries now use the SI unit \"watt\" for measurement of power. With the implementation of the EU Directive 80/181/EEC on January 1, 2010, the use of horsepower in the EU is permitted only as a supplementary unit.\n\nUnits called \"horsepower\" have differing definitions:\n\nThe development of the steam engine provided a reason to compare the output of horses with that of the engines that could replace them. In 1702, Thomas Savery wrote in \"The Miner's Friend\":\n\nSo that an engine which will raise as much water as two horses, working together at one time in such a work, can do, and for which there must be constantly kept ten or twelve horses for doing the same. Then I say, such an engine may be made large enough to do the work required in employing eight, ten, fifteen, or twenty horses to be constantly maintained and kept for doing such a work…\n\nThe idea was later used by James Watt to help market his improved steam engine. He had previously agreed to take royalties of one third of the savings in coal from the older Newcomen steam engines. This royalty scheme did not work with customers who did not have existing steam engines but used horses instead.\n\nWatt determined that a horse could turn a mill wheel 144 times in an hour (or 2.4 times a minute). The wheel was in radius; therefore, the horse travelled 2.4 × 2π × 12 feet in one minute. Watt judged that the horse could pull with a force of . So:\n\nWatt defined and calculated the horsepower as 32,572 ft·lbf/min, which was rounded to an even 33,000 ft·lbf/min.\n\nWatt determined that a pony could lift an average per minute over a four-hour working shift. Watt then judged a horse was 50% more powerful than a pony and thus arrived at the 33,000 ft·lbf/min figure. \"Engineering in History\" recounts that John Smeaton initially estimated that a horse could produce 22,916 foot-pounds per minute. John Desaguliers had previously suggested 44,000 foot-pounds per minute and Tredgold 27,500 foot-pounds per minute. \"Watt found by experiment in 1782 that a 'brewery horse' could produce 32,400 foot-pounds per minute.\" James Watt and Matthew Boulton standardized that figure at 33,000 the next year.\n\nMost observers familiar with horses and their capabilities estimate that Watt was either a bit optimistic or intended to underpromise and overdeliver; few horses can maintain that effort for long. Regardless, comparison with a horse proved to be an enduring marketing tool.\n\nA common legend states that the unit was created when one of Watt's first customers, a brewer, specifically demanded an engine that would match a horse, but tried to cheat by taking the strongest horse he had and driving it to the limit. Watt, while aware of the trick, accepted the challenge and built a machine which was actually even stronger than the figure achieved by the brewer, and it was the output of that machine which became the horsepower.\n\nIn 1993, R. D. Stevenson and R. J. Wassersug published an article calculating the upper limit to an animal's power output. The peak power over a few seconds has been measured to be as high as 14.9 hp. However, Stevenson and Wassersug observe that for sustained activity, a work rate of about 1 hp per horse is consistent with agricultural advice from both 19th and 20th century sources.\n\nWhen considering human-powered equipment, a healthy human can produce about 1.2 hp briefly (see orders of magnitude) and sustain about 0.1 hp indefinitely; trained athletes can manage up to about 2.5 hp briefly\nand 0.3 hp for a period of several hours. The Jamaican sprinter Usain Bolt produced a maximum of 3.5 hp 0.89 seconds into his 9.58 second 100m dash world record in 2009.\n\nWhen torque formula_2 is in pound-foot units, rotational speed formula_3 is in rpm and power is required in horsepower:\n\nThe constant 5252 is the rounded value of (33,000 ft·lbf/min)/(2π rad/rev).\n\nWhen torque formula_2 is in inch pounds:\n\nThe constant 63,025 is the approximation of\n\nIf torque and rotational speed are expressed in coherent SI units, the power is calculated by ;\n\nwhere formula_9 is power in watts when formula_10 is torque in newton-metres, and formula_11 is angular speed in radians per second. When using other units or if the speed is in revolutions per unit time rather than radians, a conversion factor has to be included.\n\nThe following definitions have been widely used:\n\nIn certain situations it is necessary to distinguish between the various definitions of horsepower and thus a suffix is added: hp(I) for mechanical (or imperial) horsepower, hp(M) for metric horsepower, hp(S) for boiler (or steam) horsepower and hp(E) for electrical horsepower.\n\nHydraulic horsepower is equivalent to mechanical horsepower. The formula given above is for conversion to mechanical horsepower from the factors acting on a hydraulic system.\n\nAssuming the third CGPM (1901, CR 70) definition of standard gravity, \"g\"=9.80665 m/s, is used to define the pound-force as well as the kilogram force, and the international avoirdupois pound (1959), one mechanical horsepower is:\n\nOr given that 1 hp = 550 ft·lbf/s, 1 ft = 0.3048 m, 1 lbf ≈ 4.448 N, 1 J = 1 N·m, 1 W = 1 J/s: 1 hp ≈ 746 W\n\nThe various units used to indicate this definition (\"PS\", \"cv\", \"hk\", \"pk\", \"ks\" and \"ch\") all translate to \"horse power\" in English, so it is common to see these values referred to as \"horsepower\" or \"hp\" in the press releases or media coverage of the German, French, Italian, and Japanese automobile companies. British manufacturers often intermix metric horsepower and mechanical horsepower depending on the origin of the engine in question. Sometimes the metric horsepower rating of an engine is conservative enough so that the same figure can be used for both 80/1269/EEC with metric hp and SAE J1349 with imperial hp.\n\nDIN 66036 defines one metric horsepower as the power to raise a mass of 75 kilograms against the earth's gravitational force over a distance of one metre in one second; this is equivalent to 735.49875 W or 98.6% of an imperial mechanical horsepower.\n\nIn 1972, the PS was rendered obsolete by EEC directives, when it was replaced by the kilowatt as the official power measuring unit. It is still in use for commercial and advertising purposes, in addition to the kW rating, as many customers are still not familiar with the use of kilowatts for engines.\n\nOther names for the metric horsepower are the Dutch (pk), the French (ch), the Portuguese (cv), the Russian (лс), the Swedish (hk), the Finnish (hv), the Norwegian and Danish (hk), the Hungarian (LE), the Czech and Slovak (k or ks), the Bosnian/Croatian/Serbian (KS), the Bulgarian , the Macedonian (KC), the Polish (KM), Slovenian (KM) and the Romanian (CP), which all equal the German (PS).\n\nIn the 19th century, the French had their own unit, which they used instead of the CV or horsepower. It was called the poncelet and was abbreviated \"p\".\n\nIn addition, the capital form \"CV\" is used in Italy and France as a unit for tax horsepower, short for, respectively, and (\"steam horses\"). CV is a non-linear rating of a motor vehicle for tax purposes. The CV rating, or fiscal power, is formula_12, where \"P\" is the maximum power in kilowatts and \"U\" is the amount of carbon dioxide (CO) emitted in grams per kilometre. The term for CO measurements has been included in the definition only since 1998, so older ratings in CV are not directly comparable. The fiscal power has found its way into naming of automobile models, such as the popular Citroën deux-chevaux. The (ch) unit should not be confused with the French (CV).\n\nThe horsepower used for electrical machines is defined as exactly 746 W. In the US, nameplates on electrical motors show their power output in hp, not their power input. Outside the United States watts or kilowatts are generally used for electric motor ratings and in such usage it is the output power that is stated.\n\nHydraulic horsepower can represent the power available within hydraulic machinery, power through the down-hole nozzle of a drilling rig, or can be used to estimate the mechanical power needed to generate a known hydraulic flow rate.\n\nIt may be calculated as:\n\nDrilling rigs are powered mechanically by rotating the drill pipe from above. Hydraulic power is still needed though, as between 2 and 7 hp are required to push mud through the drill bit in order to clear waste rock. This hydraulic power, considerably more than this, may also be used to drive a down-hole mud motor to power directional drilling.\n\nBoiler horsepower is a boiler's capacity to deliver steam to a steam engine and is not the same unit of power as the 550 ft-lb/s definition. One boiler horsepower is equal to the thermal energy rate required to evaporate 34.5 lb of fresh water at 212 °F in one hour. In the early days of steam use, the boiler horsepower was roughly comparable to the horsepower of engines fed by the boiler.\n\nThe term \"boiler horsepower\" was originally developed at the Philadelphia Centennial Exhibition in 1876, where the best steam engines of that period were tested. The average steam consumption of those engines (per output horsepower) was determined to be the evaporation of 30 pounds of water per hour, based on feed water at 100 °F, and saturated steam generated at 70 psig. This original definition is equivalent to a boiler heat output of 33,485 Btu/hr. Years later in 1884, the ASME re-defined the boiler horsepower as the thermal output equal to the evaporation of 34.5 pounds per hour of water \"from and at\" 212 °F. This considerably simplified boiler testing, and provided more accurate comparisons of the boilers at that time. This revised definition is equivalent to a boiler heat output of 33,469 Btu/hr. Present industrial practice is to define \"boiler horsepower\" as a boiler thermal output equal to 33,475 Btu/hr, which is very close to the original and revised definitions.\n\nBoiler horsepower is still used to measure boiler output in industrial boiler engineering in Australia, the US, and New Zealand. Boiler horsepower is abbreviated BHP, not to be confused with brake horsepower, below, which is also called BHP.\n\nDrawbar horsepower (dbhp) is the power a railway locomotive has available to haul a train or an agricultural tractor to pull an implement. This is a measured figure rather than a calculated one. A special railway car called a dynamometer car coupled behind the locomotive keeps a continuous record of the drawbar pull exerted, and the speed. From these, the power generated can be calculated. To determine the maximum power available, a controllable load is required; it is normally a second locomotive with its brakes applied, in addition to a static load.\n\nIf the drawbar force (formula_14) is measured in pounds-force (lbf) and speed (formula_15) is measured in miles per hour (mph), then the drawbar power (formula_9) in horsepower (hp) is:\n\nExample: How much power is needed to pull a drawbar load of 2,025 pounds-force at 5 miles per hour?\n\n", "id": "14019", "title": "Horsepower"}
{"url": "https://en.wikipedia.org/wiki?curid=14020", "text": "History of London\n\nLondon (the capital city of England and the United Kingdom) has a history going back over 2,000 years. In the main time, it has grown to one of the most significant financial and cultural capitals on Earth. It has experienced plague, devastating fire, civil war, aerial bombardment, terrorist attacks, and widespread rioting. The City of London is its historic core and today is its primary financial district, though it now represents a tiny part of the wider metropolis of Greater London.\n\nAccording to the legendary \"Historia Regum Britanniae\", by Geoffrey of Monmouth, London was founded by Brutus of Troy about 1000–1100 B.C. after he defeated the native giant Gogmagog; the settlement was known as ', ' (Latin for New Troy), which, according to a pseudo-etymology, was corrupted to \"Trinovantum\". Trinovantes were the Iron Age tribe who inhabited the area prior to the Romans. Geoffrey provides prehistoric London with a rich array of legendary kings, such as Lud (see also Lludd, from Welsh mythology) who, he claims, renamed the town \"Caer Ludein\", from which London was derived, and was buried at Ludgate.\n\nHowever, despite intensive excavations, archaeologists have found no evidence of a prehistoric major settlement in the area. There have been scattered prehistoric finds, evidence of farming, burial and traces of habitation, but nothing more substantial. It is now considered unlikely that a pre-Roman city existed, but as some of the Roman city remains unexcavated, it is still just possible that some major settlement may yet be discovered. London was most likely a rural area with scattered settlement. Rich finds such as the Battersea Shield, found in the Thames near Chelsea, suggest the area was important; there may have been important settlements at Egham and Brentford, and there was a hillfort at Uphall Camp, Ilford, but no city in the area of the Roman London, the present day City of London.\n\nSome recent discoveries indicate probable very early settlements near the Thames in the London area. In 1999, the remains of a Bronze Age bridge were found, again on the foreshore south of Vauxhall Bridge. This bridge either crossed the Thames, or went to a now lost island in the river. Dendrology dated the timbers to 1500BC. In 2001 a further dig found that the timbers were driven vertically into the ground on the south bank of the Thames west of Vauxhall Bridge. In 2010 the foundations of a large timber structure, dated to 4000BC, were found on the Thames foreshore, south of Vauxhall Bridge. The function of the mesolithic structure is not known. All these structures are on the south bank at a natural crossing point where the River Effra flows into the Thames.\n\nNumerous finds have been made of spear heads and weaponry from the Bronze and Iron Ages near the banks of the Thames in the London area, many of which had clearly been used in battle. This suggests that the Thames was an important tribal boundary.\n\n\"Londinium\" was established as a civilian town by the Romans about seven years after the invasion of AD 43. London, like Rome, was founded on the point of the river where it was narrow enough to bridge and the strategic location of the city provided easy access to much of Europe. Early Roman London occupied a relatively small area, roughly equivalent to the size of Hyde Park. In around AD 60, it was destroyed by the Iceni led by their queen Boudica. The city was quickly rebuilt as a planned Roman town and recovered after perhaps 10 years, the city growing rapidly over the following decades.\n\nDuring the 2nd century \"Londinium\" was at its height and replaced Colchester as the capital of Roman Britain (Britannia). Its population was around 60,000 inhabitants. It boasted major public buildings, including the largest basilica north of the Alps, temples, bath houses, an amphitheatre and a large fort for the city garrison. Political instability and recession from the 3rd century onwards led to a slow decline.\n\nAt some time between 180 and 225 AD the Romans built the defensive London Wall around the landward side of the city. The wall was about long, high, and thick. The wall would survive for another 1,600 years and define the City of London's perimeters for centuries to come. The perimeters of the present City are roughly defined by the line of the ancient wall.\n\nIn the late 3rd century, Londinium was raided on several occasions by Saxon pirates. This led, from around 255 onwards, to the construction of an additional riverside wall. Six of the traditional seven city gates of London are of Roman origin, namely: Ludgate, Newgate, Aldersgate, Cripplegate, Bishopsgate and Aldgate (Moorgate is the exception, being of medieval origin).\n\nBy the 5th century the Roman Empire was in rapid decline, and in 410 AD the Roman occupation of Britain came to an end. Following this, the Roman city also went into rapid decline and by the end of the 5th century was practically abandoned.\n\nUntil recently it was believed that Anglo-Saxon settlement initially avoided the area immediately around Londinium. However, the discovery in 2008 of an Anglo-Saxon cemetery at Covent Garden indicates that the incomers had begun to settle there at least as early as the 6th century and possibly in the 5th. The main focus of this settlement was outside the Roman walls, clustering a short distance to the west along what is now the Strand, between the Aldwych and Trafalgar Square. It was known as \"Lundenwic\", the \"-wic\" suffix here denoting a trading settlement. Recent excavations have also highlighted the population density and relatively sophisticated urban organisation of this earlier Anglo-Saxon London, which was laid out on a grid pattern and grew to house a likely population of 10-12,000.\n\nEarly Anglo-Saxon London belonged to a people known as the Middle Saxons, from whom the name of the county of Middlesex is derived, but who probably also occupied the approximate area of modern Hertfordshire and Surrey. However, by the early 7th century the London area had been incorporated into the kingdom of the East Saxons. In 604 King Saeberht of Essex converted to Christianity and London received Mellitus, its first post-Roman bishop.\n\nAt this time Essex was under the overlordship of King Æthelberht of Kent, and it was under Æthelberht's patronage that Mellitus founded the first St. Paul's Cathedral, traditionally said to be on the site of an old Roman Temple of Diana (although Christopher Wren found no evidence of this). It would have only been a modest church at first and may well have been destroyed after he was expelled from the city by Saeberht's pagan successors.\n\nThe permanent establishment of Christianity in the East Saxon kingdom took place in the reign of King Sigeberht II in the 650s. During the 8th century the kingdom of Mercia extended its dominance over south-eastern England, initially through overlordship which at times developed into outright annexation. London seems to have come under direct Mercian control in the 730s.\n\nViking attacks dominated most of the 9th century, becoming increasingly common from around 830 onwards. London was sacked in 842 and again in 851. The Danish \"Great Heathen Army\", which had rampaged across England since 865, wintered in London in 871. The city remained in Danish hands until 886, when it was captured by the forces of King Alfred the Great of Wessex and reincorporated into Mercia, then governed under Alfred's sovereignty by his son-in-law Ealdorman Æthelred.\n\nAround this time the focus of settlement moved within the old Roman walls for the sake of defence, and the city became known as \"Lundenburh\". The Roman walls were repaired and the defensive ditch re-cut, while the bridge was probably rebuilt at this time. A second fortified Borough was established on the south bank at Southwark, the \"Suthringa Geworc\" (defensive work of the men of Surrey). The old settlement of \"Lundenwic\" became known as the \"ealdwic\" or \"old settlement\", a name which survives today as Aldwich.\n\nFrom this point, the City of London began to develop its own unique local government. Following Ethelred's death in 911 it was transferred to Wessex, preceding the absorption of the rest of Mercia in 918. Although it faced competition for political pre-eminence in the united Kingdom of England from the traditional West Saxon centre of Winchester, London's size and commercial wealth brought it a steadily increasing importance as a focus of governmental activity. King Athelstan held many meetings of the \"witan\" in London and issued laws from there, while King Æthelred the Unready issued the Laws of London there in 978.\n\nFollowing the resumption of Viking attacks in the reign of Ethelred, London was unsuccessfully attacked in 994 by an army under King Sweyn Forkbeard of Denmark. As English resistance to the sustained and escalating Danish onslaught finally collapsed in 1013, London repulsed an attack by the Danes and was the last place to hold out while the rest of the country submitted to Sweyn, but by the end of the year it too capitulated and Æthelred fled abroad. Sweyn died just five weeks after having been proclaimed king and Æthelred was restored to the throne, but Sweyn's son Cnut returned to the attack in 1015.\n\nAfter Æthelred's death at London in 1016 his son Edmund Ironside was proclaimed king there by the \"witangemot\" and left to gather forces in Wessex. London was then subjected to a systematic siege by Cnut but was relieved by King Edmund's army; when Edmund again left to recruit reinforcements in Wessex the Danes resumed the siege but were again unsuccessful. However, following his defeat at the Battle of Assandun Edmund ceded to Cnut all of England north of the Thames, including London, and his death a few weeks later left Cnut in control of the whole country.\n\nA Norse saga tells of a battle when King Æthelred returned to attack Danish-occupied London. According to the saga, the Danes lined London Bridge and showered the attackers with spears. Undaunted, the attackers pulled the roofs off nearby houses and held them over their heads in the boats. Thus protected, they were able to get close enough to the bridge to attach ropes to the piers and pull the bridge down, thus ending the Viking occupation of London. This story presumably relates to Æthelred's return to power after Sweyn's death in 1014, but there is no strong evidence of any such struggle for control of London on that occasion.\n\nFollowing the extinction of Cnut's dynasty in 1042 English rule was restored under Edward the Confessor. He was responsible for the foundation of Westminster Abbey and spent much of his time at Westminster, which from this time steadily supplanted the City itself as the centre of government. Edward's death at Westminster in 1066 without a clear heir led to a succession dispute and the Norman conquest of England. Earl Harold Godwinson was elected king by the \"witangemot\" and crowned in Westminster Abbey but was defeated and killed by William the Bastard, Duke of Normandy at the Battle of Hastings. The surviving members of the \"witan\" met in London and elected King Edward's young nephew Edgar the Ætheling as king.\n\nThe Normans advanced to the south bank of the Thames opposite London, where they defeated an English attack and burned Southwark but were unable to storm the bridge. They moved upstream and crossed the river at Wallingford before advancing on London from the north-west. The resolve of the English leadership to resist collapsed and the chief citizens of London went out together with the leading members of the Church and aristocracy to submit to William at Berkhamstead, although according to some accounts there was a subsequent violent clash when the Normans reached the city. Having occupied London, William was crowned king in Westminster Abbey.\n\nThe new Norman regime established new fortresses within the city to dominate the native population. By far the most important of these was the Tower of London at the eastern end of the city, where the initial wooden fortification was rapidly replaced by the construction of the first stone castle in England. The smaller forts of Baynard's Castle and Montfichet's Castle were also established along the waterfront. King William also granted a charter in 1067 confirming the city's existing rights, privileges and laws. Its growing self-government was consolidated by the election rights granted by King John in 1199 and 1215.\n\nIn 1097 William Rufus, the son of William the Conqueror began the construction of 'Westminster Hall', which became the focus of the Palace of Westminster.\n\nIn 1176 construction began of the most famous incarnation of London Bridge (completed in 1209) which was built on the site of several earlier wooden bridges. This bridge would last for 600 years, and remained the only bridge across the River Thames until 1739.\n\nIn 1216 during the First Barons' War London was occupied by Prince Louis of France, who had been called in by the baronial rebels against King John and was acclaimed as King of England in St Paul's Cathedral. However, following John's death in 1217 Louis's supporters reverted to their Plantagenet allegiance, rallying round John's son Henry III, and Louis was forced to withdraw from England.\n\nOver the following centuries, London would shake off the heavy French cultural and linguistic influence which had been there since the times of the Norman conquest. The city would figure heavily in the development of Early Modern English.\nDuring the Peasants' Revolt of 1381 London was invaded by rebels led by Wat Tyler. A group of peasants stormed the Tower of London and executed the Lord Chancellor, Archbishop Simon Sudbury, and the Lord Treasurer. The peasants looted the city and set fire to numerous buildings. Tyler was stabbed to death by the Lord Mayor William Walworth in a confrontation at Smithfield and the revolt collapsed.\n\nTrade increased steadily during the Middle Ages, and London grew rapidly as a result. In 1100 London's population was somewhat more than 15,000. By 1300 it had grown to roughly 80,000. London lost at least half of its population during the Black Death in the mid-14th century, but its economic and political importance stimulated a rapid recovery despite further epidemics. Trade in London was organised into various guilds, which effectively controlled the city, and elected the Lord Mayor of the City of London.\n\nMedieval London was made up of narrow and twisting streets, and most of the buildings were made from combustible materials such as wood and straw, which made fire a constant threat, while sanitation in cities was poor.\n\nDuring the Reformation, London was the principal early centre of Protestantism in England. Its close commercial connections with the Protestant heartlands in northern continental Europe, large foreign mercantile communities, disproportionately large number of literate inhabitants and role as the centre of the English print trade all contributed to the spread of the new ideas of religious reform. Before the Reformation, more than half of the area of London was the property of monasteries, nunneries and other religious houses.\n\nHenry VIII's \"Dissolution of the Monasteries\" had a profound effect on the city as nearly all of this property changed hands. The process started in the mid 1530s, and by 1538 most of the larger monastic houses had been abolished. Holy Trinity Aldgate went to Lord Audley, and the Marquess of Winchester built himself a house in part of its precincts. The Charterhouse went to Lord North, Blackfriars to , the leper hospital of St Giles to Lord Dudley, while the king took for himself the leper hospital of St James, which was rebuilt as St James's Palace.\n\nThe period saw London rapidly rising in importance amongst Europe's commercial centres. Trade expanded beyond Western Europe to Russia, the Levant, and the Americas. This was the period of mercantilism and monopoly trading companies such as the Muscovy Company (1555) and the British East India Company (1600) were established in London by Royal Charter. The latter, which ultimately came to rule India, was one of the key institutions in London, and in Britain as a whole, for two and a half centuries. Immigrants arrived in London not just from all over England and Wales, but from abroad as well, for example Huguenots from France; the population rose from an estimated 50,000 in 1530 to about 225,000 in 1605. The growth of the population and wealth of London was fuelled by a vast expansion in the use of coastal shipping.\n\nThe late 16th and early 17th century saw the great flourishing of drama in London whose preeminent figure was William Shakespeare. During the mostly calm later years of Elizabeth's reign, some of her courtiers and some of the wealthier citizens of London built themselves country residences in Middlesex, Essex and Surrey. This was an early stirring of the villa movement, the taste for residences which were neither of the city nor on an agricultural estate, but at the time of Elizabeth's death in 1603, London was still very compact.\n\nXenophobia was rampant in London, and increased after the 1580s. Many immigrants became disillusioned by routine threats of violence and molestation, attempts at expulsion of foreigners, and the great difficulty in acquiring English citizenship. Dutch cities proved more hospitable, and many left London permanently.\n\nLondon's expansion beyond the boundaries of the City was decisively established in the 17th century. In the opening years of that century the immediate environs of the City, with the principal exception of the aristocratic residences in the direction of Westminster, were still considered not conducive to health. Immediately to the north was Moorfields, which had recently been drained and laid out in walks, but it was frequented by beggars and travellers, who crossed it in order to get into London. Adjoining Moorfields were Finsbury Fields, a favourite practising ground for the archers, Mile End, then a common on the Great Eastern Road and famous as a rendezvous for the troops.\n\nThe preparations for King James I becoming king were interrupted by a severe plague epidemic, which may have killed over thirty thousand people. The Lord Mayor's Show, which had been discontinued for some years, was revived by order of the king in 1609. The dissolved monastery of the Charterhouse, which had been bought and sold by the courtiers several times, was purchased by Thomas Sutton for £13,000. The new hospital, chapel, and schoolhouse were begun in 1611. Charterhouse School was to be one of the principal public schools in London until it moved to Surrey in Victorian times, and the site is still used as a medical school.\n\nThe general meeting-place of Londoners in the day-time was the nave of Old St. Paul's Cathedral. Merchants conducted business in the aisles, and used the font as a counter upon which to make their payments; lawyers received clients at their particular pillars; and the unemployed looked for work. St Paul's Churchyard was the centre of the book trade and Fleet Street was a centre of public entertainment. Under James I the theatre, which established itself so firmly in the latter years of Elizabeth, grew further in popularity. The performances at the public theatres were complemented by elaborate masques at the royal court and at the inns of court.\n\nCharles I acceded to the throne in 1625. During his reign, aristocrats began to inhabit the West End in large numbers. In addition to those who had specific business at court, increasing numbers of country landowners and their families lived in London for part of the year simply for the social life. This was the beginning of the \"London season\". Lincoln's Inn Fields was built about 1629. The piazza of Covent Garden, designed by England's first classically trained architect Inigo Jones followed in about 1632. The neighbouring streets were built shortly afterwards, and the names of Henrietta, Charles, James, King and York Streets were given after members of the royal family.\nIn January 1642 five members of parliament whom the King wished to arrest were granted refuge in the City. In August of the same year the King raised his banner at Nottingham, and during the English Civil War London took the side of the parliament. Initially the king had the upper hand in military terms and in November he won the Battle of Brentford a few miles to the west of London. The City organised a new makeshift army and Charles hesitated and retreated. Subsequently, an extensive system of fortifications was built to protect London from a renewed attack by the Royalists. This comprised a strong earthen rampart, enhanced with bastions and redoubts. It was well beyond the City walls and encompassed the whole urban area, including Westminster and Southwark. London was not seriously threatened by the royalists again, and the financial resources of the City made an important contribution to the parliamentarians' victory in the war.\n\nThe unsanitary and overcrowded City of London has suffered from the numerous outbreaks of the plague many times over the centuries, but in Britain it is the last major outbreak which is remembered as the \"Great Plague\" It occurred in 1665 and 1666 and killed around 60,000 people, which was one fifth of the population. Samuel Pepys chronicled the epidemic in his diary. On 4 September 1665 he wrote \"I have stayed in the city till above 7400 died in one week, and of them about 6000 of the plague, and little noise heard day or night but tolling of bells.\"\n\nThe Great Plague was immediately followed by another catastrophe, albeit one which helped to put an end to the plague. On the Sunday, 2 September 1666 the Great Fire of London broke out at one o'clock in the morning at a bakery in Pudding Lane in the southern part of the City. Fanned by an eastern wind the fire spread, and efforts to arrest it by pulling down houses to make firebreaks were disorganised to begin with. On Tuesday night the wind fell somewhat, and on Wednesday the fire slackened. On Thursday it was extinguished, but on the evening of that day the flames again burst forth at the Temple. Some houses were at once blown up by gunpowder, and thus the fire was finally mastered. The Monument was built to commemorate the fire: for over a century and a half it bore an inscription attributing the conflagration to a \"\"popish frenzy\"\".\nThe fire destroyed about 60% of the City, including Old St Paul's Cathedral, 87 parish churches, 44 livery company halls and the Royal Exchange. However, the number of lives lost was surprisingly small; it is believed to have been 16 at most. Within a few days of the fire, three plans were presented to the king for the rebuilding of the city, by Christopher Wren, John Evelyn and Robert Hooke.\n\nWren proposed to build main thoroughfares north and south, and east and west, to insulate all the churches in conspicuous positions, to form the most public places into large piazzas, to unite the halls of the 12 chief livery companies into one regular square annexed to the Guildhall, and to make a fine quay on the bank of the river from Blackfriars to the Tower of London. Wren wished to build the new streets straight and in three standard widths of thirty, sixty and ninety feet. Evelyn's plan differed from Wren's chiefly in proposing a street from the church of St Dunstan's in the East to the St Paul's, and in having no quay or terrace along the river. These plans were not implemented, and the rebuilt city generally followed the streetplan of the old one, and most of it has survived into the 21st century.\n\nNonetheless, the new City was different from the old one. Many aristocratic residents never returned, preferring to take new houses in the West End, where fashionable new districts such as St. James's were built close to the main royal residence, which was Whitehall Palace until it was destroyed by fire in the 1690s, and thereafter St. James's Palace. The rural lane of Piccadilly sprouted courtiers mansions such as Burlington House. Thus the separation between the middle class mercantile City of London, and the aristocratic world of the court in Westminster became complete.\n\nIn the City itself there was a move from wooden buildings to stone and brick construction to reduce the risk of fire. Parliament's Rebuilding of London Act 1666 stated \"\"building with brick [is] not only more comely and durable, but also more safe against future perils of fire\"\". From then on only doorcases, window-frames and shop fronts were allowed to be made of wood.\n\nChristopher Wren's plan for a new model London came to nothing, but he was appointed to rebuild the ruined parish churches and to replace St Paul's Cathedral. His domed baroque cathedral was the primary symbol of London for at least a century and a half. As city surveyor, Robert Hooke oversaw the reconstruction of the City's houses. The East End, that is the area immediately to the east of the city walls, also became heavily populated in the decades after the Great Fire. London's docks began to extend downstream, attracting many working people who worked on the docks themselves and in the processing and distributive trades. These people lived in Whitechapel, Wapping, Stepney and Limehouse, generally in slum conditions.\n\nIn the winter of 1683–4 a frost fair was held on the Thames. The frost, which began about seven weeks before Christmas and continued for six weeks after, was the greatest on record. The Revocation of the Edict of Nantes in 1685 led to a large migration on Huguenots to London. They established a silk industry at Spitalfields.\n\nAt this time the Bank of England was founded, and the British East India Company was expanding its influence. Lloyd's of London also began to operate in the late 17th century. In 1700 London handled 80% of England's imports, 69% of its exports and 86% of its re-exports. Many of the goods were luxuries from the Americas and Asia such as silk, sugar, tea and tobacco. The last figure emphasises London's role as an entrepot: while it had many craftsmen in the 17th century, and would later acquire some large factories, its economic prominence was never based primarily on industry. Instead it was a great trading and redistribution centre. Goods were brought to London by England's increasingly dominant merchant navy, not only to satisfy domestic demand, but also for re-export throughout Europe and beyond.\n\nWilliam III, a Dutchman, cared little for London, the smoke of which gave him asthma, and after the first fire at Whitehall Palace (1691) he purchased Nottingham House and transformed it into Kensington Palace. Kensington was then an insignificant village, but the arrival of the court soon caused it to grow in importance. The palace was rarely favoured by future monarchs, but its construction was another step in the expansion of the bounds of London. During the same reign Greenwich Hospital, then well outside the boundary of London, but now comfortably inside it, was begun; it was the naval complement to the Chelsea Hospital for former soldiers, which had been founded in 1681. During the reign of Queen Anne an act was passed authorising the building of 50 new churches to serve the greatly increased population living outside the boundaries of the City of London.\n\nThe 18th century was a period of rapid growth for London, reflecting an increasing national population, the early stirrings of the Industrial Revolution, and London's role at the centre of the evolving British Empire.\n\nIn 1707 an Act of Union was passed merging the Scottish and the English Parliaments, thus establishing the Kingdom of Great Britain. A year later, in 1708 Christopher Wren's masterpiece, St Paul's Cathedral was completed on his birthday. However, the first service had been held on 2 December 1697; more than 10 years earlier. This Cathedral replaced the original St. Paul's which had been completely destroyed in the Great Fire of London. This building is considered one of the finest in Britain and a fine example of Baroque architecture.\n\nMany tradesmen from different countries came to London to trade goods and merchandise. Also, more immigrants moved to London making the population greater. More people also moved to London for work and for business making London an altogether bigger and busier city. Britain's victory in the Seven Years' War increased the country's international standing and opened large new markets to British trade, further boosting London's prosperity.\n\nDuring the Georgian period London spread beyond its traditional limits at an accelerating pace. This is shown in a series of detailed maps, particularly John Rocque's 1741–45 map \"(see below)\" and his 1746 Map of London. New districts such as Mayfair were built for the rich in the West End, new bridges over the Thames encouraged an acceleration of development in South London and in the East End, the Port of London expanded downstream from the City. During this period was also the uprising of the American colonies. In 1780, the Tower of London held its only American prisoner, former President of the Continental Congress, Henry Laurens. In 1779 he was the Congress's representative of Holland, and got the country's support for the Revolution. On his return voyage back to America, the Royal Navy captured him and charged him with treason after finding evidence of a reason of war between Great Britain and the Netherlands. He was released from the Tower on 21 December 1781 in exchange for General Lord Cornwallis.\n\nIn 1762 George III acquired Buckingham Palace (then called Buckingham House) from the Duke of Buckingham. It was enlarged over the next 75 years by architects such as John Nash.\nA phenomenon of the era was the coffeehouse, which became a popular place to debate ideas. Growing literacy and the development of the printing press meant that news became widely available. Fleet Street became the centre of the embryonic national press during the century.\n\n18th-century London was dogged by crime, the Bow Street Runners were established in 1750 as a professional police force. Penalties for crime were harsh, with the death penalty being applied for fairly minor crimes. Public hangings were common in London, and were popular public events.\n\nIn 1780 London was rocked by the Gordon Riots, an uprising by Protestants against Roman Catholic emancipation led by Lord George Gordon. Severe damage was caused to Catholic churches and homes, and 285 rioters were killed.\n\nIn the year 1787, freed slaves from London, America, and many of Britain's colonies founded Freetown in modern-day Sierra Leone.\n\nUp until 1750, London Bridge was the only crossing over the Thames, but in that year Westminster Bridge was opened and, for the first time in history, London Bridge, in a sense, had a rival. In 1798, Frankfurt banker Nathan Mayer Rothschild arrived in London and set up a banking house in the city, with a large sum of money given to him by his father, Amschel Mayer Rothschild. The Rothschilds also had banks in Paris and Vienna. The bank financed numerous large-scale projects, especially regarding railways around the world and the Suez Canal.\n\nThe 18th century saw the breakaway of the American colonies and many other unfortunate events in London, but also great change and Enlightenment. This all led into the beginning of modern times, the 19th century.\n\nDuring the 19th century, London was transformed into the world's largest city and capital of the British Empire. Its population expanded from 1 million in 1800 to 6.7 million a century later. During this period, London became a global political, financial, and trading capital. In this position, it was largely unrivalled until the latter part of the century, when Paris and New York began to threaten its dominance.\n\nWhile the city grew wealthy as Britain's holdings expanded, 19th-century London was also a city of poverty, where millions lived in overcrowded and unsanitary slums. Life for the poor was immortalised by Charles Dickens in such novels as Oliver Twist In 1810, after the death of Sir Francis Baring and Abraham Goldsmid, Rothschild emerges as the major banker in London.\n\nIn 1829 the then Home Secretary (and future prime minister) Robert Peel established the Metropolitan Police as a police force covering the entire urban area. The force gained the nickname of \"bobbies\" or \"peelers\" named after Robert Peel.\n\n19th-century London was transformed by the coming of the railways. A new network of metropolitan railways allowed for the development of suburbs in neighbouring counties from which middle-class and wealthy people could commute to the centre. While this spurred the massive outward growth of the city, the growth of greater London also exacerbated the class divide, as the wealthier classes emigrated to the suburbs, leaving the poor to inhabit the inner city areas.\n\nThe first railway to be built in London was a line from London Bridge to Greenwich, which opened in 1836. This was soon followed by the opening of great rail termini which linked London to every corner of Britain. These included Euston station (1837), Paddington station (1838), Fenchurch Street station (1841), Waterloo station (1848), King's Cross station (1850), and St Pancras station (1863). From 1863, the first lines of the London Underground were constructed.\n\nThe urbanised area continued to grow rapidly, spreading into Islington, Paddington, Belgravia, Holborn, Finsbury, Shoreditch, Southwark and Lambeth. Towards the middle of the century, London's antiquated local government system, consisting of ancient parishes and vestries, struggled to cope with the rapid growth in population. In 1855 the Metropolitan Board of Works (MBW) was created to provide London with adequate infrastructure to cope with its growth. One of its first tasks was addressing London's sanitation problems. At the time, raw sewage was pumped straight into the River Thames. This culminated in The Great Stink of 1858. Parliament finally gave consent for the MBW to construct a large system of sewers. The engineer put in charge of building the new system was Joseph Bazalgette. In what was one of the largest civil engineering projects of the 19th century, he oversaw construction of over 2100 km of tunnels and pipes under London to take away sewage and provide clean drinking water. When the London sewerage system was completed, the death toll in London dropped dramatically, and epidemics of cholera and other diseases were curtailed. Bazalgette's system is still in use today.\n\nOne of the most famous events of 19th-century London was the Great Exhibition of 1851. Held at The Crystal Palace, the fair attracted 6 million visitors from across the world and displayed Britain at the height of its Imperial dominance.\n\nAs the capital of a massive empire, London became a magnet for immigrants from the colonies and poorer parts of Europe. A large Irish population settled in the city during the Victorian period, with many of the newcomers refugees from the Great Famine (1845–1849). At one point, Catholic Irish made up about 20% of London's population; they typically lived in overcrowded slums. London also became home to a sizable Jewish community, which was notable for its entrepreneurship in the clothing trade and merchandising.\n\nIn 1888, the new County of London was established, administered by the London County Council. This was the first elected London-wide administrative body, replacing the earlier Metropolitan Board of Works, which had been made up of appointees. The County of London covered broadly what was then the full extent of the London conurbation, although the conurbation later outgrew the boundaries of the county. In 1900, the county was sub-divided into 28 metropolitan boroughs, which formed a more local tier of administration than the county council.\n\nMany famous buildings and landmarks of London were constructed during the 19th century including:\n\nLondon entered the 20th century at the height of its influence as the capital of one of the largest empires in history, but the new century was to bring many challenges.\n\nLondon's population continued to grow rapidly in the early decades of the century, and public transport was greatly expanded. A large tram network was constructed by the London County Council, through the LCC Tramways; the first motorbus service began in the 1900s. Improvements to London's overground and underground rail network, including large scale electrification were progressively carried out.\n\nDuring World War I, London experienced its first bombing raids carried out by German zeppelin airships; these killed around 700 people and caused great terror, but were merely a foretaste of what was to come. The city of London would experience many more terrors as a result of both World Wars. The largest explosion in London occurred during World War I: the Silvertown explosion, when a munitions factory containing 50 tons of TNT exploded, killing 73 and injuring 400.\n\nThe period between the two World Wars saw London's geographical extent growing more quickly than ever before or since. A preference for lower density suburban housing, typically semi-detached, by Londoners seeking a more \"rural\" lifestyle, superseded Londoners' old predilection for terraced houses. This was facilitated not only by a continuing expansion of the rail network, including trams and the Underground, but also by slowly widening car ownership. London's suburbs expanded outside the boundaries of the County of London, into the neighbouring counties of Essex, Hertfordshire, Kent, Middlesex and Surrey.\n\nLike the rest of the country, London suffered severe unemployment during the Great Depression of the 1930s. In the East End during the 1930s, politically extreme parties of both right and left flourished. The Communist Party of Great Britain and the British Union of Fascists both gained serious support. Clashes between right and left culminated in the Battle of Cable Street in 1936. The population of London reached an all-time peak of 8.6 million in 1939.\n\nLarge numbers of Jewish immigrants fleeing from Nazi Germany settled in London during the 1930s, mostly in the East End.\n\nLabour Party politician Herbert Morrison was a dominant figure in local government in the 1920s and 1930s. He became mayor of Hackney and a member of the London County Council in 1922, and for a while was Minister of Transport in Ramsay MacDonald's cabinet. When Labour gained power in London in 1934, Morrison unified the bus, tram and trolleybus services with the Underground, by the creation of the London Passenger Transport Board (known as London Transport) in 1933., He led the effort to finance and build the new Waterloo Bridge. He designed the Metropolitan Green Belt around the suburbs and worked to clear slums, build schools, and reform public assistance.\n\nDuring World War II, London, as many other British cities, suffered severe damage, being bombed extensively by the \"Luftwaffe\" as a part of The Blitz. Prior to the bombing, hundreds of thousands of children in London were evacuated to the countryside to avoid the bombing. Civilians took shelter from the air raids in underground stations.\n\nThe heaviest bombing took place during The Blitz between 7 September 1940 and 10 May 1941. During this period, London was subjected to 71 separate raids receiving over 18,000 tonnes of high explosive. One raid in December 1940, which became known as the Second Great Fire of London, saw a firestorm engulf much of the City of London and destroy many historic buildings. St Paul's Cathedral, however, remained unscathed; a photograph showing the Cathedral shrouded in smoke became a famous image of the war.\n\nHaving failed to defeat Britain, Hitler turned his attention to the Eastern front and regular bombing raids ceased. They began again, but on a smaller scale with the \"Little Blitz\" in early 1944. Towards the end of the war, during 1944/45 London again came under heavy attack by pilotless V-1 flying bombs and V-2 rockets, which were fired from Nazi occupied Europe. These attacks only came to an end when their launch sites were captured by advancing Allied forces.\n\nLondon suffered severe damage and heavy casualties, the worst hit part being the Docklands area. By the war's end, just under 30,000 Londoners had been killed by the bombing, and over 50,000 seriously injured, tens of thousands of buildings were destroyed, and hundreds of thousands of people were made homeless.\n\nThree years after the war, the 1948 Summer Olympics were held at the original Wembley Stadium, at a time when the city had barely recovered from the war. London's rebuilding was slow to begin. However, in 1951 the Festival of Britain was held, which marked an increasing mood of optimism and forward looking.\n\nIn the immediate postwar years housing was a major issue in London, due to the large amount of housing which had been destroyed in the war. The authorities decided upon high-rise blocks of flats as the answer to housing shortages. During the 1950s and 1960s the skyline of London altered dramatically as tower blocks were erected, although these later proved unpopular. In a bid to reduce the number of people living in overcrowded housing, a policy was introduced of encouraging people to move into newly built new towns surrounding London.\n\nThrough the 19th and in the early half of the 20th century, Londoners used coal for heating their homes, which produced large amounts of smoke. In combination with climatic conditions this often caused a characteristic smog, and London became known for its typical \"London Fog\", also known as \"Pea Soupers\". London was sometimes referred to as \"The Smoke\" because of this. In 1952 this culminated in the disastrous Great Smog of 1952 which lasted for five days and killed over 4,000 people. In response to this, the Clean Air Act 1956 was passed, mandating the creating of \"smokeless zones\" where the use of \"smokeless\" fuels was required (this was at a time when most households still used open fires); the Act was effective.\nStarting in the mid-1960s, and partly as a result of the success of such UK musicians as the Beatles and The Rolling Stones, London became a centre for the worldwide youth culture, exemplified by the Swinging London subculture which made Carnaby Street a household name of youth fashion around the world. London's role as a trendsetter for youth fashion was revived strongly in the 1980s during the new wave and punk eras. In the mid-1990s this was revived to some extent with the emergence of the Britpop era.\n\nFrom the 1950s onwards London became home to a large number of immigrants, largely from Commonwealth countries such as Jamaica, India, Bangladesh, Pakistan, which dramatically changed the face of London, turning it into one of the most diverse cities in Europe. However, the integration of the new immigrants was not always easy. Racial tensions emerged in events such as the Brixton Riots in the early 1980s.\n\nFrom the beginning of \"The Troubles\" in Northern Ireland in the early 1970s until the mid-1990s, London was subjected to repeated terrorist attacks by the Provisional IRA.\n\nThe outward expansion of London was slowed by the war, and the introduction of the Metropolitan Green Belt. Due to this outward expansion, in 1965 the old County of London (which by now only covered part of the London conurbation) and the London County Council were abolished, and the much larger area of Greater London was established with a new Greater London Council (GLC) to administer it, along with 32 new London boroughs.\n\nGreater London's population declined steadily in the decades after World War II, from an estimated peak of 8.6 million in 1939 to around 6.8 million in the 1980s. However, it then began to increase again in the late 1980s, encouraged by strong economic performance and an increasingly positive image.\n\nLondon's traditional status as a major port declined dramatically in the post-war decades as the old Docklands could not accommodate large modern container ships. The principal ports for London moved downstream to the ports of Felixstowe and Tilbury. The docklands area had become largely derelict by the 1980s, but was redeveloped into flats and offices from the mid-1980s onwards. The Thames Barrier was completed in the 1980s to protect London against tidal surges from the North Sea.\n\nIn the early 1980s political disputes between the GLC run by Ken Livingstone and the Conservative government of Margaret Thatcher led to the GLC's abolition in 1986, with most of its powers relegated to the London boroughs. This left London as the only large metropolis in the world without a central administration.\n\nIn 2000, London-wide government was restored, with the creation of the Greater London Authority (GLA) by Tony Blair's government, covering the same area of Greater London. The new authority had similar powers to the old GLC, but was made up of a directly elected Mayor and a London Assembly. The first election took place on 4 May, with Ken Livingstone comfortably regaining his previous post. London was recognised as one of the nine regions of England. In global perspective, it was emerging as a World city widely compared to New York and Tokyo.\n\nAround the start of the 21st century, London hosted the much derided Millennium Dome at Greenwich, to mark the new century. Other Millennium projects were more successful. One was the largest observation wheel in the world, the \"Millennium Wheel\", or the London Eye, which was erected as a temporary structure, but soon became a fixture, and draws four million visitors a year. The National Lottery also released a flood of funds for major enhancements to existing attractions, for example the roofing of the Great Court at the British Museum.\n\nThe London Plan, published by the Mayor of London in 2004, estimated that the population would reach 8.1 million by 2016, and continue to rise thereafter. This was reflected in a move towards denser, more urban styles of building, including a greatly increased number of tall buildings, and proposals for major enhancements to the public transport network. However, funding for projects such as Crossrail remained a struggle.\n\nOn 6 July 2005 London won the right to host the 2012 Olympics and Paralympics making it the first city to host the modern games three times. However, celebrations were cut short the following day when the city was rocked by a series of terrorist attacks. More than 50 were killed and 750 injured in three bombings on London Underground trains and a fourth on a double decker bus near King's Cross.\n\nIn the public there was ambivalence leading-up to the Olympics, though public sentiment changed strongly in their favour following a successful opening ceremony and when the anticipated organisational and transport problems never occurred.\n\n\n\n\n\n\n\n\n", "id": "14020", "title": "History of London"}
{"url": "https://en.wikipedia.org/wiki?curid=14021", "text": "History of astronomy\n\nAstronomy is the oldest of the natural sciences, dating back to antiquity, with its origins in the religious, mythological, cosmological, calendrical, and astrological beliefs and practices of prehistory: vestiges of these are still found in astrology, a discipline long interwoven with public and governmental astronomy, and not completely disentangled from it until a few centuries ago in the Western World (see astrology and astronomy). In some cultures, astronomical data was used for astrological prognostication.\n\nAncient astronomers were able to differentiate between stars and planets, as stars remain relatively fixed over the centuries while planets will move an appreciable amount during a comparatively short time.\n\nEarly cultures identified celestial objects with gods and spirits. They related these objects (and their movements) to phenomena such as rain, drought, seasons, and tides. It is generally believed that the first astronomers were priests, and that they understood celestial objects and events to be manifestations of the divine, hence early astronomy's connection to what is now called astrology. Ancient structures with possibly astronomical alignments (such as Stonehenge) probably fulfilled astronomical, religious, and social functions.\n\nCalendars of the world have often been set by observations of the Sun and Moon (marking the day, month and year), and were important to agricultural societies, in which the harvest depended on planting at the correct time of year. The most common modern calendar is based on the Roman calendar, which broke the traditional link of the month to the phases of the moon and divided the year into twelve months, alternately comprising thirty and thirty-one days. In 46 BC, Julius Caesar instigated calendar reform and adopted what is now known as the Julian calendar, based upon the 365 day year length originally proposed by the 4th century BC Greek astronomer Callippus.\n\nSince 1990 our understanding of prehistoric Europeans has been radically changed by discoveries of ancient astronomical artifacts throughout Europe. The artifacts demonstrate that Neolithic and Bronze Age Europeans had a sophisticated knowledge of mathematics and astronomy.\n\nAmong the discoveries are:\n\n\nThe origins of Western astronomy can be found in Mesopotamia, the \"land between the rivers\" Tigris and Euphrates, where the ancient kingdoms of Sumer, Assyria, and Babylonia were located. A form of writing known as cuneiform emerged among the Sumerians around 3500–3000 BC. Our knowledge of Sumerian astronomy is indirect, via the earliest Babylonian star catalogues dating from about 1200 BC. The fact that many star names appear in Sumerian suggests a continuity reaching into the Early Bronze Age. Astral theology, which gave planetary gods an important role in Mesopotamian mythology and religion, began with the Sumerians. They also used a sexagesimal (base 60) place-value number system, which simplified the task of recording very large and very small numbers. The modern practice of dividing a circle into 360 degrees, of 60 minutes each, began with the Sumerians. For more information, see the articles on Babylonian numerals and mathematics.\n\nClassical sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were, in reality, priest-scribes specializing in astrology and other forms of divination.\n\nThe first evidence of recognition that astronomical phenomena are periodic and of the application of mathematics to their prediction is Babylonian. Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the \"Enūma Anu Enlil\". The oldest significant astronomical text that we possess is Tablet 63 of the \"Enūma Anu Enlil\", the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.\n\nA significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747–733 BC). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year cycle of lunar eclipses, for example. The Greek astronomer Ptolemy later used Nabonassar's reign to fix the beginning of an era, since he felt that the earliest usable observations began at this time.\n\nThe last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (323–60 BC). In the 3rd century BC, astronomers began to use \"goal-year texts\" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting past records. A notable Babylonian astronomer from this time was Seleucus of Seleucia, who was a supporter of the heliocentric model.\n\nBabylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian Iran, in Byzantium, in Syria, in Islamic astronomy, in Central Asia, and in Western Europe.\n\nAstronomy in the Indian subcontinent dates back to the period of Indus Valley Civilization during 3rd millennium BCE, when it was used to create calendars. As the Indus Valley civilization did not leave behind written documents, the oldest extant Indian astronomical text is the Vedanga Jyotisha, dating from the Vedic period. Vedanga Jyotisha describes rules for tracking the motions of the Sun and the Moon for the purposes of ritual. During 6th century, astronomy was influenced by the Greek and Byzantine astronomical traditions.\n\nAryabhata (476–550), in his magnum opus \"Aryabhatiya\" (499), propounded a computational system based on a planetary model in which the Earth was taken to be spinning on its axis and the periods of the planets were given with respect to the Sun. He accurately calculated many astronomical constants, such as the periods of the planets, times of the solar and lunar eclipses, and the instantaneous motion of the Moon. Early followers of Aryabhata's model included Varahamihira, Brahmagupta, and Bhaskara II.\n\nAstronomy was advanced during the Shunga Empire and many star catalogues were produced during this time. The Shunga period is known as the \"Golden age of astronomy in India\".\nIt saw the development of calculations for the motions and places of various planets, their rising and setting, conjunctions, and the calculation of eclipses.\n\nBhāskara II (1114–1185) was the head of the astronomical observatory at Ujjain, continuing the mathematical tradition of Brahmagupta. He wrote the \"Siddhantasiromani\" which consists of two parts: \"Goladhyaya\" (sphere) and \"Grahaganita\" (mathematics of the planets). He also calculated the time taken for the Earth to orbit the sun to 9 decimal places. The Buddhist University of Nalanda at the time offered formal courses in astronomical studies.\n\nOther important astronomers from India include Madhava of Sangamagrama, Nilakantha Somayaji and Jyeshtadeva, who were members of the Kerala school of astronomy and mathematics from the 14th century to the 16th century. Nilakantha Somayaji, in his \"Aryabhatiyabhasya\", a commentary on Aryabhata's \"Aryabhatiya\", developed his own computational system for a partially heliocentric planetary model, in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system, however, was mathematically more effient than the Tychonic system, due to correctly taking into account the equation of the centre and latitudinal motion of Mercury and Venus. Most astronomers of the Kerala school of astronomy and mathematics who followed him accepted his planetary model.\n\nThe Ancient Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their models were based on nested homocentric spheres centered upon the Earth. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis.\n\nA different approach to celestial phenomena was taken by natural philosophers such as Plato and Aristotle. They were less concerned with developing mathematical predictive models than with developing an explanation of the reasons for the motions of the Cosmos. In his \"Timaeus\", Plato described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul. Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the earth. This basic cosmological model prevailed, in various forms, until the 16th century.\n\nIn the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric system, although only fragmentary descriptions of his idea survive. Eratosthenes, using the angles of shadows created at widely separated regions, estimated the circumference of the Earth with great accuracy.\n\nGreek geometrical astronomy developed away from the model of concentric spheres to employ more complex models in which an eccentric circle would carry around a smaller circle, called an epicycle which in turn carried around a planet. The first such model is attributed to Apollonius of Perga and further developments in it were carried out in the 2nd century BC by Hipparchus of Nicea. Hipparchus made a number of other contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed our modern system of apparent magnitudes.\n\nThe Antikythera mechanism, an ancient Greek astronomical observational device for calculating the movements of the Sun and the Moon, possibly the planets, dates from about 150–100 BC, and was the first ancestor of an astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica.\n\nDepending on the historian's viewpoint, the acme or corruption of physical Greek astronomy is seen with Ptolemy of Alexandria, who wrote the classic comprehensive presentation of geocentric astronomy, the \"Megale Syntaxis\" (Great Synthesis), better known by its Arabic title \"Almagest\", which had a lasting effect on astronomy up to the Renaissance. In his \"Planetary Hypotheses\", Ptolemy ventured into the realm of cosmology, developing a physical model of his geometric system, in a universe many times smaller than the more realistic conception of Aristarchus of Samos four centuries earlier.\n\nThe precise orientation of the Egyptian pyramids affords a lasting demonstration of the high degree of technical skill in watching the heavens attained in the 3rd millennium BC. It has been shown the Pyramids were aligned towards the pole star, which, because of the precession of the equinoxes, was at that time Thuban, a faint star in the constellation of Draco. Evaluation of the site of the temple of Amun-Re at Karnak, taking into account the change over time of the obliquity of the ecliptic, has shown that the Great Temple was aligned on the rising of the midwinter sun. The length of the corridor down which sunlight would travel would have limited illumination at other times of the year.\n\nAstronomy played a considerable part in religious matters for fixing the dates of festivals and determining the hours of the night. The titles of several temple books are preserved recording the movements and phases of the sun, moon and stars. The rising of Sirius (Egyptian: Sopdet, Greek: Sothis) at the beginning of the inundation was a particularly important point to fix in the yearly calendar.\n\nWriting in the Roman era, Clement of Alexandria gives some idea of the importance of astronomical observations to the sacred rites:\nAnd after the Singer advances the Astrologer (ὡροσκόπος), with a \"horologium\" (ὡρολόγιον) in his hand, and a \"palm\" (φοίνιξ), the symbols of astrology. He must know by heart the Hermetic astrological books, which are four in number. Of these, one is about the arrangement of the fixed stars that are visible; one on the positions of the sun and moon and five planets; one on the conjunctions and phases of the sun and moon; and one concerns their risings.\n\nThe Astrologer's instruments (\"horologium\" and \"palm\") are a plumb line and sighting instrument. They have been identified with two inscribed objects in the Berlin Museum; a short handle from which a plumb line was hung, and a palm branch with a sight-slit in the broader end. The latter was held close to the eye, the former in the other hand, perhaps at arms length. The \"Hermetic\" books which Clement refers to are the Egyptian theological texts, which probably have nothing to do with Hellenistic Hermetism.\n\nFrom the tables of stars on the ceiling of the tombs of Rameses VI and Rameses IX it seems that for fixing the hours of the night a man seated on the ground faced the Astrologer in such a position that the line of observation of the pole star passed over the middle of his head. On the different days of the year each hour was determined by a fixed star culminating or nearly culminating in it, and the position of these stars at the time is given in the tables as in the centre, on the left eye, on the right shoulder, etc. According to the texts, in founding or rebuilding temples the north axis was determined by the same apparatus, and we may conclude that it was the usual one for astronomical observations. In careful hands it might give results of a high degree of accuracy.\n\nThe astronomy of East Asia began in China. Solar term was completed in Warring States period. The knowledge of Chinese astronomy was introduced into East Asia.\n\nAstronomy in China has a long history. Detailed records of astronomical observations were kept from about the 6th century BC, until the introduction of Western astronomy and the telescope in the 17th century. Chinese astronomers were able to precisely predict eclipses.\n\nMuch of early Chinese astronomy was for the purpose of timekeeping. The Chinese used a lunisolar calendar, but because the cycles of the Sun and the Moon are different, astronomers often prepared new calendars and made observations for that purpose.\n\nAstrological divination was also an important part of astronomy. Astronomers took careful note of \"guest stars\" which suddenly appeared among the fixed stars. They were the first to record a supernova, in the Astrological Annals of the Houhanshu in 185 AD. Also, the supernova that created the Crab Nebula in 1054 is an example of a \"guest star\" observed by Chinese astronomers, although it was not recorded by their European contemporaries. Ancient astronomical records of phenomena like supernovae and comets are sometimes used in modern astronomical studies.\n\nThe world's first star catalogue was made by Gan De, a , in the 4th century BC.\n\nMaya astronomical codices include detailed tables for calculating phases of the Moon, the recurrence of eclipses, and the appearance and disappearance of Venus as morning and evening star. The Maya based their calendrics in the carefully calculated cycles of the Pleiades, the Sun, the Moon, Venus, Jupiter, Saturn, Mars, and also they had a precise description of the eclipses as depicted in the Dresden Codex, as well as the ecliptic or zodiac, and the Milky Way was crucial in their Cosmology. A number of important Maya structures are believed to have been oriented toward the extreme risings and settings of Venus. To the ancient Maya, Venus was the patron of war and many recorded battles are believed to have been timed to the motions of this planet. Mars is also mentioned in preserved astronomical codices and early mythology.\n\nAlthough the Maya calendar was not tied to the Sun, John Teeple has proposed that the Maya calculated the solar year to somewhat greater accuracy than the Gregorian calendar. Both astronomy and an intricate numerological scheme for the measurement of time were vitally important components of Maya religion.\n\nThe Arabic and the Persian world under Islam had become highly cultured, and many important works of knowledge from Greek astronomy and Indian astronomy and Persian astronomy were translated into Arabic, used and stored in libraries throughout the area. An important contribution by Islamic astronomers was their emphasis on observational astronomy This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. Zij star catalogues were produced at these observatories.\n\nIn the 10th century, Abd al-Rahman al-Sufi (Azophi) carried out observations on the stars and described their positions, magnitudes, brightness, and colour and drawings for each constellation in his \"Book of Fixed Stars\". He also gave the first descriptions and pictures of \"A Little Cloud\" now known as the Andromeda Galaxy. He mentions it as lying before the mouth of a Big Fish, an Arabic constellation. This \"cloud\" was apparently commonly known to the Isfahan astronomers, very probably before 905 AD. The first recorded mention of the Large Magellanic Cloud was also given by al-Sufi. In 1006, Ali ibn Ridwan observed SN 1006, the brightest supernova in recorded history, and left a detailed description of the temporary star.\n\nIn the late 10th century, a huge observatory was built near Tehran, Iran, by the astronomer Abu-Mahmud al-Khujandi who observed a series of meridian transits of the Sun, which allowed him to calculate the tilt of the Earth's axis relative to the Sun. He noted that measurements by earlier (Indian, then Greek) astronomers had found higher values for this angle, possible evidence that the axial tilt is not constant but was in fact decreasing. In 11th-century Persia, Omar Khayyám compiled many tables and performed a reformation of the calendar that was more accurate than the Julian and came close to the Gregorian.\n\nOther Muslim advances in astronomy included the collection and correction of previous astronomical data, resolving significant problems in the Ptolemaic model, the development of the universal latitude-independent astrolabe by Arzachel, the invention of numerous other astronomical instruments, Ja'far Muhammad ibn Mūsā ibn Shākir's belief that the heavenly bodies and celestial spheres were subject to the same physical laws as Earth, the first elaborate experiments related to astronomical phenomena, the introduction of exacting empirical observations and experimental techniques, and the introduction of empirical testing by Ibn al-Shatir, who produced the first model of lunar motion which matched physical observations.\n\nNatural philosophy (particularly Aristotelian physics) was separated from astronomy by Ibn al-Haytham (Alhazen) in the 11th century, by Ibn al-Shatir in the 14th century, and Qushji in the 15th century, leading to the development of an astronomical physics.\n\nAfter the significant contributions of Greek scholars to the development of astronomy, it entered a relatively static era in Western Europe from the Roman era through the 12th century. This lack of progress has led some astronomers to assert that nothing happened in Western European astronomy during the Middle Ages. Recent investigations, however, have revealed a more complex picture of the study and teaching of astronomy in the period from the 4th to the 16th centuries.\n\nWestern Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production. The advanced astronomical treatises of classical antiquity were written in Greek, and with the decline of knowledge of that language, only simplified summaries and practical texts were available for study. The most influential writers to pass on this ancient tradition in Latin were Macrobius, Pliny, Martianus Capella, and Calcidius. In the 6th century Bishop Gregory of Tours noted that he had learned his astronomy from reading Martianus Capella, and went on to employ this rudimentary astronomy to describe a method by which monks could determine the time of prayer at night by watching the stars.\n\nIn the 7th century the English monk Bede of Jarrow published an influential text, On the Reckoning of Time, providing churchmen with the practical astronomical knowledge needed to compute the proper date of Easter using a procedure called the \"computus\". This text remained an important element of the education of clergy from the 7th century until well after the rise of the Universities in the 12th century.\n\nThe range of surviving ancient Roman writings on astronomy and the teachings of Bede and his followers began to be studied in earnest during the revival of learning sponsored by the emperor Charlemagne. By the 9th century rudimentary techniques for calculating the position of the planets were circulating in Western Europe; medieval scholars recognized their flaws, but texts describing these techniques continued to be copied, reflecting an interest in the motions of the planets and in their astrological significance.\n\nBuilding on this astronomical background, in the 10th century European scholars such as Gerbert of Aurillac began to travel to Spain and Sicily to seek out learning which they had heard existed in the Arabic-speaking world. There they first encountered various practical astronomical techniques concerning the calendar and timekeeping, most notably those dealing with the astrolabe. Soon scholars such as Hermann of Reichenau were writing texts in Latin on the uses and construction of the astrolabe and others, such as Walcher of Malvern, were using the astrolabe to observe the time of eclipses in order to test the validity of computistical tables.\n\nBy the 12th century, scholars were traveling to Spain and Sicily to seek out more advanced astronomical and astrological texts, which they translated into Latin from Arabic and Greek to further enrich the astronomical knowledge of Western Europe. The arrival of these new texts coincided with the rise of the universities in medieval Europe, in which they soon found a home. Reflecting the introduction of astronomy into the universities, John of Sacrobosco wrote a series of influential introductory astronomy textbooks: the Sphere, a Computus, a text on the Quadrant, and another on Calculation.\n\nIn the 14th century, Nicole Oresme, later bishop of Liseux, showed that neither the scriptural texts nor the physical arguments advanced against the movement of the Earth were demonstrative and adduced the argument of simplicity for the theory that the earth moves, and \"not\" the heavens. However, he concluded \"everyone maintains, and I think myself, that the heavens do move and not the earth: For God hath established the world which shall not be moved.\" In the 15th century, cardinal Nicholas of Cusa suggested in some of his scientific writings that the Earth revolved around the Sun, and that each star is itself a distant sun. He was not, however, describing a scientifically verifiable theory of the universe.\n\nThe renaissance came to astronomy with the work of Nicolaus Copernicus, who proposed a heliocentric system, in which the planets revolved around the Sun and not the Earth. His \"De revolutionibus\" provided a full mathematical discussion of his system, using the geometrical techniques that had been traditional in astronomy since before the time of Ptolemy. His work was later defended, expanded upon and modified by Galileo Galilei and Johannes Kepler.\n\nGalileo was considered the father of observational astronomy. He was among the first to use a telescope to observe the sky and after constructing a 20x refractor telescope he discovered the four largest moons of Jupiter in 1610. This was the first observation of satellites orbiting another planet. He also found that our Moon had craters and observed (and correctly explained) sunspots. Galileo noted that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these observations supported the Copernican system and were, to some extent, incompatible with the favored model of the Earth at the center of the universe. He may have even observed the planet Neptune in 1612 and 1613, over 200 years before it was discovered, but it is unclear if he was aware of what he was looking at.\n\nAlthough the motions of celestial bodies had been qualitatively explained in physical terms since Aristotle introduced celestial movers in his Metaphysics and a fifth element in his On the Heavens, Johannes Kepler was the first to attempt to derive mathematical predictions of celestial motions from assumed physical causes. Combining his physical insights with the unprecedentedly accurate naked-eye observations made by Tycho Brahe, Kepler discovered the three laws of planetary motion that now carry his name.\n\nIsaac Newton developed further ties between physics and astronomy through his law of universal gravitation. Realising that the same force that attracted objects to the surface of the Earth held the moon in orbit around the Earth, Newton was able to explain – in one theoretical framework – all known gravitational phenomena. In his Philosophiae Naturalis Principia Mathematica, he derived Kepler's laws from first principles. Newton's theoretical developments lay many of the foundations of modern physics.\n\nOutside of England, Newton's theory took some time to become established. Descartes' theory of vortices held sway in France, and Huygens, Leibniz and Cassini accepted only parts of Newton's system, preferring their own philosophies. It wasn't until Voltaire published a popular account in 1738 that the tide changed. In 1748, the French Academy of Sciences offered a reward for solving the perturbations of Jupiter and Saturn which was eventually solved by Euler and Lagrange. Laplace completed the theory of the planets towards the end of the century.\n\nEdmund Halley succeeded Flamsteed as Astronomer Royal in England and succeeded in predicting the return in 1758 of the comet that bears his name. Sir William Herschel found the first new planet, Uranus, to be observed in modern times in 1781. The gap between the planets Mars and Jupiter disclosed by the Titius–Bode law was filled by the discovery of the asteroids Ceres and Pallas in 1801 with many more following.\n\nAt first, astronomical thought in America was based on Aristotelian philosophy, but interest in the new astronomy began to appear in Almanacs as early as 1659.\n\nIn the 19th century it was discovered that, when decomposing the light from the Sun, a multitude of spectral lines were observed (regions where there was less or no light). Experiments with hot gases showed that the same lines could be observed in the spectra of gases, specific lines corresponding to unique elements. It was proved that the chemical elements found in the Sun (chiefly hydrogen and helium) were also found on Earth.\nDuring the 20th century spectroscopy (the study of these lines) advanced, especially because of the advent of quantum physics, that was necessary to understand the observations.\n\nAlthough in previous centuries noted astronomers were exclusively male, at the turn of the 20th century women began to play a role in the great discoveries. In this period prior to modern computers, women at the United States Naval Observatory (USNO), Harvard University, and other astronomy research institutions began to be hired as human \"computers,\" who performed the tedious calculations while scientists performed research requiring more background knowledge. A number of discoveries in this period were originally noted by the women \"computers\" and reported to their supervisors. For example, at the Harvard Observatory Henrietta Swan Leavitt discovered the cepheid variable star period-luminosity relation which she further developed into a method of measuring distance outside of our solar system. Annie Jump Cannon, also at Harvard, organized the stellar spectral types according to stellar temperature. In 1847, Maria Mitchell discovered a comet using a telescope. According to Lewis D. Eigen, Cannon alone, \"in only 4 years discovered and catalogued more stars than all the men in history put together.\"\nMost of these women received little or no recognition during their lives due to their lower professional standing in the field of astronomy. Although their discoveries and methods are taught in classrooms around the world, few students of astronomy can attribute the works to their authors or have any idea that there were active female astronomers at the end of the 19th century.\n\nMost of our current knowledge was gained during the 20th century. With the help of the use of photography, fainter objects were observed. Our sun was found to be part of a galaxy made up of more than 10 stars (10 billion stars). The existence of other galaxies, one of the matters of \"the great debate\", was settled by Edwin Hubble, who identified the Andromeda nebula as a different galaxy, and many others at large distances and receding, moving away from our galaxy.\n\nPhysical cosmology, a discipline that has a large intersection with astronomy, made huge advances during the 20th century, with the model of the hot big bang heavily supported by the evidence provided by astronomy and physics, such as the redshifts of very distant galaxies and radio sources, the cosmic microwave background radiation, Hubble's law and cosmological abundances of elements.\n\nIn the 19th century, scientists began discovering forms of light which were invisible to the naked eye: X-Rays, gamma rays, radio waves, microwaves, ultraviolet radiation, and infrared radiation. This had a major impact on astronomy, spawning the fields of infrared astronomy, radio astronomy, x-ray astronomy and finally gamma-ray astronomy. With the advent of spectroscopy it was proven that other stars were similar to our own sun, but with a range of temperatures, masses and sizes. The existence of our galaxy, the Milky Way, as a separate group of stars was only proven in the 20th century, along with the existence of \"external\" galaxies, and soon after, the expansion of the universe seen in the recession of most galaxies from us.\n\n\n\n\n\n", "id": "14021", "title": "History of astronomy"}
{"url": "https://en.wikipedia.org/wiki?curid=14022", "text": "Haber process\n\nThe Haber process, also called the Haber–Bosch process, is an artificial nitrogen fixation process and is the main industrial procedure for the production of ammonia today. It is named after its inventors, the German chemists Fritz Haber and Carl Bosch, who developed it in the first half of the 20th century. The process converts atmospheric nitrogen (N) to ammonia (NH) by a reaction with hydrogen (H) using a metal catalyst under high temperatures and pressures:\n\nBefore the development of the Haber process, ammonia had been difficult to produce on an industrial scale with early methods such as the Birkeland–Eyde process and Frank–Caro process all being highly inefficient.\n\nAlthough the Haber process is mainly used to produce fertilizer today, during World War I it provided Germany with a source of ammonia for the production of explosives, compensating for the Allied trade blockade on Chilean saltpeter.\n\nThroughout the 19th century the demand for nitrates and ammonia for use as fertilizers and industrial feedstocks had been steadily increasing. The main source was mining niter deposits. At the beginning of the 20th century it was being predicted that these reserves could not satisfy future demand and research into new potential sources of ammonia became more important. The obvious source was atmospheric nitrogen (N), comprising nearly 80% of the air, however N is exceptionally stable and will not readily react with other chemicals. Converting N into ammonia posed a chemical challenge for chemists globally.\n\nHaber, with his assistant Robert Le Rossignol, developed the high-pressure devices and catalysts needed to demonstrate the Haber process at laboratory scale. They demonstrated their process in the summer of 1909 by producing ammonia from air, drop by drop, at the rate of about per hour. The process was purchased by the German chemical company BASF, which assigned Carl Bosch the task of scaling up Haber's tabletop machine to industrial-level production. He succeeded in 1910. Haber and Bosch were later awarded Nobel prizes, in 1918 and 1931 respectively, for their work in overcoming the chemical and engineering problems of large-scale, continuous-flow, high-pressure technology.\n\nAmmonia was first manufactured using the Haber process on an industrial scale in 1913 in BASF's Oppau plant in Germany, reaching 20 tonnes per day the following year.\n\nDuring World War I, the production of munitions required large amounts of nitrate. The Allies had access to large sodium nitrate deposits in Chile (so called \"Chile saltpetre\") controlled by British companies. Germany had no such resources, so the Haber process proved essential to the German war effort. Synthetic ammonia from the Haber process was used for the production of nitric acid, a precursor to the nitrates used in explosives.\n\nThis conversion is typically conducted at and between , as the gases (nitrogen and hydrogen) are passed over four beds of catalyst, with cooling between each pass so as to maintain a reasonable equilibrium constant. On each pass only about 15% conversion occurs, but any unreacted gases are recycled, and eventually an overall conversion of 97% is achieved.\n\nThe steam reforming, shift conversion, carbon dioxide removal, and methanation steps each operate at pressures of about , and the ammonia synthesis loop operates at pressures ranging from , depending upon which proprietary process is used.\n\nThe major source of hydrogen is methane from natural gas. The conversion, steam reforming, is conducted with steam in a high temperature and pressure tube inside a reformer with a nickel catalyst, separating the carbon and hydrogen molecules in the natural gas.\n\nNitrogen (N) is very unreactive because the molecules are held together by strong triple bonds. The Haber process relies on catalysts that accelerate the scission of this triple bond.\n\nTwo opposing considerations are relevant to this synthesis: the position of the equilibrium and the rate of reaction. At room temperature, the equilibrium is strongly in favor of ammonia, but the reaction doesn't proceed at a detectable rate. The obvious solution is to raise the temperature, but because the reaction is exothermic, the equilibrium constant (using bar or atm units) becomes 1 around . (See Le Châtelier's principle.)\nAbove this temperature, the equilibrium quickly becomes quite unfavorable at atmospheric pressure, according to the Van 't Hoff equation. Thus one might suppose that a low temperature is to be used and some other means to increase rate. However, the catalyst itself requires a temperature of at least 400 °C to be efficient.\n\nPressure is the obvious choice to favor the forward reaction because there are 4 moles of reactant for every 2 moles of product (see entropy), and the pressure used () alters the equilibrium concentrations to give a profitable yield.\n\nEconomically, pressure is an expensive commodity. Pipes and reaction vessels need to be strengthened, valves more rigorous, and there are safety considerations of working at 20 MPa. In addition, running pumps and compressors takes considerable energy. Thus the compromise used gives a single pass yield of around 15%.\n\nAnother way to increase the yield of the reaction would be to remove the product (i.e. ammonia gas) from the system. In practice, gaseous ammonia is not removed from the reactor itself, since the temperature is too high; it is removed from the equilibrium mixture of gases leaving the reaction vessel. The hot gases are cooled enough, whilst maintaining a high pressure, for the ammonia to condense and be removed as liquid. Unreacted hydrogen and nitrogen gases are then returned to the reaction vessel to undergo further reaction.\n\nThe most popular catalysts are based on iron promoted with KO, CaO, SiO, and AlO. The original Haber–Bosch reaction chambers used osmium as the catalyst, but it was available in extremely small quantities. Haber noted uranium was almost as effective and easier to obtain than osmium. Under Bosch's direction in 1909, the BASF researcher Alwin Mittasch discovered a much less expensive iron-based catalyst, which is still used today. Some ammonia production utilizes ruthenium-based catalysts (the KAAP process). Ruthenium forms more active catalysts that allows milder operating pressures. Such catalysts are prepared by decomposition of triruthenium dodecacarbonyl on graphite.\n\nIn industrial practice, the iron catalyst is obtained from finely ground iron powder, which is usually obtained by reduction of high purity magnetite (FeO). The pulverized iron metal is burnt (oxidized) to give magnetite of a defined particle size. The magnetite particles are then partially reduced, removing some of the oxygen in the process. The resulting catalyst particles consist of a core of magnetite, encased in a shell of wüstite (FeO), which in turn is surrounded by an outer shell of iron metal. The catalyst maintains most of its bulk volume during the reduction, resulting in a highly porous high surface area material, which enhances its effectiveness as a catalyst. Other minor components of the catalyst include calcium and aluminium oxides, which support the iron catalyst and help it maintain its surface area. These oxides of Ca, Al, K, and Si are unreactive to reduction by the hydrogen.\nThe reaction mechanism, involving the heterogeneous catalyst, is believed to involve the following steps:\n\n\nReaction 5 occurs in three steps, forming NH, NH, and then NH. Experimental evidence points to reaction 2 as being the slow, rate-determining step. This is not unexpected since the bond broken, the nitrogen triple bond, is the strongest of the bonds that must be broken.\n\nA major contributor to the elucidation of this mechanism is Gerhard Ertl.\n\nWhen it was first invented, the Haber process needed to compete against another industrial process, the Cyanamide process. However, the Cyanamide process consumed large amounts of electrical power and was more labor-intensive than the Haber process.\n\nThe Haber process now produces 450 million tonnes of nitrogen fertilizer per year, mostly in the form of anhydrous ammonia, ammonium nitrate, and urea. Three to five percent of the world's natural gas production is consumed in the Haber process (around 1–2% of the world's annual energy supply). In combination with pesticides, these fertilizers have quadrupled the productivity of agricultural land:\n\nDue to its dramatic impact on the human ability to grow food, the Haber process served as the \"detonator of the population explosion\", enabling the global population to increase from 1.6 billion in 1900 to today's 7 billion. Nearly 80% of the nitrogen found in human tissues originated from the Haber-Bosch process. Since nitrogen use efficiency is typically less than 50%, our heavy use of industrial nitrogen fixation is disruptive to our biological habitat.\n\n\n", "id": "14022", "title": "Haber process"}
{"url": "https://en.wikipedia.org/wiki?curid=14023", "text": "Hot or Not\n\nHot or Not is a rating site that allows users to rate the attractiveness of photos submitted voluntarily by others. The site offers a matchmaking engine called 'Meet Me' and an extended profile feature called \"Hotlists\". The domain hotornot.com is currently owned by Hot Or Not Limited, and was previously owned by Avid Life Media. 'Hot or Not' was a significant influence on the people who went on to create the social media sites Facebook and YouTube.\n\nUsers would submit photographs of themselves to the site for the purpose of other users to rate said person's attractiveness on a scale of 1 - 10, with the cumulative average acting as the overall score for a given photograph.\n\nThe site was founded in October 2000 by James Hong and Jim Young, two friends and Silicon Valley-based engineers. Both graduated from the University of California, Berkeley in electrical engineering, with Young pursuing a Ph.D at the time. It was inspired on some other developer's ideas.\n\nThe site was a technical solution to a disagreement they made one day over a passing woman's attractiveness. The site was originally called \"Am I Hot or Not\". Within a week of launching, it had reached almost two million page views per day. Within a few months, the site was immediately behind CNET and NBCi on NetNielsen Rating's Top 25 advertising domains. To keep up with rising costs Hong and Young added a matchmaking component to their website called \"Meet Me at Hot or Not\", i.e. a system of range voting. The matchmaking service has been especially successful and the site continues to generate most of its revenue through subscriptions. In the December 2006 issue of \"Time\" magazine, the founders of YouTube stated that they originally set out to make a version of Hot or Not with Video before developing their more inclusive site. Mark Zuckerberg of Facebook similarly got his start by creating a Hot or Not type site called FaceMash, where he posted photos from Harvard's Facebook for the university's community to rate.\n\nHot or Not was sold for a rumored $20 million on February 8, 2008 to Avid Life Media, owners of Ashley Madison. Annual revenue reached $7.5 million, with net profits of $5.5 million. They initially started off $60,000 in debt due to tuition fees James paid for his MBA. On July 31, 2008, Hot or Not launched Hot or Not Gossip and a Baresi rate box (a \"hot meter\") – a subdivision to expand their market, run by former radio DJ turned celebrity blogger Zack Taylor.\n\nHot or Not was preceded by the rating sites RateMyFace, which was registered a year earlier in the summer of 1999, and AmIHot.com, which was registered in January 2000 by MIT freshman Daniel Roy. Regardless, despite any head starts of its predecessors, Hot or Not quickly became the most popular. Since AmIHotOrNot.com's launch, the concept has spawned many imitators. The concept always remained the same, but the subject matter varied greatly. The concept has also been integrated with a wide variety of dating and matchmaking systems. In 2007 BecauseImHot.com launched and deleted anyone with a rating below 7 after a voting audit or the first 50 votes (whichever is first).\n\nVariations on the Hot or Not concept include voting via a Condorcet method where a candidate is compared with other candidates in a series of pairwise comparisons in order to gauge their popularity. Another variation used a four-way comparison of candidates to gauge their popularity and show a 'type' match for candidates who most closely match the average preferences shown by the user making the choices.\n\nIn 2005, as an example of using image morphing methods to study the effects of averageness, imaging researcher Pierre Tourigny created a composite of about 30 faces to find out the current standard of good looks on the Internet (as shown above). On the Hot or Not web site, people rate others' attractiveness on a scale of 1 to 10. An average score based on hundreds or even thousands of individual ratings takes only a few days to emerge. To make this hot or not palette of morphed images, photos from the site were sorted by rank and used SquirlzMorph to create multi-morph composites from them. Unlike projects like Face of Tomorrow, where the subjects are posed for the purpose, the portraits are blurry because the source images are low resolution with differences in posture, hair styles, glasses, etc., so that here images could use only 36 control points for the morphs. A similar study was done with Miss Universe contestants, as shown in the averageness article, as well as one for age, as shown in youthfulness article.\n\nA 2006 \"hot\" or \"not\" style study, involving 264 women and 18 men, at the Washington University School of Medicine, as published online in the journal \"Brain Research\", indicates that a person's brain determines whether an image is erotic long before the viewer is even aware they are seeing the picture. Moreover, according to these researchers, one of the basic functions of the brain is to classify images into a hot or not type categorization. The study's researchers also discovered that sexy shots induce a uniquely powerful reaction in the brain, equal in effect for both men and women, and that erotic images produced a strong reaction in the hypothalamus.\n\n\n", "id": "14023", "title": "Hot or Not"}
{"url": "https://en.wikipedia.org/wiki?curid=14024", "text": "H.263\n\nH.263 is a video compression standard originally designed as a low-bit-rate compressed format for videoconferencing. It was developed by the ITU-T Video Coding Experts Group (VCEG) in a project ending in 1995/1996 as one member of the H.26x family of video coding standards in the domain of the ITU-T, and it was later extended to add various additional enhanced features in 1998 and 2000. Smaller additions were also made in 1997 and 2001, and a unified edition was produced in 2005.\n\nThe H.263 standard was first designed to be utilized in H.324 based systems (PSTN and other circuit-switched network videoconferencing and videotelephony), but it also found use in H.323 (RTP/IP-based videoconferencing), H.320 (ISDN-based videoconferencing), RTSP (streaming media) and SIP (IP-based videoconferencing) solutions.\n\nH.263 is a required video coding format in ETSI 3GPP technical specifications for IP Multimedia Subsystem (IMS), Multimedia Messaging Service (MMS) and Transparent end-to-end Packet-switched Streaming Service (PSS). In 3GPP specifications, H.263 video is usually used in 3GP container format.\n\nH.263 also found many applications on the internet: much Flash Video content (as used on sites such as YouTube, Google Video, MySpace, etc.) used to be encoded in Sorenson Spark format (an incomplete implementation of H.263). The original version of the RealVideo codec was based on H.263 until the release of RealVideo 8.\n\nH.263 was developed as an evolutionary improvement based on experience from H.261, the previous ITU-T standard for video compression, and the MPEG-1 and MPEG-2 standards. Its first version was completed in 1995 and provided a suitable replacement for H.261 at all bit rates. It was further enhanced in projects known as H.263v2 (also known as H.263+ or H.263 1998), MPEG-4 Part 2 and H.263v3 (also known as H.263++ or H.263 2000). MPEG-4 Part 2 is H.263 compatible in the sense that basic \"baseline\" H.263 bitstreams are correctly decoded by an MPEG-4 Video decoder.\n\nThe next enhanced format developed by ITU-T VCEG (in partnership with MPEG) after H.263 was the H.264 standard, also known as AVC and MPEG-4 part 10. As H.264 provides a significant improvement in capability beyond H.263, the H.263 standard is now considered a legacy design. Most new videoconferencing products now include H.264 as well as H.263 and H.261 capabilities. An even-newer standard format, HEVC, has also been developed by VCEG and MPEG, and has begun to emerge in some applications.\n\nSince the original ratification of H.263 in March 1996 (approving a document that was produced in November 1995), there have been two subsequent additions which improved on the original standard by additional optional extensions (for example, the H.263v2 project added a deblocking filter in its Annex J).\n\nThe original version of H.263 specified the following annexes:\n\nThe first version of H.263 supported a limited set of picture sizes:\n\nIn March 1997, an informative Appendix I describing Error Tracking – an encoding technique for providing improved robustness to data losses and errors, was approved to provide information for the aid of implementers having an interest in such techniques.\n\nH.263v2 (also known as \"H.263+\", or as \"the 1998 version of H.263\") is the informal name of the second edition of the ITU-T H.263 international video coding standard. It retained the entire technical content of the original version of the standard, but enhanced H.263 capabilities by adding several annexes which can substantially improve encoding efficiency and provide other capabilities (such as enhanced robustness against data loss in the transmission channel). The H.263+ project was ratified by the ITU in February 1998. It added the following Annexes:\nH.263v2 also added support for flexible customized picture formats and custom picture clock frequencies. As noted above, the only picture formats previously supported in H.263 had been Sub-QCIF, QCIF, CIF, 4CIF, and 16CIF, and the only picture clock frequency had been 30000/1001 (approximately 29.97) clock ticks per second.\n\nH.263v2 specified a set of recommended modes in an informative appendix (Appendix II, since deprecated):\nThe definition of H.263v3 (also known as H.263++ or as the 2000 version of H.263) added three annexes. These annexes and an additional annex that specified profiles (approved the following year) were originally published as separate documents from the main body of the standard itself. The additional annexes specified are:\n\nThe prior informative Appendix II (recommended optional enhancement) was obsoleted by the creation of the normative Annex X.\n\nIn June 2001, another informative appendix (Appendix III, Examples for H.263 encoder/decoder implementations) was approved. It describes techniques for encoding and for error/loss concealment by decoders.\n\nIn January 2005, a unified H.263 specification document was produced (with the exception of Appendix III, which remains as a separately-published document).\n\nIn August 2005, an implementors guide was approved to correct a small error in the seldom-used Annex Q reduced-resolution update mode.\n\nIn countries without software patents, H.263 video can be legally encoded and decoded with the free LGPL-licensed libavcodec library (part of the FFmpeg project) which is used by programs such as ffdshow, VLC media player and MPlayer.\n\n\n", "id": "14024", "title": "H.263"}
{"url": "https://en.wikipedia.org/wiki?curid=14026", "text": "House of Orange (disambiguation)\n\nHouse of Orange may refer to:\n\n\n", "id": "14026", "title": "House of Orange (disambiguation)"}
{"url": "https://en.wikipedia.org/wiki?curid=14029", "text": "Histone\n\nIn biology, histones are highly alkaline proteins found in eukaryotic cell nuclei that package and order the DNA into structural units called nucleosomes. They are the chief protein components of chromatin, acting as spools around which DNA winds, and playing a role in gene regulation. Without histones, the unwound DNA in chromosomes would be very long (a length to width ratio of more than 10 million to 1 in human DNA). For example, each human diploid cell (containing 23 pairs of chromosomes) has about 1.8 meters of DNA, but wound on the histones it has about 90 micrometers (0.09 mm) of chromatin, which, when duplicated and condensed during mitosis, result in about 120 micrometers of chromosomes.\n\nFive major families of histones exist: H1/H5, H2A, H2B, H3, and H4. Histones H2A, H2B, H3 and H4 are known as the core histones, while histones H1/H5 are known as the linker histones.\n\nThe core histones all exist as dimers, which are similar in that they all possess the histone fold domain; three alpha helices linked by two loops. It is this helical structure that allows for interaction between distinct dimers, particularly in a head-tail fashion (also called the handshake motif). The resulting four distinct dimers then come together to form one octameric nucleosome core, approximately 63 Angstroms in diameter (a solenoid (DNA)-like particle). Around 146 base pairs (bp) of DNA wrap around this core particle 1.65 times in a left-handed super-helical turn to give a particle of around 100 Angstroms across. The linker histone H1 binds the nucleosome at the entry and exit sites of the DNA, thus locking the DNA into place and allowing the formation of higher order structure. The most basic such formation is the 10 nm fiber or beads on a string conformation. This involves the wrapping of DNA around nucleosomes with approximately 50 base pairs of DNA separating each pair of nucleosomes (also referred to as linker DNA). Higher-order structures include the 30 nm fiber (forming an irregular zigzag) and 100 nm fiber, these being the structures found in normal cells. During mitosis and meiosis, the condensed chromosomes are assembled through interactions between nucleosomes and other regulatory proteins.\n\nHistones are subdivided into canonical replication-dependent histones that are expressed during the S-phase of cell cycle and replication-independent histone variants, expressed during the whole cell cycle. In animals, genes encoding canonical histones are typically clustered along the chromosome, lack introns and use a stem loop structure at the 3’ end instead of a polyA tail. Genes encoding histone variants are usually not clustered, have introns and their mRNAs are regulated with polyA tails. Complex multicellular organisms typically have a higher number of histone variants providing a variety of different functions. Recent data are accumulating about the roles of diverse histone variants highlighting the functional links between variants and the delicate regulation of organism development. Histone variants from different organisms, their classification and variant specific features can be found in \"HistoneDB 2.0 - Variants\" database.\n\nThe following is a list of human histone proteins:\n\nThe nucleosome core is formed of two H2A-H2B dimers and a H3-H4 tetramer, forming two nearly symmetrical halves by tertiary structure (C2 symmetry; one macromolecule is the mirror image of the other). The H2A-H2B dimers and H3-H4 tetramer also show pseudodyad symmetry. The 4 'core' histones (H2A, H2B, H3 and H4) are relatively similar in structure and are highly conserved through evolution, all featuring a 'helix turn helix turn helix' motif (which allows the easy dimerisation). They also share the feature of long 'tails' on one end of the amino acid structure - this being the location of post-translational modification (see below).\n\nIt has been proposed that histone proteins are evolutionarily related to the helical part of the extended AAA+ ATPase domain, the C-domain, and to the N-terminal substrate recognition domain of Clp/Hsp100 proteins. Despite the differences in their topology, these three folds share a homologous helix-strand-helix (HSH) motif.\n\nUsing an electron paramagnetic resonance spin-labeling technique, British researchers measured the distances between the spools around which eukaryotic cells wind their DNA. They determined the spacings range from 59 to 70 Å.\n\nIn all, histones make five types of interactions with DNA:\n\nThe highly basic nature of histones, aside from facilitating DNA-histone interactions, contributes to their water solubility.\n\nHistones are subject to post translational modification by enzymes primarily on their N-terminal tails, but also in their globular domains. Such modifications include methylation, citrullination, acetylation, phosphorylation, SUMOylation, ubiquitination, and ADP-ribosylation. This affects their function of gene regulation.\n\nIn general, genes that are active have less bound histone, while inactive genes are highly associated with histones during interphase. It also appears that the structure of histones has been evolutionarily conserved, as any deleterious mutations would be severely maladaptive. All histones have a highly positively charged N-terminus with many lysine and arginine residues.\n\nHistones were discovered in 1884 by Albrecht Kossel. The word \"histone\" dates from the late 19th century and is from the German word \"\"Histon\"\", a word itself of uncertain origin - perhaps from the Greek \"histanai\" or \"histos\". Until the early 1990s, histones were dismissed by most as inert packing material for eukaryotic nuclear DNA, a view based in part on the models of Mark Ptashne and others, who believed that transcription was activated by protein-DNA and protein-protein interactions on largely naked DNA templates, as is the case in bacteria.\n\nDuring the 1980s, Yahli Lorch and Roger Kornberg showed that a nucleosome on a core promoter prevents the initiation of transcription in vitro, and Michael Grunstein demonstrated that histones repress transcription in vivo, leading to the idea of the nucleosome as a general gene repressor. Relief from repression is believed to involve both histone modification and the action of chromatin-remodeling complexes. Vincent Allfrey and Alfred Mirsky earlier proposed a role of histone modification in transcriptional activation, regarded as a molecular manifestation of epigenetics. Michael Grunstein and David Allis found support for this proposal, in the importance of histone acetylation for transcription in yeast and the activity of the transcriptional activator Gcn5 as a histone acetyltransferase.\n\nThe discovery of the H5 histone appears to date back to the 1970s, and it is now considered an isoform of Histone H1.\n\nHistones are found in the nuclei of eukaryotic cells, and in certain Archaea, namely Thermoproteales and Euryarchaea, but not in bacteria. The unicellular algae known as dinoflagellates were previously thought to be the only eukaryotes that completely lack histones, however, later studies showed that their DNA still encodes histone genes.\n\nArchaeal histones may well resemble the evolutionary precursors to eukaryotic histones. Histone proteins are among the most highly conserved proteins in eukaryotes, emphasizing their important role in the biology of the nucleus. In contrast mature sperm cells largely use protamines to package their genomic DNA, most likely because this allows them to achieve an even higher packaging ratio.\n\nCore histones are highly conserved proteins; that is, there are very few differences among the amino acid sequences of the histone proteins of different species.\n\nThere are some \"variant\" forms in some of the major classes. They share amino acid sequence homology and core structural similarity to a specific class of major histones but also have their own feature that is distinct from the major histones. These \"minor histones\" usually carry out specific functions of the chromatin metabolism. For example, histone H3-like CenpA is associated with only the centromere region of the chromosome. Histone H2A variant H2A.Z is associated with the promoters of actively transcribed genes and also involved in the prevention of the spread of silent heterochromatin. Furthermore, H2A.Z has roles in chromatin for genome stability. Another H2A variant H2A.X is phosphorylated at S139 in regions around double-strand breaks and marks the region undergoing DNA repair. Histone H3.3 is associated with the body of actively transcribed genes.\n\nHistones act as spools around which DNA winds. This enables the compaction necessary to fit the large genomes of eukaryotes inside cell nuclei: the compacted molecule is 40,000 times shorter than an unpacked molecule.\n\nHistones undergo posttranslational modifications that alter their interaction with DNA and nuclear proteins. The H3 and H4 histones have long tails protruding from the nucleosome, which can be covalently modified at several places. Modifications of the tail include methylation, acetylation, phosphorylation, ubiquitination, SUMOylation, citrullination, and ADP-ribosylation. The core of the histones H2A and H2B can also be modified. Combinations of modifications are thought to constitute a code, the so-called \"histone code\". Histone modifications act in diverse biological processes such as gene regulation, DNA repair, chromosome condensation (mitosis) and spermatogenesis (meiosis).\n\nThe common nomenclature of histone modifications is:\n\nSo H3K4me1 denotes the monomethylation of the 4th residue (a lysine) from the start (i.e., the N-terminal) of the H3 protein.\n\nExamples of histone modifications in transcription regulation include:\n\nA huge catalogue of histone modifications have been described, but a functional understanding of most is still lacking. Collectively, it is thought that histone modifications may underlie a histone code, whereby combinations of histone modifications have specific meanings. However, most functional data concerns individual prominent histone modifications that are biochemically amenable to detailed study.\n\nThe addition of one, two or three methyl groups to lysine has little effect on the chemistry of the histone; methylation leaves the charge of the lysine intact and adds a minimal number of atoms so steric interactions are mostly unaffected. However, proteins containing Tudor, chromo or PHD domains, amongst others, can recognise lysine methylation with exquisite sensitivity and differentiate mono, di and tri-methyl lysine, to the extent that, for some lysines (e.g.: H4K20) mono, di and tri-methylation appear to have different meanings. Because of this, lysine methylation tends to be a very informative mark and dominates the known histone modification functions.\nWhat was said above of the chemistry of lysine methylation also applies to arginine methylation, and some protein domains—e.g., Tudor domains—can be specific for methyl arginine instead of methyl lysine. Arginine is known to be mono- or di-methylated, and methylation can be symmetric or asymmetric, potentially with different meanings.\n\nEnzymes called peptidylarginine deiminases (PADs) hydrolyze the imine group of arginines and attach a keto group, so that there is one less positive charge on the amino acid residue. This process has been involved in the activation of gene expression by making the modified histones less tightly bound to DNA and thus making the chromatin more accessible. PADs can also produce the opposite effect by removing or inhibiting mono-methylation of arginine residues on histones and thus antagonizing the positive effect arginine methylation has on transcriptional activity.\n\nAddition of an acetyl group has a major chemical effect on lysine as it neutralises the positive charge. This reduces electrostatic attraction between the histone and the negatively charged DNA backbone, loosening the chromatin structure; highly acetylated histones form more accessible chromatin and tend to be associated with active transcription. Lysine acetylation appears to be less precise in meaning than methylation, in that histone acetyltransferases tend to act on more than one lysine; presumably this reflects the need to alter multiple lysines to have a significant effect on chromatin structure. The modification includes H3K27ac.\nAddition of a negatively charged phosphate group can lead to major changes in protein structure, leading to the well-characterised role of phosphorylation in controlling protein function. It is not clear what structural implications histone phosphorylation has, but histone phosphorylation has clear functions as a post-translational modification, and binding domains such as BRCT have been characterised.\n\nMost well-studied histone modifications are involved in control of transcription.\n\nTwo histone modifications are particularly associated with active transcription:\n\nThree histone modifications are particularly associated with repressed genes:\n\nAnalysis of histone modifications in embryonic stem cells (and other stem cells) revealed many gene promoters carrying both H3K4Me3 and H3K27Me3, in other words these promoters display both activating and repressing marks simultaneously. This peculiar combination of modifications marks genes that are poised for transcription; they are not required in stem cells, but are rapidly required after differentiation into some lineages. Once the cell starts to differentiate, these bivalent promoters are resolved to either active or repressive states depending on the chosen lineage.\n\nMarking sites of DNA damage is an important function for histone modifications. It also protects DNA from getting destroyed by ultraviolet radiation of sun.\n\n\n\n\n", "id": "14029", "title": "Histone"}
{"url": "https://en.wikipedia.org/wiki?curid=14031", "text": "Hierarchical organization\n\nA hierarchical organization is an organizational structure where every entity in the organization, except one, is subordinate to a single other entity. This arrangement is a form of a hierarchy. In an organization, the hierarchy usually consists of a singular/group of power at the top with subsequent levels of power beneath them. This is the dominant mode of organization among large organizations; most corporations, governments, and organized religions are hierarchical organizations with different levels of management, power or authority. For example, the broad, top-level overview of the general organization of the Catholic Church consists of the Pope, then the Cardinals, then the Archbishops, and so on. \n\nMembers of hierarchical organizational structures chiefly communicate with their immediate superior and with their immediate subordinates. Structuring organizations in this way is useful partly because it can reduce the communication overhead by limiting information flow; this is also its major limitation.\n\nA hierarchy is typically visualized as a pyramid, where the height of the ranking or person depicts their power status and the width of that level represents how many people or business divisions are at that level relative to the whole—the highest-ranking people are at the apex, and there are very few of them; the base may include thousands of people who have no subordinates. These hierarchies are typically depicted with a tree or triangle diagram, creating an organizational chart or organigram. Those nearest the top have more power than those nearest the bottom, and there being fewer people at the top than at the bottom. As a result, superiors in a hierarchy generally have higher status and command greater rewards than their subordinates.\n\nAll governments and most companies have similar structures. Traditionally, the monarch was the pinnacle of the state. In many countries, feudalism and manorialism provided a formal social structure that established hierarchical links at every level of society, with the monarch at the top. \n\nIn modern post-feudal states the nominal top of the hierarchy still remains the head of state, which may be a president or a constitutional monarch, although in many modern states the powers of the head of state are delegated among different bodies. Below the head, there is commonly a senate, parliament or congress, which in turn often delegate the day-to-day running of the country to a prime minister. In many democracies, the people are considered to be the notional top of the hierarchy, over the head of state; in reality, the people's power is restricted to voting in elections.\n\nIn business, the business owner traditionally occupied the pinnacle of the organization. In most modern large companies, there is now no longer a single dominant shareholder, and the collective power of the business owners is for most purposes delegated to a board of directors, which in turn delegates the day-to-day running of the company to a managing director or CEO. Again, although the shareholders of the company are the nominal top of the hierarchy, in reality many companies are run at least in part as personal fiefdoms by their management; corporate governance rules are an attempt to mitigate this tendency.\n\nThe organizational development theorist Elliott Jacques identified a special role for hierarchy in his concept of requisite organization. \nThe iron law of oligarchy, introduced by Robert Michels, describes the inevitable tendency of hierarchical organizations to become oligarchic in their decision making.\n\nThe Peter Principle is a term coined by Laurence J. Peter in which the selection of a candidate for a position in an hierarchical organization is based on the candidate's performance in their current role, rather than on abilities relevant to the intended role. Thus, employees only stop being promoted once they can no longer perform effectively, and managers in an hierarchical organization \"rise to the level of their incompetence.\" \n\nHierarchiology is another term coined by Laurence J. Peter, described in his humorous book of the same name, to refer to the study of hierarchical organizations and the behavior of their members.\n\nThe IRG Solution – hierarchical incompetence and how to overcome it argued that hierarchies were inherently incompetent, and were only able to function due to large amounts of informal lateral communication fostered by private informal networks.\n\nNarcissists may like hierarchical organisations because they might think they will rise to high ranks and reap status and power. Narcissists may be less interested in hierarchies where there is little opportunity for upward mobility.\n\nIn the work of diverse theorists such as William James (1842–1910), Michel Foucault (1926–1984) and Hayden White, important critiques of hierarchical epistemology are advanced. James famously asserts in his work \"Radical Empiricism\" that clear distinctions of type and category are a constant but unwritten goal of scientific reasoning, so that when they are discovered, success is declared. But if aspects of the world are organized differently, involving inherent and intractable ambiguities, then scientific questions are often considered unresolved. A hesitation to declare success upon the discovery of ambiguities leaves heterarchy at an artificial and subjective disadvantage in the scope of human knowledge. This bias is an artifact of an aesthetic or pedagogical preference for hierarchy, and not necessarily an expression of objective observation.\n\nHierarchies and hierarchical thinking has been criticized by many people, including Susan McClary and one political philosophy which is vehemently opposed to hierarchical organization: anarchism is generally opposed to hierarchical organization in any form of human relations. Heterarchy is the most commonly proposed alternative to hierarchy and this has been combined with responsible autonomy by Gerard Fairtlough in his work on Triarchy theory.\n\nAmidst constant innovation in information and communication technologies, hierarchical authority structures are giving way to greater decision-making latitude for individuals and more flexible definitions of job activities and this new style of work presents a challenge to existing organizational forms, with some research studies contrasting traditional organizational forms against groups that operate as online communities that are characterized by personal motivation and the satisfaction of making one's own decisions. With all levels of an organization having access to information and communication via digital means, power structures align more as a wirearchy, enabling the flow of power and authority to be based not on hierarchical levels, but on information, trust, credibility, and a focus on results.\n", "id": "14031", "title": "Hierarchical organization"}
{"url": "https://en.wikipedia.org/wiki?curid=14033", "text": "Harry Secombe\n\nSir Harry Donald Secombe, CBE (8 September 1921 – 11 April 2001) was a Welsh comedian and singer. He played Neddie Seagoon, a central character in the BBC radio comedy series \"The Goon Show\" (1951–60). He also appeared in musicals and films and, in his later years, was a presenter of television shows incorporating hymns and other devotional songs.\n\nSecombe was born in St Thomas, Swansea, the third of four children of Nellie Jane Gladys (née Davies), a shop manageress, and Frederick Ernest Secombe, a grocer. From the age of 11 he attended Dynevor School, a state grammar school in central Swansea.\n\nHis family were regular churchgoers, belonging to the congregation of St Thomas Church. A member of the choir, Secombe would – from the age of 12 – perform a sketch entitled \"The Welsh Courtship\" at church socials, acting as \"feed\" to his sister Carol. His elder brother, Fred Secombe, was the author of several books about his experiences as an Anglican priest and rector.\n\nAfter leaving school in 1937, Secombe became a pay clerk at Baldwin's store. With war looming, he decided in 1938 that he would join the Territorial Army (United Kingdom). Very short sighted, he got a friend to tell him the sight test, and then learnt it by heart. He served as a Lance Bombardier in No.132 Field Regiment of the Royal Artillery. He would refer to the unit in which he served during World War II in the North African Campaign, Sicily, and Italy, as \"The Five-Mile Snipers\". While in North Africa Secombe met Spike Milligan for the first time. In Sicily he joined a concert party and developed his own comedy routines to entertain the troops.\n\nWhen Secombe visited the Falkland Islands to entertain the troops after the 1982 Falklands War, his old regiment promoted him to the rank of sergeant – 37 years after he had been demobbed.\n\nHe made his first radio broadcast in May 1944 on a variety show aimed at the services. Following the end of fighting in the war but prior to demobilisation Secombe joined a pool of entertainers in Naples and formed a comedy duo with Spike Milligan.\n\nSecombe joined the cast of the Windmill Theatre in 1946, using a routine he had developed in Italy about how people shaved. Secombe always claimed that his ability to sing could always be counted on to save him when he bombed. Both Milligan and Sellers credited him with keeping the act on the bill when club owners had wanted to sack them.\n\nAfter a regional touring career, his first break came in radio when he was chosen as resident comedian for the Welsh series \"Welsh Rarebit,\" followed by appearances on \"Variety Bandbox\" and a regular role in \"Educating Archie\".\n\nSecombe met Michael Bentine at the Windmill Theatre, and was introduced to Peter Sellers by his agent Jimmy Grafton. Together with Spike Milligan, the four wrote a comedy radio script, and \"Those Crazy People\" was commissioned and first broadcast on 28 May 1951. Produced by Peter Ross, this would soon become \"The Goon Show\" and the show remained on the air until 1960. Secombe mainly played Neddie Seagoon, around whom the show's absurd plots developed.\n\nWith the success of \"The Goon Show\", Secombe developed a dual career as both a comedy actor and a singer. At the beginning of his career as an entertainer his act would end with a joke version of the duet \"Sweethearts,\" in which he sang both the baritone and falsetto parts. Trained under Italian maestro Manlio di Veroli, he emerged as a \"bel canto\" tenor (characteristically, he insisted that in his case this meant \"can belto\") and had a long list of best-selling record albums to his credit.\n\nIn 1958 he appeared in the film \"Jet Storm,\" which starred Dame Sybil Thorndike and Richard Attenborough and in the same year Secombe starred in the title role in \"Davy\", one of Ealing Studios' last films.\n\nThe power of his voice allowed Secombe to appear in many stage musicals. This included 1963's \"Pickwick,\" based on Charles Dickens' \"The Pickwick Papers\", which gave him the number 18 hit single \"If I Ruled the World\" – his later signature tune. In 1965 the show was produced on tour in the United States, where on Broadway he garnered a nomination for a Tony Award for Best Actor in a Musical. He also appeared in the musical \"The Four Musketeers\" (1967) at Drury Lane, as Mr. Bumble in Carol Reed's film of \"Oliver!\" (1968), and in the Envy segment of \"The Magnificent Seven Deadly Sins\" (1971).\n\nHe would go on to star in his own television show, \"The Harry Secombe Show\", which debuted on Christmas Day 1968 on BBC 1 and ran for thirty-one episodes until 1973. A sketch comedy show featuring Julian Orchard as Secombe's regular sidekick, the series also featured guest appearances by fellow Goon Spike Milligan as well as leading performers such as Ronnie Barker and Arthur Lowe. Secombe later starred in similar vehicles such as \"Sing a Song of Secombe\" and ITV's \"Secombe with Music\" during the 1970s.\n\nLater in life, Secombe (whose brother Fred Secombe was a priest in the Church in Wales, part of the Anglican Communion) attracted new audiences as a presenter of religious programmes, such as the BBC's \"Songs of Praise\" and ITV's \"Stars on Sunday\" and \"Highway\". He was also a special programming consultant to Harlech Television. and hosted a Thames Television programme in 1979 entitled \"Cross on the Donkey's Back\". In the latter half of the 1980s, Secombe personally sponsored a football team for boys aged 9–11 in the local West Sutton Little League, 'Secombes Knights'.\n\nIn 1990, he was one of a few to be honoured by a second appearance on \"This Is Your Life\", when he was surprised by Michael Aspel at a book signing in a London branch of WH Smith. Secombe had been a subject of the show previously in March 1958 when Eamonn Andrews surprised him at the BBC Television Theatre.\n\nIn 1963 he was appointed a Commander of the Order of the British Empire (CBE).\n\nHe was knighted in 1981, and jokingly referred to himself as Sir Cumference (in recognition of his rotund figure). The motto he chose for his coat of arms was \"GO ON\", a reference to goon.\n\nSecombe suffered from peritonitis in 1980. Within two years, taking advice from doctors, he had lost five stone in weight. He had a stroke in 1997, from which he made a slow recovery. He was then diagnosed with prostate cancer in September 1998. After suffering a second stroke in 1999, he was forced to abandon his television career, but made a documentary about his condition in the hope of giving encouragement to other sufferers. Secombe had diabetes in the latter part of his life.\n\nSecombe died on 11 April 2001 at the age of 79, from prostate cancer, in hospital in Guildford, Surrey. His ashes are interred at the parish church of Shamley Green, and a later memorial service to celebrate his life was held at Westminster Abbey on 26 October 2001. As well as family members and friends, the service was also attended by Charles, Prince of Wales and representatives of Prince Philip, Duke of Edinburgh, Anne, Princess Royal, Princess Margaret, Countess of Snowdon and Prince Edward, Duke of Kent. On his tombstone is the inscription: \"To know him was to love him.\"\n\nUpon hearing of his old friend's death, Spike Milligan quipped, \"I'm glad he died before me, because I didn't want him to sing at my funeral.\" But Secombe would have the last laugh: upon Milligan's own death the following year, a recording of Secombe singing was played at Spike's memorial service.\n\nThe Secombe Theatre at Sutton, London, bears his name in memory of this former local personality. He is also fondly remembered at the London Welsh Centre, where he opened the bar on St Patrick's Day (17 March) 1971.\n\nSecombe met Myra Atherton at the Mumbles dance hall in 1946. The couple were married from 1948 until his death, and had four children:\n\n\n\n\n\n\n", "id": "14033", "title": "Harry Secombe"}
{"url": "https://en.wikipedia.org/wiki?curid=14034", "text": "Heroin\n\nHeroin, also known as diamorphine among other names, is an opiate typically used as a recreational drug for its euphoric effects. Medically it is occasionally used to relieve pain and as a form of opioid replacement therapy alongside counseling. Heroin is typically injected, usually into a vein; however, it can also be smoked, snorted or inhaled. Onset of effects is usually rapid and lasts for a few hours.\nCommon side effects include respiratory depression (decreased breathing) and about a quarter of those who use heroin become physically dependent. Other side effects can include abscesses, infected heart valves, blood borne infections, constipation, and pneumonia. After a history of long-term use, withdrawal symptoms can begin within hours of last use. When given by injection into a vein, heroin has two to three times the effect as a similar dose of morphine. It typically comes as a white or brown powder.\nTreatment of heroin addiction often includes behavioral therapy and medications. Medications used may include methadone or naltrexone. A heroin overdose may be treated with naloxone. An estimated 17 million people as of 2015 use opiates such as heroin, which together with opioids resulted in 122,000 deaths. The total number of opiate users has increased from 1998 to 2007 after which it has remained more or less stable. In the United States about 1.6 percent of people have used heroin at some point in time. When people die from overdosing on a drug, the drug is usually an opioid.\nHeroin was first made by C. R. Alder Wright in 1874 from morphine, a natural product of the opium poppy. Internationally, heroin is controlled under Schedules I and IV of the Single Convention on Narcotic Drugs. It is generally illegal to make, possess, or sell heroin without a license. In 2015 Afghanistan produced about 66% of the world's opium. Often heroin, which is illegally sold, is mixed with other substances such as sugar or strychnine.\n\nThe original trade name of heroin is typically used in non-medical settings. It is used as a recreational drug for the euphoria it induces. Anthropologist Michael Agar once described heroin as \"the perfect whatever drug.\" Tolerance develops quickly, and increased doses are needed in order to achieve the same effects. Its popularity with recreational drug users, compared to morphine, reportedly stems from its perceived different effects. In particular, users report an intense rush, an acute transcendent state of euphoria, which occurs while diamorphine is being metabolized into 6-monoacetylmorphine (6-MAM) and morphine in the brain. Some believe that heroin produces more euphoria than other opioids; one possible explanation is the presence of 6-monoacetylmorphine, a metabolite unique to heroin – although a more likely explanation is the rapidity of onset. While other opioids of recreational use produce only morphine, heroin also leaves 6-MAM, also a psycho-active metabolite. However, this perception is not supported by the results of clinical studies comparing the physiological and subjective effects of injected heroin and morphine in individuals formerly addicted to opioids; these subjects showed no preference for one drug over the other. Equipotent injected doses had comparable action courses, with no difference in subjects' self-rated feelings of euphoria, ambition, nervousness, relaxation, drowsiness, or sleepiness.\n\nShort-term addiction studies by the same researchers demonstrated that tolerance developed at a similar rate to both heroin and morphine. When compared to the opioids hydromorphone, fentanyl, oxycodone, and pethidine (meperidine), former addicts showed a strong preference for heroin and morphine, suggesting that heroin and morphine are particularly susceptible to abuse and addiction. Morphine and heroin were also much more likely to produce euphoria and other positive subjective effects when compared to these other opioids.\n\nSome researchers have attempted to explain heroin use and the culture that surrounds it through the use of sociological theories. In \"Righteous Dopefiend\", Philippe Bourgois and Jeff Schonberg use anomie theory to explain why people begin using heroin. By analyzing a community in San Francisco, they demonstrated that heroin use was caused in part by internal and external factors such as violent homes and parental neglect. This lack of emotional, social, and financial support causes strain and influences individuals to engage in deviant acts, including heroin usage. They further found that heroin users practiced \"retreatism\", a behavior first described by Howard Abadinsky, in which those suffering from such strain reject society's goals and institutionalized means of achieving them.\n\nIn the United States heroin is not accepted as medically useful.\n\nUnder the generic name diamorphine, heroin is prescribed as a strong pain medication in the United Kingdom, where it is given via subcutaneous, intramuscular, intrathecal or intravenously. Its use includes treatment for acute pain, such as in severe physical trauma, myocardial infarction, post-surgical pain, and chronic pain, including end-stage cancer and other terminal illnesses. In other countries it is more common to use morphine or other strong opioids in these situations. In 2004 the National Institute for Health and Clinical Excellence produced guidance on the management of caesarian section, which recommended the use of intrathecal or epidural diamorphine for post-operative pain relief.\n\nDiamorphine continues to be widely used in palliative care in the UK, where it is commonly given by the subcutaneous route, often via a syringe driver, if patients cannot easily swallow morphine solution. The advantage of diamorphine over morphine is that diamorphine is more fat soluble and therefore more potent by injection, so smaller doses of it are needed for the same effect on pain. Both of these factors are advantageous if giving high doses of opioids via the subcutaneous route, which is often necessary in palliative care.\n\nA number of European countries including the United Kingdom allow the prescribing of heroin for heroin addiction.\n\nDiamorphine is also used as a maintenance drug to assist the treatment of opiate addiction, normally in long-term chronic intravenous (IV) heroin users. It is only prescribed following exhaustive efforts at treatment via other means. It is sometimes thought that heroin users can walk into a clinic and walk out with a prescription, but the process takes many weeks before a prescription for diamorphine is issued. Though this is somewhat controversial among proponents of a zero-tolerance drug policy, it has proven superior to methadone in improving the social and health situations of addicts.\n\nThe UK Department of Health's Rolleston Committee Report in 1926 established the British approach to diamorphine prescription to users, which was maintained for the next 40 years: dealers were prosecuted, but doctors could prescribe diamorphine to users when withdrawing from it would cause harm or severe distress to the patient. This \"policing and prescribing\" policy effectively controlled the perceived diamorphine problem in the UK until 1959 when the number of diamorphine addicts doubled every 16 months during the ten years from 1959 to 1968. In 1964 the Brain Committee recommended that only selected approved doctors working at approved specialised centres be allowed to prescribe diamorphine and benzoylmethylecgonine (cocaine) to users. The law was made more restrictive in 1968. Beginning in the 1970s, the emphasis shifted to abstinence and the use of methadone; currently only a small number of users in the UK are prescribed diamorphine.\n\nIn 1994 Switzerland began a trial diamorphine maintenance program for users that had failed multiple withdrawal programs. The aim of this program was to maintain the health of the user by avoiding medical problems stemming from the illicit use of diamorphine. The first trial in 1994 involved 340 users, although enrollment was later expanded to 1000 based on the apparent success of the program.\n\nThe trials proved diamorphine maintenance to be superior to other forms of treatment in improving the social and health situation for this group of patients. It has also been shown to save money, despite high treatment expenses, as it significantly reduces costs incurred by trials, incarceration, health interventions and delinquency. Patients appear twice daily at a treatment center, where they inject their dose of diamorphine under the supervision of medical staff. They are required to contribute about 450 Swiss francs per month to the treatment costs. A national referendum in November 2008 showed 68% of voters supported the plan, introducing diamorphine prescription into federal law. The previous trials were based on time-limited executive ordinances. The success of the Swiss trials led German, Dutch, and Canadian cities to try out their own diamorphine prescription programs. Some Australian cities (such as Sydney) have instituted legal diamorphine supervised injecting centers, in line with other wider harm minimization programs.\n\nSince January 2009, Denmark has prescribed diamorphine to a few addicts that have tried methadone and subutex without success. Beginning in February 2010, addicts in Copenhagen and Odense became eligible to receive free diamorphine. Later in 2010 other cities including Århus and Esbjerg joined the scheme. It was supposed that around 230 addicts would be able to receive free diamorphine.\nHowever, Danish addicts would only be able to inject heroin according to the policy set by Danish National Board of Health. Of the estimated 1500 drug users who did not benefit from the then-current oral substitution treatment, approximately 900 would not be in the target group for treatment with injectable diamorphine, either because of \"massive multiple drug abuse of non-opioids\" or \"not wanting treatment with injectable diamorphine\".\n\nIn July 2009 the German Bundestag passed a law allowing diamorphine prescription as a standard treatment for addicts; a large-scale trial of diamorphine prescription had been authorized in that country in 2002.\n\nOn August 26 2016 Health Canada issued regulations amending prior regulations it had issued under the Controlled Drugs and Substances Act; the \"New Classes of Practitioners Regulations\", the \"Narcotic Control Regulations\", and the \"Food and Drug Regulations\", to allow doctors to prescribe diamorphine to people who have a severe opioid addiction who haven't responded to other treatments. The prescription heroin can be accessed by doctors through Health Canada's Special Access Programme (SAP) for \"emergency access to drugs for patients with serious or life-threatening conditions when conventional treatments have failed, are unsuitable, or are unavailable.\"\n\nThe onset of heroin's effects depends upon the route of administration. Studies have shown that the subjective pleasure of drug use (the reinforcing component of addiction) is proportional to the rate at which the blood level of the drug increases. Intravenous injection is the fastest route of drug administration, causing blood concentrations to rise the most quickly, followed by smoking, suppository (anal or vaginal insertion), insufflation (snorting), and ingestion (swallowing).\n\nIngestion does not produce a rush as forerunner to the high experienced with the use of heroin, which is most pronounced with intravenous use. While the onset of the rush induced by injection can occur in as little as a few seconds, the oral route of administration requires approximately half an hour before the high sets in. Thus, with both higher the dosage of heroin used and faster the route of administration used, the higher potential risk for psychological addiction.\n\nLarge doses of heroin can cause fatal respiratory depression, and the drug has been used for suicide or as a murder weapon. The serial killer Harold Shipman used diamorphine on his victims, and the subsequent Shipman Inquiry led to a tightening of the regulations surrounding the storage, prescribing and destruction of controlled drugs in the UK. John Bodkin Adams is also known to have used heroin as a murder weapon.\n\nBecause significant tolerance to respiratory depression develops quickly with continued use and is lost just as quickly during withdrawal, it is often difficult to determine whether a heroin lethal overdose was accidental, suicide or homicide. Examples include the overdose deaths of Sid Vicious, Janis Joplin, Tim Buckley, Hillel Slovak, Layne Staley, Bradley Nowell, Ted Binion, and River Phoenix.\n\nChronic use of heroin and other opioids has been shown to be a potential cause of hyponatremia, resultant because of excess vasopressin secretion.\n\nOral use of heroin is less common than other methods of administration, mainly because there is little to no \"rush\", and the effects are less potent. Heroin is entirely converted to morphine by means of first-pass metabolism, resulting in deacetylation when ingested. Heroin's oral bioavailability is both dose-dependent (as is morphine's) and significantly higher than oral use of morphine itself, reaching up to 64.2% for high doses and 45.6% for low doses; opiate-naive users showed far less absorption of the drug at low doses, having bioavailabilities of only up to 22.9%. The maximum plasma concentration of morphine following oral administration of heroin was around twice as much as that of oral morphine.\n\nInjection, also known as \"slamming\", \"banging\", \"shooting up\", \"digging\" or \"mainlining\", is a popular method which carries relatively greater risks than other methods of administration. Heroin base (commonly found in Europe), when prepared for injection, will only dissolve in water when mixed with an acid (most commonly citric acid powder or lemon juice) and heated. Heroin in the east-coast United States is most commonly found in the hydrochloride salt form, requiring just water (and no heat) to dissolve. Users tend to initially inject in the easily accessible arm veins, but as these veins collapse over time, users resort to more dangerous areas of the body, such as the femoral vein in the groin. Users who have used this route of administration often develop a deep vein thrombosis. Intravenous users can use a various single dose range using a hypodermic needle. The dose of heroin used for recreational purposes is dependent on the frequency and level of use: thus a first-time user may use between 5 and 20 mg, while an established addict may require several hundred mg per day. As with the injection of any drug, if a group of users share a common needle without sterilization procedures, blood-borne diseases, such as HIV or hepatitis, can be transmitted.\nThe use of a common dispenser for water for the use in the preparation of the injection, as well as the sharing of spoons and/or filters can also cause the spread of blood-borne diseases. Many countries now supply small sterile spoons and filters for single use in order to prevent the spread of disease.\n\nSmoking heroin refers to vaporizing it to inhale the resulting fumes, not burning it to inhale the resulting smoke. It is commonly smoked in glass pipes made from glassblown Pyrex tubes and light bulbs. It can also be smoked off aluminium foil, which is heated underneath by a flame and the resulting smoke is inhaled through a tube of rolled up foil, This method is also known as \"chasing the dragon\" (whereas smoking methamphetamine is known as \"chasing the \"white\" dragon\").\n\nAnother popular route to intake heroin is insufflation (snorting), where a user crushes the heroin into a fine powder and then gently inhales it (sometimes with a straw or a rolled-up banknote, as with cocaine) into the nose, where heroin is absorbed through the soft tissue in the mucous membrane of the sinus cavity and straight into the bloodstream. This method of administration redirects first-pass metabolism, with a quicker onset and higher bioavailability than oral administration, though the duration of action is shortened. This method is sometimes preferred by users who do not want to prepare and administer heroin for injection or smoking, but still experience a fast onset. Snorting heroin becomes an often unwanted route, once a user begins to inject the drug. The user may still get high on the drug from snorting, and experience a nod, but will not get a rush. A \"rush\" is caused by a large amount of heroin entering the body at once. When the drug is taken in through the nose, the user does not get the rush because the drug is absorbed slowly rather than instantly.\n\nLittle research has been focused on the suppository (anal insertion) or pessary (vaginal insertion) methods of administration, also known as \"plugging\". These methods of administration are commonly carried out using an oral syringe. Heroin can be dissolved and withdrawn into an oral syringe which may then be lubricated and inserted into the anus or vagina before the plunger is pushed. The rectum or the vaginal canal is where the majority of the drug would likely be taken up, through the membranes lining their walls.\n\nLike most opioids, unadulterated heroin does not cause many long-term complications other than dependence and constipation. The purity of street heroin varies greatly. This variation has led to individuals inadvertently experiencing overdoses when the purity of the drug was higher than they expected. Intravenous use of heroin (and any other substance) with needles and syringes or other related equipment may lead to:\n\nMany countries and local governments have begun funding programs that supply sterile needles to people who inject illegal drugs in an attempt to reduce these contingent risks, and especially the spread of blood-borne diseases. The Drug Policy Alliance reports that up to 75% of new AIDS cases among women and children are directly or indirectly a consequence of drug use by injection. The United States federal government does not operate needle exchanges, although some state and local governments do support such programs.\n\nAnthropologists Philippe Bourgois and Jeff Schonberg performed a decade of fieldwork among homeless heroin and cocaine addicts in San Francisco, published in 2009. They reported that the African-American addicts they observed were more inclined to \"direct deposit\" heroin into a vein, while \"skin-popping\" was a far more widespread practice: \"By the midpoint of our fieldwork, most of the whites had given up searching for operable veins and skin-popped. They sank their needles perfunctorily, often through their clothing, into their fatty tissue.\" Bourgois and Schonberg describes how the cultural difference between the African-Americans and the whites leads to this contrasting behavior, and also points out that the two different ways to inject heroin comes with different health risks. Skin-popping more often results in abscesses, and direct injection more often leads to fatal overdose and also to hepatitis C and HIV infection.\n\nA small percentage of heroin smokers, and occasionally IV users, may develop symptoms of toxic leukoencephalopathy. The cause has yet to be identified, but one speculation is that the disorder is caused by an uncommon adulterant that is only active when heated. Symptoms include slurred speech and difficulty walking.\n\nCocaine is sometimes used in combination with heroin, and is referred to as a speedball when injected or \"moonrocks\" when smoked together. Cocaine acts as a stimulant, whereas heroin acts as a depressant. Coadministration provides an intense rush of euphoria with a high that combines both effects of the drugs, while excluding the negative effects, such as anxiety and sedation. The effects of cocaine wear off far more quickly than heroin, so if an overdose of heroin was used to compensate for cocaine, the end result is fatal respiratory depression.\n\nThe withdrawal syndrome from heroin (the so-called \"cold turkey\") may begin within 6–24 hours of discontinuation of the drug; however, this time frame can fluctuate with the degree of tolerance as well as the amount of the last consumed dose. Symptoms may include: sweating, malaise, anxiety, depression, akathisia, priapism, extra sensitivity of the genitals in females, general feeling of heaviness, excessive yawning or sneezing, tears, rhinorrhea, sleep difficulties (insomnia), cold sweats, chills, severe muscle and bone aches, nausea, vomiting, diarrhea, cramps, watery eyes, fever and cramp-like pains and involuntary spasms in the limbs (thought to be an origin of the term \"kicking the habit\").\n\nHeroin overdose is usually treated with an opioid antagonist, such as naloxone (Narcan), or naltrexone. This reverses the effects of heroin and other opioids and causes an immediate return of consciousness but may result in withdrawal symptoms. The half-life of naloxone is shorter than most opioids, so that it has to be administered multiple times until the opioid has been metabolized by the body.\n\nDepending on drug interactions and numerous other factors, death from overdose can take anywhere from several minutes to several hours. Death usually occurs due to lack of oxygen resulting from the lack of breathing caused by the opioid. Heroin overdoses can occur because of an unexpected increase in the dose or purity or because of diminished opioid tolerance. However, many fatalities reported as overdoses are probably caused by interactions with other depressant drugs such as alcohol or benzodiazepines. It should also be noted that since heroin can cause nausea and vomiting, a significant number of deaths attributed to heroin overdose are caused by aspiration of vomit by an unconscious person. Some sources quote the median lethal dose (for an average 75 kg opiate-naive individual) as being between 75 and 600  mg. Illicit heroin is of widely varying and unpredictable purity. This means that the user may prepare what they consider to be a moderate dose while actually taking far more than intended. Also, tolerance typically decreases after a period of abstinence. If this occurs and the user takes a dose comparable to their previous use, the user may experience drug effects that are much greater than expected, potentially resulting in an overdose. It has been speculated that an unknown portion of heroin-related deaths are the result of an overdose or allergic reaction to quinine, which may sometimes be used as a cutting agent.\n\nWhen taken orally, heroin undergoes extensive first-pass metabolism via deacetylation, making it a prodrug for the systemic delivery of morphine. When the drug is injected, however, it avoids this first-pass effect, very rapidly crossing the blood–brain barrier because of the presence of the acetyl groups, which render it much more fat soluble than morphine itself. Once in the brain, it then is deacetylated variously into the inactive 3-monoacetylmorphine and the active 6-monoacetylmorphine (6-MAM), and then to morphine, which bind to μ-opioid receptors, resulting in the drug's euphoric, analgesic (pain relief), and anxiolytic (anti-anxiety) effects; heroin itself exhibits relatively low affinity for the μ receptor. Unlike hydromorphone and oxymorphone, however, administered intravenously, heroin creates a larger histamine release, similar to morphine, resulting in the feeling of a greater subjective \"body high\" to some, but also instances of pruritus (itching) when they first start using.\n\nBoth morphine and 6-MAM are μ-opioid agonists that bind to receptors present throughout the brain, spinal cord, and gut of all mammals. The μ-opioid receptor also binds endogenous opioid peptides such as β-endorphin, Leu-enkephalin, and Met-enkephalin. Repeated use of heroin results in a number of physiological changes, including an increase in the production of μ-opioid receptors (upregulation). These physiological alterations lead to tolerance and dependence, so that stopping heroin use results in uncomfortable symptoms including pain, anxiety, muscle spasms, and insomnia called the opioid withdrawal syndrome. Depending on usage it has an onset 4–24 hours after the last dose of heroin. Morphine also binds to δ- and κ-opioid receptors.\n\nThere is also evidence that 6-MAM binds to a subtype of μ-opioid receptors that are also activated by the morphine metabolite morphine-6β-glucuronide but not morphine itself. The third subtype of third opioid type is the mu-3 receptor, which may be a commonality to other six-position monoesters of morphine. The contribution of these receptors to the overall pharmacology of heroin remains unknown.\n\nA subclass of morphine derivatives, namely the 3,6 esters of morphine, with similar effects and uses, includes the clinically used strong analgesics nicomorphine (Vilan), and dipropanoylmorphine; there is also the latter's dihydromorphine analogue, diacetyldihydromorphine (Paralaudin). Two other 3,6 diesters of morphine invented in 1874–75 along with diamorphine, dibenzoylmorphine and acetylpropionylmorphine, were made as substitutes after it was outlawed in 1925 and, therefore, sold as the first \"designer drugs\" until they were outlawed by the League of Nations in 1930.\n\nThe major metabolites of diamorphine, 6-MAM, morphine, morphine-3-glucuronide and morphine-6-glucuronide, may be quantitated in blood, plasma or urine to monitor for abuse, confirm a diagnosis of poisoning or assist in a medicolegal death investigation. Most commercial opiate screening tests cross-react appreciably with these metabolites, as well as with other biotransformation products likely to be present following usage of street-grade diamorphine such as 6-acetylcodeine and codeine. However, chromatographic techniques can easily distinguish and measure each of these substances. When interpreting the results of a test, it is important to consider the diamorphine usage history of the individual, since a chronic user can develop tolerance to doses that would incapacitate an opiate-naive individual, and the chronic user often has high baseline values of these metabolites in his system. Furthermore, some testing procedures employ a hydrolysis step before quantitation that converts many of the metabolic products to morphine, yielding a result that may be 2 times larger than with a method that examines each product individually.\n\nThe opium poppy was cultivated in lower Mesopotamia as long ago as 3400 BCE. The chemical analysis of opium in the 19th century revealed that most of its activity could be ascribed to two alkaloids, codeine and morphine.\n\nDiamorphine was first synthesized in 1874 by C. R. Alder Wright, an English chemist working at St. Mary's Hospital Medical School in London. He had been experimenting with combining morphine with various acids. He boiled anhydrous morphine alkaloid with acetic anhydride for several hours and produced a more potent, acetylated form of morphine, now called \"diacetylmorphine\" or \"morphine diacetate\". The compound was sent to F. M. Pierce of Owens College in Manchester for analysis. Pierce told Wright:\n\nWright's invention did not lead to any further developments, and diamorphine became popular only after it was independently re-synthesized 23 years later by another chemist, Felix Hoffmann. Hoffmann, working at Bayer pharmaceutical company in Elberfeld, Germany, was instructed by his supervisor Heinrich Dreser to acetylate morphine with the objective of producing codeine, a constituent of the opium poppy, pharmacologically similar to morphine but less potent and less addictive. Instead, the experiment produced an acetylated form of morphine one and a half to two times more potent than morphine itself. The head of Bayer's research department reputedly coined the drug's new name, \"heroin,\" based on the German \"heroisch\", which means \"heroic, strong\" (from the ancient Greek word \"heros, ήρως\"). Bayer scientists were not the first to make heroin, but their scientists discovered ways to make it, and Bayer led commercialization of heroin.\n\nFrom 1898 through to 1910, diamorphine was marketed under the trademark name Heroin as a non-addictive morphine substitute and cough suppressant. In the 11th edition of Encyclopædia Britannica (1910), the article on morphine states: \"In the cough of phthisis minute doses [of morphine] are of service, but in this particular disease morphine is frequently better replaced by codeine or by heroin, which checks irritable coughs without the narcotism following upon the administration of morphine.\"\n\nIn the U.S., the Harrison Narcotics Tax Act was passed in 1914 to control the sale and distribution of diacetylmorphine and other opioids, which allowed the drug to be prescribed and sold for medical purposes. In 1924, the United States Congress banned its sale, importation, or manufacture. It is now a Schedule I substance, which makes it illegal for non-medical use in signatory nations of the Single Convention on Narcotic Drugs treaty, including the United States.\n\nThe Health Committee of the League of Nations banned diacetylmorphine in 1925, although it took more than three years for this to be implemented. In the meantime, the first designer drugs, viz. 3,6 diesters and 6 monoesters of morphine and acetylated analogues of closely related drugs like hydromorphone and dihydromorphine, were produced in massive quantities to fill the worldwide demand for diacetylmorphine—this continued until 1930 when the Committee banned diacetylmorphine analogues with no therapeutic advantage over drugs already in use, the first major legislation of this type.\n\nLater, as with Aspirin, Bayer lost some of its trademark rights to heroin under the 1919 Treaty of Versailles following the German defeat in World War I.\n\nIn 1895, the German drug company Bayer marketed diacetylmorphine as an over-the-counter drug under the trademark name Heroin. The name was derived from the Greek word \"heros\" because of its perceived \"heroic\" effects upon a user. It was developed chiefly as a morphine substitute for cough suppressants that did not have morphine's addictive side-effects. Morphine at the time was a popular recreational drug, and Bayer wished to find a similar but non-addictive substitute to market. However, contrary to Bayer's advertising as a \"non-addictive morphine substitute,\" heroin would soon have one of the highest rates of addiction among its users.\n\n\"Diamorphine\" is the Recommended International Nonproprietary Name and British Approved Name. Other synonyms for heroin include: diacetylmorphine, and morphine diacetate. Heroin is also known by many street names including dope, H, smack, junk, horse, and brown, among others.\n\nIn Hong Kong, diamorphine is regulated under Schedule 1 of Hong Kong's Chapter 134 \"Dangerous Drugs Ordinance\". It is available by prescription. Anyone supplying diamorphine without a valid prescription can be fined $10,000 (HKD). The penalty for trafficking or manufacturing diamorphine is a $50,000 (HKD) fine and life imprisonment. Possession of diamorphine without a license from the Department of Health is illegal with a $10,000 (HKD) fine and/or 7 years of jail time.\n\nIn the Netherlands, diamorphine is a List I drug of the Opium Law. It is available for prescription under tight regulation exclusively to long-term addicts for whom methadone maintenance treatment has failed. It cannot be used to treat severe pain or other illnesses.\n\nIn the United Kingdom, diamorphine is available by prescription, though it is a restricted Class A drug. According to the 50th edition of the British National Formulary (BNF), diamorphine hydrochloride may be used in the treatment of acute pain, myocardial infarction, acute pulmonary oedema, and chronic pain. The treatment of chronic non-malignant pain must be supervised by a specialist. The BNF notes that all opioid analgesics cause dependence and tolerance but that this is \"no deterrent in the control of pain in terminal illness\". When used in the palliative care of cancer patients, diamorphine is often injected using a syringe driver.\n\nIt is controlled in the UK by the Misuse of Drugs Act 1971. In the UK it is a class A controlled drug and as such is subject to guidelines surrounding its storage, administration and destruction. Possession of diamorphine without a prescription is an arrestable offence. When diamorphine is prescribed in a hospital or similar environment, its administration must be supervised by two people who must then complete and sign a controlled drugs register (CD register) detailing the patient's name, amount, time, date and route of administration. In the case of a physician administering diamorphine, then he/she may administer the drug alone, however the rule requiring two registered practitioners, such as a nurse, midwife or another physician to sign the CD register still applies. The use of a witness when administering diamorphine is to avoid the possibility of the drug being diverted onto the black market.\n\nFor safety reasons, many UK National Health Service hospitals only permit the administration of intravenous diamorphine in designated areas. In practice this usually means a critical care unit, an accident and emergency department, operating theatres by an anaesthetist or nurse anaesthetist or other such areas where close monitoring and support from senior staff is immediately available. However, administration by other routes is permitted in other areas of the hospital. This includes subcutaneous, intramuscular, intravenously as part of a patient controlled analgesia setup, and as an already established epidural infusion pump. Subcutaneous infusion, along with subcutaneous and intramuscular injection (bolus administration), is often used in the patient's own home, in order to treat severe pain in terminal illness.\n\nIn Australia diamorphine is listed as a schedule 9 prohibited substance under the Poisons Standard (October 2015). A schedule 9 drug is outlined in the Poisons Act 1964 as \"Substances which may be abused or misused, the manufacture, possession, sale or use of which should be prohibited by law except when required for medical or scientific research, or for analytical, teaching or training purposes with approval of the CEO.\" \n\nIn Canada, diamorphine is a controlled substance under Schedule I of the Controlled Drugs and Substances Act (CDSA). Any person seeking or obtaining diamorphine without disclosing authorization 30 days before obtaining another prescription from a practitioner is guilty of an indictable offense and subject to imprisonment for a term not exceeding seven years. Possession of diamorphine for the purpose of trafficking is an indictable offense and subject to imprisonment for life.\n\nIn the United States, diamorphine is a Schedule I drug according to the Controlled Substances Act of 1970, making it illegal to possess without a DEA license. Possession of more than 100 grams of diamorphine or a mixture containing diamorphine is punishable with a minimum mandatory sentence of 5 years of imprisonment in a federal prison.\n\nDiamorphine is produced from acetylation of morphine derived from natural opium sources. Numerous mechanical and chemical means are used to purify the final product. The final products have a different appearance depending on purity and have different names.\n\nHeroin purity has been classified into four grades. No.4 is the purest form – white powder (salt) to be easily dissolved and injected. No.3 is \"brown sugar\" for smoking (base). No.1 and No.2 are unprocessed raw heroin (salt or base).\n\nTraffic is heavy worldwide, with the biggest producer being Afghanistan. According to a U.N. sponsored survey, in 2004, Afghanistan accounted for production of 87 percent of the world's diamorphine. Afghan opium kills around 100,000 people annually.\n\nIn 2003 \"The Independent\" reported:\nOpium production in that country has increased rapidly since, reaching an all-time high in 2006. War in Afghanistan once again appeared as a facilitator of the trade. Some 3.3 million Afghans are involved in producing opium.\n\nAt present, opium poppies are mostly grown in Afghanistan (), and in Southeast Asia, especially in the region known as the Golden Triangle straddling Burma (), Thailand, Vietnam, Laos () and Yunnan province in China. There is also cultivation of opium poppies in Pakistan (), Mexico () and in Colombia (). According to the DEA, the majority of the heroin consumed in the United States comes from Mexico (50%) and Colombia (43-45%) via Mexican criminal cartels such as Sinaloa Cartel. However, these statistics may be significantly unreliable, the DEA's 50/50 split between Colombia and Mexico is contradicted by the amount of hectares cultivated in each country and in 2014, the DEA claimed most of the heroin in the US came from Colombia.\n, the Sinaloa Cartel is the most active drug cartel involved in smuggling illicit drugs such as heroin into the United States and trafficking them throughout the United States. According to the Royal Canadian Mounted Police, 90% of the heroin seized in Canada (where the origin was known) came from Afghanistan. Pakistan is the destination and transit point for 40 percent of the opiates produced in Afghanistan, other destinations of Afghan opiates are Russia, Europe and Iran.\n\nConviction for trafficking heroin carries the death penalty in most Southeast Asian, some East Asian and Middle Eastern countries (see Use of death penalty worldwide for details), among which Malaysia, Singapore and Thailand are the most strict. The penalty applies even to citizens of countries where the penalty is not in place, sometimes causing controversy when foreign visitors are arrested for trafficking, for example the arrest of nine Australians in Bali, the death sentence given to Nola Blake in Thailand in 1987, or the hanging of an Australian citizen Van Tuong Nguyen in Singapore.\n\nThe origins of the present international illegal heroin trade can be traced back to laws passed in many countries in the early 1900s that closely regulated the production and sale of opium and its derivatives including heroin. At first, heroin flowed from countries where it was still legal into countries where it was no longer legal. By the mid-1920s, heroin production had been made illegal in many parts of the world. An illegal trade developed at that time between heroin labs in China (mostly in Shanghai and Tianjin) and other nations. The weakness of government in China and conditions of civil war enabled heroin production to take root there. Chinese triad gangs eventually came to play a major role in the illicit heroin trade. The French Connection route started in the 1930s.\n\nHeroin trafficking was virtually eliminated in the U.S. during World War II because of temporary trade disruptions caused by the war. Japan's war with China had cut the normal distribution routes for heroin and the war had generally disrupted the movement of opium.\n\nAfter World War II, the Mafia took advantage of the weakness of the postwar Italian government and set up heroin labs in Sicily. The Mafia took advantage of Sicily's location along the historic route opium took westward into Europe and the United States.\n\nLarge-scale international heroin production effectively ended in China with the victory of the communists in the civil war in the late 1940s. The elimination of Chinese production happened at the same time that Sicily's role in the trade developed.\n\nAlthough it remained legal in some countries until after World War II, health risks, addiction, and widespread recreational use led most western countries to declare heroin a controlled substance by the latter half of the 20th century.\n\nIn late 1960s and early 1970s, the CIA supported anti-Communist Chinese Nationalists settled near the Sino-Burmese border and Hmong tribesmen in Laos. This helped the development of the Golden Triangle opium production region, which supplied about one-third of heroin consumed in US after the 1973 American withdrawal from Vietnam. In 1999, Burma, the heartland of the Golden Triangle, was the second largest producer of heroin, after Afghanistan.\n\nThe Soviet-Afghan war led to increased production in the Pakistani-Afghan border regions, as U.S.-backed mujaheddin militants raised money for arms from selling opium, contributing heavily to the modern Golden Crescent creation. By 1980, 60 percent of heroin sold in the U.S. originated in Afghanistan. It increased international production of heroin at lower prices in the 1980s. The trade shifted away from Sicily in the late 1970s as various criminal organizations violently fought with each other over the trade. The fighting also led to a stepped-up government law enforcement presence in Sicily.\nFollowing the discovery at a Jordanian airport of a toner cartridge that had been modified into an improvised explosive device, the resultant increased level of airfreight scrutiny led to a major shortage (drought) of heroin from October 2010 until April 2011. This was reported in most of mainland Europe and the UK which led to a price increase of approximately 30 percent in the cost of street heroin and an increased demand for diverted methadone. The number of addicts seeking treatment also increased significantly during this period.\nOther heroin droughts (shortages) have been attributed to cartels restricting supply in order to force a price increase and also to a fungus that attacked the opium crop of 2009. Many people thought that the American government had introduced pathogens into the Afghanistan atmosphere in order to destroy the opium crop and thus starve insurgents of income.\n\nOn 13 March 2012, Haji Bagcho, with ties to the Taliban, was convicted by a U.S. District Court of conspiracy, distribution of heroin for importation into the United States and narco-terrorism. Based on heroin production statistics compiled by the United Nations Office on Drugs and Crime, in 2006, Bagcho's activities accounted for approximately 20 percent of the world's total production for that year.\n\nThe European Monitoring Centre for Drugs and Drug Addiction reports that the retail price of brown heroin varies from €14.5 per gram in Turkey to €110 per gram in Sweden, with most European countries reporting typical prices of €35–40 per gram. The price of white heroin is reported only by a few European countries and ranged between €27 and €110 per gram.\n\nThe United Nations Office on Drugs and Crime claims in its 2008 World Drug Report that typical US retail prices are US$172 per gram.\n\nHarm reduction is a public health philosophy that seeks to reduce the harms associated with the use of diamorphine. One aspect of harm reduction initiatives focuses on the behaviour of individual users. This includes promoting safer means of taking the drug, such as smoking, nasal use, oral or rectal insertion. This attempts to avoid the higher risks of overdose, infections and blood-borne viruses associated with injecting the drug. Other measures include using a small amount of the drug first to gauge the strength, and minimize the risks of overdose. For the same reason, poly drug use (the use of two or more drugs at the same time) is discouraged. Injecting diamorphine users are encouraged to use new needles, syringes, spoons/steri-cups and filters every time they inject and not share these with other users. Users are also encouraged to not use it on their own, as others can assist in the event of an overdose.\n\nGovernments that support a harm reduction approach usually fund needle and syringe exchange programs, which supply new needles and syringes on a confidential basis, as well as education on proper filtering before injection, safer injection techniques, safe disposal of used injecting gear and other equipment used when preparing diamorphine for injection may also be supplied including citric acid sachets/vitamin C sachets, steri-cups, filters, alcohol pre-injection swabs, sterile water ampules and tourniquets (to stop use of shoe laces or belts).\n\nAnother harm reduction measure employed for example in Europe, Canada and Australia are safe injection sites where users can inject diamorphine and cocaine under the supervision of medically trained staff. Safe injection sites are low threshold and allow social services to approach problem users that would otherwise be hard to reach.\nIn the UK the Criminal Justice System has a protocol in place that requires that any individual that is arrested and is suspected of having a substance misuse problem be offered the chance to enter a treatment program. This has had the effect of drastically reducing an area's crime rate as individuals arrested for theft in order to supply the funds for their drugs are no longer in the position of having to steal to purchase heroin because they have been placed onto a methadone program, quite often more quickly than would have been possible had they not been arrested. This aspect of harm reduction is seen as being beneficial to both the individual and the community at large, who are then protected from the possible theft of their goods.\n\nDuring the late 1980s and early 1990s, Swiss authorities ran the ZIPP-AIDS (Zurich Intervention Pilot Project), handing out free syringes in the officially tolerated drug scene in Platzspitz park. In 1994, Zurich started a pilot project using prescription heroin in heroin-assisted treatment (HAT) which allowed users to obtain heroin and inject it under medical supervision. The HAT program proved to be cost-beneficial to society and improve patients overall health and social stability and has since been introduced in multiple European countries.\n\nHeroin is mentioned in hundreds of films. Sometimes the use or trafficking of the drug is the central theme of the film but many times it is almost incidental as part of a crime in a police drama, for example.\n\nUse of heroin by jazz musicians in particular was prevalent in the mid-twentieth century, including Billie Holiday, sax legends Charlie Parker and Art Pepper, guitarist Joe Pass and piano player/singer Ray Charles; a \"staggering number of jazz musicians were addicts\". It was also a problem with many rock musicians, particularly from the late 1960s through the 1990s. Pete Doherty is also a self-confessed user of heroin. Nirvana frontman Kurt Cobain's heroin addiction was well documented. Pantera frontman, Phil Anselmo, turned to heroin while touring during the 1990s to cope with his back pain.\n\nHeroin is mentioned explicitly in a number of rock songs:\n\nResearchers are attempting to reproduce the biosynthetic pathway that produces morphine in genetically engineered yeast. In June 2015 the \"S\"-reticuline could be produced from sugar and \"R\"-reticuline could be converted to morphine, but the intermediate reaction could not be performed.\n\n\n", "id": "14034", "title": "Heroin"}
{"url": "https://en.wikipedia.org/wiki?curid=14035", "text": "Hellas Verona F.C.\n\nHellas Verona Football Club (commonly known simply as Verona, or Hellas within the city of Verona itself) is a professional Italian football club, based in Verona, Veneto. The team won the Serie A Championship in 1984–85, and currently plays in Serie B.\n\nFounded in 1903 by a group of high school students, the club was named \"Hellas\" (the Greek word for Greece), at the request of a professor of classics. At a time in which football was played seriously only in the larger cities of the northwest of Italy, most of Verona was indifferent to the growing sport. However, when in 1906 two city teams chose the city's Roman amphitheatre as a venue to showcase the game, crowd enthusiasm and media interest began to rise.\n\nDuring these first few years, Hellas was one of three or four area teams playing mainly at a municipal level while fighting against city rivals Bentegodi to become the city's premier football outfit. By the 1907–08 season, Hellas was playing against regional teams and an intense rivalry with Vicenza that lasts to this day was born.\n\nFrom 1898 to 1926, Italian football was organised into regional groups. In this period, Hellas was one of the founding teams of the early league and often among its top final contenders. In 1911, the city helped Hellas replace the early, gritty football fields with a proper venue. This allowed the team to take part in its first regional tournament, which until 1926, was the qualifying stage for the national title.\n\nIn 1919, following a return to activity after a four-year suspension of all football competition in Italy during World War I, the team merged with city rival Verona and changed its name to Hellas Verona. Between 1926 and 1929, the elite \"\"Campionato Nazionale\"\" assimilated the top sides from the various regional groups and Hellas Verona joined the privileged teams, yet struggled to remain competitive.\n\nSerie A, as it is structured today, began in 1929, when the \"Campionato Nazionale\" turned into a professional league. Still an amateur team, Hellas merged with two city rivals, Bentegodi and Scaligera, to form AC Verona. Hoping to build a first class contender for future years the new team debuted in Serie B in 1929. It would take the \"gialloblu\" 28 years to finally achieve their goal. After first being promoted to Serie A for one season in 1957–58, in 1959, the team merged with another city rival (called Hellas) and commemorated its beginnings by changing its name to Hellas Verona AC.\n\nCoached by Nils Liedholm, the team returned to Serie A in 1968 and remained in the elite league almost without interruption until 1990. Along the way, it scored a famous 5–3 win in the 1972–73 season that cost Milan the \"scudetto\" (the Serie A title). The fact that the result came late during the last matchday of the season makes the sudden and unexpected end to the \"rossoneri\"'s title ambitions all the more memorable.\n\nIn 1973–74, Hellas finished the season in fourth-last, just narrowly avoiding relegation, but were nonetheless sent down to Serie B during the summer months as a result of a scandal involving team president Saverio Garonzi. After a year in Serie B, Hellas returned to Serie A.\n\nIn the 1975–76 season, the team had a successful run in the Coppa Italia, eliminating highly rated teams such as Torino, Cagliari and Internazionale from the tournament. However, in their first ever final in the competition, Hellas were trounced 4–0 by Napoli.\n\nUnder the leadership of coach Osvaldo Bagnoli, in 1982–83 the team secured a fourth-place in Serie A (its highest finish at the time) and even led the Serie A standings for a few weeks. The same season Hellas again reached the Coppa Italia final. After a 2–0 home victory, Hellas then travelled to Turin to play Juventus but were defeated 3–0 after extra time.\n\nFurther disappointment followed in the 1983–84 season when the team again reached the Coppa Italia final, only to lose the Cup in the final minutes of the return match against defending Serie A champions Roma\n\nThe team made its first European appearance in the 1983-84 UEFA Cup and were knocked out in the second round of the tournament by Sturm Graz. Hellas were eliminated from the 1985–86 European Cup in the second round by defending champions and fellow Serie A side Juventus after a contested game, the result of a scandalous arbitrage by the French Wurtz, having beaten PAOK of Greece in the first round.\n\nIn 1988, the team had their best international result when they reached the UEFA Cup quarterfinals with four victories and three draws. The decisive defeat came from German side Werder Bremen.\n\nAlthough the 1984–85 season squad was made up of a mix of emerging players and mature stars, at the beginning of the season no one would have regarded the team as having the necessary ingredients to make it to the end. Certainly, the additions of Hans-Peter Briegel in midfield and of Danish striker Preben Elkjær to an attack that already featured the wing play of Pietro Fanna, the creative abilities of Antonio Di Gennaro and the scoring touch of Giuseppe Galderisi were to prove crucial.\n\nTo mention a few of the memorable milestones on the road to the \"scudetto\": a decisive win against Juventus (2–0), with a goal scored by Elkjær after having lost a boot in a tackle just outside the box, set the stage early in the championship; an away win over Udinese (5–3) ended any speculation that the team was losing energy at the midway point; three straight wins (including a hard fought 1–0 victory against a strong Roma side) served notice that the team had kept its polish and focus intact during their rival's final surge; and a 1–1 draw in Bergamo against Atalanta secured the title with a game in hand.\n\nHellas finished the year with a 15–13–2 record and 43 points, four points ahead of Torino with Internazionale and Sampdoria rounding out the top four spots. This unusual final table of the Serie A (with the most successful Italian teams of the time, Juventus and Roma, ending up much lower than expected) has led to many speculations. The 1984–85 season was the only season when referees were assigned to matches by way of a random draw. Before then each referee had always been assigned to a specific match by a special commission of referees (\"designatori arbitrali\"). After the betting scandal of the early 1980 (the Calcio Scommesse scandal), it was decided to clean up the image of Italian football by assigning referees randomly instead of picking them, to clear up all the suspicions and accusations always accompanying Italy's football life. This resulted in a quieter championship and in a completely unexpected final table.\n\nIn the following season, won again by Juventus, the choice of the referees went back in the hands of the \"designatori arbitrali\". In 2006, a major scandal in Italian football revealed that certain clubs had been illegally influencing the referee selection process in an attempt to ensure that certain referees were assigned to their matches.\n\nThese were more than mere modest achievements for a mid-size city with a limited appeal to fans across the nation. But soon enough financial difficulties caught up with team managers. In 1991 the team folded and was reborn as Verona, regularly moving to and fro between Serie A and Serie B for several seasons. In 1995 the name was officially changed back to Hellas Verona.\n\nAfter a three-year stay, their last stint in Serie A ended in grief in 2002. That season emerging international talents such as Adrian Mutu, Mauro Camoranesi, Alberto Gilardino, Martin Laursen, Massimo Oddo, Marco Cassetti and coach Alberto Malesani failed to capitalise on an excellent start and eventually dropped into fourth-to-last place for the first time all season on the very last matchday, enforcing relegation into Serie B.\n\nFollowing the 2002 relegation to Serie B, team fortunes continued to slip throughout the decade. In the 2003–04 season Hellas Verona struggled in Serie B and spent most of the season fighting off an unthinkable relegation to Serie C1. Undeterred, the fans supported their team and a string of late season wins eventually warded off the danger. Over 5,000 of them followed Hellas to Como on the final day of the season to celebrate.\n\nIn 2004–05, things looked much brighter for the team. After a rocky start, Hellas put together a string of results and climbed to third spot. The \"gialloblù\" held on to the position until January 2005, when transfers weakened the team, yet they managed to take the battle for Serie A to the last day of the season.\n\nThe 2006–07 Serie B seemed to start well, due to the club takeover by Pietro Arvedi D'Emilei, which ended nine years of controversial leadership under chairman Gianbattista Pastorello, heavily contested by the supporters in his later years at Verona. However, Verona was immediately involved in the relegation battle, and Massimo Ficcadenti was replaced in December 2006 by Giampiero Ventura. Despite a recovery in the results, Verona ended in an 18th place, thus being forced to play a two-legged playoff against 19th-placed Spezia to avert relegation. A 2–1 away loss in the first leg at La Spezia was followed by a 0–0 home tie, and Verona were relegated to Serie C1 after 64 years of play in the two highest divisions.\n\nVerona appointed experienced coach Franco Colomba for the new season with the aim to return to Serie B as soon as possible. However, despite being widely considered the division favourite, the \"gialloblù\" spent almost the entire season in last place. After seven matches, club management sacked Colomba in early October and replaced him with youth team coach (and former Verona player) Davide Pellegrini. A new property acquired the club in late 2007, appointing in December Giovanni Galli as new director of football and Maurizio Sarri as new head coach. Halfway through the 2007–08 season, the team remained at the bottom of Serie C1, on the brink of relegation to the fourth level (Serie C2). In response, club management sacked Sarri and brought back Pellegrini. Thanks to a late-season surge the \"scaligeri\" avoided direct relegation by qualifying for the relegation play-off, and narrowly averted dropping to Lega Pro Seconda Divisione in the final game, beating Pro Patria 2–1 on aggregate. However, despite the decline in results, attendance and season ticket sales remained on 15,000 average.\n\nFor the 2008–09 season, Verona appointed former Sassuolo and Piacenza manager Gian Marco Remondina with the aim to win promotion to Serie B. However, the season did not start impressively, with Verona being out of the playoff zone by mid-season, and club chairman Pietro Arvedi D'Emilei entering into a coma after being involved in a car crash on his way back from a league match in December 2008. Arvedi died in March 2009, two months after the club was bought by new chairman Giovanni Martinelli.\n\nThe following season looked promising, as new transfer players were brought aboard, and fans enthusiastically embraced the new campaign. Season ticket figures climbed to over 10,000, placing Verona ahead of several Serie A teams and all but Torino in Serie B attendance. The team led the standings for much of the season, accumulating a seven-point lead by early in the spring. However, the advantage was gradually squandered, and the team dropped to second place on the second-last day of the season, with a chance to regain first place in the final regular season match against Portogruaro on home soil. Verona, however, disappointed a crowd of over 25,000 fans and, with the loss, dropped to third place and headed towards the play-offs. A managerial change for the post-season saw the firing of Remondina and the arrival of Giovanni Vavassori. After eliminating Rimini in the semi-finals (1–0; 0–0) Verona lost the final to Pescara (2–2 on home soil and 0–1 in the return match) and were condemned to a fourth-straight year of third division football.\n\nFormer 1990 World Cup star Giuseppe Giannini (a famous captain of Roma for many years) signed as manager for the 2010–11 campaign. Once again, the team was almost entirely revamped during the transfer season. The squad struggled in the early months and Giannini was eventually sacked and replaced by former Internazionale defender Andrea Mandorlini, who succeeded in reorganising the team's play and bringing discipline both on and off the pitch. In the second half of the season, Verona climbed back from the bottom of the division to clinch a play-off berth (fifth place) on the last day of the regular season. The team advanced to the play-off final after eliminating Sorrento in the semi-finals 3–1 on aggregate. Following the play-off final, after four years of Lega Pro football, Verona were promoted back to Serie B after a 2–1 aggregate win over Salernitana on 19 June 2011.\n\nOn 18 May 2013, Verona finished second in Serie B and were promoted to Serie A after an 11-year absence. Their return to the top flight began against title contenders Milan and Roma, beating the former 2–1 and losing to the latter 3–0. The team continued at a steady pace, finishing the first half of the season with 32 points and sitting in sixth place—11 points behind the closest UEFA Champions League spot—and tied with Internazionale for the final UEFA Europa League spot. Verona, however, ultimately finished the year in tenth.\n\nDuring the 2015–16 season, Verona hadn't won a single match since the beginning of the campaign until the club edged Atalanta 2–1 on 3 February 2016 in a win at home; coming twenty-three games into the season. Consequently, Verona were relegated from Serie A.\n\nThe team's colours are yellow and blue and \"gialloblu\" (literally, \"yellow-blue\" in Italian) is the team's most widely used nickname. The colours represent the city itself and Verona's emblem (a yellow cross on a blue shield) appears on most team apparel. Two more team nicknames are \"Mastini\" (the mastiffs) and \"Scaligeri\", both references to Mastino I della Scala of the Della Scala princes that ruled the city during the 13th and 14th centuries.\n\nThe Scala family coat of arms is depicted on the team's jersey and on its trademark logo as a stylised image of two large, powerful mastiffs facing opposite directions. In essence, the term \"\"scaligeri\"\" is synonymous with Veronese, and therefore can describe anything or anyone from Verona (e.g., ChievoVerona, a different team that also links itself to the Scala family – specifically to Cangrande I della Scala).\n\nSince 1963, the club have played at the Stadio Marc'Antonio Bentegodi, which has a capacity of 39,211. The ground is shared with Hellas' rivals, ChievoVerona. It was used as a venue for the 1990 FIFA World Cup.\n\nThe intercity fixtures against Chievo Verona are known as the \"Derby della Scala.\" The name refers to the Scaligeri or della Scala aristocratic family, who were rulers of Verona during the Middle Ages and early Renaissance. In the season 2001–02, both Hellas Verona and the city rivals of Chievo Verona were playing in the Serie A. The first ever derby of Verona in Serie A took place on 18 November 2001, while both teams were ranked among the top four. The match was won by Hellas, 3–2. Chievo got revenge in the return match in spring 2002, winning 2–1. Verona became so the fifth city in Italy, after Milan, Rome, Turin and Genoa to host a cross-city derby in Serie A.\n\n\n\n\n\n", "id": "14035", "title": "Hellas Verona F.C."}
{"url": "https://en.wikipedia.org/wiki?curid=14036", "text": "Hinayana\n\nHīnayāna is a Sanskrit term literally meaning the \"smaller vehicle\", applied to the \"Śrāvakayāna\", the Buddhist path followed by a śrāvaka who wishes to become an arhat. The term appeared around the first or second century. Hīnayāna is often contrasted with \"Mahāyāna\", which means the \"great vehicle\".\n\nThe term was widely used in the past by Western scholars to cover \"the earliest system of Buddhist doctrine\", as the \"Monier-Williams Sanskrit-English Dictionary\" put it. However, Buddhist scholarship uses the term \"Nikaya Buddhism\" to refer to early Buddhist schools. \"Hinayana\" has also been used as a synonym for Theravada, which is the main tradition of Buddhism in Sri Lanka and Southeast Asia; this is considered inaccurate and derogatory. Robert Thurman writes, \"'Nikaya Buddhism' is a coinage of Professor Masatoshi Nagatomi of Harvard University, who suggested it to me as a usage for the eighteen schools of Indian Buddhism to avoid the term 'Hinayana Buddhism,' which is found offensive by some members of the Theravada tradition.\" In 1950 the World Fellowship of Buddhists declared that the term Hīnayana should not be used when referring to any form of Buddhism existing today.\n\nWithin Mahayana Buddhism, there are a variety of interpretations as to who or what the term \"Hinayana\" refers. Kalu Rinpoche stated the \"lesser\" or \"greater\" designation \"does not refer to economic or social status, but concerns the spiritual capacities of the practitioner\".\nThe Chinese monk Yijing, who visited India in the 7th century, distinguished Mahāyāna from Hīnayāna as follows:\nThe word \"hīnayāna\" is formed of \"hīna\": \"little\", \"poor\", \"inferior\", \"abandoned\", \"deficient\", \"defective\"; and \"yāna\" (यान): \"vehicle\", where \"vehicle\" means \"a way of going to enlightenment\". The Pali Text Society's \"Pali-English Dictionary\" (1921–25) defines \"hīna\" in even stronger terms, with a semantic field that includes \"poor, miserable; vile, base, abject, contemptible\", and \"despicable\".\n\nThe term was translated by Kumārajīva and others into Classical Chinese as \"small vehicle\" (小 meaning \"small\", 乘 meaning \"vehicle\"), although earlier and more accurate translations of the term also exist. In Mongolian (\"Baga Holgon\") the term for hinayana also means \"small\" or \"lesser\" vehicle, while in Tibetan there are at least two words to designate the term, \"theg chung\" meaning \"small vehicle\" and \"theg dman\" meaning \"inferior vehicle\" or \"inferior spiritual approach\".\n\nThrangu Rinpoche has emphasized that \"hinayana\" is in no way implying \"inferior\". In his translation and commentary of Asanga's \"Distinguishing Dharma from Dharmata\", he writes, \"all three traditions of hinayana, mahayana, and vajrayana were practiced in Tibet and that the hinayana which literally means \"lesser vehicle\" is in no way inferior to the mahayana.\"\n\nAccording to Jan Nattier, it is most likely that the term Hīnayāna postdates the term Mahāyāna and was only added at a later date due to antagonism and conflict between the bodhisattva and śrāvaka ideals. The sequence of terms then began with the term \"Bodhisattvayāna\" \"bodhisattva-vehicle\", which was given the epithet Mahāyāna \"Great Vehicle\". It was only later, after attitudes toward the bodhisattva teachings had become more critical, that the term Hīnayāna was created as a back-formation, contrasting with the already established term Mahāyāna. The earliest Mahāyāna texts often use the term Mahāyāna as an epithet and synonym for Bodhisattvayāna, but the term Hīnayāna is comparatively rare in early texts, and is usually not found at all in the earliest translations. Therefore, the often-perceived symmetry between Mahāyāna and Hīnayāna can be deceptive, as the terms were not actually coined in relation to one another in the same era.\n\nAccording to Paul Williams, \"the deep-rooted misconception concerning an unfailing, ubiquitous fierce criticism of the Lesser Vehicle by the [Mahāyāna] is not supported by our texts.\" Williams states that while evidence of conflict is present in some cases, there is also substantial evidence demonstrating peaceful coexistence between the two traditions.\n\nAlthough the 18–20 early Buddhist schools are sometimes loosely classified as Hīnayāna in modern times, this is not necessarily accurate. There is no evidence that Mahāyāna ever referred to a separate formal school of Buddhism but rather as a certain set of ideals, and later doctrines. Paul Williams has also noted that the Mahāyāna never had nor ever attempted to have a separate vinaya or ordination lineage from the early Buddhist schools, and therefore bhikṣus and bhikṣuṇīs adhering to the Mahāyāna formally adheres to the vinaya of an early school. This continues today with the Dharmaguptaka ordination lineage in East Asia and the Mūlasarvāstivāda ordination lineage in Tibetan Buddhism. Mahāyāna was never a separate sect of the early schools. From Chinese monks visiting India, we now know that both Mahāyāna and non-Mahāyāna monks in India often lived in the same monasteries side by side.\n\nThe Chinese Buddhist monk and pilgrim Yijing wrote about the relationship between the various \"vehicles\" and the early Buddhist schools in India. He wrote, \"There exist in the West numerous subdivisions of the schools which have different origins, but there are only four principal schools of continuous tradition.\" These schools are the Mahāsāṃghika Nikāya, Sthavira nikāya, Mūlasarvāstivāda Nikāya, and Saṃmitīya Nikāya. Explaining their doctrinal affiliations, he then writes, \"Which of the four schools should be grouped with the Mahāyāna or with the Hīnayāna is not determined.\" That is to say, there was no simple correspondence between a Buddhist school and whether its members learn \"Hīnayāna\" or \"Mahāyāna\" teachings.\n\nTo identify entire schools as \"Hīnayāna\" that contained not only śrāvakas and pratyekabuddhas but also Mahāyāna bodhisattvas would be attacking the schools of their fellow Mahāyānists as well as their own. Instead, what is demonstrated in the definition of \"Hīnayāna\" given by Yijing is that the term referred to individuals based on doctrinal differences.\n\nScholar Isabelle Onians asserts that although \"the Mahāyāna ... very occasionally referred to earlier Buddhism as the Hinayāna, the Inferior Way, [...] the preponderance of this name in the secondary literature is far out of proportion to occurrences in the Indian texts.\" She notes that the term Śrāvakayāna was \"the more politically correct and much more usual\" term used by Mahāyānists. Jonathan Silk has argued that the term \"Hinayana\" was used to refer to whomever one wanted to criticize on any given occasion, and did not refer to any definite grouping of Buddhists.\n\nIn the 7th century, the Chinese Buddhist monk Xuanzang describes the concurrent existence of the Mahāvihara and the Abhayagiri vihāra in Sri Lanka. He refers to the monks of the Mahāvihara as the \"Hīnayāna Sthaviras\" and the monks of Abhayagiri vihāra as the \"Mahāyāna Sthaviras\". Xuanzang further writes, \"The Mahāvihāravāsins reject the Mahāyāna and practice the Hīnayāna, while the Abhayagirivihāravāsins study both Hīnayāna and Mahāyāna teachings and propagate the \"Tripiṭaka\".\"\n\nMahayanists were primarily in philosophical dialectic with the Vaibhāṣika school of Sarvāstivāda, which had by far the most \"comprehensive edifice of doctrinal systematics\" of the nikāya schools. With this in mind it is sometimes argued that the Theravada would not have been considered a \"Hinayana\" school by Mahayanists because, unlike the now-extinct Sarvastivada school, the primary object of Mahayana criticism, the Theravada school does not claim the existence of independent dharmas; in this it maintains the attitude of early Buddhism. Additionally, the concept of the bodhisattva as one who puts off enlightenment rather than reaching awakening as soon as possible, has no roots in Theravada textual or cultural contexts, current or historical. Aside from the Theravada schools being geographically distant from the Mahayana, the Hinayana distinction is used in reference to certain views and practices that had become found within the Mahayana tradition itself. Theravada, as well as Mahayana schools stress the urgency of one's own awakening in order to end suffering. Some contemporary Theravadin figures have thus indicated a sympathetic stance toward the Mahayana philosophy found in the \"Heart Sutra\" and the \"Mūlamadhyamakakārikā\".\n\nThe Mahayanists were bothered by the substantialist thought of the Sarvāstivādins and Sautrāntikins, and in emphasizing the doctrine of śūnyatā, David Kalupahana holds that they endeavored to preserve the early teaching. The Theravadins too refuted the Sarvāstivādins and Sautrāntikins (and followers of other schools) on the grounds that their theories were in conflict with the non-substantialism of the canon. The Theravada arguments are preserved in the \"Kathavatthu\".\n\nMost western scholars regard the Theravada school to be one of the Hinayana schools referred to in Mahayana literature, or regard Hinayana as a synonym for Theravada. These scholars understand the term to refer to schools of Buddhism that did not accept the teachings of the Mahāyāna sūtras as authentic teachings of the Buddha. At the same time, scholars have objected to the pejorative connotation of the term Hinayana and some scholars do not use it for any school.\n\n\n", "id": "14036", "title": "Hinayana"}
{"url": "https://en.wikipedia.org/wiki?curid=14045", "text": "Humphrey Bogart\n\nHumphrey DeForest Bogart (; December 25, 1899January 14, 1957) was an American screen and stage actor whose performances in 1940s films noir such as \"The Maltese Falcon\", \"Casablanca\", and \"The Big Sleep\" earned him status as a cultural icon.\n\nBogart began acting in 1921 after a hitch in the U.S. Navy in World War I and little success in various jobs in finance and the production side of the theater. Gradually he became a regular in Broadway shows in the 1920s and 1930s. When the stock market crash of 1929 reduced the demand for plays, Bogart turned to film. His first great success was as Duke Mantee in \"The Petrified Forest\" (1936), and this led to a period of typecasting as a gangster with films such as \"Angels with Dirty Faces\" (1938) and B-movies like \"The Return of Doctor X\" (1939).\n\nBogart's breakthrough as a leading man came in 1941 with \"High Sierra\" and \"The Maltese Falcon.\" The next year, his performance in \"Casablanca\" (1943; Oscar nomination) raised him to the peak of his profession and, at the same time, cemented his trademark film persona, that of the hard-boiled cynic who ultimately shows his noble side. Other successes followed, including \"To Have and Have Not\" (1944), \"The Big Sleep\" (1946), \"Dark Passage\" (1947), and \"Key Largo\" (1948), all four with his wife Lauren Bacall; \"The Treasure of the Sierra Madre\" (1948); \"In a Lonely Place\" (1950); \"The African Queen\" (1951; Oscar winner); \"Sabrina\" (1954); \"The Caine Mutiny\" (1954; Oscar nomination); and \"We're No Angels\" (1955). His last film was \"The Harder They Fall\" (1956).\n\nDuring a film career of almost 30 years, Bogart appeared in more than 75 feature films. In 1999, the American Film Institute ranked Bogart as the greatest male star of Classic American cinema. Over his career, he received three Academy Award nominations for Best Actor, winning one (for \"The African Queen\").\n\nBogart was born on Christmas Day, 1899, in New York City, the eldest child of Belmont DeForest Bogart (1867 – 1934) and Maud Humphrey (1868 – 1940). Belmont was the only child of the unhappy marriage of Adam Watkins Bogart, a Canandaigua, New York innkeeper, and his wife, Julia, a wealthy heiress. He attended Phillips Academy Andover from 1917-1918 before being expelled. The name \"Bogart\" comes from the Dutch surname \"Bogaert\". Belmont and Maud married in June 1898; he was Presbyterian of English and Dutch descent, she an Episcopalian of English heritage, also a descendant of \"Mayflower\" passenger John Howland. Young Humphrey was raised in the Episcopal faith, but was non-practicing for most of his adult life.\n\nThe precise date of Bogart's birth was long a matter of dispute, but has been cleared up. Warner Bros listed his birthdate as Christmas Day, 1899, throughout his career; but film historian Clifford McCarty later maintained that the Warner publicity department had altered it from January 23, 1900 \"...to foster the view that a man born on Christmas Day couldn't really be as villainous as he appeared to be on screen\". The \"corrected\" January birthdate subsequently appeared—and in some cases, remains—in many otherwise authoritative sources. Biographers A.M. Sperber and Eric Lax documented, however, that Bogart always celebrated his birthday on December 25, and consistently listed it as such on official records, such as his marriage license.\nLauren Bacall confirmed in her autobiography that his birthday was always celebrated on Christmas Day, adding that he joked that he was cheated out of a present every year because of it. Sperber and Lax also noted that a birth announcement, printed in the \"Ontario County Times\" on January 10, 1900, effectively rules out the possibility of a January 23 birthdate; and state and federal census records from 1900 report a Christmas 1899 birthdate as well.\n\nBogart's father, Belmont, was a cardiopulmonary surgeon. His mother, Maud, was a commercial illustrator who received her art training in New York and France, including study with James McNeill Whistler. Later she became art director of the fashion magazine \"The Delineator\" and a militant suffragette. She used a drawing of baby Humphrey in a well-known advertising campaign for Mellins Baby Food. In her prime, she made over $50,000 a year, then a vast sum and far more than her husband's $20,000. The Bogarts lived in a fashionable Upper West Side apartment, and had an elegant cottage on a 55-acre estate on Canandaigua Lake in upstate New York. As a youngster, Humphrey's gang of friends at the lake would put on theatricals.\n\nHumphrey had two younger sisters, Frances (\"Pat\") and Catherine Elizabeth (\"Kay\"). His parents were busy in their careers and frequently fought. Very formal, they showed little emotion towards their children. Maud told her offspring to call her \"Maud\" not \"Mother\", and showed little if any physical affection for them. When pleased she \"[c]lapped you on the shoulder, almost the way a man does\", Bogart recalled. \"I was brought up very unsentimentally but very straightforwardly. A kiss, in our family, was an event. Our mother and father didn't glug over my two sisters and me.\"\n\nAs a boy, Bogart was teased for his curls, tidiness, the \"cute\" pictures his mother had him pose for, the Little Lord Fauntleroy clothes she dressed him in, and even for the name \"Humphrey\". From his father, Bogart inherited a tendency to needle, fondness for fishing, lifelong love of boating, and an attraction to strong-willed women.\n\nBogart attended the private Delancey School until fifth grade, then the prestigious Trinity School. He was an indifferent, sullen student who showed no interest in after-school activities. Later he went to the equally elite boarding school Phillips Academy, where he was admitted based on family connections. His parents hoped he would go on to Yale, but in 1918 Bogart was expelled. Several reasons have been given: one claims that it was for throwing the headmaster (or a groundskeeper) into Rabbit Pond on campus.\n\nAnother cites smoking, drinking, poor academic performance, and possibly some inappropriate comments made to the staff. A third has him withdrawn by his father for failing to improve his grades. Whatever caused his premature departure, his parents were deeply dismayed and rued their failed plans for his future.\n\nWith no viable career options, Bogart followed his passion for the sea and enlisted in the United States Navy in the spring of 1918. He recalled later, \"At eighteen, war was great stuff. Paris! Sexy French girls! Hot damn!\" Bogart is recorded as a model sailor who spent most of his sea time after the Armistice ferrying troops back from Europe.\n\nIt was during his naval stint that Bogart may have received his trademark scar and developed his characteristic lisp, though the actual circumstances are unclear. In one account his lip was cut by shrapnel when his ship, the \"\", was shelled, although some claim Bogart did not make it to sea until after the Armistice had been signed. Another version, which Bogart's long-time friend, author Nathaniel Benchley, holds to, is that Bogart was injured while taking a prisoner to Portsmouth Naval Prison in Kittery, Maine.\n\nChanging trains in Boston the handcuffed prisoner allegedly asked Bogart for a cigarette, then while Bogart looked for a match, the prisoner smashed him across the mouth with the cuffs, cutting Bogart's lip and fleeing. Recaptured, the prisoner was taken to jail. An alternate version has Bogart struck in the mouth by a handcuff loosened while freeing his charge, the other still around the prisoner's wrist.\n\nBy the time Bogart was treated by a doctor, a scar had already formed. David Niven said that when he first asked Bogart about his scar, he said it was caused by a childhood accident. \"Goddamn doctor\", Bogart later told Niven, \"instead of stitching it up, he screwed it up.\" Niven claims the stories that Bogart got the scar during wartime were made up by the studios to inject glamour.\n\nHis post-service physical makes no mention of the lip scar, even though it mentions many smaller scars. When actress Louise Brooks met Bogart in 1924, he had some scar-tissue on his upper lip, which Brooks said that Bogart may have had partially repaired before entering films in 1930. She believed his scar had nothing to do with his distinctive speech pattern, and said his \"lip wound gave him no speech impediment, either before or after it was mended. Over the years, Bogart practiced all kinds of lip gymnastics, accompanied by nasal tones, snarls, lisps and slurs. His painful wince, his leer, his fiendish grin were the most accomplished ever seen on film.\"\n\nBogart returned home to find his father suffering from poor health, his medical practice faltering, and much of the family's wealth lost on bad investments in timber. During his naval days, Bogart's character and values developed independently of family influence, and he began to rebel somewhat against their values. He came to be a liberal who hated pretensions, phonies, and snobs, and at times defied conventional behavior and authority, traits he displayed in both life and the movies. He did not, however, forsake good manners, articulateness, punctuality, modesty, and a dislike of being touched. After his naval service, he worked as a shipper and then bond salesman. He joined the Naval Reserve.\n\nBogart resumed his friendship with boyhood pal Bill Brady, Jr., whose father had show business connections. Eventually Bogart got an office job working for William A. Brady Sr.'s new company, World Films. Bogart was able to try his hand at screenwriting, directing, and production, but excelled at none. For a while he was stage manager for Brady's daughter Alice's play \"A Ruined Lady\". A few months later he made his stage debut as a Japanese butler in Alice's 1921 play \"Drifting\", nervously speaking one line of dialog. Several appearances followed in her subsequent plays.\n\nWhile Bogart had been raised to believe that acting was beneath a gentleman, he liked the late hours actors kept and enjoyed the attention gotten on stage. He stated, \"I was born to be indolent and this was the softest of rackets.\" He spent a lot of his free time in speakeasies and became a heavy drinker. A barroom brawl during this time joins the list of purported causes of Bogart's lip damage, and coincides better with the Brooks account.\n\nPreferring to learn as he went, Bogart never took acting lessons. He was persistent and worked steadily at his craft, appearing in at least seventeen Broadway productions between 1922 and 1935. He played juveniles or romantic second-leads in drawing room comedies, and is said to have been the first actor to ask \"Tennis, anyone?\" on stage. Critic Alexander Woollcott wrote of Bogart's early work that he \"is what is usually and mercifully described as inadequate.\" Some reviews were kinder.\n\nHeywood Broun, reviewing \"Nerves\" wrote, \"Humphrey Bogart gives the most effective performance ... both dry and fresh, if that be possible\". He played juvenile lead, reporter Gregory Brown, in the comedy \"Meet the Wife\", written by Lynn Starling, which had a successful run of 232 performances at the Klaw Theatre from November 1923 through July 1924. Bogart loathed these trivial, effeminate parts he had to play early in his career, calling them \"White Pants Willie\" roles.\n\nEarly in his career, while playing double roles in the play \"Drifting\" at the Playhouse Theatre in 1922, Bogart met actress Helen Menken. They were married on May 20, 1926, at the Gramercy Park Hotel in New York City. Divorced on November 18, 1927, they remained friends. On April 3, 1928, he married Mary Philips, whom he'd met when they appeared in the play \"Nerves\" during its very brief run at the Comedy Theatre in September 1924, at her mother's apartment in Hartford, Connecticut. She, like Menken, had a fiery temper, and, like every other Bogart spouse, was an actress.\n\nAfter the stock market crash of 1929, stage production dropped off sharply, and many of the more photogenic actors headed for Hollywood. Bogart's film debut was with Helen Hayes in the 1928 two-reeler \"The Dancing Town\", of which a complete copy has never been found. He also appeared with Joan Blondell and Ruth Etting in a Vitaphone short, \"Broadway's Like That\" (1930) which was re-discovered in 1963.\n\nBogart then signed a contract with Fox Film Corporation for $750 a week. There he met Spencer Tracy, a serious Broadway actor whom Bogart liked and admired, and they became close friends and drinking companions. It was Tracy, in 1930, who first called him \"Bogie\". Tracy made his film debut in the only film in which he and Bogart appeared together, John Ford's early sound film \"Up the River\" (1930). Both had major roles as inmates. Tracy received top billing and Bogart's face was featured on the film's posters instead of Tracy's.\n\nBogart then had a minor supporting role in \"Bad Sister\" with Bette Davis in 1931. Decades later, Tracy and Bogart planned to make \"The Desperate Hours\" together, but both sought top billing, so Tracy dropped out and was replaced by Fredric March.\n\nBogart shuttled back and forth between Hollywood and the New York stage from 1930 to 1935, suffering long periods without work. His parents had separated, his father dying in 1934 in debt, which Bogart eventually paid off. Bogart inherited his father's gold ring which he always wore, even in many of his films. At his father's deathbed, Bogart finally told him how much he loved him. His second marriage was on the rocks, and he was less than happy with his acting career. He became depressed, irritable, and drank heavily.\n\nBogart starred in the Broadway play \"Invitation to a Murder\" at the Theatre Masque, now the John Golden Theatre, in 1934. The producer Arthur Hopkins heard the play from off-stage and sent for Bogart to play escaped murderer Duke Mantee in Robert E. Sherwood's new play, \"The Petrified Forest\". Hopkins recalled:\n\nThe play had 197 performances at the Broadhurst Theatre in New York in 1935. Leslie Howard, though, was the star. \"New York Times\" critic Brooks Atkinson said of the play, \"a peach ... a roaring Western melodrama ... Humphrey Bogart does the best work of his career as an actor.\" Bogart said the play \"marked my deliverance from the ranks of the sleek, sybaritic, stiff-shirted, swallow-tailed 'smoothies' to which I seemed condemned to life.\" However, he was still feeling insecure.\n\nWarner Bros. bought the screen rights to \"The Petrified Forest\". The play seemed perfect for the studio, which was famous for its socially realistic, urban, low-budget action pictures, especially for a public entranced by real-life criminals like John Dillinger (whom Bogart resembled) and Dutch Schultz. Bette Davis and Leslie Howard were cast. Howard, who held production rights, made it clear he wanted Bogart to star with him.\n\nThe studio tested several Hollywood veterans for the Duke Mantee role, and chose Edward G. Robinson, who had first-rank star appeal and was due to make a film to fulfill his expensive contract. Bogart cabled news of this to Howard in Scotland, who replied: \"Att: Jack Warner Insist Bogart Play Mantee No Bogart No Deal L.H.\". When Warner Bros. saw Howard would not budge, they gave in and cast Bogart. Jack Warner, famous for butting heads with his stars, tried to get Bogart to adopt a stage name, but Bogart stubbornly refused.\n\nThe film was highly successful, earning $500,000 at the box office, and making Bogart a star. He never forgot Howard's favor, and in 1952 named his only daughter \"Leslie Howard Bogart\" after Howard, who had died in World War II under mysterious circumstances. Robert E. Sherwood remained a close friend of Bogart's.\n\nThe film version of \"The Petrified Forest\" was released in 1936. Bogart's performance was called \"brilliant\", \"compelling\", and \"superb.\" Despite his success in an \"A movie,\" Bogart received a tepid twenty-six week contract at $550 per week and was typecast as a gangster in a series of \"B movie\" crime dramas. Bogart was proud of his success, but the fact that it came from playing a gangster weighed on him. He once said: \"I can't get in a mild discussion without turning it into an argument. There must be something in my tone of voice, or this arrogant face—something that antagonizes everybody. Nobody likes me on sight. I suppose that's why I'm cast as the heavy.\"\n\nBogart's roles were not only repetitive, but physically demanding and draining (studios were not yet air-conditioned), and his regimented, tightly scheduled job at Warners was anything but the indolent and \"peachy\" actor's life he hoped for. However, he was always professional and generally respected by other actors. He used these \"B movie\" years to start developing his enduring film persona—the wounded, stoical, cynical, charming, vulnerable, self-mocking loner with a code of honor.\n\nIn spite of his success, Warner Bros. had no interest in making Bogart a top star. Shooting on a new movie might begin days or only hours after the previous one wrapped. The studio system, then at its most entrenched, restricted actors to their home lot, with only occasional loan-outs. Any actor who refused a role could be suspended without pay. Bogart disliked the roles chosen for him, but he worked steadily. Between 1936 and 1940 he averaged a movie every two months, at times working on two simultaneously.\n\nAmenities at Warners were few compared to the prestigious Metro-Goldwyn-Mayer. Bogart thought that the Warners wardrobe department was cheap, and often wore his own suits in his movies. In \"High Sierra\", Bogart used his own pet dog Zero to play his character's dog, Pard. Bogart's disputes with Warner Bros. over roles and money were similar to those the studio waged with other high-spirited, less-than-obedient stars such as Bette Davis, James Cagney, Errol Flynn, and Olivia de Havilland.\n\nThe leading men ahead of Bogart at Warner Bros. included not only such marquee names as James Cagney and Edward G. Robinson, but also journeymen leads such as Victor McLaglen, George Raft, and Paul Muni. Most of the studio's better movie scripts went to them, leaving Bogart with what was left. He made films like \"Racket Busters\", \"San Quentin\", and \"You Can't Get Away with Murder\". The only substantial leading role he got during this period was in \"Dead End\" (1937), while loaned to Samuel Goldwyn, where he portrayed a gangster modeled after Baby Face Nelson.\n\nBogart played violent roles so often that in Nevil Shute's 1939 novel \"What Happened to the Corbetts\" the protagonist, when asked whether he knows how to operate an automatic weapon, jokes \"I've seen Humphrey Bogart with one often enough ...\". He did play a variety of interesting supporting roles, such as in \"Angels with Dirty Faces\" (1938) (in which his character got shot by James Cagney's). Bogart was gunned down on film repeatedly by Cagney and Edward G. Robinson, among others. In \"Black Legion\" (1937), for a change, he played a good man caught up and destroyed by a racist organization, a movie Graham Greene described as \"intelligent and exciting, if rather earnest\".\n\nIn 1938, Warner Bros. put Bogart in a \"hillbilly musical\" called \"Swing Your Lady\" as a wrestling promoter; he later apparently considered this his worst film performance. In 1939, Bogart played a mad scientist in \"The Return of Doctor X\",his only horror film. He cracked, \"If it'd been Jack Warner's blood ... I wouldn't have minded so much. The trouble was they were drinking mine and I was making this stinking movie.\" During this time his wife Mary had a stage hit in \"A Touch of Brimstone\" (1935), and refused to give up her Broadway career to go to Hollywood. After the play closed she relented, but insisted on continuing her career and the couple divorced in 1937.\nOn August 21, 1938, Bogart entered into a disastrous third marriage, with actress Mayo Methot, a lively, friendly woman when sober but paranoid and physical when drunk. She became convinced Bogart was cheating on her. The more the two drifted apart, the more she drank, in her fury throwing plants, crockery, anything close at hand, at him. She set their house on fire, stabbed him with a knife, and slashed her wrists on several occasions. Bogart for his part needled her mercilessly and seemed to enjoy confrontation. Sometimes he turned violent. The press accurately dubbed them \"the Battling Bogarts.\"\n\n\"The Bogart-Methot marriage was the sequel to the Civil War,\" said their friend Julius Epstein. A wag observed that there was \"madness in his Methot.\" During this time, Bogart bought a motor launch, which he named \"Sluggy,\" his nickname for hot-tempered Methot. Despite his proclamations that, \"I like a jealous wife,\" \"We get on so well together (because) we don't have illusions about each other,\" and, \"I wouldn't give you two cents for a dame without a temper,\" it was a highly destructive relationship.\n\nBogart had a lifelong disgust for the pretentious, fake or phony. Sensitive yet caustic, he was once again disgusted by the inferior movies he was performing in. He rarely saw his own films and avoided premieres. He even issued phony press releases about his private life to satisfy the curiosity of newspapers and the public. When he thought an actor, director, or a movie studio had done something shoddy, he spoke up about it and was willing to be quoted. He advised Robert Mitchum that the only way to stay alive in Hollywood was to be an \"againster.\" As a result, he was not the most popular of actors, and some in the Hollywood community shunned him privately to avoid trouble with the studios. But the Hollywood press, unaccustomed to candor, was delighted. Bogart once said:\n\n\"High Sierra\", a 1941 film directed by Raoul Walsh, had a screenplay written by Bogart's friend and drinking partner, John Huston, adapted from the novel by W. R. Burnett (\"Little Caesar\", etc.). Both Paul Muni and George Raft turned down the lead role, giving Bogart the opportunity to play a character of some depth, although legendary director Walsh initially fought the casting of supporting player Bogart as a leading man, much preferring Raft for the part. The film was Bogart's last major film playing a gangster (only a supporting role in 1942's \"The Big Shot\" followed). Bogart worked well with Ida Lupino, and her relationship with him was close, provoking jealousy from Bogart's wife, Mayo.\n\nThe film cemented a strong personal and professional connection between Bogart and Huston. Bogart admired and somewhat envied Huston for his skill as a writer. Though a poor student, Bogart was a lifelong reader. He could quote Plato, Pope, Ralph Waldo Emerson, and over a thousand lines of Shakespeare. He subscribed to the \"Harvard Law Review\". He admired writers, and some of his best friends were screenwriters, including Louis Bromfield, Nathaniel Benchley, and Nunnally Johnson. Bogart enjoyed intense, provocative conversation and stiff drinks, as did Huston. Both were rebellious and liked to play childish pranks. Huston was reported to be easily bored during production, and admired Bogart (also bored easily off camera) not just for his acting talent but for his intense concentration on the set.\n\nNow regarded as a classic film noir, the \"The Maltese Falcon\" (1941) was John Huston's directorial debut. Originally a novel written by Dashiell Hammett, it was first published in the pulp magazine \"Black Mask\" in 1929, and had also served as the basis of two other movie versions including \"Satan Met a Lady\" (1936) starring Bette Davis. Producer Hal Wallis initially offered the leading man role to George Raft, a more established box office name than Bogart whose contract stipulated he did not have to appear in remakes. Fearing it would be no more than a cleaned-up version of the pre-Production Code \"The Maltese Falcon\" (1931), Raft turned it down in order to make \"Manpower\" with director Raoul Walsh and cast members Edward G. Robinson and Marlene Dietrich. Eagerly, Huston accepted Bogart as his Sam Spade.\n\nComplementing Bogart were co-stars Sydney Greenstreet, Peter Lorre, Elisha Cook, Jr., and Mary Astor as the treacherous female foil. Bogart's sharp timing and facial expressions were praised by the cast and director as vital to the quick action and rapid-fire dialogue. The film was a huge hit in theaters and a major triumph for Huston. Bogart was unusually happy with it, remarking, \"it is practically a masterpiece. I don't have many things I'm proud of ... but that's one\".\n\nBogart gained his first real romantic lead in 1942's \"Casablanca\", playing Rick Blaine, a hard-pressed expatriate nightclub owner hiding from a shady past while negotiating a fine line among Nazis, the French underground, the Vichy prefect and unresolved feelings for his ex-girlfriend. The film was directed by Michael Curtiz and produced by Hal Wallis, and featured Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre and Dooley Wilson. An avid chess player, Bogart reportedly had the idea that Rick Blaine be portrayed as one, a metaphor for the sparring relationship he maintained with friends, enemies, and tenuous allies. In real life Bogart played tournament level chess one division below master, often enjoying games with crew members and cast, but finding his better in the superior Paul Henreid.\n\nThe on-screen magic of Bogart and Bergman was the result of two actors working at their best, not any real-life sparks, though Bogart's perennially jealous wife assumed otherwise. Off the set, the co-stars hardly spoke. Bergman, who had a reputation for affairs with her leading men, later said of Bogart, \"I kissed him but I never knew him.\" Because Bergman was taller, Bogart had blocks attached to his shoes in certain scenes.\n\n\"Casablanca\" won the 1943 Academy Award for Best Picture. Bogart was nominated for Best Actor in a Leading Role, but lost to Paul Lukas for his performance in \"Watch on the Rhine\". The film vaulted Bogart from fourth place to first in the studio's roster, finally overtaking James Cagney. By 1946 he'd more than doubled his annual salary to over $460,000, making him the highest-paid actor in the world.\n\nDuring part of 1943 and 1944, Bogart went on USO and War Bond tours accompanied by Methot, enduring arduous travels to Italy and North Africa, including Casablanca.\n\nBogart met Lauren Bacall while filming \"To Have and Have Not\" (1944), a loose adaptation of the Ernest Hemingway novel. The movie has many similarities with \"Casablanca\"—the same enemies, the same kind of hero, even a piano player sidekick (played by Hoagy Carmichael). When they met, Bacall was 19 and Bogart 44. He nicknamed her \"Baby.\" She had been a model since 16 and had acted in two failed plays. Bogart was drawn to Bacall's high cheekbones, green eyes, tawny blond hair, and lean body, as well as her poise and earthy, outspoken honesty. Reportedly he said, \"I just saw your test. We'll have a lot of fun together\". Their physical and emotional rapport was very strong from the start, their age difference and disparity in acting experience allowing the dynamic of a mentor-student relationship to emerge. Quite contrary to Hollywood norm, their affair was Bogart's first with a leading lady. He was still married and his early meetings with Bacall were discreet and brief, their separations bridged by ardent love letters. The relationship made it much easier for the newcomer to make her first film, and Bogart did his best to put her at ease with jokes and quiet coaching. He let her steal scenes and even encouraged it. Howard Hawks, for his part, also did his best to boost her performance and highlight her role, and found Bogart easy to direct.\n\nHawks at some point began to disapprove of the pair. He considered himself Bacall's protector and mentor, and Bogart was usurping that role. Married, and not usually drawn to his starlets, he too fell for Bacall, telling her she meant nothing to Bogart and even threatening to send her to Monogram, the worst studio in Hollywood. Bogart calmed her down and then went after Hawks. Jack Warner settled the dispute and filming resumed. Hawks said of Bacall: \"Bogie fell in love with the character she played, so she had to keep playing it the rest of her life.\"\n\nJust months after wrapping the film, Bogart and Bacall were reunited for an encore, the film noir \"The Big Sleep\", based on the novel by Raymond Chandler, again with script help from William Faulkner. Chandler thoroughly admired Bogart's performance: \"Bogart can be tough without a gun. Also, he has a sense of humor that contains that grating undertone of contempt.\" The film was completed and slated for release in 1945, then withdrawn and substantially re-edited to add new, juiced-up scenes exploiting both the box office chemistry that shone between Bogart and Bacall in \"To Have and Have Not\", and the notoriety of their personal relationship.\n\nAt director Howard Hawks' urging production partner Charles K. Feldman agreed to Bacall's scenes being re-written to heighten the 'insolent' quality that had intrigued critics and audiences in that film. By chance, a 35-mm nitrate composite master positive (fine grain) of the 1945 version survived. The UCLA Film Archive, in association with Turner Entertainment and with funding provided by Hugh Hefner, restored and released it in 1996.\n\nThroughout filming Bogart was still torn between his new love and his sense of duty to his marriage. The mood on the set was tense, the actors both emotionally exhausted as Bogart tried to find a way out of his dilemma. The dialogue, especially in the newly shot scenes, was full of sexual innuendo supplied by Hawks, and Bogart proves convincing and enduring as private detective Philip Marlowe. In the end, the film was successful, though some critics found the plot confusing and overcomplicated. Reportedly a bemused Chandler himself could not answer baffled screenwriters' question over who killed the limousine driver early in the story.\n\nBogart filed for divorce from Methot in February 1945. He and Bacall married in a small ceremony at the country home of Bogart's close friend, Pulitzer Prize-winning author Louis Bromfield, at Malabar Farm near Lucas, Ohio, on May 21, 1945.\n\nBogart and Bacall moved into a $160,000 ($ today) white brick mansion in an exclusive neighborhood in Los Angeles's Holmby Hills. The marriage proved a happy one, though there were tensions due to their differences. Bogart's drinking sometimes inflamed tensions. He was a homebody and she liked nightlife; he loved the sea, which made her seasick.\n\nIn California in 1945, Bogart bought a sailing yacht, the \"Santana\", from actor Dick Powell. The sea was his sanctuary, spending about thirty weekends a year on the water, with a particular fondness for sailing around Catalina Island. He once said, \"An actor needs something to stabilize his personality, something to nail down what he really is, not what he is currently pretending to be.\" He also joined the Coast Guard Temporary Reserve offering the use of his own yacht, Santana, for Coast Guard use. It was rumored Bogart attempted to enlist but was turned down because of his age.\n\nThe suspenseful \"Dark Passage\" (1947) was Bogart and Bacall's next pairing.\nIts first third is shot from the Bogart's character's point of view, with the camera seeing what he sees. After his plastic surgery, the rest of the movie is shot normally, with Bogart intent on finding the real murderer in a crime he was blamed for and sentenced to prison.\n\nThe couple next starred in the future classic, \"Key Largo\". Directed by John Huston, the film highlighted Edward G. Robinson as gangster \"Johnny Rocco,\" a seething older synthesis of many of his vicious early bad guy roles. The characters are trapped during a spectacular hurricane in a hotel owned by Bacall's screen father-in-law, played by Lionel Barrymore. Claire Trevor won an Academy Award for Best Supporting Actress for her heart-wrenching performance as Rocco's physically abused alcoholic girlfriend. Though Robinson had always had top billing over Bogart in their previous films together, this time Robinson's name appears to the right of Bogart's, but placed a little higher on the posters and in the film's opening credits, to signify Robinson's near-equal status. Robinson's image was also markedly larger and centered on the original poster, with Bogart relegated to the background.\n\nIn the film's trailer, Bogart is repeatedly mentioned first, but Robinson's name is listed above Bogart's in a cast list at the trailer's end. Robinson's role is evocative of Duke Mantee in \"The Petrified Forest\" (1936), a Bogart leading man breakthrough the studio had originally earmarked for Robinson.\n\nBogart became a first-time father at age 49 when Bacall gave birth to Stephen Humphrey Bogart on January 6, 1949, during the filming of \"Tokyo Joe\". The name was drawn from Bogart's character's nickname in \"To Have and Have Not\", \"Steve\". Stephen would go on to become an author and biographer, later hosting a television special about his father on Turner Classic Movies. Three years later the couple's daughter, Leslie Howard Bogart, would draw her name from Bogart's friend and \"The Petrified Forest\" co-star, British actor Leslie Howard.\n\nThe enormous success of \"Casablanca\" redefined Bogart's career. For the first time, Bogart could be cast successfully as both a tough, strong man and vulnerable love interest. Despite his elevated standing, he did not yet have a contractual right of script refusal. When he got weak scripts he simply dug in his heels and locked horns again with the front office, as he did on the film \"Conflict\" (1945). Though he submitted to Jack Warner on it, he successfully turned down \"God is My Co-Pilot\" (1945).\n\nRiding high in 1947 with a new contract which provided limited script refusal and the right to form his own production company, Bogart reunited with John Huston for \"The Treasure of the Sierra Madre\", a stark tale of greed played out by three gold prospectors in Mexico. Without either a love interest or happy ending it was deemed a risky project. Bogart later said of co-star (and John Huston's father) Walter Huston, \"He's probably the only performer in Hollywood to whom I'd gladly lost a scene\".\n\nThe film was shot in the heat of summer for greater realism and atmosphere, proving grueling to make. James Agee wrote, \"Bogart does a wonderful job with this character ... miles ahead of the very good work he has done before\". John Huston won the Academy Award for direction and screenplay and his father won Best Supporting Actor, but the film had mediocre box office results. Bogart complained, \"An intelligent script, beautifully directed—something different—and the public turned a cold shoulder on it\".\n\nBogart, a liberal Democrat, organized a delegation to Washington, D.C., called the Committee for the First Amendment, against what he perceived to be the House Un-American Activities Committee's harassment of Hollywood screenwriters and actors. He subsequently wrote an article \"I'm No Communist\" in the March 1948 edition of \"Photoplay\" magazine in which he distanced himself from The Hollywood Ten to counter the negative publicity resulting from his appearance. Bogart wrote: \"The ten men cited for contempt by the House Un-American Activities Committee were not defended by us.\"\n\nIn addition to being offered better, more diverse roles, Bogart started his own production company in 1948, Santana Productions, named after his sailing yacht (which also lent her name to the cabin cruiser featured in the climax of that year's smash, \"Key Largo\"). Earning the right to create his own production company had left Warner Bros. head Jack Warner furious, and afraid other stars would do the same and further erode the major studios' power. In addition to the pressure they were bearing from freelancing actors like Bogart, James Stewart, Henry Fonda and others, they were beginning to buckle from the eroding impact of television and enforcement of anti-trust laws breaking up theater chains. Bogart performed in his final films for Warners, \"Chain Lightning\", released early in 1950, and \"The Enforcer\", early in 1951.\n\nBogart's Santana Productions released its films through Columbia Pictures. Without letting up, Bogart starred in \"Knock on Any Door\" (1949), \"Tokyo Joe\" (1949), \"In a Lonely Place\" (1950), \"Sirocco\" (1951) and \"Beat the Devil\" (1954). Santana made two other films without him: \"And Baby Makes Three\" (1949) and \"The Family Secret\" (1951).\n\nWhile the majority lost money at the box office, ultimately forcing Santana's sale, at least two are well remembered today: \"In a Lonely Place\" is considered by many a high point in film noir. Bogart plays embittered writer Dixon Steele, whose history of violence lands him as top suspect in a murder case. At the same time he falls in love with an alluring but failed actress played by Gloria Grahame. It is considered among his best performances, and many Bogart biographers and actress/writer Louise Brooks feel the role is the closest to the real Bogart of any he played. She wrote that the film \"gave him a role that he could play with complexity, because the film character's pride in his art, his selfishness, drunkenness, lack of energy stabbed with lightning strokes of violence were shared by the real Bogart\". The character even mimics some of Bogart's personal habits, including twice ordering Bogart's favorite meal of ham and eggs.\n\nSomething of a parody of \"The Maltese Falcon\", \"Beat the Devil\" (1953), was Bogart's last film with his close friend and favorite director John Huston. Co-written by Truman Capote, the eccentrically filmed tale follows an amoral group of rogues chasing an unattainable treasure.\n\nBogart sold his interest in Santana to Columbia for over $1 million in 1955.\n\nWorking outside of his own Santana Productions, Bogart starred with Katharine Hepburn in the John Huston directed \"The African Queen\" in 1951. The C.S. Forester novel on which it was based was overlooked and left undeveloped for fifteen years until producer Sam Spiegel and Huston bought the rights. Spiegel sent Katharine Hepburn the book and she suggested Bogart for the male lead, firmly believing that \"he was the only man who could have played that part\". Huston's love of adventure, deep, longstanding friendship–and success–with Bogart, and a chance to work with Hepburn, convinced the actor to leave the comfortable confines of Hollywood for a difficult shoot on location in the Belgian Congo in Africa. Bogart was to get 30 percent of the profits and Hepburn 10 percent, plus a relatively small salary for both. The stars met up in London and announced the happy prospect of working together.\n\nBacall came for the four-month-plus duration, leaving their young child to be cared for in L.A. The Bogarts started the trip with a junket through Europe, including a visit with Pope Pius XII. Later, the glamor would be gone and Bacall would make herself useful as a cook, nurse and clothes washer, earning her husband's praise: \"I don't know what we'd have done without her. She Luxed my undies in darkest Africa\". Just about everyone in the cast came down with dysentery except Bogart and Huston, who subsisted on canned food and alcohol. Bogart explained: \"All I ate was baked beans, canned asparagus and Scotch whisky. Whenever a fly bit Huston or me, it dropped dead.\" Hepburn, a teetotaler in and out of character, fared worse in the difficult conditions, losing weight and at one point falling very ill. Bogart resisted Huston's insistence on using real leeches in a key scene where Charlie has to drag his tramp steamer through an infested marsh, until reasonable fakes were employed. In the end, the crew overcame illness, soldier ant invasions, leaking boats, poor food, attacking hippos, poor water filters, fierce heat, isolation, and a boat fire to complete a memorable film. Despite the discomfort of jumping from the boat into swamps, rivers and marshes the film apparently rekindled Bogart's early love of boats. On his return to California he bought a classic mahogany Hacker-Craft runabout, which he kept until his death.\n\nThe role of cantankerous skipper Charlie Allnutt won Bogart his only Academy Award in three nominations, for Best Actor in a Leading Role in 1951. Bogart considered his performance to be the best of his film career. He had vowed to friends that if he won, his speech would break the convention of thanking everyone in sight. He advised Claire Trevor, when she had been nominated for \"Key Largo\", to \"just say you did it all yourself and don't thank anyone\". But when Bogart won the Academy Award, which he truly coveted despite his well-advertised disdain for Hollywood, he said \"It's a long way from the Belgian Congo to the stage of this theatre. It's nicer to be here. Thank you very much ... No one does it alone. As in tennis, you need a good opponent or partner to bring out the best in you. John and Katie helped me to be where I am now\". Despite the thrilling win and the recognition, Bogart later commented, \"The way to survive an Oscar is never to try to win another one ... too many stars ... win it and then figure they have to top themselves ... they become afraid to take chances. The result: A lot of dull performances in dull pictures\".\n\n\"The African Queen\" was the first Technicolor film in which Bogart appeared. He appeared in relatively few color films of any kind during the rest of his career, which continued for another five years.\n\nJust three years after his Best Actor triumph in \"African Queen\", Bogart dropped his asking price to get the role of Captain Queeg in Edward Dmytryk's 1954 drama \"The Caine Mutiny\". Though he griped with some of his old bitterness about having to do so, he delivered a strong performance in the lead, earning him his final Oscar nomination as well as being the subject of the cover story in the June 7, 1954 issue of TIME. Yet for all his success, Bogart was still his melancholy old self, grumbling and feuding with the studio, while his health was beginning to deteriorate. The character of Queeg mirrored in some ways those Bogart had played in \"The Maltese Falcon\", \"Casablanca\" and \"The Big Sleep\"–the wary loner who trusts no one—but without either the warmth or humor of those roles. Like his portrayal of Fred C. Dobbs in \"The Treasure of the Sierra Madre\", Bogart played a paranoid, self-pitying character whose small-mindedness eventually destroyed him. Three months before the film's release, Bogart appeared as Queeg on the cover of \"TIME\" magazine, while on Broadway Henry Fonda was starring in the stage version (in a different role), both of which generated strong publicity for the film.\n\nIn \"Sabrina\", Billy Wilder wished to cast Cary Grant as the older male lead. Unable, he chose Bogart to play the elder, conservative brother who competes with his younger playboy sibling (William Holden) for the affection of the Cinderella-like Sabrina (Audrey Hepburn). Bogart was lukewarm about the part, but agreed to it on a handshake with Wilder, sans finished script but with the director's assurances he would take good care of Bogart during the filming. Nevertheless, Bogart got on poorly with his director and co-stars. He complained about the script and its last-minute drafting and delivery, and accused Wilder of favoring Hepburn and Holden on and off the set. At the root was Wilder being the opposite of Bogart's ideal director, John Huston, in both style and personality. Bogart groused to the press that Wilder was \"overbearing\" and \"is the kind of Prussian German with a riding crop. He is the type of director I don't like to work with ... the picture is a crock of crap. I got sick and tired of who gets Sabrina.\" Wilder later claimed, \"We parted as enemies but finally made up.\" Despite the acrimony, the film was successful. \"The New York Times\" crowed that Bogart was \"incredibly adroit ... the skill with which this old rock-ribbed actor blends the gags and such duplicities with a manly manner of melting is one of the incalculable joys of the show.\"\n\n\"The Barefoot Contessa\", directed by Joseph Mankiewicz, was filmed in Rome, and released in 1954. In this Hollywood back-story Bogart is again a broken-down man, the cynical director-narrator who saves his career by making a star of a flamenco dancer modeled on real life movie sex goddess Rita Hayworth. Bogart was uneasy with Ava Gardner in the female lead, as she had just split from close \"Rat Pack\" buddy Frank Sinatra and was carrying on an affair with bullfighter Luis Miguel Dominguín. Bogart told her, \"Half the world's female population would throw themselves at Frank's feet and here you are flouncing around with guys who wear capes and little ballerina slippers.\" He was also annoyed by her inexperienced performance. Later, Gardner credited Bogart with helping her both on and offscreen. Bogart's performance was generally praised as the strongest part of the film. During the filming, while Bacall was home, Bogart resumed his discreet affair with Verita Peterson, his long-time studio assistant, whom he took sailing and enjoyed drinking with. When his wife suddenly arrived on the scene discovering them together, she took it quite well, extracting an expensive shopping spree from her husband, the three traveling together after the shooting.\n\nBogart could be generous with actors, particularly those who were blacklisted, down on their luck, or having personal problems. During the filming of the Edward Dmytryk directed \"The Left Hand of God\" (1955), he noticed his co-star Gene Tierney having a hard time remembering her lines and behaving oddly. He coached Tierney, feeding her lines. He was familiar with mental illness from his sister's bouts of depression, and encouraged Tierney to seek treatment. He also stood behind Joan Bennett and insisted on her as his co-star in Michael Curtiz's \"We're No Angels\" when an ugly public scandal made her persona non grata with Jack Warner.\n\nBogart rounded out 1955 with \"The Desperate Hours\", directed by William Wyler. Mark Robson's \"The Harder They Fall\" (1956) was his last film.\n\nWhile Bogart rarely performed on television, he and Bacall appeared on Edward R. Murrow's \"Person to Person\" in which they disagreed in answering every question. Bogart was also featured on \"The Jack Benny Show\". The surviving kinescope of the live telecast captures him in his only TV sketch comedy outing.\n\nBogart and Bacall also worked together on an early color telecast in 1955, an NBC adaptation of \"The Petrified Forest\" for \"Producers' Showcase\", with Bogart receiving top billing and Henry Fonda playing Leslie Howard's role; a black and white kinescope of the live telecast has also survived.\n\nBogart performed radio adaptations of some of his best known films, such as \"Casablanca\" and \"The Maltese Falcon\". He also recorded a radio series called \"Bold Venture\" with Lauren Bacall.\n\nIn 1995 newly developed digital technology allowed Bogart's image to be inserted in the \"Tales from the Crypt\" television episode \"You, Murderer\" as one of its many \"Casablanca\" references. The \"Ingrid Bergman\" character was played by her daughter Isabella Rossellini.\n\nBogart was a founding member and original leader of the so-called Hollywood Rat Pack. In the spring of 1955, after a long party in Las Vegas attended by Frank Sinatra, Judy Garland, her husband Sid Luft, Mike Romanoff and wife Gloria, David Niven, Angie Dickinson and others, Lauren Bacall surveyed the wreckage and declared, \"You look like a goddamn rat pack.\"\n\nThe name stuck and was made official at Romanoff's in Beverly Hills. Sinatra was tabbed Pack Leader, Bacall Den Mother, Bogie Director of Public Relations, and Sid Luft Acting Cage Manager. When asked by columnist Earl Wilson what the group's purpose was, Bacall stated: \"To drink a lot of bourbon and stay up late.\"\n\nOnce, after signing a long-term deal with Warner Bros., Bogart had predicted with glee that his teeth and hair would fall out before the contract ended. By the mid-1950s, though he was well established as an independent producer, the sometime actor's health was failing. In the wake of Santana Productions he had formed a new company and had anxious plans for a film \"Melville Goodwin, U.S.A.\", in which he would play a general and Bacall a press magnate. However, his persistent cough and difficulty eating became too serious to ignore and he dropped the project.\n\nBogart, a heavy smoker and drinker, had developed cancer of the esophagus. He almost never spoke of his failing health and refused to see a doctor until January 1956. A diagnosis was made several weeks later, but by then removal of his esophagus, two lymph nodes, and a rib on March 1, 1956, was too late to halt the disease, even with chemotherapy. He underwent corrective surgery in November 1956 after the cancer had spread. With time, he grew too weak to walk up and down stairs, valiantly fighting the pain yet still able to joke: \"Put me in the dumbwaiter and I'll ride down to the first floor in style.\" It was then altered to accommodate his wheelchair. Frank Sinatra was a frequent visitor, as were Katharine Hepburn and Spencer Tracy. In an interview, Hepburn described the last time she and Tracy saw their dear friend, on January 13, 1957:\n\nBogart fell into a coma and died in his bed the next day. He had just turned 57 twenty days prior and weighed only 80 pounds (36 kg). His simple funeral was held at All Saints Episcopal Church, with musical selections from favorite composers Johann Sebastian Bach and Claude Debussy. The ceremony was attended by some of Hollywood's biggest stars, including Hepburn, Tracy, Judy Garland, David Niven, Ronald Reagan, James Mason, Bette Davis, Danny Kaye, Joan Fontaine, Marlene Dietrich, James Cagney, Errol Flynn, Gregory Peck and Gary Cooper, as well as Billy Wilder and Jack Warner. Bacall had asked Tracy to give the eulogy, but he was too upset, so John Huston spoke instead. He reminded the gathered mourners that while Bogart's life had ended far too soon, it had been a rich one:\n\nBogart's cremated remains were interred in Forest Lawn Memorial Park Cemetery, Glendale, California in the Garden of Memory, Columbarium of Eternal Light, Garden Niche 647. He was buried with a small, gold whistle once part of a charm bracelet he had given to Lauren Bacall before they married. On it was inscribed an allusion to a line from their first movie together, in 1944, \"To Have and Have Not\", where Bacall had said to him shortly after their first meeting: \"You know how to whistle don't you Steve? You just put your lips together and blow\". The inscription read: \"If you want anything, just whistle.\"\n\nThe probate value of Bogart's estate was $910,146 gross and $737,668 net ($ million and $ million today).\n\nAfter his death, a \"Bogie Cult\" formed at the Brattle Theatre in Cambridge, Massachusetts, as well as Greenwich Village, Manhattan, New York, and in France, which contributed to his spike in popularity in the late 1950s and 1960s. In 1997, \"Entertainment Weekly\" magazine named Bogart the number one movie legend of all time. In 1999, the American Film Institute ranked him the Greatest Male Star of Classic Hollywood.\n\nJean-Luc Godard's \"Breathless\" (1960) was the first film to pay tribute to Bogart. Later, in Woody Allen's comic paean to Bogart, \"Play It Again, Sam\" (1972), Bogart's ghost comes to the aid of Allen's bumbling character, a movie critic with women troubles whose \"sex life has turned into the 'Petrified Forest'\".\n\nOn August 21, 1946, Bogart was honored in a ceremony at Grauman's Chinese Theater to record his hand and footprints in cement. On February 8, 1960, he was posthumously given a star on the Hollywood Walk of Fame at 6322 Hollywood Boulevard. During his career, Bogart was nominated for several awards including the BAFTA award for best foreign actor in 1952 for \"The African Queen\" and three Academy Awards.\n\nIn 1997, the United States Postal Service honored Bogart with a stamp bearing his image in its \"Legends of Hollywood\" series as the third figure to be recognized. At a formal ceremony attended by Lauren Bacall, and the Bogart children, Stephen and Leslie, Tirso del Junco, the chairman of the governing board of the USPS, provided an eloquent tribute:\n\n\"Today, we mark another chapter in the Bogart legacy. With an image that is small and yet as powerful as the ones he left in celluloid, we will begin today to bring his artistry, his power, his unique star quality, to the messages that travel the world.\"\n\nOn June 24, 2006, a section of 103rd Street, between Broadway and West End Avenue, in New York City was renamed \"Humphrey Bogart Place.\" Lauren Bacall and her son Stephen Bogart were present at the commemorative event. \"Bogie would never have believed it,\" Lauren Bacall expressed to the assembled group of city officials and onlookers in attendance.\n\nHumphrey Bogart's life has inspired writers and others:\n\nBogart is credited with five of the American Film Institute's top 100 quotations in American cinema, the most by any actor:\n\nBogart is also credited with one of the top movie misquotations, \"Play it again, Sam\". In \"Casablanca\", neither his Rick Blaine character nor anyone else says the line, although it is widely credited to him and is the verbatim title of a Woody Allen tribute movie.\n\nWhen Blaine's former love, Ilsa (Ingrid Bergman), first enters his \"Café Americain\", she spots Sam, the piano player (Dooley Wilson), and asks him to \"Play it once, Sam, for old times' sake.\" When he feigns ignorance, she persists, \"Play it, Sam. Play 'As Time Goes By.'\" Later that night, alone with Sam, Rick demands, \"You played it for her - you can play it for me.\" Sam once again resists, prompting Blaine to shout: \"If she can stand it, I can! Play it!\"\n\n\n", "id": "14045", "title": "Humphrey Bogart"}
{"url": "https://en.wikipedia.org/wiki?curid=14051", "text": "History painting\n\nHistory painting is a genre in painting defined by its subject matter rather than artistic style. History paintings usually depict a moment in a narrative story, rather than a specific and static subject, as in a portrait. The term is derived from the wider senses of the word \"historia\" in Latin and Italian, meaning \"story\" or \"narrative\", and essentially means \"story painting\". Most history paintings are not of scenes from history, especially paintings from before about 1850. In modern English, historical painting is sometimes used to describe the painting of scenes from history in its narrower sense, especially for 19th-century art, excluding religious, mythological and allegorical subjects, which are included in the broader term history painting, and before the 19th century were the most common subjects for history paintings. \n\nHistory paintings almost always contain a number of figures, often a large number, and normally show some type of action that is a moment in a narrative. The genre includes depictions of moments in religious narratives, above all the \"Life of Christ\", as well as narrative scenes from mythology, and also allegorical scenes. These groups were for long the most frequently painted; works such as Michelangelo's Sistine Chapel ceiling are therefore history paintings, as are most very large paintings before the 19th century. The term covers large paintings in oil on canvas or fresco produced between the Renaissance and the late 19th century, after which the term is generally not used even for the many works that still meet the basic definition.\n\nHistory painting may be used interchangeably with historical painting, and was especially so used before the 20th century. Where a distinction is made \"historical painting\" is the painting of scenes from secular history, whether specific episodes or generalized scenes. In the 19th century historical painting in this sense became a distinct genre. In phrases such as \"historical painting materials\", \"historical\" means in use before about 1900, or some earlier date.\n\nHistory paintings were traditionally regarded as the highest form of Western painting, occupying the most prestigious place in the hierarchy of genres, and considered the equivalent to the epic in literature. In his \"De Pictura\" of 1436, Leon Battista Alberti had argued that multi-figure history painting was the noblest form of art, as being the most difficult, which required mastery of all the others, because it was a visual form of history, and because it had the greatest potential to move the viewer. He placed emphasis on the ability to depict the interactions between the figures by gesture and expression.\n\nThis view remained general until the 19th century, when artistic movements began to struggle against the establishment institutions of academic art, which continued to adhere to it. At the same time there was from the latter part of the 18th century an increased interest in depicting in the form of history painting moments of drama from recent or contemporary history, which had long largely been confined to battle-scenes and scenes of formal surrenders and the like. Scenes from ancient history had been popular in the early Renaissance, and once again became common in the Baroque and Rococo periods, and still more so with the rise of Neoclassicism. In some 19th or 20th century contexts, the term may refer specifically to paintings of scenes from secular history, rather than those from religious narratives, literature or mythology.\n\nThe term is generally not used in art history in speaking of medieval painting, although the Western tradition was developing in large altarpieces, fresco cycles, and other works, as well as miniatures in illuminated manuscripts. It comes to the fore in Italian Renaissance painting, where a series of increasingly ambitious works were produced, many still religious, but several, especially in Florence, which did actually feature near-contemporary historical scenes such as the set of three huge canvases on \"The Battle of San Romano\" by Paolo Uccello, the abortive \"Battle of Cascina\" by Michelangelo and the \"Battle of Anghiari\" by Leonardo da Vinci, neither of which were completed. Scenes from ancient history and mythology were also popular. Writers such as Alberti and the following century Giorgio Vasari in his \"Lives of the Artists\", followed public and artistic opinion in judging the best painters above all on their production of large works of history painting (though in fact the only modern (post-classical) work described in \"De Pictura\" is Giotto's huge \"Navicella\" in mosaic). Artists continued for centuries to strive to make their reputation by producing such works, often neglecting genres to which their talents were better suited.\n\nThere was some objection to the term, as many writers preferred terms such as \"poetic painting\" (\"poesia\"), or wanted to make a distinction between the \"true\" \"istoria\", covering history including biblical and religious scenes, and the \"fabula\", covering pagan myth, allegory, and scenes from fiction, which could not be regarded as true. The large works of Raphael were long considered, with those of Michelangelo, as the finest models for the genre.\n\nIn the Raphael Rooms in the Vatican Palace, allegories and historical scenes are mixed together, and the Raphael Cartoons show scenes from the Gospels, all in the Grand Manner that from the High Renaissance became associated with, and often expected in, history painting. In the Late Renaissance and Baroque the painting of actual history tended to degenerate into panoramic battle-scenes with the victorious monarch or general perched on a horse accompanied with his retinue, or formal scenes of ceremonies, although some artists managed to make a masterpiece from such unpromising material, as Velázquez did with his \"The Surrender of Breda\".\n\nAn influential formulation of the hierarchy of genres, confirming the history painting at the top, was made in 1667 by André Félibien, a historiographer, architect and theoretician of French classicism became the classic statement of the theory for the 18th century:Celui qui fait parfaitement des païsages est au-dessus d'un autre qui ne fait que des fruits, des fleurs ou des coquilles. Celui qui peint des animaux vivants est plus estimable que ceux qui ne représentent que des choses mortes & sans mouvement ; & comme la figure de l'homme est le plus parfait ouvrage de Dieu sur la Terre, il est certain aussi que celui qui se rend l'imitateur de Dieu en peignant des figures humaines, est beaucoup plus excellent que tous les autres ... un Peintre qui ne fait que des portraits, n'a pas encore cette haute perfection de l'Art, & ne peut prétendre à l'honneur que reçoivent les plus sçavans. Il faut pour cela passer d'une seule figure à la représentation de plusieurs ensemble ; il faut traiter l'histoire & la fable ; il faut représenter de grandes actions comme les historiens, ou des sujets agréables comme les Poëtes ; & montant encore plus haut, il faut par des compositions allégoriques, sçavoir couvrir sous le voile de la fable les vertus des grands hommes, & les mystères les plus relevez.\n\nHe who produces perfect landscapes is above another who only produces fruit, flowers or seashells. He who paints living animals is more than those who only represent dead things without movement, and as man is the most perfect work of God on the earth, it is also certain that he who becomes an imitator of God in representing human figures, is much more excellent than all the others ... a painter who only does portraits still does not have the highest perfection of his art, and cannot expect the honour due to the most skilled. For that he must pass from representing a single figure to several together; history and myth must be depicted; great events must be represented as by historians, or like the poets, subjects that will please, and climbing still higher, he must have the skill to cover under the veil of myth the virtues of great men in allegories, and the mysteries they reveal\".\n\nBy the late 18th century, with both religious and mytholological painting in decline, there was an increased demand for paintings of scenes from history, including contemporary history. This was in part driven by the changing audience for ambitious paintings, which now increasingly made their reputation in public exhibitions rather than by impressing the owners of and visitors to palaces and public buildings. Classical history remained popular, but scenes from national histories were often the best-received. From 1760 onwards, the Society of Artists of Great Britain, the first body to organize regular exhibitions in London, awarded two generous prizes each year to paintings of subjects from British history. \nThe unheroic nature of modern dress was regarded as a serious difficulty. When, in 1770, Benjamin West proposed to paint \"The Death of General Wolfe\" in contemporary dress, he was firmly instructed to use classical costume by many people. He ignored these comments and showed the scene in modern dress. Although George III refused to purchase the work, West succeeded both in overcoming his critics' objections and inaugurating a more historically accurate style in such paintings. Other artists depicted scenes, regardless of when they occurred, in classical dress and for a long time, especially during the French Revolution, history painting often focused on depictions of the heroic male nude.\n\nThe large production, using the finest French artists, of propaganda paintings glorifying the exploits of Napoleon, were matched by works, showing both victories and losses, from the anti-Napoleonic alliance by artists such as Goya and J.M.W. Turner. Théodore Géricault's \"The Raft of the Medusa\" (1818–1819) was a sensation, appearing to update the history painting for the 19th century, and showing anonymous figures famous only for being victims of what was then a famous and controversial disaster at sea. Conveniently their clothes had been worn away to classical-seeming rags by the point the painting depicts. At the same time the demand for traditional large religious history paintings very largely fell away.\nIn the mid-nineteenth century there arose a style known as historicism, which marked a formal imitation of historical styles and/or artists. Another development in the nineteenth century was the treatment of historical subjects, often on a large scale, with the values of genre painting, the depiction of scenes of everyday life, and anecdote. Grand depictions of events of great public importance were supplemented with scenes depicting more personal incidents in the lives of the great, or of scenes centred on unnamed figures involved in historical events, as in the Troubadour style. At the same time scenes of ordinary life with moral, political or satirical content became often the main vehicle for expressive interplay between figures in painting, whether given a modern or historical setting.\n\nBy the later 19th century, history painting was often explicitly rejected by avant-garde movements such as the Impressionists (except for Édouard Manet) and the Symbolists, and according to one recent writer \"Modernism was to a considerable extent built upon the rejection of History Painting... All other genres are deemed capable of entering, in one form or another, the 'pantheon' of modernity considered, but History Painting is excluded\".\n\nInitially, \"history painting\" and \"historical painting\" were used interchangeably in English, as when Sir Joshua Reynolds in his fourth \"Discourse\" uses both indiscriminately to cover \"history painting\", while saying \"...it ought to be called poetical, as in reality it is\", reflecting the French term \"peinture historique\", one equivalent of \"history painting\". The terms began to separate in the 19th century, with \"historical painting\" becoming a sub-group of \"history painting\" restricted to subjects taken from history in its normal sense. In 1853 John Ruskin asked his audience: \"What do you at present \"mean\" by historical painting? Now-a-days it means the endeavour, by the power of imagination, to portray some historical event of past days.\" So for example Harold Wethey's three-volume catalogue of the paintings of Titian (Phaidon, 1969–75) is divided between \"Religious Paintings\", \"Portraits\", and \"Mythological and Historical Paintings\", though both volumes I and III cover what is included in the term \"History Paintings\". This distinction is useful but is by no means generally observed, and the terms are still often used in a confusing manner. Because of the potential for confusion modern academic writing tends to avoid the phrase \"historical painting\", talking instead of \"historical subject matter\" in history painting, but where the phrase is still used in contemporary scholarship it will normally mean the painting of subjects from history, very often in the 19th century. \"Historical painting\" may also be used, especially in discussion of painting techniques in conservation studies, to mean \"old\", as opposed to modern or recent painting.\n\nIn 19th-century British writing on art the terms \"subject painting\" or \"anecdotic\" painting were often used for works in a line of development going back to William Hogarth of monoscenic depictions of crucial moments in an implied narrative with unidentified characters, such as William Holman Hunt's 1853 painting \"The Awakening Conscience\" or Augustus Egg's \"Past and Present\", a set of three paintings, updating sets by Hogarth such as \"Marriage à-la-mode\".\n\nHistory painting was the dominant form of academic painting in the various national academies in the 18th century, and for most of the 19th, and increasingly historical subjects dominated. During the Revolutionary and Napoleonic periods the heroic treatment of contemporary history in a frankly propagandistic fashion by Antoine-Jean, Baron Gros, Jacques-Louis David, Carle Vernet and others was supported by the French state, but after the fall of Napoleon in 1815 the French governments were not regarded as suitable for heroic treatment and many artists retreated further into the past to find subjects, though in Britain depicting the victories of the Napoleonic Wars mostly occurred after they were over. Another path was to choose contemporary subjects that were oppositional to government either at home and abroad, and many of what were arguably the last great generation of history paintings were protests at contemporary episodes of repression or outrages at home or abroad: Goya's \"The Third of May 1808\" (1814), Théodore Géricault's \"The Raft of the Medusa\" (1818–19), Eugène Delacroix's \"The Massacre at Chios\" (1824) and \"Liberty Leading the People\" (1830). These were heroic, but showed heroic suffering by ordinary civilians.\nRomantic artists such as Géricault and Delacroix, and those from other movements such as the English Pre-Raphaelite Brotherhood continued to regard history painting as the ideal for their most ambitious works. Others such as Jan Matejko in Poland, Vasily Surikov in Russia, and Paul Delaroche in France became specialized painters of large historical subjects. The \"style troubadour\" (\"troubadour style\") was a somewhat derisive French term for earlier paintings of medieval and Renaissance scenes, which were often small and depicting moments of anecdote rather than drama; Ingres, Richard Parkes Bonington and Henri Fradelle painted such works. Sir Roy Strong calls this type of work the \"Intimate Romantic\", and in French it was known as the \"peinture de genre historique\" or \"peinture anecdotique\" (\"historical genre painting\" or \"anecdotal painting\").\n\nChurch commissions for large group scenes from the Bible had greatly reduced, and historical painting became very significant. Especially in the early 19th century, much historical painting depicted specific moments from historical literature, with the novels of Sir Walter Scott a particular favourite, in France and other European countries as much as Great Britain. By the middle of the century medieval scenes were expected to be very carefully researched, using the work of historians of costume, architecture and all elements of decor that were becoming available. The provision of examples and expertise for artists, as well as revivalist industrial designers, was one of the motivations for the establishment of museums like the Victoria and Albert Museum in London. New techniques of printmaking such as the chromolithograph made good quality monochrome print reproductions both relatively cheap and very widely accessible, and also hugely profitable for artist and publisher, as the sales were so large. Historical painting often had a close relationship with Nationalism, and painters like Matejko in Poland could play an important role in fixing the prevailing historical narrative of national history in the popular mind. In France, \"L'art Pompier\" (\"Fireman art\") was a derisory term for official academic historical painting, and in a final phase, \"History painting of a debased sort, scenes of brutality and terror, purporting to illustrate episodes from Roman and Moorish history, were Salon sensations. On the overcrowded walls of the exhibition galleries, the paintings that shouted loudest got the attention\". Orientalist painting was an alternative genre that offered similar exotic costumes and decor, and at least as much opportunity to depict sex and violence.\n\n\n\n", "id": "14051", "title": "History painting"}
{"url": "https://en.wikipedia.org/wiki?curid=14052", "text": "Hyperbola\n\nIn mathematics, a hyperbola (plural \"hyperbolas\" or \"hyperbolae\") is a type of smooth curve lying in a plane, defined by its geometric properties or by equations for which it is the solution set. A hyperbola has two pieces, called connected components or branches, that are mirror images of each other and resemble two infinite bows. The hyperbola is one of the three kinds of conic section, formed by the intersection of a plane and a double cone. (The other conic sections are the parabola and the ellipse. A circle is a special case of an ellipse). If the plane intersects both halves of the double cone but does not pass through the apex of the cones, then the conic is a hyperbola.\n\nHyperbolas arise in many ways: as the curve representing the function formula_1 in the Cartesian plane, as the path followed by the shadow of the tip of a sundial, as the shape of an open orbit (as distinct from a closed elliptical orbit), such as the orbit of a spacecraft during a gravity assisted swing-by of a planet or more generally any spacecraft exceeding the escape velocity of the nearest planet, as the path of a single-apparition comet (one travelling too fast ever to return to the solar system), as the scattering trajectory of a subatomic particle (acted on by repulsive instead of attractive forces but the principle is the same), and so on.\n\nEach branch of the hyperbola has two arms which become straighter (lower curvature) further out from the center of the hyperbola. Diagonally opposite arms, one from each branch, tend in the limit to a common line, called the asymptote of those two arms. So there are two asymptotes, whose intersection is at the center of symmetry of the hyperbola, which can be thought of as the mirror point about which each branch reflects to form the other branch. In the case of the curve formula_1 the asymptotes are the two coordinate axes.\n\nHyperbolas share many of the ellipses' analytical properties such as eccentricity, focus, and directrix. Typically the correspondence can be made with nothing more than a change of sign in some term. Many other mathematical objects have their origin in the hyperbola, such as hyperbolic paraboloids (saddle surfaces), hyperboloids (\"wastebaskets\"), hyperbolic geometry (Lobachevsky's celebrated non-Euclidean geometry), hyperbolic functions (sinh, cosh, tanh, etc.), and gyrovector spaces (a geometry proposed for use in both relativity and quantum mechanics which is not Euclidean).\n\nThe word \"hyperbola\" derives from the Greek , meaning \"over-thrown\" or \"excessive\", from which the English term hyperbole also derives. Hyperbolae were discovered by Menaechmus in his investigations of the problem of doubling the cube, but were then called sections of obtuse cones. The term hyperbola is believed to have been coined by Apollonius of Perga (c. 262–c. 190 BC) in his definitive work on the conic sections, the \"Conics\". For comparison, the other two general conic sections, the ellipse and the parabola, derive from the corresponding Greek words for \"deficient\" and \"comparable\"; these terms may refer to the eccentricity of these curves, which is greater than one (hyperbola), less than one (ellipse) and exactly one (parabola).\n\nSimilar to a parabola, a hyperbola is an open curve, meaning that it continues indefinitely to infinity, rather than closing on itself as an ellipse does. A hyperbola consists of two disconnected curves called its arms or branches.\n\nThe points on the two branches that are closest to each other are called the vertices; they are the points where the curve has its smallest radius of curvature. The line segment connecting the vertices is called the \"transverse axis\" or \"major axis\", corresponding to the major diameter of an ellipse. The midpoint of the transverse axis is known as the hyperbola's \"center\". The distance \"a\" from the center to each vertex is called the semi-major axis. Outside of the transverse axis but on the same line are the two \"focal points (foci)\" of the hyperbola. The line through these five points is one of the two principal axes of the hyperbola, the other being the perpendicular bisector of the transverse axis. The hyperbola has mirror symmetry about its principal axes, and is also symmetric under a 180° turn about its center.\n\nAt large distances from the center, the hyperbola approaches two lines, its asymptotes, which intersect at the hyperbola's center. A hyperbola approaches its asymptotes arbitrarily closely as the distance from its center increases, but it never intersects them; however, a degenerate hyperbola consists only of its asymptotes. Consistent with the symmetry of the hyperbola, if the transverse axis is aligned with the \"x\"-axis of a Cartesian coordinate system, the slopes of the asymptotes are equal in magnitude but opposite in sign, ±, where \"b\" = \"a\" × tan(θ) and where θ is the angle between the transverse axis and either asymptote. The distance \"b\" (not shown) is the length of the perpendicular segment from either vertex to the asymptotes.\n\nA \"conjugate axis\" of length 2\"b\", corresponding to the \"minor axis\" of an ellipse, is sometimes drawn on the non-transverse principal axis; its endpoints ±\"b\" lie on the minor axis at the height of the asymptotes over/under the hyperbola's vertices. Because of the minus sign in some of the formulas below, it is also called the \"imaginary axis\" of the hyperbola.\n\nIf , the angle 2θ between the asymptotes equals 90° and the hyperbola is said to be \"rectangular\" or \"equilateral\". In this special case, the rectangle joining the four points on the asymptotes directly above and below the vertices is a square, since the lengths of its sides \"2a\" \"2b\".\n\nIf the transverse axis of any hyperbola is aligned with the \"x\"-axis of a Cartesian coordinate system and is centered on the origin, the equation of the hyperbola can be written as\n\nA hyperbola aligned in this way is called an \"East-West opening hyperbola\". Likewise, a hyperbola with its transverse axis aligned with the \"y\"-axis is called a \"North–South opening hyperbola\" and has equation\n\nEvery hyperbola is congruent to the origin-centered East-West opening hyperbola sharing its same scale and eccentricity \"e\" (its shape, or degree of \"spread\"), and is also congruent to the origin-centered North–South opening hyperbola with identical scale and eccentricity \"e\" — that is, it can be rotated so that it opens in the desired direction and can be translated (rigidly moved in the plane) so that it is centered at the origin. For convenience, hyperbolas are usually analyzed in terms of their centered East-West opening form.\n\nIf formula_5 is the distance from the center to either focus, then formula_6.\nThe shape of a hyperbola is defined entirely by its eccentricity \"e\", which is a dimensionless number always greater than one. The distance \"c\" from the center to the foci equals \"ae\". The eccentricity can also be defined as the ratio of the distances to either focus and to a corresponding line known as the directrix; hence, the distance from the center to the directrices equals \"a\"/\"e\". In terms of the parameters \"a\", \"b\", \"c\" and the angle θ, the eccentricity equals\n\nFor example, the eccentricity of a rectangular hyperbola , equals the square root of two: \"e\" =  formula_8.\n\nEvery hyperbola has a conjugate hyperbola, in which the transverse and conjugate axes are exchanged without changing the asymptotes. The equation of the conjugate hyperbola of formula_9 is formula_10. If the graph of the conjugate hyperbola is rotated 90° to restore the east-west opening orientation (so that \"x\" becomes \"y\" and vice versa), the equation of the resulting rotated conjugate hyperbola is the same as the equation of the original hyperbola except with \"a\" and \"b\" exchanged. For example, the angle θ of the conjugate hyperbola equals 90° minus the angle of the original hyperbola. Thus, the angles in the original and conjugate hyperbolas are complementary angles, which implies that they have different eccentricities unless θ = 45° (a rectangular hyperbola). Hence, the conjugate hyperbola does \"not\" in general correspond to a 90° rotation of the original hyperbola; the two hyperbolas are generally different in shape.\n\nA few other lengths are used to describe hyperbolas. Consider a line perpendicular to the transverse axis (i.e., parallel to the conjugate axis) that passes through one of the hyperbola's foci. The line segment connecting the two intersection points of this line with the hyperbola is known as the \"latus rectum\" and has a length formula_11. The \"semi-latus rectum\" \"l\" is half of this length, i.e., formula_12. The \"focal parameter\" \"p\" is the distance from a focus to its corresponding directrix, and equals formula_13.\n\nA hyperbola can be defined mathematically in several equivalent ways.\n\nA hyperbola may be defined as the curve of intersection between a right circular conical surface and a plane that cuts through both halves of the cone. The other major types of conic sections are the ellipse and the parabola; in these cases, the plane cuts through only one half of the double cone. If the plane passes through the central apex of the double cone a degenerate hyperbola results — two straight lines that cross at the apex point.\n\nA hyperbola may be defined equivalently as the locus of points where the absolute value of the \"difference\" of the distances to the two foci is a constant equal to 2\"a\", the distance between its two vertices. This definition accounts for many of the hyperbola's applications, such as multilateration; this is the problem of determining position from the \"difference\" in arrival times of synchronized signals, as in GPS.\n\nThis definition may be expressed also in terms of tangent circles. The center of any circles externally tangent to two given circles lies on a hyperbola, whose foci are the centers of the given circles and where the vertex distance 2\"a\" equals the difference in radii of the two circles. As a special case, one given circle may be a point located at one focus; since a point may be considered as a circle of zero radius, the other given circle—which is centered on the other focus—must have radius 2\"a\". This provides a simple technique for constructing a hyperbola, as shown below. It follows from this definition that a tangent line to the hyperbola at a point P bisects the angle formed with the two foci, i.e., the angle FP F. Consequently, the feet of perpendiculars drawn from each focus to such a tangent line lies on a circle of radius \"a\" that is centered on the hyperbola's own center.\n\nA proof that this characterization of the hyperbola is equivalent to the conic-section characterization can be done without coordinate geometry by means of Dandelin spheres.\n\nA hyperbola can be defined as the locus of points for which the ratio of the distances to one focus and to a line (called the directrix) is a constant \"e\" that is larger than 1. This constant is the eccentricity of the hyperbola. The eccentricity equals the secant of half the angle between the asymptotes of the hyperbola, so the eccentricity of the hyperbola \"xy\" = 1 equals the square root of 2.\n\nBy symmetry a hyperbola has two directrices, which are parallel to the conjugate axis and are between it and the tangent to the hyperbola at a vertex. One directrix and its focus is enough to produce both arms of the hyperbola.\n\nThe reciprocation of a circle \"B\" in a circle \"C\" always yields a conic section such as a hyperbola. The process of \"reciprocation in a circle \"C\"\" consists of replacing every line and point in a geometrical figure with their corresponding pole and polar, respectively. The \"pole\" of a line is the inversion of its closest point to the circle \"C\", whereas the polar of a point is the converse, namely, a line whose closest point to \"C\" is the inversion of the point.\n\nThe eccentricity of the conic section obtained by reciprocation is the ratio of the distances between the two circles' centers to the radius \"r\" of reciprocation circle \"C\". If B and C represent the points at the centers of the corresponding circles, then\n\nSince the eccentricity of a hyperbola is always greater than one, the center B must lie outside of the reciprocating circle \"C\".\n\nThis definition implies that the hyperbola is both the locus of the poles of the tangent lines to the circle \"B\", as well as the envelope of the polar lines of the points on \"B\". Conversely, the circle \"B\" is the envelope of polars of points on the hyperbola, and the locus of poles of tangent lines to the hyperbola. Two tangent lines to \"B\" have no (finite) poles because they pass through the center C of the reciprocation circle \"C\"; the polars of the corresponding tangent points on \"B\" are the asymptotes of the hyperbola. The two branches of the hyperbola correspond to the two parts of the circle \"B\" that are separated by these tangent points.\n\nA hyperbola can also be defined as a second-degree equation in the Cartesian coordinates (\"x\", \"y\") in the plane,\n\nprovided that the constants \"A\", \"A\", \"A\", \"B\", \"B\", and \"C\" satisfy the determinant condition\n\nThis determinant is conventionally called the discriminant of the conic section.\n\nA special case of a hyperbola—the \"degenerate hyperbola\" consisting of two intersecting lines—occurs when another determinant is zero:\n\nThis determinant Δ is sometimes called the discriminant of the conic section.\n\nGiven the above general parametrization of the hyperbola in Cartesian coordinates, the eccentricity can be found using the formula in Conic section#Eccentricity in terms of parameters of the quadratic form.\n\nThe center (\"x\", \"y\") of the hyperbola may be determined from the formulae\n\nIn terms of new coordinates, and , the defining equation of the hyperbola can be written\n\nThe principal axes of the hyperbola make an angle \"φ\" with the positive \"x\"-axis that is given by\n\nRotating the coordinate axes so that the \"x\"-axis is aligned with the transverse axis brings the equation into its canonical form\n\nThe major and minor semiaxes \"a\" and \"b\" are defined by the equations\n\nwhere λ and λ are the roots of the quadratic equation\n\nFor comparison, the corresponding equation for a degenerate hyperbola (consisting of two intersecting lines) is\n\nThe tangent line to a given point (\"x\", \"y\") on the hyperbola is defined by the equation\n\nwhere \"E\", \"F\" and \"G\" are defined by\n\nThe normal line to the hyperbola at the same point is given by the equation\n\nThe normal line is perpendicular to the tangent line, and both pass through the same point (\"x\", \"y\").\n\nFrom the equation\nthe left focus is formula_33 and the right focus is formula_34 where is the eccentricity. Denote the distances from a point (\"x, y\") to the left and right foci as formula_35 and formula_36 For a point on the right branch,\n\nand for a point on the left branch,\n\nThis can be proved as follows:\n\nIf (\"x\",\"y\") is a point on the hyperbola the distance to the left focal point is\n\nTo the right focal point the distance is\n\nIf (\"x,y\") is a point on the right branch of the hyperbola then formula_41 and\n\nSubtracting these equations one gets\n\nIf (\"x,y\") is a point on the left branch of the hyperbola then formula_45 and\n\nSubtracting these equations one gets\n\nIn the section above it is shown that using the coordinate system in which the equation of the hyperbola takes its canonical form\n\nthe distance formula_50 from a point formula_51 on the left branch of the hyperbola to the left focal point formula_52 is\n\nIntroducing polar coordinates formula_54 with origin at the left focal point, the coordinates relative to the canonical coordinate system are\n\nand the equation above takes the form\n\nfrom which it follows that\n\nThis is the representation of the near branch of a hyperbola in polar coordinates with respect to a focal point.\n\nThe polar angle formula_59 of a point on a hyperbola relative to the near focal point as described above is called the true anomaly of the point.\n\nSimilar to the ellipse, a hyperbola can be constructed using a taut thread. A straightedge of length \"S\" is attached to one focus F at one of its corners A so that it is free to rotate about that focus. A thread of length is attached between the other focus F and the other corner B of the straightedge. A sharp pencil is held up against the straightedge, sandwiching the thread tautly against the straightedge. Let the position of the pencil be denoted as P. The total length \"L\" of the thread equals the sum of the distances \"L\" from F to P and \"L\" from P to B. Similarly, the total length \"S\" of the straightedge equals the distance \"L\" from F to P and \"L\". Therefore, the difference in the distances to the foci, equals the constant 2\"a\"\n\nA second construction uses intersecting circles, but is likewise based on the constant difference of distances to the foci. Consider a hyperbola with two foci F and F, and two vertices P and Q; these four points all lie on the transverse axis. Choose a new point T also on the transverse axis and to the right of the rightmost vertex P; the difference in distances to the two vertices, = 2\"a\", since 2\"a\" is the distance between the vertices. Hence, the two circles centered on the foci F and F of radius QT and PT, respectively, will intersect at two points of the hyperbola.\n\nA third construction relies on the definition of the hyperbola as the reciprocation of a circle. Consider the circle centered on the center of the hyperbola and of radius \"a\"; this circle is tangent to the hyperbola at its vertices. A line \"g\" drawn from one focus may intersect this circle in two points M and N; perpendiculars to \"g\" drawn through these two points are tangent to the hyperbola. Drawing a set of such tangent lines reveals the envelope of the hyperbola.\n\nA fourth construction is using the parallelogram method. It is similar to such method for parabola and ellipse construction: certain equally spaced points lying on parallel lines are connected with each other by two straight lines and their intersection point lies on the hyperbola.\n\nThe ancient Greek geometers recognized a reflection property of hyperbolas. If a ray of light emerges from one focus and is reflected from either branch of the hyperbola, the light-ray appears to have come from the other focus. Equivalently, by reversing the direction of the light, rays directed at one of the foci are reflected towards the other focus. This property is analogous to the property of ellipses that a ray emerging from one focus is reflected from the ellipse directly \"towards\" the other focus (rather than \"away\" as in the hyperbola). Expressed mathematically, lines drawn from each focus to the same point on the hyperbola intersect it at equal angles; the tangent line to a hyperbola at a point P bisects the angle formed with the two foci, FPF.\n\nTangent lines to a hyperbola have another remarkable geometrical property. If a tangent line at a point T intersects the asymptotes at two points K and L, then T bisects the line segment KL, and the product of distances to the hyperbola's center, OK×OL is a constant.\n\nThe angle containing part of one arm of the hyperbola, formed at a point on the major axis by a tangent line to that arm and the major axis, has measure greater than 45°.\n\nJust as the sine and cosine functions give a parametric equation for the ellipse, so the hyperbolic sine and hyperbolic cosine give a parametric equation for the hyperbola.\n\nAs\n\none has for any hyperbolic angle formula_62 that the point\n\nsatisfies the equation\n\nwhich is the equation of a hyperbola relative its canonical coordinate system.\n\nWhen \"μ\" varies over the interval formula_66 one gets with this formula all points formula_67 on the right branch of the hyperbola.\n\nThe left branch for which formula_68 is in the same way obtained as\n\nIn the figure the points formula_71 given by\n\nfor\n\non the left branch of a hyperbola with eccentricity 1.2 are marked as dots.\n\nThere are three types of conic sections: hyperbolas, ellipses and parabolas. Since the parabola may be seen as a limiting case poised exactly between an ellipse and a hyperbola, there are only two non-limiting types, ellipses and hyperbolas. These two types are related in that formulae for one type can often be applied to the other.\n\nThe canonical equation for a hyperbola is\n\nAny hyperbola can be rotated so that it is east-west opening and positioned with its center at the origin, so that the equation describing it is this canonical equation.\n\nThe canonical equation for the hyperbola may be seen as a version of the corresponding ellipse equation\n\nin which the semi-minor axis length \"b\" is imaginary. That is, if in the ellipse equation \"b\" is replaced by \"ib\" where \"b\" is real, one obtains the hyperbola equation.\n\nSimilarly, the parametric equations for a hyperbola and an ellipse are expressed in terms of hyperbolic and trigonometric functions, respectively, which are again related by an imaginary circular angle, for example,\n\nHence, many formulae for the ellipse can be extended to hyperbolas by adding the imaginary unit \"i\" in front of the semi-minor axis \"b\" and the angle. For example, the arc length of a segment of an ellipse can be determined using an incomplete elliptic integral of the second kind. The corresponding arclength of a hyperbola is given by the same function with imaginary parameters \"b\" and μ, namely, \"ib E\"(\"iμ\", \"c\").\n\nBesides providing a uniform description of circles, ellipses, parabolas, and hyperbolas, conic sections can also be understood as a natural model of the geometry of perspective in the case where the scene being viewed consists of a circle, or more generally an ellipse. The viewer is typically a camera or the human eye. In the simplest case the viewer's lens is just a pinhole; the role of more complex lenses is merely to gather far more light while retaining as far as possible the simple pinhole geometry in which all rays of light from the scene pass through a single point. Once through the lens, the rays then spread out again, in air in the case of a camera, in the vitreous humor in the case of the eye, eventually distributing themselves over the film, imaging device, or retina, all of which come under the heading of image plane. The lens plane is a plane parallel to the image plane at the lens; all rays pass through a single point on the lens plane, namely the lens itself.\n\nWhen the circle directly faces the viewer, the viewer's lens is on-axis, meaning on the line normal to the circle through its center (think of the axle of a wheel). The rays of light from the circle through the lens to the image plane then form a cone with circular cross section whose apex is the lens. The image plane concretely realizes the abstract cutting plane in the conic section model.\n\nWhen in addition the viewer directly faces the circle, the circle is rendered faithfully on the image plane without perspective distortion, namely as a scaled-down circle. When the viewer turns attention or gaze away from the center of the circle the image plane then cuts the cone in an ellipse, parabola, or hyperbola depending on how far the viewer turns, corresponding exactly to what happens when the surface cutting the cone to form a conic section is rotated.\n\nA parabola arises when the lens plane is tangent to (touches) the circle. A viewer with perfect 180-degree wide-angle vision will see the whole parabola; in practice this is impossible and only a finite portion of the parabola is captured on the film or retina.\n\nWhen the viewer turns further so that the lens plane cuts the circle in two points, the shape on the image plane becomes that of a hyperbola. The viewer still sees only a finite curve, namely a portion of one branch of the hyperbola, and is unable to see the second branch at all, which corresponds to the portion of the circle behind the viewer, more precisely, on the same side of the lens plane as the viewer. In practice the finite extent of the image plane makes it impossible to see any portion of the circle near where it is cut by the lens plane. Further back however one could imagine rays from the portion of the circle well behind the viewer passing through the lens, were the viewer transparent. In this case the rays would pass through the image plane before the lens, yet another impracticality ensuring that no portion of the second branch could possibly be visible.\n\nThe tangents to the circle where it is cut by the lens plane constitute the asymptotes of the hyperbola. Were these tangents to be drawn in ink in the plane of the circle, the eye would perceive them as asymptotes to the visible branch. Whether they converge in front of or behind the viewer depends on whether the lens plane is in front of or behind the center of the circle respectively.\n\nIf the circle is drawn on the ground and the viewer gradually transfers gaze from straight down at the circle up towards the horizon, the lens plane eventually cuts the circle producing first a parabola then a hyperbola on the image plane as shown in Figure 10. As the gaze continues to rise the asymptotes of the hyperbola, if realized concretely, appear coming in from left and right, swinging towards each other and converging at the horizon when the gaze is horizontal. Further elevation of the gaze into the sky then brings the point of convergence of the asymptotes towards the viewer.\n\nBy the same principle with which the back of the circle appears on the image plane were all the physical obstacles to its projection to be overcome, the portion of the two tangents behind the viewer appear on the image plane as an extension of the visible portion of the tangents in front of the viewer. Like the second branch this extension materializes in the sky rather than on the ground, with the horizon marking the boundary between the physically visible (scene in front) and invisible (scene behind), and the visible and invisible parts of the tangents combining in a single X shape. As the gaze is raised and lowered about the horizon, the X shape moves oppositely, lowering as the gaze is raised and vice versa but always with the visible portion being on the ground and stopping at the horizon, with the center of the X being on the horizon when the gaze is horizontal.\n\nAll of the above was for the case when the circle faces the viewer, with only the viewer's gaze varying. When the circle starts to face away from the viewer the viewer's lens is no longer on-axis. In this case the cross section of the cone is no longer a circle but an ellipse (never a parabola or hyperbola). However the principle of conic sections does not depend on the cross section of the cone being circular, and applies without modification to the case of eccentric cones.\n\nIt is not difficult to see that even in the off-axis case a circle can appear circular, namely when the image plane (and hence lens plane) is parallel to the plane of the circle. That is, to see a circle as a circle when viewing it obliquely, look not at the circle itself but at the plane in which it lies. From this it can be seen that when viewing a plane filled with many circles, all of them will appear circular simultaneously when the plane is looked at directly.\n\nA common misperception about the hyperbola is that it is a mathematical curve rarely if ever encountered in daily life. The reality is that one sees a hyperbola whenever catching sight of portion of a circle cut by one's lens plane (and a parabola when the lens plane is tangent to, i.e. just touches, the circle). The inability to see very much of the arms of the visible branch, combined with the complete absence of the second branch, makes it virtually impossible for the human visual system to recognize the connection with hyperbolas such as \"y\" = 1/\"x\" where both branches are on display simultaneously.\n\nSeveral other curves can be derived from the hyperbola by inversion, the so-called inverse curves of the hyperbola. If the center of inversion is chosen as the hyperbola's own center, the inverse curve is the lemniscate of Bernoulli; the lemniscate is also the envelope of circles centered on a rectangular hyperbola and passing through the origin. If the center of inversion is chosen at a focus or a vertex of the hyperbola, the resulting inverse curves are a limaçon or a strophoid, respectively.\n\nAn east-west opening hyperbola centered at (\"h\",\"k\") has the equation\nThe major axis runs through the center of the hyperbola and intersects both arms of the hyperbola at the vertices (bend points) of the arms. The foci lie on the extension of the major axis of the hyperbola.\n\nThe minor axis runs through the center of the hyperbola and is perpendicular to the major axis.\n\nIn both formulas \"a\" is the semi-major axis (half the distance between the two arms of the hyperbola measured along the major axis), and \"b\" is the semi-minor axis (half the distance between the asymptotes along a line tangent to the hyperbola at a vertex).\n\nIf one forms a rectangle with vertices on the asymptotes and two sides that are tangent to the hyperbola, the sides tangent to the hyperbola are \"2b\" in length while the sides that run parallel to the line between the foci (the major axis) are \"2a\" in length. Note that \"b\" may be larger than \"a\" despite the names \"minor\" and \"major\".\n\nIf one calculates the distance from any point on the hyperbola to each focus, the absolute value of the difference of those two distances is always \"2a\".\n\nThe eccentricity is given by\n\nIf \"c\" equals the distance from the center to either focus, then\nwhere\nThe distance \"c\" is known as the linear eccentricity of the hyperbola. The distance between the foci is 2\"c\" or 2\"ae\".\n\nThe foci for an east-west opening hyperbola are given by\nand for a north-south opening hyperbola are given by\n\nThe directrices for an east-west opening hyperbola are given by\nand for a north-south opening hyperbola are given by\n\nThe polar coordinates used most commonly for the hyperbola are defined relative to the Cartesian coordinate system that has its origin in a focus and its x-axis pointing towards the origin of the \"canonical coordinate system\" as illustrated in the figure of the section True anomaly.\n\nRelative to this coordinate system one has that\n\nand the range of the true anomaly formula_87 is\n\nWith polar coordinate relative to the \"canonical coordinate system\"\n\none has that\n\nFor the right branch of the hyperbola the range of formula_92 is\n\n\"East-west opening hyperbola:\"\n\n\"North-south opening hyperbola:\"\n\nIn all formulae (\"h\",\"k\") are the center coordinates of the hyperbola, \"a\" is the length of the semi-major axis, and \"b\" is the length of the semi-minor axis.\n\nA family of confocal hyperbolas is the basis of the system of elliptic coordinates in two dimensions. These hyperbolas are described by the equation\n\nwhere the foci are located at a distance \"c\" from the origin on the \"x\"-axis, and where θ is the angle of the asymptotes with the \"x\"-axis. Every hyperbola in this family is orthogonal to every ellipse that shares the same foci. This orthogonality may be shown by a conformal map of the Cartesian coordinate system \"w\" = \"z\" + 1/\"z\", where \"z\"= \"x\" + \"iy\" are the original Cartesian coordinates, and \"w\"=\"u\" + \"iv\" are those after the transformation.\n\nOther orthogonal two-dimensional coordinate systems involving hyperbolas may be obtained by other conformal mappings. For example, the mapping \"w\" = \"z\" transforms the Cartesian coordinate system into two families of orthogonal hyperbolas.\n\nA rectangular hyperbola, equilateral hyperbola, or right hyperbola is a hyperbola for which the asymptotes are perpendicular.\n\nRectangular hyperbolas with the coordinate axes parallel to their asymptotes have the equation\n\nRectangular hyperbolas have eccentricity formula_98 with semi-major axis and semi-minor axis given by formula_99.\n\nThe simplest example of rectangular hyperbolas occurs when the center (\"h\", \"k\") is at the origin:\n\ndescribing quantities \"x\" and \"y\" that are inversely proportional. By rotating the coordinate axes counterclockwise by 45 degrees, with the new coordinate axes labelled formula_101 the equation of the hyperbola is given by canonical form\n\nIf the scale factor \"m\"=1/2, then this canonical rectangular hyperbola is the unit hyperbola.\n\nA circumconic passing through the orthocenter of a triangle is a rectangular hyperbola.\n\n\nHyperbolas may be seen in many sundials. On any given day, the sun revolves in a circle on the celestial sphere, and its rays striking the point on a sundial traces out a cone of light. The intersection of this cone with the horizontal plane of the ground forms a conic section. At most populated latitudes and at most times of the year, this conic section is a hyperbola. In practical terms, the shadow of the tip of a pole traces out a hyperbola on the ground over the course of a day (this path is called the \"declination line\"). The shape of this hyperbola varies with the geographical latitude and with the time of the year, since those factors affect the cone of the sun's rays relative to the horizon. The collection of such hyperbolas for a whole year at a given location was called a \"pelekinon\" by the Greeks, since it resembles a double-bladed axe.\n\nA hyperbola is the basis for solving multilateration problems, the task of locating a point from the differences in its distances to given points — or, equivalently, the difference in arrival times of synchronized signals between the point and the given points. Such problems are important in navigation, particularly on water; a ship can locate its position from the difference in arrival times of signals from a LORAN or GPS transmitters. Conversely, a homing beacon or any transmitter can be located by comparing the arrival times of its signals at two separate receiving stations; such techniques may be used to track objects and people. In particular, the set of possible positions of a point that has a distance difference of 2\"a\" from two given points is a hyperbola of vertex separation 2\"a\" whose foci are the two given points.\n\nThe path followed by any particle in the classical Kepler problem is a conic section. In particular, if the total energy \"E\" of the particle is greater than zero (i.e., if the particle is unbound), the path of such a particle is a hyperbola. This property is useful in studying atomic and sub-atomic forces by scattering high-energy particles; for example, the Rutherford experiment demonstrated the existence of an atomic nucleus by examining the scattering of alpha particles from gold atoms. If the short-range nuclear interactions are ignored, the atomic nucleus and the alpha particle interact only by a repulsive Coulomb force, which satisfies the inverse square law requirement for a Kepler problem.\n\nThe hyperbolic trig function formula_109 appears as one solution to the Korteweg–de Vries equation which describes the motion of a soliton wave in a canal.\n\nAs shown first by Apollonius of Perga, a hyperbola can be used to trisect any angle, a well studied problem of geometry. Given an angle, first draw a circle centered at its vertex O, which intersects the sides of the angle at points A and B. Next draw the line segment with endpoints A and B and its perpendicular bisector formula_110. Construct a hyperbola of eccentricity \"e\"=2 with formula_110 as directrix and B as a focus. Let P be the intersection (upper) of the hyperbola with the circle. Angle POB trisects angle AOB. To prove this, reflect the line segment OP about the line formula_110 obtaining the point P' as the image of P. Segment AP' has the same length as segment BP due to the reflection, while segment PP' has the same length as segment BP due to the eccentricity of the hyperbola. As OA, OP', OP and OB are all radii of the same circle (and so, have the same length), the triangles OAP', OPP' and OPB are all congruent. Therefore, the angle has been trisected, since 3×POB = AOB.\n\nIn portfolio theory, the locus of mean-variance efficient portfolios (called the efficient frontier) is the upper half of the east-opening branch of a hyperbola drawn with the portfolio return's standard deviation plotted horizontally and its expected value plotted vertically; according to this theory, all rational investors would choose a portfolio characterized by some point on this locus.\n\nThe three-dimensional analog of a hyperbola is a hyperboloid. Hyperboloids come in two varieties, those of one sheet and those of two sheets. A simple way of producing a hyperboloid is to rotate a hyperbola about the axis of its foci or about its symmetry axis perpendicular to the first axis; these rotations produce hyperboloids of two and one sheet, respectively.\n\n", "id": "14052", "title": "Hyperbola"}
{"url": "https://en.wikipedia.org/wiki?curid=14055", "text": "Humayun\n\nIn December 1530 Humayun succeeded his father as ruler of the Mughal territories in the Indian subcontinent. At the age of 23, Humayun was an inexperienced ruler when he came to power. His half-brother Kamran Mirza inherited Kabul and Lahore, the northernmost parts of their father's empire. Mirza was to become a bitter rival of Humayun.\n\nHumayun lost Mughal territories to the Pashtun noble, Sher Shah Suri, but regained them 15 years later with Safavid aid. Humayun's return from Persia was accompanied by a large retinue of Persian noblemen and signalled an important change in Mughal court culture. The Central Asian origins of the dynasty were largely overshadowed by the influences of Persian art, architecture, language and literature. There are many stone carvings and thousands of Persian manuscripts in India dating from the time of Humayun.\n\nSubsequently, Humayun further expanded the Empire in a very short time, leaving a substantial legacy for his son, Akbar. His peaceful personality, patience and non-provocative methods of speech earned him the title \"’Insān-i-Kamil\" (Perfect Man), among the Mughals.\n\nThe decision of Babur to divide the territories of his empire between two of his sons was unusual in India, although it had been a common Central Asian practice since the time of Genghis Khan. Unlike most monarchies, which practised primogeniture, the Timurids followed the example of Genghis and did not leave an entire kingdom to the eldest son. Although under that system only a Chingissid could claim sovereignty and khanal authority, any male Chinggisid within a given sub-branch had an equal right to the throne (though the Timurids were not Chinggisid in their paternal ancestry). While Genghis Khan's Empire had been peacefully divided between his sons upon his death, almost every Chinggisid succession since had resulted in fratricide.\n\nTimur himself had divided his territories among Pir Muhammad, Miran Shah, Khalil Sultan and Shah Rukh, which resulted in inter-family warfare. Upon Babur's death, Humayun's territories were the least secure. He had ruled only four years, and not all \"umarah\" (nobles) viewed Humayun as the rightful ruler. Indeed, earlier, when Babur had become ill, some of the nobles had tried to install his uncle, Mahdi Khwaja, as ruler. Although this attempt failed, it was a sign of problems to come.\n\nWhen Humayun came to the throne of the Mughal Empire, several of his brothers revolted against him. Another brother Khalil Mirza (1509–30) supported Humayun but was assassinated. The Emperor commenced construction of a tomb for his brother in 1538, but this was not yet finished when Humayun was forced to flee to Persia. Sher Shah destroyed the structure and no further work was done on it after Humayun's restoration.\n\nHumayun had two major rivals for his lands: Sultan Bahadur of Gujarat to the southwest and Sher Shah Suri (Sher Khan) settled along the river Ganges in Bihar to the east. Humayun's first campaign was to confront Sher Shah Suri. Halfway through this offensive Humayun had to abandon it and concentrate on Gujarat, where a threat from Ahmed Shah had to be met. Humayun was victorious annexing Gujarat, Malwa, Champaner and the great fort of Mandu.\n\nDuring the first five years of Humayun's reign, Bahadur and Sher Khan extended their rule, although Sultan Bahadur faced pressure in the east from sporadic conflicts with the Portuguese. While the Mughals had obtained firearms via the Ottoman Empire, Bahadur's Gujarat had acquired them through a series of contracts drawn up with the Portuguese, allowing the Portuguese to establish a strategic foothold in north western India.\n\nIn 1535 Humayun was made aware that the Sultan of Gujarat was planning an assault on the Mughal territories with Portuguese aid. Humayun gathered an army and marched on Bahadur. Within a month he had captured the forts of Mandu and Champaner. However, instead of pressing his attack, Humayun ceased the campaign and consolidated his newly conquered territory. Sultan Bahadur, meanwhile escaped and took up refuge with the Portuguese.\n\nShortly after Humayun had marched on Gujarat, Sher Shah Suri saw an opportunity to wrest control of Agra from the Mughals. He began to gather his army together hoping for a rapid and decisive siege of the Mughal capital. Upon hearing this alarming news, Humayun quickly marched his troops back to Agra allowing Bahadur to easily regain control of the territories Humayun had recently taken. In February 1537, however, Bahadur was killed when a botched plan to kidnap the Portuguese viceroy ended in a fire-fight that the Sultan lost.\n\nWhilst Humayun succeeded in protecting Agra from Sher Shah, the second city of the Empire, Gaur the capital of the \"vilayat\" of Bengal, was sacked. Humayun's troops had been delayed while trying to take Chunar, a fort occupied by Sher Shah's son, in order to protect his troops from an attack from the rear. The stores of grain at Gauri, the largest in the empire, were emptied, and Humayun arrived to see corpses littering the roads. The vast wealth of Bengal was depleted and brought East, giving Sher Shah a substantial war chest.\n\nSher Shah withdrew to the east, but Humayun did not follow: instead he \"shut himself up for a considerable time in his Harem, and indulged himself in every kind of luxury.\" Hindal, Humayun's 19-year-old brother, had agreed to aid him in this battle and protect the rear from attack, but he abandoned his position and withdrew to Agra, where he decreed himself acting emperor. When Humayun sent the grand \"Mufti\", Sheikh Buhlul, to reason with him; the Sheikh was killed. Further provoking the rebellion, Hindal ordered that the \"Khutba\", or sermon, in the main mosque at Agra be read in his name, a sign of assumption of sovereignty. When Hindal withdrew from protecting the rear of Humayun's troops, Sher Shah's troop quickly reclaimed these positions, leaving Humayun surrounded.\n\nHumayun's other brother, Kamran Mirza, marched from his territories in the Punjab, ostensibly to aid Humayun. However, his return home had treacherous motives as he intended to stake a claim for Humayun's apparently collapsing empire. He brokered a deal with Hindal providing that his brother would cease all acts of disloyalty in return for a share in the new empire, which Kamran would create once Humayun was deposed.\n\nIn June 1539 Sher Shah met Humayun in the Battle of Chausa on the banks of the Ganges, near Buxar. This was to become an entrenched battle in which both sides spent a lot of time digging themselves into positions. The major part of the Mughal army, the artillery, was now immobile, and Humayun decided to engage in some diplomacy using Muhammad Aziz as ambassador. Humayun agreed to allow Sher Shah to rule over Bengal and Bihar, but only as provinces granted to him by his Emperor, Humayun, falling short of outright sovereignty. The two rulers also struck a bargain in order to save face: Humayun's troops would charge those of Sher Shah whose forces then retreat in feigned fear. Thus honour would, supposedly, be satisfied.\n\nOnce the Army of Humayun had made its charge and Sher Shah's troops made their agreed-upon retreat, the Mughal troops relaxed their defensive preparations and returned to their entrenchments without posting a proper guard. Observing the Mughals' vulnerability, Sher Shah reneged on his earlier agreement. That very night, his army approached the Mughal camp and finding the Mughal troops unprepared with a majority asleep, they advanced and killed most of them. The Emperor survived by swimming across the Ganges using an air filled \"water skin,\" and quietly returned to Agra. Humayun was assisted across the Ganges by Shams al-Din Muhammad.\n\nWhen Humayun returned to Agra, he found that all three of his brothers were present. Humayun once again not only pardoned his brothers for plotting against him, but even forgave Hindal for his outright betrayal. With his armies travelling at a leisurely pace, Sher Shah was gradually drawing closer and closer to Agra. This was a serious threat to the entire family, but Humayun and Kamran squabbled over how to proceed. Kamran withdrew after Humayun refused to make a quick attack on the approaching enemy, instead opting to build a larger army under his own name.\n\nWhen Kamran returned to Lahore, his troops followed him shortly afterwards, and Humayun, with his other brothers Askari and Hindal, marched to meet Sher Shah just east of Agra at the battle of Kannauj on 17 May 1540. Humayun once again made some tactical errors, and his army was soundly defeated. He and his brothers quickly retreated back to Agra, but they chose not to stay and retreated to Lahore, since Sher Shah followed them. The founding of the short-lived Sur Dynasty (which contained only him and his son) of northern India, with its capital at Delhi, resulted in Humayun's exile for 15 years in the court of Shah Tahmasp I.\n\nThe four brothers were united in Lahore, but every day they were informed that Sher Shah was getting closer and closer. When he reached Sirhind, Humayun sent an ambassador carrying the message \"I have left you the whole of Hindustan (\"i.e.\" the lands to the East of Punjab, comprising most of the Ganges Valley). Leave Lahore alone, and let Sirhind be a boundary between you and me.\" Sher Shah, however, replied \"I have left you Kabul. You should go there.\" Kabul was the capital of the empire of Humayun's brother Kamran, who was far from willing to hand over any of his territories to his brother. Instead, Kamran approached Sher Shah and proposed that he actually revolt against his brother and side with Sher Shah in return for most of the Punjab. Sher Shah dismissed his help, believing it not to be required, though word soon spread to Lahore about the treacherous proposal, and Humayun was urged to make an example of Kamran and kill him. Humayun refused, citing the last words of his father, Babur, \"Do nothing against your brothers, even though they may deserve it.\"\n\nHumayun decided it would be wise to withdraw still further. He and his army rode out through and across the Thar Desert, when the Hindu ruler Rao Maldeo Rathore allied with Sher Shah Suri against the Mughal Empire. In many accounts Humayun mentions how he and his pregnant wife had to trace their steps through the desert at the hottest time of year. All the wells had been filled with sand by the nearby Hindu inhabitants in order to starve and further exhaust the Mughals, leaving them with nothing but berries to eat. When Hamida's horse died, no one would lend the Queen (who was now eight months pregnant) a horse, so Humayun did so himself, resulting in him riding a camel for six kilometrees (four miles), although Khaled Beg then offered him his mount. Humayun was later to describe this incident as the lowest point in his life.\n\nHe asked that his brothers join him as he fell back into Sindh. While the previously rebellious Hindal Mirza remained loyal and was ordered to join his brothers in Kandahar. Kamran Mirza and Askari Mirza instead decided to head to the relative peace of Kabul. This was to be a definitive schism in the family.\n\nHumayun expected aid from the Emir of Sindh, Hussein Umrani, whom he had appointed and who owed him his allegiance. The Emir Hussein Umrani welcomed Humayun's presence and was loyal to Humayun just as he had been loyal to Babur against the renegade Arghuns. Whilst in the oasis garrison of Umerkot in Sindh, Hamida daughter of noble Sindhi, gave birth to Akbar on 15 October 1542, the heir-apparent to the 34-year-old Humayun. The date was special because Humayun consulted his Astronomer to utilise the astrolabe and check the location of the planets.\n\nWhile in Sindh, Humayun alongside Emir Hussein Umrani, gathered horses and weapons and formed new alliances that helped regain lost territories. Until finally Humayun had gathered hundreds of Sindhi and Baloch tribesmen alongside his Mughals and then marched towards Kandahar and later Kabul, thousands more gathered by his side as Humayun continually declared himself the rightful Timurid heir of the first Mughal Emperor, Babur.\n\nAfter Humayun set out from his expedition in Sindh, along with 300 camels (mostly wild) and 2000 loads of grain, he set off to join his brothers in Kandahar after crossing the Indus River on 11 July 1543 along with the ambition to regain the Mughal Empire and overthrow the Suri dynasty. Among the tribes that had sworn allegiance to Humayun were the Magsi, Rind and many others.\n\nIn Kamran Mirza's territory, Hindal Mirza had been placed under house arrest in Kabul after refusing to have the \"Khutba\" recited in Kamran Mirza's name. His other brother Askari Mirza was now ordered to gather an army and march on Humayun. When Humayun received word of the approaching hostile army he decided against facing them, and instead sought refuge elsewhere. Akbar was left behind in camp close to Kandahar for, as it was December it would have been too cold and dangerous to include the 14-month-old toddler in the forthcoming march through the dangerous and snowy mountains of the Hindu Kush. Askari Mirza found Akbar in the camp, and embraced him, and allowed his own wife to parent him, she apparently started treating him as her own.\n\nOnce again Humayun turned toward Kandahar where his brother Kamran Mirza was in power, but he received no help and had to seek refuge with the Shah of Persia.\n\nHumayun fled to the refuge of the Safavid Empire in Persia, marching with 40 men and his wife and her companion through mountains and valleys. Amongst other trials the Imperial party were forced to live on horse meat boiled in the soldiers' helmets. These indignities continued during the month it took them to reach Herat, however after their arrival they were reintroduced to the finer things in life. Upon entering the city his army was greeted with an armed escort, and they were treated to lavish food and clothing. They were given fine accommodations and the roads were cleared and cleaned before them. Shah Tahmasp, unlike Humayun's own family, actually welcomed the Mughal, and treated him as a royal visitor. Here Humayun went sightseeing and was amazed at the Persian artwork and architecture he saw: much of this was the work of the Timurid Sultan Husayn Bayqarah and his ancestor, princess Gauhar Shad, thus he was able to admire the work of his relatives and ancestors at first hand.\n\nHe was introduced to the work of the Persian miniaturists, and Kamaleddin Behzad had two of his pupils join Humayun in his court. Humayun was amazed at their work and asked if they would work for him if he were to regain the sovereignty of Hindustan: they agreed. With so much going on Humayun did not even meet the Shah until July, some six months after his arrival in Persia. After a lengthy journey from Herat the two met in Qazvin where a large feast and parties were held for the event. The meeting of the two monarchs is depicted in a famous wall-painting in the Chehel Sotoun (Forty Columns) palace in Esfahan.\n\nThe Shah urged that Humayun convert from Sunni to Shia Islam, and Humayun eventually and reluctantly accepted, in order to keep himself and several hundred followers alive. Although the Mughals initially disagreed to their conversion they knew that with this outward acceptance of Shi'ism, Shah Tahmasp was eventually prepared to offer Humayun more substantial support. When Humayun's brother, Kamran Mirza, offered to cede Kandahar to the Persians in exchange for Humayun, dead or alive, Shah Tahmasp refused. Instead the Shah staged a celebration for Humayun, with 300 tents, an imperial Persian carpet, 12 musical bands and \"meat of all kinds\". Here the Shah announced that all this, and 12,000 elite cavalry were his to lead an attack on his brother Kamran. All that Shah Tahmasp asked for was that, if Humayun's forces were victorious, Kandahar would be his.\n\nWith this Persian Safavid aid Humayun took Kandahar from Askari Mirza after a two-week siege. He noted how the nobles who had served Askari Mirza quickly flocked to serve him, \"in very truth the greater part of the inhabitants of the world are like a flock of sheep, wherever one goes the others immediately follow\". Kandahar was, as agreed, given to the Shah of Persia who sent his infant son, Murad, as the Viceroy. However, the baby soon died and Humayun thought himself strong enough to assume power.\n\nHumayun now prepared to take Kabul, ruled by his brother Kamran Mirza. In the end, there was no actual siege. Kamran Mirza was detested as a leader and as Humayun's Persian army approached the city hundreds of Kamran Mirza's troops changed sides, flocking to join Humayun and swelling his ranks. Kamran Mirza absconded and began building an army outside the city. In November 1545, Hamida and Humayun were reunited with their son Akbar, and held a huge feast. They also held another, larger, feast in the child's honour when he was circumcised.\n\nHowever, while Humayun had a larger army than his brother and had the upper hand, on two occasions his poor military judgement allowed Kamran Mirza to retake Kabul and Kandahar, forcing Humayun to mount further campaigns for their recapture. He may have been aided in this by his reputation for leniency towards the troops who had defended the cities against him, as opposed to Kamran Mirza, whose brief periods of possession were marked by atrocities against the inhabitants who, he supposed, had helped his brother.\n\nHis youngest brother, Hindal Mirza, formerly the most disloyal of his siblings, died fighting on his behalf. His brother Askari Mirza was shackled in chains at the behest of his nobles and aides. He was allowed go on Hajj, and died en route in the desert outside Damascus.\n\nHumayun's other brother, Kamran Mirza, had repeatedly sought to have Humayun killed. In 1552 Kamran Mirza attempted to make a pact with Islam Shah, Sher Shah's successor, but was apprehended by a Gakhar. The Gakhars were one of the minority of tribal groups who had consistently remained loyal to their oath to the Mughals. Sultan Adam of the Gakhars handed Kamran Mirza over to Humayun. Humayun was inclined to forgive his brother. However he was warned that allowing Kamran Mirza's repeated acts of treachery to go unpunished could foment rebellion amongst his own supporters. So, instead of killing his brother, Humayun had Kamran Mirza blinded which would end any claim by the latter to the throne. Humayun sent Kamran Mirza on Hajj, as he hoped to see his brother thereby absolved of his offences. However Kamran Mirza died close to Mecca in the Arabian Peninsula in 1557.\n\nSher Shah Suri had died in 1545; his son and successor Islam Shah died too, in 1554. These two deaths left the dynasty reeling and disintegrating. Three rivals for the throne all marched on Delhi, while in many cities leaders tried to stake a claim for independence. This was a perfect opportunity for the Mughals to march back to India.\n\nThe Mughal Emperor Humayun, gathered a vast army and attempted the challenging task of retaking the throne in Delhi. Humayun placed the army under the able leadership of Bairam Khan. This was a wise move given Humayun's own record of military ineptitude, and turned out to be prescient, as Bairam was to prove himself a great tactician.\n\nThe \"Gazetteer of Ulwur\" states:\n\nBairam Khan led the army through the Punjab virtually unopposed. The fort of Rohtas, which was built in 1541–43 by Sher Shah Suri to crush the Gakhars who were loyal to Humayun, was surrendered without a shot by a treacherous commander. The walls of the Rohtas Fort measure up to 12.5 meters in thickness and up to 18.28 meters in height. They extend for 4 km and feature 68 semi-circular bastions. Its sandstone gates, both massive and ornate, are thought to have exerted a profound influence on Mughal military architecture.\n\nThe only major battle faced by Humayun's armies was against Sikander Suri in Sirhind, where Bairam Khan employed a tactic whereby he engaged his enemy in open battle, but then retreated quickly in apparent fear. When the enemy followed after them they were surprised by entrenched defensive positions and were easily annihilated.\n\nFrom here on most towns and villages chose to welcome the invading army as it made its way to the capital. On 23 July 1555, Humayun once again sat on Babur's throne in Delhi.\n\nWith all of Humayun's brothers now dead, there was no fear of another usurping his throne during his military campaigns. He was also now an established leader, and could trust his generals. With this new-found strength Humayun embarked on a series of military campaigns aimed at extending his reign over areas in eastern and western India. His sojourn in exile seems to have reduced Humayun's reliance on astrology, and his military leadership came to imitate the more effective methods that he had observed in Persia.\n\nAfter defeating Bahadur Shah's confederacy in Gujarat, Humayun placed the following generals in Gujarat:\n\n\nIn the year 1540, Humayun met the Ottoman Admiral Seydi Ali Reis. During their discussions in the Durbar, Humayun asked which of the two empires was bigger and Seydi Ali Reis, stated that the Ottoman Empire was \"ten times bigger\", Humayun was very inspired and he turned towards his nobles and remarked without resentment: \"Indeed Suleiman the Magnificent, deserves to be called the only Padshah on Earth\".\n\nOn 27 January 1556, Humayun, with his arms full of books, was descending the staircase from his library when the muezzin announced the Azaan (the call to prayer). It was his habit, wherever he heard the summons, to bow his knee in holy reverence. Trying to kneel, he caught his foot in his robe, tumbled down several steps and hit his temple on a rugged stone edge. He died three days later. His body was laid to rest in Purana Quila initially, but because of attack by Hemu on Delhi and capture of Purana Qila, Humayun's body was exhumed by the fleeing army and transferred to Kalanaur in Punjab where Akbar was crowned. His tomb stands in Delhi, where he was later buried in a grand way.\n\nHis full title as Emperor of the Mughal Empire was \"Al-Sultan al-'Azam wal Khaqan al-Mukarram, Jam-i-Sultanat-i-haqiqi wa Majazi, Sayyid al-Salatin, Abu'l Muzaffar Nasir ud-din Muhammad Humayun Padshah Ghazi, Zillu'llah\"\n\n\n\n", "id": "14055", "title": "Humayun"}
{"url": "https://en.wikipedia.org/wiki?curid=14056", "text": "Prince-elector\n\nThe prince-electors (or simply electors) of the Holy Roman Empire ( (), pl. \"Kurfürsten\", , ) were the members of the electoral college of the Holy Roman Empire.\n\nFrom the 13th century onwards, the Prince-Electors had the privilege of electing the King of the Romans, who would be crowned by the Pope as Holy Roman Emperor. Charles V was the last to be a crowned Emperor (elected 1519, crowned 1530); his successors were elected Emperors directly by the electoral college, each being titled \"Elected Emperor of the Romans\" (; ). In practice, all but one Emperor (Charles VII from the House of Wittelsbach) from 1440 onwards came from the Austrian House of Habsburg, and the Electors merely ratified the Habsburg succession.\n\nThe dignity of Elector carried great prestige and was considered to be second only to that of King or Emperor. The Electors had exclusive privileges that were not shared with the other princes of the Empire, and they continued to hold their original titles alongside that of Elector. The heir apparent to a secular prince-elector was known as an electoral prince ().\n\nThe German practice of electing monarchs began when ancient Germanic tribes formed \"ad hoc\" coalitions and elected the leaders thereof. Elections were irregularly held by the Franks, whose successor states include France and the Holy Roman Empire. The French monarchy eventually became hereditary, but the Holy Roman Emperors remained elective, at least in theory, although the Habsburgs provided most of the later monarchs. While all free men originally exercised the right to vote in such elections, suffrage eventually came to be limited to the leading men of the realm. In the election of Lothar II in 1125, a small number of eminent nobles chose the monarch and then submitted him to the remaining magnates for their approbation.\n\nSoon, the right to choose the monarch was settled on an exclusive group of princes, and the procedure of seeking the approval of the remaining nobles was abandoned. The college of electors was mentioned in 1152 and again in 1198. The composition of electors at that time is unclear, but appears to have included representatives of the church and the dukes of the four nations of Germany: the Franks (Duchy of Franconia), Swabians (Duchy of Swabia), Saxons (Duchy of Saxony) and Bavarians (Duchy of Bavaria).\n\nA letter of Pope Urban IV suggests that by \"immemorial custom\", seven princes had the right to elect the King and future Emperor. The seven have been mentioned as the vote-casters in the election of 1257 that resulted in two kings becoming elected.\n\n\nThe three Archbishops oversaw the most venerable and powerful sees in Germany, while the other four were supposed to represent the dukes of the four nations. The Count Palatine of the Rhine held most of the former Duchy of Franconia after the last Duke died in 1039. The Margrave of Brandenburg became an Elector when the Duchy of Swabia was dissolved after the last Duke of Swabia was beheaded in 1268. Saxony, even with diminished territory, retained its eminent position.\n\nThe Palatinate and Bavaria were originally held by the same individual, but in 1253, they were divided between two members of the House of Wittelsbach. The other electors refused to allow two princes from the same dynasty to have electoral rights, so a heated rivalry arose between the Count Palatine and the Duke of Bavaria over who should hold the Wittelsbach seat.\n\nMeanwhile, the King of Bohemia, who held the ancient imperial office of Arch-Cupbearer, asserted his right to participate in elections. Sometimes he was challenged on the grounds that his kingdom was not German, though usually he was recognized, instead of Bavaria which after all was just a younger line of Wittelsbachs.\n\nThe Declaration of Rhense issued in 1338 had the effect that election by the majority of the electors automatically conferred the royal title and rule over the empire, without papal confirmation. The Golden Bull of 1356 finally resolved the disputes among the electors. Under it, the Archbishops of Mainz, Trier, and Cologne, as well as the King of Bohemia, the Count Palatine of the Rhine, the Duke of Saxony, and the Margrave of Brandenburg held the right to elect the King.\n\nThe college's composition remained unchanged until the 17th century, although the Electorate of Saxony was transferred from the senior to the junior branch of the Wettin family in 1547, in the aftermath of the Schmalkaldic War.\n\nIn 1621, the Elector Palatine, Frederick V, came under the imperial ban after participating in the Bohemian Revolt (a part of the Thirty Years' War). The Elector Palatine's seat was conferred on the Duke of Bavaria, the head of a junior branch of his family. Originally, the Duke held the electorate personally, but it was later made hereditary along with the duchy. When the Thirty Years' War concluded with the Peace of Westphalia in 1648, a new electorate was created for the Count Palatine of the Rhine. Since the Elector of Bavaria retained his seat, the number of electors increased to eight; the two Wittelsbach lines now sufficiently estranged so as not to pose a combined potential threat.\n\nIn 1685, the religious composition of the College of Electors was disrupted when a Catholic branch of the Wittelsbach family inherited the Palatinate. A new Protestant electorate was created in 1692 for the Duke of Brunswick-Lüneburg, who became known as the Elector of Hanover (the Imperial Diet officially confirmed the creation in 1708). The Elector of Saxony converted to Catholicism in 1697 so that he could become King of Poland, but no additional Protestant electors were created. Although the Elector of Saxony was personally Catholic, the Electorate itself remained officially Protestant, and the Elector even remained the leader of the Protestant body in the Reichstag.\n\nIn 1706, the Elector of Bavaria and Archbishop of Cologne were banned during the War of the Spanish Succession, but both were restored in 1714 after the Peace of Baden. In 1777, the number of electors was reduced to eight when the Elector Palatine inherited Bavaria.\n\n\nMany changes to the composition of the college were necessitated by Napoleon's aggression during the early 19th century. The Treaty of Lunéville (1801), which ceded territory on the Rhine's left bank to France, led to the abolition of the archbishoprics of Trier and Cologne, and the transfer of the remaining spiritual Elector from Mainz to Regensburg. In 1803, electorates were created for the Duke of Württemberg, the Margrave of Baden, the Landgrave of Hesse-Kassel, and the Duke of Salzburg, bringing the total number of electors to ten. When Austria annexed Salzburg under the Treaty of Pressburg (1805), the Duke of Salzburg moved to the Grand Duchy of Würzburg and retained his electorate. None of the new electors, however, had an opportunity to cast votes, as the Holy Roman Empire was abolished in 1806, and the new electorates were never confirmed by the Emperor.\n\nAfter the abolition of the Holy Roman Empire in August 1806, the Electors continued to reign over their territories, many of them taking higher titles. The Electors of Bavaria, Württemberg, and Saxony styled themselves Kings, while the Electors of Baden, Hesse-Darmstadt, Regensburg, and Würzburg became Grand Dukes. The Elector of Hesse-Kassel, however, retained the meaningless title \"Elector of Hesse\", thus distinguishing himself from other Hessian princes (the Grand Duke of Hesse-Darmstadt and the Landgrave of Hesse-Homburg). Napoleon soon exiled him and Kassel was annexed to the Kingdom of Westphalia, a new creation. The King of Great Britain remained at war with Napoleon and continued to style himself Elector of Hanover, while the Hanoverian government continued to operate in London.\n\nThe Congress of Vienna accepted the Electors of Bavaria, Württemberg, and Saxony as Kings, along with the newly created Grand Dukes. The Elector of Hanover finally joined his fellow Electors by declaring himself the King of Hanover. The restored Elector of Hesse, a Napoleonic creation, tried to be recognized as the King of the Chatti. However, the European powers refused to acknowledge this title at the Congress of Aix-la-Chapelle (1818) and instead listed him with the grand dukes as a \"Royal Highness.\" Believing the title of Prince-Elector to be superior in dignity to that of Grand Duke, the Elector of Hesse-Kassel chose to remain an Elector, even though there was no longer a Holy Roman Emperor to elect. Hesse-Kassel remained the only Electorate in Germany until 1866, when the country backed the losing side in the Austro-Prussian War and was absorbed into Prussia.\n\nThe German element \"Kur-\" is based on the Middle High German irregular verb \"kiesen\" and is related etymologically to the English word \"choose\" (cf. Old English \"ceosan\" , participle \"coren\" 'having been chosen' and Gothic \"kiusan\"). In English, the \"s\"/\"r\" mix in the Germanic verb conjugation has been regularized to \"s\" throughout, while German retains the \"r\" in \"Kur-\". There is also a modern German verb \"küren\" which means 'to choose' in a ceremonial sense. \"Fürst\" is German for 'prince', but while the German language distinguishes between the head of a principality (\"der Fürst\") and the son of a monarch (\"der Prinz\"), English uses \"prince\" for both concepts. \"Fürst\" itself is related to English \"first\" and is thus the 'foremost' person in his realm. Note that 'prince' derives from Latin \"princeps\", which carried the same meaning.\n\nElectors were \"reichsstände\" (Imperial Estates), enjoying precedence over the other princes. They were, until the 18th century, exclusively entitled to be addressed with the title \"Durchlaucht\" (Serene Highness). In 1742, the electors became entitled to the superlative \"Durchläuchtigste\" (Most Serene Highness), while other princes were promoted to \"Durchlaucht\".\n\nAs Imperial Estates, the electors enjoyed all the privileges of the other princes enjoying that status, including the right to enter into alliances, autonomy in relation to dynastic affairs and precedence over other subjects. The Golden Bull had granted them the Privilegium de non appellando, which prevented their subjects from lodging an appeal to a higher Imperial court. However, while this privilege, and some others, were automatically granted to Electors, they were not exclusive to them and many of the larger Imperial Estates were also to be individually granted some or all those rights and privileges.\n\nThe electors, like the other princes ruling States of the Empire, were members of the Imperial Diet, which was divided into three \"collegia\": the Council of Electors, the Council of Princes, and the Council of Cities. In addition to being members of the Council of Electors, several lay electors were therefore members of the Council of Princes as well by virtue of other territories they possessed. In many cases, the lay electors ruled numerous States of the Empire, and therefore held several votes in the Council of Princes. In 1792, the King of Bohemia held three votes, the Elector of Bavaria six votes, the Elector of Brandenburg eight votes, and the Elector of Hanover six votes. Thus, of the hundred votes in the Council of Princes in 1792, twenty-three belonged to electors. The lay electors therefore exercised considerable influence, being members of the small Council of Electors and holding a significant number of votes in the Council of Princes. The assent of both bodies was required for important decisions affecting the structure of the Empire, such as the creation of new electorates or States of the Empire.\n\nIn addition to voting by colleges or councils, the Imperial Diet also voted on religious lines, as provided for by the Peace of Westphalia. The Archbishop of Mainz presided over the Catholic body, or \"corpus catholicorum\", while the Elector of Saxony presided over the Protestant body, or \"corpus evangelicorum\". The division into religious bodies was on the basis of the official religion of the state, and not of its rulers. Thus, even when the Electors of Saxony were Catholics during the eighteenth century, they continued to preside over the \"corpus evangelicorum\", since the state of Saxony was officially Protestant.\n\nThe individual chosen by the electors assumed the title \"King of the Romans\", though he actually reigned in Germany. The King of the Romans became Holy Roman Emperor only when crowned by the Pope. On many occasions, a Pope refused to crown a king with whom he was engaged in a dispute, but a lack of a papal coronation deprived a king of only the title Emperor and not of the power to govern (cf Declaration of Rhense). The Habsburg dynasty stopped the practice of papal coronations. After Charles V, all individuals chosen by the electors were merely \"Emperors elect\".\n\nThe electors were originally summoned by the Archbishop of Mainz within one month of an Emperor's death, and met within three months of being summoned. During the \"interregnum\", imperial power was exercised by two imperial vicars. Each vicar, in the words of the Golden Bull, was \"the administrator of the empire itself, with the power of passing judgments, of presenting to ecclesiastical benefices, of collecting returns and revenues and investing with fiefs, of receiving oaths of fealty for and in the name of the holy empire\". The Elector of Saxony was vicar in areas operating under Saxon law (Saxony, Westphalia, Hanover, and northern Germany), while the Elector Palatine was vicar in the remainder of the Empire (Franconia, Swabia, the Rhine, and southern Germany). The Elector of Bavaria replaced the Elector Palatine in 1623, but when the latter was granted a new electorate in 1648, there was a dispute between the two as to which was vicar. In 1659, both purported to act as vicar, but the other vicar recognised the Elector of Bavaria. Later, the two electors made a pact to act as joint vicars, but the Imperial Diet rejected the agreement. In 1711, while the Elector of Bavaria was under the ban of the Empire, the Elector Palatine again acted as vicar, but his cousin was restored to his position upon his restoration three years later. Finally, in 1745, the two agreed to alternate as vicars, with Bavaria starting first. This arrangement was upheld by the Imperial Diet in 1752. In 1777 the question became moot when the Elector Palatine inherited Bavaria. On many occasions, however, there was no interregnum, as a new king had been elected during the lifetime of the previous Emperor.\n\nFrankfurt regularly served as the site of the election from the fifteenth century on, but elections were also held at Cologne (1531), Regensburg (1575 and 1636), and Augsburg (1653 and 1690). An elector could appear in person or could appoint another elector as his proxy. More often, an electoral suite or embassy was sent to cast the vote; the credentials of such representatives were verified by the Archbishop of Mainz, who presided over the ceremony. The deliberations were held at the city hall, but voting occurred in the cathedral. In Frankfurt, a special electoral chapel, or \"Wahlkapelle\", was used for elections. Under the Golden Bull, a majority of electors sufficed to elect a king, and each elector could cast only one vote. Electors were free to vote for whomsoever they pleased (including themselves), but dynastic considerations played a great part in the choice. Electors drafted a \"Wahlkapitulation\", or electoral capitulation, which was presented to the king-elect. The capitulation may be described as a contract between the princes and the king, the latter conceding rights and powers to the electors and other princes. Once an individual swore to abide by the electoral capitulation, he assumed the office of King of the Romans.\n\nIn the 10th and 11th centuries, princes often acted merely to confirm hereditary succession in the Saxon Ottonian dynasty and Franconian Salian dynasty. But with the actual formation of the prince-elector class, elections became more open, starting with the election of Lothair II in 1125. The Staufen dynasty managed to get its sons formally elected in their fathers' lifetimes almost as a formality. After these lines ended in extinction, the electors began to elect kings from different families so that the throne would not once again settle within a single dynasty. For some two centuries, the monarchy was elective both in theory and in practice; the arrangement, however, did not last, since the powerful House of Habsburg managed to secure succession within their dynasty during the fifteenth century. All kings elected from 1438 onwards were from among the Habsburg Archdukes of Austria (and later Kings of Hungary and Bohemia) until 1740, when the archduchy was inherited by a woman, Maria Theresa, sparking the War of the Austrian Succession. A representative of the House of Wittelsbach was elected for a short period of time, but in 1745, Maria Theresa's husband, Francis I of the Habsburg-Lorraine dynasty, became King. All of his successors were also from the same family. Hence, for the greater part of the Empire's history, the role of the electors was largely ceremonial.\n\nEach elector held a \"High Office of the Empire\" (\"Reichserzämter\") and was a member of the (ceremonial) Imperial Household. The three spiritual electors were all Arch-Chancellors (, ): the Archbishop of Mainz was Arch-Chancellor of Germany, the Archbishop of Cologne was Arch-Chancellor of Italy, and the Archbishop of Trier was Arch-Chancellor of Burgundy. The other offices were as follows:\n\nWhen the Duke of Bavaria replaced the Elector Palatine in 1623, he assumed the latter's office of Arch-Steward. When the Count Palatine was granted a new electorate, he assumed the position of Arch-Treasurer of the Empire. When the Duke of Bavaria was banned in 1706, the Elector Palatine returned to the office of Arch-Steward, and in 1710 the Elector of Hanover was promoted to the post of Arch-Treasurer. Matters were complicated by the Duke of Bavaria's restoration in 1714; the Elector of Bavaria resumed the office of Arch-Steward, while the Elector Palatine returned to the post of Arch-Treasurer, and the Elector of Hanover was given the new office of Archbannerbearer. The Electors of Hanover, however, continued to be styled Arch-Treasurers, though the Elector Palatine was the one who actually exercised the office until 1777, when he inherited Bavaria and the Arch-Stewardship. After 1777, no further changes were made to the Imperial Household; new offices were planned for the Electors admitted in 1803, but the Empire was abolished before they could be created. The Duke of Württemberg, however, started to adopt the trappings of the Arch-Bannerbearer.\n\nMany High Officers were entitled to use augmentations on their coats of arms; these augmentations, which were special marks of honour, appeared in the centre of the electors' shields (as shown in the image above) atop the other charges (in heraldic terms, the augmentations appeared in the form of inescutcheons). The Arch-Steward used \"gules an orb Or\" (a gold orb on a red field). The Arch-Marshal utilised the more complicated \"per fess sable and argent, two swords in saltire gules\" (two red swords arranged in the form of a saltire, on a black and white field). The Arch-Chamberlain's augmentation was \"azure a sceptre palewise Or\" (a gold sceptre on a blue field), while the Arch-Treasurer's was \"gules the crown of Charlemagne Or\" (a gold crown on a red field). As noted above, the Elector Palatine and the Elector of Hanover styled themselves Arch-Treasurer from 1714 until 1777; during this time, both electors used the corresponding augmentations. The three Arch-Chancellors and the Arch-Cupbearer did not use any augmentations.\n\nThe electors discharged the ceremonial duties associated with their offices only during coronations, where they bore the crown and regalia of the Empire. Otherwise, they were represented by holders of corresponding \"Hereditary Offices of the Household\". The Arch-Butler was represented by the Butler (Cupbearer) (the Count of Althann), the Arch-Seneschal by the Steward (the Count of Waldburg), the Arch-Chamberlain by the Chamberlain (the Count of Hohenzollern), the Arch-Marshal by the Marshal (the Count of Pappenheim), and the Arch-Treasurer by the Treasurer (the Count of Sinzendorf). The Duke of Württemberg assigned the count of Zeppelin-Aschhausen as hereditary Bannerbearer.\n\nCoat of arms of the states granted the electoral dignity:\n\nThree ecclesiastic/spiritual electors (archbishops):\n\nFour secular electors:\n\nElectors added in 17th century:\n\nDuring the collapse of the Holy Roman Empire, between 1803 and 1806:\n\n\n\n", "id": "14056", "title": "Prince-elector"}
{"url": "https://en.wikipedia.org/wiki?curid=14059", "text": "Howard Hughes\n\nHoward Robard Hughes Jr. (December 24, 1905 – April 5, 1976) was an American entrepreneur, known during his lifetime as one of the most financially successful individuals in the world. He first made a name for himself as a film producer, and then became an influential figure in the aviation industry. Later in life, he became known for his eccentric behavior and reclusive lifestyle, oddities that were caused in part by a worsening obsessive–compulsive disorder (OCD) and chronic pain from a plane crash.\n\nAs a maverick film tycoon, Hughes gained prominence in Hollywood beginning in the late 1920s, when he made big-budget and often controversial films like \"The Racket\" (1928), \"Hell's Angels\" (1930), and \"Scarface\" (1932). Later he controlled the RKO film studio.\n\nHughes formed the Hughes Aircraft Company in 1932, hiring numerous engineers and designers. He spent the rest of the 1930s setting multiple world air speed records and building the Hughes H-1 Racer and H-4 Hercules (the \"Spruce Goose\"). He acquired and expanded Trans World Airlines and later acquired Air West, renaming it Hughes Airwest. Hughes was included in \"Flying\" Magazine's list of the 51 Heroes of Aviation, ranked at 25. Today, his legacy is maintained through the Howard Hughes Medical Institute.\n\nThe birthplace of Howard Hughes is recorded as either Humble or Houston, Texas. The date remains uncertain due to conflicting dates from various sources. He repeatedly claimed that his birthday was on Christmas Eve. A 1941 affidavit birth certificate of Hughes that was signed by his aunt Annette Gano Lummis and Estelle Boughton Sharp states that he was born on December 24, 1905, in Harris County, Texas. However, his certificate of baptism recorded on October 7, 1906, in the parish register of St. John's Episcopal Church in Keokuk, Iowa, listed his birth as September 24, 1905 without any reference to the place of birth.\n\nHughes was the son of Allene Stone Gano and Howard R. Hughes Sr., a successful inventor and businessman from Missouri. He was of English, and some French Huguenot, ancestry, and was a descendant of John Gano, a minister who allegedly baptized George Washington. His father had patented the two-cone roller bit, which allowed rotary drilling for petroleum in previously inaccessible places. The senior Hughes made the shrewd and lucrative decision to commercialize the invention by leasing the bits instead of selling them, and founded the Hughes Tool Company in 1909. Hughes' uncle was the famed novelist, screenwriter, and film director Rupert Hughes.\n\nAt a young age, Hughes demonstrated interest in science and technology. In particular, he had great engineering aptitude and built Houston's first \"wireless\" radio transmitter at age 11. He went on to be one of the first licensed ham radio operators in Houston, having the assigned callsign W5CY (originally 5CY). At 12, Hughes was photographed in the local newspaper, identified as being the first boy in Houston to have a \"motorized\" bicycle, which he had built from parts from his father's steam engine. He was an indifferent student, with a liking for mathematics, flying, and mechanics. He took his first flying lesson at 14, and later attended math and aeronautical engineering courses at Caltech. The red brick house where Hughes lived as a teenager at 3921 Yoakum St., Houston today serves as the headquarters of the Theology Department of the University of St. Thomas. \n\nHis mother Allene died in March 1922 from complications of an ectopic pregnancy. Howard Hughes Sr. died of a heart attack in 1924. Their deaths apparently inspired Hughes to include the creation of a medical research laboratory in the will that he signed in 1925 at age 19. Howard Sr.'s will had not been updated since Allene's death, and Hughes inherited 75% of the family fortune. On his 19th birthday, Hughes was declared an emancipated minor, enabling him to take full control of his life.\n\nFrom a young age, Hughes was an excellent and enthusiastic golfer. He often scored near par figures and played the game to a three handicap during his twenties. He played frequently with top players, including Gene Sarazen. Hughes rarely played competitively and gradually gave up his passion for the sport to pursue other interests.\n\nHughes withdrew from Rice University shortly after his father's death. On June 1, 1925, he married Ella Botts Rice, daughter of David Rice and Martha Lawson Botts of Houston. They moved to Los Angeles, where he hoped to make a name for himself as a filmmaker.\n\nHughes enjoyed a highly successful business career beyond engineering, aviation, and filmmaking, though many of his career endeavors involved varying entrepreneurial roles. The Summa Corporation was the name adopted for the business interests of Howard Hughes after he sold the tool division of Hughes Tool Company in 1972. The company serves as the principal holding company for Hughes' business ventures and investments. It is primarily involved in aerospace and defense, electronics, mass media, manufacturing, and hospitality industries, but has maintained a strong presence in a wide variety of industries including real estate, petroleum drilling and oilfield services, consulting, entertainment, and engineering. Much of his fortune was later used for philanthropic causes, notably towards health care and medical research.\n\nHughes entered the entertainment industry after dropping out of Rice University and moving to Los Angeles. His first two films, \"Everybody's Acting\" (1927) and \"Two Arabian Knights\" (1928), were financial successes, the latter winning the first Academy Award for Best Director of a comedy picture. \"The Racket\" (1928) and \"The Front Page\" (1931) were also nominated for Academy Awards.\n\nHughes spent $3.8 million to make the flying film \"Hell's Angels\" (1930). It earned nearly $8 million, about double the production and advertising costs. \"Hell's Angels\" received one Academy Award nomination for Best Cinematography. He produced another hit, \"Scarface\" (1932), a production delayed by censors' concern over its violence.\n\n\"The Outlaw\" (1943) was completed in 1941 and featured Jane Russell. It also received considerable attention from industry censors, this time owing to Russell's revealing costumes. Hughes designed a special bra for his leading lady, although Russell said it was uncomfortable and decided against wearing it.\n\nDuring the 1940s to the late 1950s, The Hughes Tool Company ventured into the film industry when it obtained partial ownership of the RKO companies which included RKO Pictures, RKO Studios, a chain of movie theaters known as RKO Theatres and a network of radio stations known as the RKO Radio Network.\n\nIn 1948, Hughes gained control of RKO, a struggling major Hollywood studio, by acquiring 25% of the outstanding stock from Floyd Odlum's Atlas Corporation. Within weeks of acquiring the studio, Hughes dismissed three-quarters of the work force and production was shut down for six months during which time investigations were conducted of each employee who remained with RKO as far as their political leanings were concerned. Only after ensuring that the stars under contract to RKO had no suspect affiliations would Hughes approve completed pictures to be sent back for re-shooting. This was especially true of the women who were under contract to RKO at that time. If Hughes felt that his stars did not properly represent the political views of his liking or if a film's anti-communist politics were not sufficiently clear, he pulled the plug. In 1952, an abortive sale to a Chicago-based group connected to the mafia with no experience in the industry also disrupted studio operations at RKO even further.\n\nIn 1953, Hughes was involved with a high profile lawsuit as part of the settlement of the \"United States v. Paramount Pictures, Inc.\" Antitrust Case. As a result of the hearings, the shaky status of RKO became increasingly apparent. A steady stream of lawsuits from RKO's minority shareholders had grown to be extremely annoying to Hughes. They had accused him with financial misconduct and corporate mismanagement. Since Hughes wanted to focus primarily on his aircraft manufacturing and TWA holdings during the Korean War years, Hughes offered to buy out all other stockholders in order to dispense with their distractions.\n\nHe had gained near-total control of RKO by the end of 1954 at a cost of nearly $24 million, becoming the closest thing to a sole owner of a Hollywood studio seen in three decades. Six months later, Hughes sold the studio to the General Tire and Rubber Company for $25 million. Hughes retained the rights to pictures that he had personally produced, including those made at RKO. He also retained Jane Russell's contract. For Howard Hughes, this was the virtual end of his 25-year involvement in the motion picture industry. However, his reputation as a financial wizard emerged unscathed. During that time period, RKO became known as the home of film noir classic productions thanks in part to the limited budgets required to make such films during Hughes' tenure. Hughes reportedly walked away from RKO having made $6.5 million in personal profit.\n\nGeneral Tire was interested mainly in exploiting the value of the RKO library for television programming even though it made some attempts to continue producing films. After a year and a half of mixed success, General Tire shut down film production entirely at RKO at the end of January 1957. The studio lots in Hollywood and Culver City were sold to Desilu Productions later that year for $6.15 million.\n\nBeyond extending his business prowess in the manufacturing, aviation, entertainment, and hospitality industries, Hughes was a successful real estate investor. Hughes was deeply involved in the American real estate industry where he amassed vast holdings of undeveloped land both in Las Vegas and in the desert surrounding the city that had gone unused during his lifetime. In 1968, the Hughes Tool Company purchased the North Las Vegas Air Terminal.\n\nOriginally known as Summa Corporation, The Howard Hughes Corporation was formed in 1972 when the oil tools business of Hughes Tool Company, then owned by Howard Hughes Jr., was floated on the New York Stock Exchange under the Hughes Tool name. This forced the remaining businesses of the \"original\" Hughes Tool to adopt a new corporate name Summa. The name \"Summa\"Latin for \"highest\"was adopted without the approval of Hughes himself, who preferred to keep his own name on the business, and suggested HRH Properties (for Hughes Resorts and Hotels, and also his own initials).\n\nInitially staying in the Desert Inn, Hughes refused to vacate his room, and instead decided to purchase the entire hotel. Hughes extended his financial empire to include Las Vegas real estate, hotels and media outlets, spending an estimated $300 million, and using his considerable powers to take-over many of the well known hotels, especially the organized crime connected venues. He quickly became one of the most powerful men in Las Vegas. He was instrumental in changing the image of Las Vegas from its Wild West roots into a more refined cosmopolitan city.\n\nAnother portion of Hughes' business interests lay in aviation, airlines, and the aerospace and defense industries. Hughes was a lifelong aircraft enthusiast and pilot. He survived four airplane accidents: one while filming \"Hell's Angels\", one while setting the air speed record in the Hughes Racer, one at Lake Mead in 1943, and the near fatal crash of the Hughes XF-11 in 1946. At Rogers Airport in Los Angeles, he learned to fly from pioneer aviators, including Moye Stephens. He set many world records and commissioned the construction of custom aircraft for himself while heading Hughes Aircraft at the airport in Glendale, CA. Operating from there, the most technologically important aircraft he commissioned was the Hughes H-1 Racer. On September 13, 1935, Hughes, flying the H-1, set the landplane airspeed record of over his test course near Santa Ana, California (Giuseppe Motta reached 362 mph in 1929 and George Stainforth reached 407.5 mph in 1931, both in seaplanes). This was the last time in history that the world airspeed record was set in an aircraft built by a private individual. A year and a half later, on January 19, 1937, flying the same H-1 Racer fitted with longer wings, Hughes set a new transcontinental airspeed record by flying non-stop from Los Angeles to Newark in 7 hours, 28 minutes and 25 seconds (beating his own previous record of 9 hours, 27 minutes). His average ground speed over the flight was .\n\nThe H-1 Racer featured a number of design innovations: it had retractable landing gear (as Boeing Monomail had five years before) and all rivets and joints set flush into the body of the aircraft to reduce drag. The H-1 Racer is thought to have influenced the design of a number of World War II fighters such as the Mitsubishi A6M Zero, the Focke-Wulf Fw 190 and the F8F Bearcat; although that has never been reliably confirmed. The H-1 Racer was donated to the Smithsonian in 1975 and is on display at the National Air and Space Museum.\n\nOn July 14, 1938, Hughes set another record by completing a flight around the world in just 91 hours (3 days, 19 hours, 17 minutes), beating the previous record set in 1933 by Wiley Post in a single engine Lockheed Vega by almost four days. Hughes returned home ahead of photographs of his flight. Taking off from New York City, Hughes continued to Paris, Moscow, Omsk, Yakutsk, Fairbanks, Minneapolis, then returning to New York City. For this flight he flew a Lockheed 14 Super Electra (NX18973, a twin-engine transport with a four-man crew) fitted with the latest radio and navigational equipment. Hughes wanted the flight to be a triumph of American aviation technology, illustrating that safe, long-distance air travel was possible. While he had previously been relatively obscure despite his wealth, being better known for dating Katharine Hepburn, New York City now gave Hughes a ticker-tape parade in the Canyon of Heroes. In 1938, the William P. Hobby Airport in Houston, Texas—known at the time as Houston Municipal Airport—was renamed after Hughes, but the name was changed back after people objected to naming the airport after a living person. Originally from Mystic, Iowa, Albert I Lodwick provided excellent organizational skills as his flight operations manager which directly contributed to Hughes' record breaking flight.\n\nHughes also had a role in the design and financing of both the Boeing 307 Stratoliner and Lockheed L-049 Constellation.\n\nHe received many awards as an aviator, including the Harmon Trophy in 1936 and 1938, the Collier Trophy and the Bibesco Cup of the Fédération Aéronautique Internationale in 1938, the Octave Chanute Award in 1940, and a special Congressional Gold Medal in 1939 \"in recognition of the achievements of Howard Hughes in advancing the science of aviation and thus bringing great credit to his country throughout the world\". According to his obituary in the \"New York Times\", Hughes never bothered to come to Washington to pick up the Congressional Gold Medal, which was eventually mailed to him.\n\nThe Hughes D-2 was conceived in 1939 as a bomber with five crew members, powered by 42-cylinder Wright R-2160 Tornado engines. In the end it appeared as two-seat fighter-reconnaissance aircraft designated the D-2A, powered by two Pratt & Whitney R-2800-49 engines. The aircraft was constructed using the Duramold process. The prototype was brought to Harper's Dry Lake California in great secrecy in 1943 and first flew on June 20 of that year. Acting on a recommendation of the president's son, Colonel Elliott Roosevelt, who had become friends with Hughes, in September 1943 the USAAF ordered 100 of a reconnaissance development of the D-2, known as the F-11. Hughes then attempted to get the military to pay for the development of the D-2. In November 1944, the hangar containing the D-2A was reportedly hit by lightning and the aircraft was destroyed. The D-2 design was abandoned, but led to the extremely controversial Hughes XF-11. The XF-11 was a large all-metal, two-seat reconnaissance aircraft, powered by two Pratt & Whitney R-4360-31 engines, each driving a set of contra-rotating propellers. Only the two prototypes were completed; the second one with a single propeller per side.\n\nIn the spring of 1943 Hughes spent nearly a month in Las Vegas, test flying his Sikorsky S-43 amphibian aircraft, practicing touch-and-go landings on Lake Mead in preparation for flying the H-4 Hercules. The weather conditions at the lake during the day were ideal and he enjoyed Las Vegas at night. On May 17, 1943, Hughes flew the Sikorsky from California carrying two CAA aviation inspectors, two of his employees and actress Ava Gardner. Hughes dropped Gardner off in Las Vegas and proceeded to Lake Mead to conduct qualifying tests in the S-43. The test flight did not go well. The Sikorsky crashed into Lake Mead, killing CAA inspector Ceco Cline and Hughes employee Richard Felt. Hughes suffered a severe gash on the top of his head when he hit the upper control panel and had to be rescued by one of the others on board. Hughes paid divers $100,000 to raise the aircraft and later spent more than $500,000 restoring it.\n\nHughes was involved in a near-fatal aircraft accident on July 7, 1946, while performing the first flight of the prototype U.S. Army Air Forces reconnaissance aircraft, the XF-11, near Hughes airfield at Culver City, California. An oil leak caused one of the contra-rotating propellers to reverse pitch, causing the aircraft to yaw sharply and lose altitude rapidly. Hughes attempted to save the aircraft by landing it at the Los Angeles Country Club golf course, but just seconds before reaching the course, the XF-11 started to drop dramatically and crashed in the Beverly Hills neighborhood surrounding the country club.\n\nWhen the XF-11 finally came to a halt after destroying three houses, the fuel tanks exploded, setting fire to the aircraft and a nearby home at 808 North Whittier Drive, owned by Lt Col. Charles E. Meyer. Hughes managed to pull himself out of the flaming wreckage but lay beside the aircraft until he was rescued by Marine Master Sgt. William L. Durkin, who happened to be in the area visiting friends. Hughes sustained significant injuries in the crash, including a crushed collar bone, multiple cracked ribs, crushed chest with collapsed left lung, shifting his heart to the right side of the chest cavity, and numerous third-degree burns. An oft-told story said that Hughes sent a check to the Marine weekly for the remainder of his life as a sign of gratitude. However, Durkin's daughter denied that he took any money for the rescue.\n\nDespite his physical injuries, Hughes was proud that his mind was still working. As he lay in his hospital bed, he decided that he did not like the bed's design. He called in plant engineers to design a customized bed, equipped with hot and cold running water, built in six sections, and operated by 30 electric motors, with push-button adjustments. The hospital bed was designed by Hughes specifically to alleviate the pain caused by moving with severe burn injuries. Despite the fact that he never had the chance to use the bed that he designed, Hughes' bed served as a prototype for the modern hospital bed. Hughes' doctors considered his recovery almost miraculous. Hughes, however, believed that neither miracle nor modern medicine contributed to his recovery, instead asserting the natural life-giving properties of fresh squeezed orange juice were responsible.\n\nMany attribute his long-term dependence on opiates to his use of codeine as a painkiller during his convalescence. The trademark mustache he wore afterward was used to hide a scar on his upper lip resulting from the accident.\n\nThe War Production Board (not the military) originally contracted with Henry Kaiser and Hughes to produce the gigantic HK-1 Hercules flying boat for use during World War II to transport troops and equipment across the Atlantic as an alternative to seagoing troop transport ships that were vulnerable to German U-boats. The project was opposed by the military services, thinking it would siphon resources from higher priority programs, but was advocated by Hughes' powerful allies in Washington, D.C. After disputes, Kaiser withdrew from the project and Hughes elected to continue it as the H-4 Hercules. However, the aircraft was not completed until after the end of World War II.\n\nThe Hercules was the world's largest flying boat, the largest aircraft made from wood, and, at , had the longest wingspan of any aircraft (the next largest wingspan was about ). (The Hercules is no longer the longest or heaviest aircraft ever built; both of those titles are currently held by the Antonov An-225 \"Mriya\".)\n\nThe Hercules flew only once for one mile (1.6 km), and above the water, with Hughes at the controls, on November 2, 1947.\n\nThe Hercules was nicknamed the \"Spruce Goose\" by its critics, but it was actually made largely from birch, not spruce, rather than of aluminum, because the contract required that Hughes build the aircraft of \"non-strategic materials\". It was built in Hughes' Westchester, California, facility. In 1947, Howard Hughes was summoned to testify before the Senate War Investigating Committee to explain why the H-4 development had been so troubled, and why $22 million had produced only two prototypes of the F-11. General Elliott Roosevelt and numerous other USAAF officers were also called to testify in hearings that transfixed the nation during August and November 1947. In hotly disputed testimony over TWA's route awards and malfeasance in the defense acquisition process, Hughes turned the tables on his main interlocutor, Maine Senator Owen Brewster, and the hearings were widely interpreted as a Hughes victory. After being displayed at the harbor of Long Beach, California, the Hercules was moved to McMinnville, Oregon, where it is now part of the Evergreen Aviation Museum.\n\nHughes Aircraft Company, a division of Hughes Tool Company, was originally founded by Hughes in 1932, in a rented corner of a Lockheed Aircraft Corporation hangar in Burbank, California, to build the H-1 racer. During and after World War II, Hughes fashioned his company into a major defense contractor. The Hughes Helicopters division started in 1947 when helicopter manufacturer Kellett sold their latest design to Hughes for production. The company was a major American aerospace and defense contractor manufacturing numerous technology related products that include spacecraft vehicles, military aircraft, radar systems, electro-optical systems, the first working laser, aircraft computer systems, missile systems, ion-propulsion engines (for space travel), commercial satellites, and other electronics systems.\n\nIn 1948, Hughes created a new division of the company, the Hughes Aerospace Group. The Hughes Space and Communications Group and the Hughes Space Systems Division were later spun off in 1948 to form their own divisions and ultimately became the Hughes Space and Communications Company in 1961. In 1953, Howard Hughes gave all his stock in the Hughes Aircraft Company to the newly formed Howard Hughes Medical Institute, thereby turning the aerospace and defense contractor into a tax-exempt charitable organization. The Howard Hughes Medical Institute sold Hughes Aircraft in 1985 to General Motors for $5.2 billion. In 1997, General Motors sold Hughes Aircraft to Raytheon and in 2000, sold Hughes Space & Communications to Boeing. A combination of Boeing, GM and Raytheon acquired the Hughes Research Laboratories, where it focused on advanced developments in microelectronics, information & systems sciences, materials, sensors, and photonics; their workspace spans from basic research to product delivery. It has particularly emphasized capabilities in high performance integrated circuits, high power lasers, antennas, networking, and smart materials.\n\nIn 1939, at the urging of Jack Frye, president of Trans World Airlines (TWA), Hughes quietly purchased a majority share of TWA stock for nearly $7 million and took control of the airline. Upon assuming ownership, Hughes was prohibited by federal law from building his own aircraft. Seeking an aircraft that would perform better than TWA's fleet of Boeing 307 Stratoliners, Hughes and Frye approached Boeing's competitor, Lockheed. Hughes had a good relationship with Lockheed since they had built the aircraft he used in his record flight around the world in 1938. Lockheed agreed to Hughes and Frye's request that the new aircraft be built in secrecy. The result was the revolutionary Constellation and TWA purchased the first 40 of the new airliners off the production line.\n\nIn 1956, Hughes placed an order for 63 Convair 880s for TWA at a cost of $400 million. Although Hughes was extremely wealthy at this time, outside creditors demanded that Hughes relinquish control of TWA in return for providing the money. In 1960, Hughes was ultimately forced out of TWA, although he owned 78% of the company and battled to regain control.\n\nBefore Hughes' removal, the TWA jet financing issue precipitated the end of Hughes' relationship with Noah Dietrich. Dietrich claimed Hughes developed a plan by which Hughes Tool Company profits would be inflated to sell the company for a windfall that would pay the bills for the 880s. Dietrich agreed to go to Texas to implement the plan on the condition that Hughes agreed to a capital gains arrangement he had long promised Dietrich. When Hughes balked, Dietrich resigned immediately. \"Noah\", Dietrich quoted Hughes as replying, \"I cannot exist without you!\" Dietrich stood firm and eventually had to sue to retrieve personal possessions from his office after Hughes ordered it locked.\n\nIn 1966, a U.S. federal court forced Hughes to sell his TWA shares because of concerns over conflict of interest between his ownership of both TWA and Hughes Aircraft. The sale of his TWA shares netted him a profit of $547 million.\n\nIn 1970, Hughes went back into the airline business, buying San Francisco-based Air West and renaming it Hughes Airwest. Air West had been formed in 1968 by the merger of Bonanza Air Lines, Pacific Air Lines and West Coast Airlines, all of which operated in the western U.S. By the late 1970s, Hughes Airwest operated an all-jet fleet of Boeing 727-200, Douglas DC-9-10, and McDonnell Douglas DC-9-30 jetliners serving an extensive route network in the western U.S. with flights to Mexico and western Canada as well. By 1980, the airline's route system reached as far east as Houston Hobby Airport and Milwaukee with a total of forty-two (42) destinations being served. Hughes Airwest was then acquired by and merged into Republic Airlines (1979–1986) in late 1980. Republic was subsequently acquired by and merged into Northwest Airlines which in turn was eventually merged into Delta Air Lines.\n\nIn 1953, Hughes launched the Howard Hughes Medical Institute in Miami, Florida, and currently located in Chevy Chase, Maryland, formed with the expressed goal of basic biomedical research, including trying to understand, in Hughes' words, the \"genesis of life itself,\" due to his lifelong interest in science and technology. Hughes' first will, which he signed in 1925 at the age of 19, stipulated that a portion of his estate should be used to create a medical institute bearing his name. When a major battle with the IRS loomed ahead, Hughes gave all his stock in the Hughes Aircraft Company to the institute, thereby turning the aerospace and defense contractor into a for-profit entity of a fully tax-exempt charity. Hughes' internist, Verne Mason, who treated Hughes after his 1946 aircraft crash, was chairman of the institute's medical advisory committee. The Howard Hughes Medical Institute's new board of trustees sold Hughes Aircraft in 1985 to General Motors for $5.2 billion, allowing the institute to grow dramatically.\n\nThe deal was the topic of a protracted legal battle between Hughes and the Internal Revenue Service, which Hughes ultimately won. After his death in 1976, many thought that the balance of Hughes' estate would go to the institute, although it was ultimately divided among his cousins and other heirs, given the lack of a will to the contrary. The HHMI was the fourth largest private organization as of 2007 and the largest devoted to biological and medical research, with an endowment of $16.3 billion as of June 2007.\n\nIn 1972, Hughes was approached by the CIA to help secretly recover Soviet submarine K-129, which had sunk near Hawaii four years earlier. The recovery plan used the special-purpose salvage vessel \"Glomar Explorer\". Hughes' involvement provided the CIA with a plausible cover story, having to do with civilian marine research at extreme depths and the mining of undersea manganese nodules. In the summer of 1974, \"Glomar Explorer\" attempted to raise the Soviet vessel.\n\nHowever, during the recovery a mechanical failure in the ship's grapple caused half of the submarine to break off and fall to the ocean floor. This section is believed to have held many of the most sought-after items, including its code book and nuclear missiles. Two nuclear-tipped torpedoes and some cryptographic machines were recovered, along with the bodies of six Soviet submariners who were subsequently given formal burial at sea in a filmed ceremony. The operation, known as Project Azorian (but incorrectly referred to by the press as Project Jennifer), became public in February 1975 after secret documents were released, obtained by burglars from Hughes' headquarters in June 1974. Though he lent his name to the operation, Hughes and his companies had no actual involvement in the project. The \"Glomar Explorer\" was eventually acquired by Transocean Inc., an offshore oil and gas drilling rig company.\n\nIn 1929 Hughes' wife, Ella, returned to Houston and filed for divorce. Hughes dated many famous women, many of them decades younger, including Billie Dove, Faith Domergue, Bette Davis, Ava Gardner, Olivia de Havilland, Katharine Hepburn, Ginger Rogers, Rita Hayworth and Gene Tierney. He also proposed to Joan Fontaine several times, according to her autobiography \"No Bed of Roses\". Jean Harlow accompanied him to the premiere of \"Hell's Angels\", but Noah Dietrich wrote many years later that the relationship was strictly professional, as Hughes apparently personally disliked Harlow. In his 1971 book, \"Howard: The Amazing Mr. Hughes\", Dietrich said that Hughes genuinely liked and respected Jane Russell, but never sought romantic involvement with her. According to Russell's autobiography, however, Hughes once tried to bed her after a party. Russell (who was married at the time) refused him, and Hughes promised it would never happen again. The two maintained a professional and private friendship for many years. Hughes remained good friends with Tierney who, after his failed attempts to seduce her, was quoted as saying \"I don't think Howard could love anything that did not have a motor in it.\" Later, when Tierney's daughter Daria was born deaf and blind and with a severe learning disability, because of Tierney's being exposed to rubella during her pregnancy, Hughes saw to it that Daria received the best medical care and paid all expenses.\n\nIn 1933, Hughes made a purchase of an unseen luxury steam yacht named the \"Rover\", which was previously owned by British shipping magnate Lord Inchcape. \"I have never seen the \"Rover\" but bought it on the blue prints, photographs and the reports of Lloyd's surveyors. My experience is that the English are the most honest race in the world.\" Hughes renamed the yacht \"Southern Cross\" and later sold her to Swedish entrepreneur Axel Wenner-Gren.\n\nOn July 11, 1936, Hughes struck and killed a pedestrian named Gabriel S. Meyer with his car at the corner of 3rd Street and Lorraine in Los Angeles. After the accident, Hughes was taken to the hospital and certified as sober, but an attending doctor made a note that Hughes had been drinking. A witness to the accident told police that Hughes was driving erratically and too fast, and that Meyer had been standing in the safety zone of a streetcar stop. Hughes was booked on suspicion of negligent homicide and held overnight in jail until his attorney, Neil S. McCarthy, obtained a writ of habeas corpus for his release pending a coroner's inquest. By the time of the coroner's inquiry, however, the witness had changed his story and claimed that Meyer had moved directly in front of Hughes' car. Nancy Bayly (Watts), who was in the car with Hughes at the time of the accident, corroborated this version of the story. On July 16, 1936, Hughes was held blameless by a coroner's jury at the inquest into Meyer's death. Hughes told reporters outside the inquiry, \"I was driving slowly and a man stepped out of the darkness in front of me.\"\n\nOn January 12, 1957, Hughes married actress Jean Peters. The couple met in the 1940s, before Peters became a film actress. They had a highly publicized romance in 1947 and there was talk of marriage, but she said she could not combine it with her career. Some later claimed that Peters was \"the only woman [Hughes] ever loved,\" and he reportedly had his security officers follow her everywhere even when they were not in a relationship. Such reports were confirmed by actor Max Showalter, who became a close friend of Peters while shooting \"Niagara\" (1953). Showalter told in an interview that because he frequently met with Peters, Hughes' men threatened to ruin his career if he did not leave her alone.\n\nShortly before the 1960 Presidential election, Richard Nixon was alarmed when it was revealed that his brother, Donald, received a $205,000 loan from Hughes. It has long been speculated that Nixon's drive to learn what the Democrats were planning in 1972 was based in part on his belief that the Democrats knew about a later bribe that his friend Bebe Rebozo had received from Hughes after Nixon took office.\n\nIn late 1971, Donald Nixon was collecting intelligence for his brother in preparation for the upcoming presidential election. One of Donald's sources was John H. Meier, a former business adviser of Hughes who had also worked with Democratic National Committee Chair Larry O'Brien.\n\nMeier, in collaboration with former Vice President Hubert Humphrey and others, wanted to feed misinformation to the Nixon campaign. Meier told Donald that he was sure the Democrats would win the election because Larry O'Brien had a great deal of information on Richard Nixon's illicit dealings with Howard Hughes that had never been released; O'Brien didn't actually have any such information, but Meier wanted Nixon to think he did. Donald told his brother that O'Brien was in possession of damaging Hughes information that could destroy his campaign. Terry Lenzner, who was the chief investigator for the Senate Watergate Committee, speculates that it was Nixon's desire to know what O'Brien knew about Nixon's dealings with Hughes that may have partially motivated the Watergate break-in.\n\nHughes was eccentric. Close friends of Hughes reported that he was obsessed with the size of peas, one of his favorite foods, and used a special fork to sort them by size.\n\nWhile directing \"The Outlaw\", Hughes became fixated on a small flaw in one of Jane Russell's blouses, claiming that the fabric bunched up along a seam and gave the appearance of two nipples on each breast. He reportedly wrote a detailed memorandum to the crew on how to fix the problem. Richard Fleischer, who directed \"His Kind of Woman\" with Hughes as executive producer, wrote at length in his autobiography about the difficulty of dealing with the tycoon. In his book, \"Just Tell Me When to Cry\", Fleischer explained that Hughes was fixated on trivial details and was alternately indecisive and obstinate. He also revealed that Hughes' unpredictable mood swings made him wonder if the film would ever be completed.\n\nIn 1947, after the U.S. Government rejected his massive H-4 Hercules, Hughes, who had suffered a near-fatal aircraft crash in 1946, told his aides that he wanted to screen some movies at a film studio near his home. He stayed in the studio's darkened screening room for more than four months, never leaving. He ate only chocolate bars and chicken and drank only milk, later urinating in the empty bottles and containers. He was surrounded by dozens of Kleenex boxes that he continuously stacked and re-arranged. He wrote detailed memos to his aides giving them explicit instructions not to look at him nor speak to him unless spoken to. Throughout this period, Hughes sat fixated in his chair, often naked, continually watching movies. When he finally emerged in the spring of 1948, his hygiene was terrible. He had not bathed nor cut his hair and nails for weeks, although this may have been due to allodynia (pain response to stimuli that would normally not cause pain).\n\nAfter the screening room incident, Hughes moved into a bungalow at the Beverly Hills Hotel where he also rented rooms for his aides, his wife, and numerous girlfriends. He would sit naked in his bedroom with a pink hotel napkin placed over his genitals, watching movies. This may have been because Hughes found the touch of clothing painful due to his allodynia. He may have watched movies to distract him from his pain—a common practice among patients with intractable pain, especially those who do not receive adequate treatment. In one year, Hughes spent an estimated $11 million at the hotel.\n\nHughes began purchasing all restaurant chains and four star hotels that had been founded within the state of Texas. This included, if for only a short period, many unknown franchises currently out of business. He placed ownership of the restaurants with the Howard Hughes Medical Institute, and all licenses were resold shortly after.\n\nAnother time, he became obsessed with the 1968 film \"Ice Station Zebra\", and had it run on a continuous loop in his home. According to his aides, he watched it 150 times.\n\nHughes insisted on using tissues to pick up objects to insulate himself from germs. He would also notice dust, stains or other imperfections on people's clothes and demand that they take care of them. Once one of the most visible men in America, Hughes ultimately vanished from public view, although tabloids continued to follow rumors of his behavior and whereabouts. He was reported to be terminally ill, mentally unstable, or even dead.\n\nInjuries from numerous aircraft crashes caused Hughes to spend much of his later life in pain, and he eventually became addicted to codeine, which he injected intramuscularly. Hughes had his hair cut and nails trimmed only once a year, likely due to the pain caused by the RSD/CRPS, which was caused by the plane crashes.\n\nThe wealthy and aging Hughes, accompanied by his entourage of personal aides, began moving from one hotel to another, always taking up residence in the top floor penthouse. In the last ten years of his life, 1966 to 1976, Hughes lived in hotels in many cities—including Beverly Hills, Boston, Las Vegas, Nassau, Freeport, Vancouver, London, Managua, and Acapulco.\n\nOn November 24, 1966 (Thanksgiving Day), Hughes arrived in Las Vegas by railroad car and moved into the Desert Inn. Because he refused to leave the hotel and to avoid further conflicts with the owners, Hughes bought the Desert Inn in early 1967. The hotel's eighth floor became the nerve center of Hughes' empire and the ninth-floor penthouse became his personal residence. Between 1966 and 1968, he bought several other hotel-casinos, including the Castaways, New Frontier, the Landmark Hotel and Casino, and the Sands. He bought the small Silver Slipper casino just so he could have its trademark neon silver slipper moved. Visible from Hughes' bedroom, it had apparently kept him awake at night.\n\nAfter Hughes left the Desert Inn, hotel employees discovered that his drapes had not been opened in the nine years he lived there and had rotted through. An unusual incident marked an earlier Hughes connection to Las Vegas: during his 1954 engagement at the Last Frontier hotel, flamboyant entertainer Liberace mistook Howard Hughes for his lighting director, instructing him to instantly bring up a blue light should he start to play \"Clair de lune\". Hughes nodded in compliance—but the hotel's entertainment director arrived and introduced Hughes to Liberace.\n\nHughes wanted to change the image of Las Vegas to something more glamorous. As Hughes wrote in a memo to an aide, \"I like to think of Las Vegas in terms of a well-dressed man in a dinner jacket and a beautifully jeweled and furred female getting out of an expensive car.\" Hughes bought several local television stations (including KLAS-TV).\n\nHughes' considerable business holdings were overseen by a small panel unofficially dubbed \"The Mormon Mafia\" because of the many Latter-day Saints on the committee, led by Frank William Gay. In addition to supervising day-to-day business operations and Hughes' health, they also went to great pains to satisfy Hughes' every whim. Hughes once became fond of Baskin-Robbins' banana nut ice cream, so his aides sought to secure a bulk shipment for him, only to discover that Baskin-Robbins had discontinued the flavor. They put in a request for the smallest amount the company could provide for a special order, 350 gallons (1,300 L), and had it shipped from Los Angeles. A few days after the order arrived, Hughes announced he was tired of banana nut and wanted only French vanilla ice cream. The Desert Inn ended up distributing free banana nut ice cream to casino customers for a year. In a 1996 interview, ex–Howard Hughes communicator Robert Maheu said, \"There is a rumor that there is still some banana nut ice cream left in the freezer. It is most likely true.\"\n\nAs an owner of several major Las Vegas businesses, Hughes wielded much political and economic influence in Nevada and elsewhere. During the 1960s and early 1970s, he disapproved of underground nuclear testing at the Nevada Test Site. Hughes was concerned about the risk from residual nuclear radiation, and attempted to halt the tests. When the tests finally went through despite Hughes' efforts, the detonations were powerful enough that the entire hotel where he was staying trembled due to the shock waves. In two separate, last-ditch maneuvers, Hughes instructed his representatives to offer million-dollar bribes to both presidents Lyndon B. Johnson and Richard Nixon.\n\nIn 1970, Jean Peters filed for divorce. The two had not lived together for many years. Peters requested a lifetime alimony payment of $70,000 a year, adjusted for inflation, and waived all claims to Hughes' estate. Hughes offered her a settlement of over a million dollars, but she declined it. Hughes did not insist on a confidentiality agreement from Peters as a condition of the divorce. Aides reported that Hughes never spoke ill of her. She refused to discuss her life with Hughes and declined several lucrative offers from publishers and biographers. Peters would state only that she had not seen Hughes for several years before their divorce and had only dealt with him by phone.\n\nHughes was living in the Intercontinental Hotel near Lake Managua in Nicaragua, seeking privacy and security, when a magnitude 6.5 earthquake damaged Managua in December 1972. As a precaution, Hughes moved to the Nicaraguan National Palace and stayed there as a guest of Anastasio Somoza Debayle before leaving for Florida on a private jet the following day. He subsequently moved into the Penthouse at the Xanadu Princess Resort on Grand Bahama Island, which he had recently purchased. He lived almost exclusively in the penthouse of the Xanadu Beach Resort & Marina for the last four years of his life. Hughes had spent a total of $300 million on his many properties in Las Vegas.\n\nIn 1972, author Clifford Irving caused a media sensation when he claimed he had co-written an authorized autobiography of Hughes. Hughes was so reclusive that he did not immediately publicly refute Irving's statement, leading many to believe the Irving book was genuine. However, before the book's publication Hughes finally denounced Irving in a teleconference and the entire project was eventually exposed as a hoax. Irving was later convicted of fraud and spent 17 months in prison. In 1974, the Orson Welles film \"F for Fake\" included a section on the Hughes biography hoax. In 1977, \"The Hoax\" by Clifford Irving was published in the United Kingdom, telling his story of these events. The 2006 film \"The Hoax\", starring Richard Gere, is also based on these events.\n\nHughes was reported to have died on April 5, 1976, at 1:27 p.m. on board an aircraft owned by Robert Graf and piloted by Jeff Abrams. He was en route from his penthouse at the Acapulco Fairmont Princess Hotel in Mexico to the Methodist Hospital in Houston, Texas. Other accounts indicate that he died on the flight from Freeport, Grand Bahama, to Houston.\n\nAfter receiving a call, his senior counsel, Frank P. Morse, ordered his staff to get his body on a plane and return him to the United States. It was common that foreign countries would hold a corpse as ransom so that an estate could not be settled. Morse ordered the pilots to announce Hughes' death once they entered U.S. airspace.\n\nHis reclusive activities and possible drug use made him practically unrecognizable. His hair, beard, fingernails, and toenails were long—his tall frame now weighed barely , and the FBI had to use fingerprints to conclusively identify the body. Howard Hughes' alias, John T. Conover, was used when his body arrived at a morgue in Houston on the day of his death.\n\nA subsequent autopsy recorded kidney failure as the cause of death. Hughes was in extremely poor physical condition at the time of his death. He suffered from malnutrition. While his kidneys were damaged, his other internal organs, including his brain, were deemed perfectly healthy. X-rays revealed five broken-off hypodermic needles in the flesh of his arms. To inject codeine into his muscles, Hughes had used glass syringes with metal needles that easily became detached.\n\nHughes is buried next to his parents at Glenwood Cemetery in Houston, Texas.\n\nApproximately three weeks after Hughes' death a handwritten will was found on the desk of an official of The Church of Jesus Christ of Latter-Day Saints in Salt Lake City, Utah. The so-called \"Mormon Will\" gave $1.56 billion to various charitable organizations (including $625 million to the Howard Hughes Medical Institute), nearly $470 million to the upper management in Hughes' companies and to his aides, $156 million to first cousin William Lummis, and $156 million split equally between his two ex-wives Ella Rice and Jean Peters.\n\nIn this will Hughes left his entire estate to the Hughes Medical Institute as he had no connection to family and was seriously ill. This is contrary to the many wills that have surfaced after his death. The original will that included payments to aides never surfaced. It was apparently in a home surrounding the Desert Inn Golf Course belonging to the mother of an assistant. He had no desire to leave any money to family, aides, churches, including William Gay and Frank Morse. Hughes was not Mormon and had no reason to leave his estate to that church. Frank P. Morse is still the attorney of record for Hughes. Gay has devoted his life to the Church of Jesus Christ of Latter-day Saints.\n\nA further $156 million was endowed to a gas-station owner named Melvin Dummar who told reporters that late one evening in 1967, he found a disheveled and dirty man lying along U.S. Highway 95, north of Las Vegas. The man asked for a ride to Vegas. Dropping him off at the Sands Hotel, Dummar said the man told him he was Hughes. Dummar later claimed that days after Hughes' death a \"mysterious man\" appeared at his gas station, leaving an envelope containing the will on his desk. Unsure if the will was genuine and unsure of what to do, Dummar left the will at the LDS Church office. In 1978, after a seven-month trial, a Nevada court rejected the Mormon Will as a forgery, and declared that Hughes had died intestate. In 1980, Jonathan Demme's film, \"Melvin and Howard\", was based on Dummar's story.\n\nHughes' $2.5 billion estate was eventually split in 1983 among 22 cousins, including William Lummis, who serves as a trustee of the Howard Hughes Medical Institute. The Supreme Court of the United States ruled that Hughes Aircraft was owned by the Howard Hughes Medical Institute which sold it to General Motors in 1985 for $5.2 billion. The court rejected suits by the states of California and Texas that claimed they were owed inheritance tax. In 1984 Hughes' estate paid an undisclosed amount to Terry Moore, who claimed she and Hughes had secretly married on a yacht in international waters off Mexico in 1949 and never divorced. Moore never produced proof of a marriage, but her book, \"The Beauty and the Billionaire,\" became a bestseller.\n\n\nThe moving image collection of Howard Hughes is held at the Academy Film Archive. The collection consists of over 200 items including 35mm and 16mm elements of feature films, documentaries, and television programs made or accumulated by Hughes.\n\n\n\n\n", "id": "14059", "title": "Howard Hughes"}
{"url": "https://en.wikipedia.org/wiki?curid=14062", "text": "Hook of Holland\n\nThe Hook of Holland () is a town in the southwestern corner of Holland (hence the name; \"hoek\" means \"corner\"), at the mouth of the New Waterway shipping canal into the North Sea. The town is administered by the municipality of Rotterdam as a district of that city. Its district covers an area of 16.7 km, of which 13.92 km is land. On 1 January 1999 it had an estimated population of 9,400.\n\nTowns near the Hook include Monster, 's-Gravenzande, Naaldwijk and Delft to the northeast, and Maassluis to the southeast. On the other side of the river is the Europort and the Maasvlakte. The wide sandy beach, one section of which is designated for use by naturists, runs for approximately 18 kilometres to Scheveningen and for most of this distance is backed by extensive sand dunes through which there are foot and cycle paths.\n\nOn the north side of the New Waterway, to the west of the town, is a pier part of which is accessible to pedestrians and cyclists.\n\nThe Berghaven is a small harbour on the New Waterway where the Rotterdam and Europort pilots are based. This small harbour is only for the use of the pilot service, government vessels and the Hook of Holland lifeboat.\n\nDuring World War II this was one of the most important places for the Germans to hold because of the harbour.\n\nThere are two railway stations, Hook of Holland Strand, which is at the end of the line and closest to the beach and Hook of Holland Haven, which is close to the town centre, adjacent to the ferry terminal and the small harbour, the Berghaven. The railway line connects the Hook to Rotterdam via Maassluis, Vlaardingen and Schiedam, the trains running every half hour during the day.\n\nThe ferry terminal is operated by Stena Line with the passenger terminal and access for passenger cars being next to Hook of Holland Holland Haven Station and the freight entrance being to the east of the town.\n\nA ferry service to eastern England has operated from the Hook since 1893 with only the two World Wars interrupting the service. Currently two routes are being operated: one, a day and night freight and passenger service, to Harwich, Essex and the other, a night, freight-only service to North Killingholme Haven, Lincolnshire. From 1 March 2009 the Harwich service will be departing at 2:30 pm and 10:45 pm and the Killingholme service at 9:15 pm.\n\nA local ferry operated by RET links the Hook with the Maasvlakte part of the Port of Rotterdam.\n\nIn just 10 kilometres one has access to the A20 heading east towards Rotterdam and Utrecht. At 17 kilometres, one could access the A4 heading north towards the Hague and Amsterdam.\n\n", "id": "14062", "title": "Hook of Holland"}
{"url": "https://en.wikipedia.org/wiki?curid=14063", "text": "Hugh Binning\n\nHugh Binning (1627–1653) was a Scottish philosopher and theologian. Binning was born in Scotland during the reign of Charles I, ordained in the (Presbyterian) Church of Scotland and died during the time of Oliver Cromwell and the Commonwealth of England.\n\nA precocious child, Binning at age 13 was admitted to the study of philosophy at the University of Glasgow. By the age of 19, he was appointed regent and professor of philosophy at the University of Glasgow. Three years later, he was called to be minister and presided at a church in Govan, adjacent to the city of Glasgow; a post he held until his untimely death of consumption at the age of 26. He was a follower of James Dalrymple. In later life he was well known as an evangelical Christian.\n\nHugh Binning was born two years after Charles I ascended to the thrones of England, Ireland, and Scotland. At the time, each was an independent country sharing the same monarch. The Acts of Union 1707 integrated Scotland and England to form the Kingdom of Great Britain; the Acts of Union 1800 integrated Ireland to form United Kingdom of Great Britain and Ireland.\n\nThe period was dominated by both political and religious strife between the three independent countries. The religious dispute centered on whether religion was to be dictated by the monarch or was to be the choice of the people; whether people have a direct relationship with God or they needed to use an intermediary. The civil disputes centered on the extent of the king's power, a question of the Divine right of kings; specifically whether the King has right to raise taxes and armed forces without the Consent of the governed. These wars ultimately changed the relationship between king and subjects.\n\nIn 1638 the General Assembly of the Church of Scotland voted to remove bishops and the Book of Common Prayer that had been introduced by Charles I to impose the Anglican model on the Presbyterian Church of Scotland. Public riots occurred. The result was the Wars of the Three Kingdoms, an interrelated series of conflicts that took place in the three countries sharing the same monarch. The first of the conflicts was in 1639, the First of the Bishops' Wars, a single border skirmish between England and Scotland; also known as \"the war the armys did not wanted to fight.\"\n\nTo maintain his English power base Charles I made secret alliances with Catholic Ireland and Presbyterian Scotland to invade Anglican England, promising that each country could establish their own separate state religion. Once these secret entreaties became known to the English Long Parliament, the Congregationalist faction (of which Oliver Cromwell was a primary spokesman) took matters into their own hands and Parliament established an army separate from the King. Then, Charles I was executed in January 1649, which led to the rule of Cromwell and the establishment of the Commonwealth. The conflicts concluded with The English Restoration of the monarchy with the return of Charles II, in 1660.\n\nThe Act of Classes was passed by the Parliament of Scotland on 23 January 1649; the act banned Royalists (people supporting the monarchy) from holding political or military office. In exile, Charles II signed the Treaty of Breda (1650) with the Scottish Parliament; among other things, the treaty established Presbyterianism as the national religion. Charles was crowned King of Scots at Scone in January 1651. By September 1651 Scotland was annexed by England, its legislative institutions abolished, Presbyterianism dis-established, and Charles was forced into exile in France.\n\nThe Scottish Parliament rescinded the Act of Classes in 1651, which produced a split within Scottish Society. The sides of the conflict were called the Resolutioners (who supported the rescission of the act – supported the monarchy and the Scottish House of Stewart) and the Protesters (who supported Cromwell and the Commonwealth); Binning sided with the Resolutioners.\n\nWhen Cromwell sent troops to Scotland, he was also attempting to dis-establish Presbyterianism and the Church of Scotland, Binning spoke against Cromwell's act. On Saturday 19 April 1651, Cromwell entered Glasgow and the next day he heard a sermon by three ministers who condemned Cromwell for invading Scotland. That evening, Cromwell summoned those ministers and others, to a debate on the issue. At the debate, Rev Hugh Binning is said to have out-debated Cromwell’s ministers so completely that he silenced Cromwell’s ministers.\n\nHugh Binning political views were based on his theology. Binning was a Covenanter, a movement that began in Scotland at Greyfriars Kirkyard in 1638 with the National Covenant and continued with the 1643 Solemn League and Covenant – in effect a treaty between the English Long Parliament and Scotland for the preservation of the reformed religion in exchange for troops to confront the threat of Irish Catholic troops joining the Royalist army. Binning could also be described as a Resolutioners; both political positions were taken because of their religious implications. However, he saw the evils of the politics of his day was not a “fomenter of factions” writing “A Treatise of Christian Love” as a response.\n\nBecause of the turmoil time in which Hugh Binning lived, politics and religion were inexorably intertwined. Binning was a Calvinist and follower of John Knox. As a profession, Binning was trained as a Philosopher, and he believed that philosophy was the servant of theology. He thought that both Philosophy and Theology should be taught in parallel. Binning’s writing, which are primarily a collection of his sermons, “forms an important bridge between the 17th century, when philosophy in Scotland was heavily dominated by Calvinism, and the 18th century when figures such as Francis Hutcheson re-asserted a greater degree of independence between the two and allied philosophy with the developing human sciences.”\nReligiously, Hugh Binning was, what we would call today, an Evangelical Calvinist. He spoke on the primacy of God’s love as the ground of salvation: \n\nWith regards to the extent of the ‘atonement’, Hugh Binning, like many Scottish theologians of his day, was not a ‘hyper-Calvinist,’ since it was not until the Synod of Dort in 1619 that the Reformed tradition come to accept limited atonement, one of the primary tenets of the Five Points of Calvinism. Binning did not hold that the offer of redemption applied only to the few that are elect but said that “the ultimate ground of faith is in the electing will of God.” In Scotland during the 1600s the questions concerning atonement revolved around the terms in which the offer was expressed.\n\nBinning believed that \"forgiveness is based on Christ's death, understood as a satisfaction and as a sacrifice: 'If he had pardoned sin without any satisfaction what rich grace it had\nbeen! But truly, to provide the Lamb and sacrifice himself, to find out\nthe ransom, and to exact it of his own Son, in our name, is a testimony\nof mercy and grace far beyond that. But then, his justice is very conspicuous\nin this work.'\"\n\nAll of the works of Hugh Binning were published posthumously and were primarily collections of his sermons. Of his speaking style, it was said: \"There is originality without any affectation, a rich imagination, without anything fanciful or extravert, the utmost simplicity, without an thing mean or trifling.\" \n\n\nHugh Binning was the son of John Binning and Margaret M'Kell. Margaret was the daughter of Rev. Matthew M'Kell,\nwho was a minister in the parish of Bothwell, Scotland, and sister of Hugh M'Kell, a minister in Edinburgh.\nHugh Binning was born on the estate of his father in Dalvennan, Straiton, in the shire of Ayr. The family owned other land in the parishes of Straiton and Colmonell as well as Maybole in Carrick. \nIn 1645, James Dalrymple, 1st Viscount of Stair, who was Hugh’s master (primary professor) in the study of philosophy, announced he was retiring from the University of Glasgow. After a national search for a replacement on the faculty, three men were selected to compete for the position. Hugh was one of those selected, but was at a disadvantage because of his extreme youth and because he was not of noble birth. However, he had strong support from the existing faculty, who suggested that the candidates speak extemporaneously on any topic of the candidate’s choice. After hearing Hugh speak, the other candidates withdrew, making Hugh a regent and professor of philosophy, while he was still 18 years old.\n\nOn 7 February 1648 (at the age of 21) Hugh was appointed an Advocate before the Court of Sessions (an attorney). In the same year he married Barbara Simpson (sometimes called Mary), daughter of Rev. James Simpson a minister in Ireland. Their son, John, was born in 1650.\n\nHugh died around September 1653 and was buried in the churchyard of Govan, where Patrick Gillespie, then principal of the University of Glasgow, ordered a monument inscribed in Latin, roughly translated: \n\nHugh’s widow, Barbara (sometimes called Mary), then remarried James Gordon, an Anglican priest at Cumber in Ireland. Together they had a daughter, Jean who married Daniel MacKenzie, who was on the winning side of the Battle of Bothwell Bridge serving as an ensign under Lieutenant-Colonel William Ramsay (who became the third Earl of Dalhousie), in the Earl of Mar’s Regiment of Foot.\n\nHugh’s son, John Binning, married Hanna Keir, who was born in Ireland. The Binning’s were Covenanters, a resistance movement that objected to the return of Charles II (who was received into the Catholic Church on his deathbed). They were on the losing side in the 1679 Battle of Bothwell Bridge. Most of the rebels who were not executed were exiled to the Americas; about 30 Covenanters were exiled to the Carolinas on the Carolina Merchant in 1684. After the battle, John and Hanna were separated. \nIn the aftermath of the battle at Bothwell Bridge, Hugh’s widow (now Barbara Gordon) tried to reclaim the family estate at Dalvennan by saying that John and his wife owed his step father a considerable some of money. The legal action was successful and Dalvennan became the possession of John’s half sister Jean, and her husband Daniel MacKenzie. In addition, Jean came into possession Hanna Keir's property in Ireland.\n\nBy 1683, Jean was widowed. John Binning was branded a traitor, was sentenced to death and forfeited his property to the Crown. John’s wife (Hanna Keir) was branded as a traitor and forfeited her property in Ireland. In 1685 Jean \"donated\" the Binning family's home at Dalvennan and other properties, along with the Keir properties to Roderick MacKenzie, who was a Scottish advocate of James II (James VII of Scotland), and the baillie of Carrick. According to an act of the Scottish Parliament, Roderick MacKenzie was also very effective in “suppressing the rebellious, fanatical party in the western and other shires of this realm, and putting the laws to vigorous execution against them”\n\nSince Bothwell Bridge, Hanna had been hiding from the authorities. In 1685 Hanna was in Edinburgh where she was found during a sweep for subversives and imprisoned in the Tolbooth of Edinburgh, a combination city hall and prison. Those arrested with Hanna were exiled to North America, however she developed Dysentery and remained behind. By 1687, near death, Hanna petitioned the Privy Council of Scotland for her release; she was exiled to her family in Ireland, where she died around 1692.\nIn 1690 the Scottish Parliament rescinded John's fines and forfeiture, but he was not able to recover his family’s estates, the courts suggesting that John had relinquished his claim to Dalvennan in exchange for forgiveness of debt, rather than forfeiture.\n\nThere is little documentation about John after his wife's death. John received a small income from royalties on his father Hugh’s works after parliament extended copyrights on Hugh’s writings to him. However, the income was not significant and John made several petitions to the Scottish parliament for money, the last occurring in 1717. It is thought that John died in Somerset county, in southwestern England. \n", "id": "14063", "title": "Hugh Binning"}
{"url": "https://en.wikipedia.org/wiki?curid=14064", "text": "Henry Home, Lord Kames\n\nHenry Home, Lord Kames (169627 December 1782) was a Scottish advocate, judge, philosopher, writer and agricultural improver. A central figure of the Scottish Enlightenment, a founder member of the Philosophical Society of Edinburgh, and active in the Select Society, his protégés included David Hume, Adam Smith, and James Boswell.\n\nBorn at Kames House, between Eccles and Birgham, Berwickshire, he was educated at home by a private tutor. He studied law at Edinburgh, was called to the bar in 1724, and became an advocate. He soon acquired reputation by a number of publications on the civil and Scottish law, and was one of the leaders of the Scottish Enlightenment. In 1752, he was \"raised to the bench\", thus acquiring the title of Lord Kames.\n\nHome was on the panel of judges in the Joseph Knight case which ruled that there could be no slavery in Scotland.\n\nHis address in 1775 is shown as New Street on the Canongate.\n\nHome wrote much about the importance of property to society. In his \"Essay Upon Several Subjects Concerning British Antiquities\", written just after the Jacobite rising of 1745 he described how the politics of Scotland were not based on loyalty to Kings or Queens as Jacobites had said but on royal land grants given in return for loyalty.\n\nIn \"Historical Law Tracts\" and later in \"Sketches on the History of Man\" he described human history as having four distinct stages. The first was as a hunter-gatherer where people avoided each other out of competition. The second stage he described was a herder of domestic animals which required forming larger societies. No laws were needed at these stages except those given by the head of the family or society. Agriculture was the third stage requiring greater cooperation and new relationships to allow for trade or employment (or slavery). He argued that 'the intimate union among a multitude of individuals, occasioned by agriculture' required a new set of rights and obligations in society. This requires laws and law enforcers. A fourth stage moves from villages and farms to seaports and market towns requiring yet more laws and complexity but also much to benefit from. Kames could see these stages within Scotland itself, with the pastoral/agricultural highlands, the agricultural/industrial lowlands and the growing commercial (\"polite\") towns of Glasgow and Edinburgh.\n\nHome was a polygenist, he believed God had created different races on earth in separate regions. In his book \"Sketches of the History of Man\", in 1774, Home claimed that the environment, climate, or state of society could not account for racial differences, so that the races must have come from distinct, separate stocks.\n\nThe above studies created the genre of the story of civilization and defined the fields of anthropology and sociology and therefore the modern study of history for two hundred years.\n\nIn the popular book \"Elements of Criticism\" (1762) Home interrogated the notion of fixed or arbitrary rules of literary composition, and endeavoured to establish a new theory based on the principles of human nature. The late eighteenth-century tradition of sentimental writing was associated with his notion that 'the genuine rules of criticism are all of them derived from the human heart. Prof Neil Rhodes has argued that Lord Kames played a significant role in the development of English as an academic discipline in the Scottish Universities.\n\nHe enjoyed intelligent conversation and cultivated a large number of intellectual associates, among them John Home, David Hume and James Boswell.. Lord Monboddo was also a frequent debater of Kames, although these two usually had a fiercely competitive and adversarial relationship.\n\n\n\n", "id": "14064", "title": "Henry Home, Lord Kames"}
{"url": "https://en.wikipedia.org/wiki?curid=14065", "text": "Harwich\n\nHarwich is a town in Essex, England and one of the Haven ports, located on the coast with the North Sea to the east. It is in the Tendring district. Nearby places include Felixstowe to the northeast, Ipswich to the northwest, Colchester to the southwest and Clacton-on-Sea to the south. It is the northernmost coastal town within Essex.\n\nIts position on the estuaries of the Stour and Orwell rivers and its usefulness to mariners as the only safe anchorage between the Thames and the Humber led to a long period of maritime significance, both civil and military. The town became a naval base in 1657 and was heavily fortified, with Harwich Redoubt, Beacon Hill Battery, and Bath Side Battery.\n\nHarwich today is contiguous with Dovercourt and the two, along with Parkeston, are often referred to collectively as Harwich.\n\nThe town's name means \"military settlement,\" from Old English \"here-wic\".\n\nThe town received its charter in 1238, although there is evidence of earlier settlement – for example, a record of a chapel in 1177, and some indications of a possible Roman presence.\n\nBecause of its strategic position, Harwich was the target for the invasion of Britain by William of Orange on 11 November 1688. However, unfavourable winds forced his fleet to sail into the English Channel instead and eventually land at Torbay. Due to the involvement of the Schomberg family in the invasion, Charles Louis Schomberg was made Marquess of Harwich.\n\nWriter Daniel Defoe devotes a few pages to the town in \"A tour thro' the Whole Island of Great Britain\". Visiting in 1722, he noted its formidable fort and harbour \"of a vast extent\". The town, he recounts, was also known for an unusual spring rising on Beacon Hill (a promontory to the north-east of the town), which \"petrified\" clay, allowing it to be used to pave Harwich's streets and build its walls. The locals also claimed that \"the same spring is said to turn wood into iron\", but Defoe put this down to the presence of \"copperas\" in the water. Regarding the atmosphere of the town, he states: \"Harwich is a town of hurry and business, not much of gaiety and pleasure; yet the inhabitants seem warm in their nests and some of them are very wealthy\".\n\nHarwich Dockyard was established as a Royal Navy Dockyard in 1652. It ceased to operate as a Royal Dockyard in 1713 (though a Royal Navy presence was maintained until 1829). During the Second World War parts of Harwich were again requisitioned for naval use, and ships were based at HMS Badger; Badger was decommissioned in 1946, but the Royal Naval Auxiliary Service maintained a headquarters on the site until 1992.\n\nThe Royal Navy is no longer present in Harwich but Harwich International Port at nearby Parkeston continues to offer regular ferry services to the Hook of Holland (Hoek van Holland) in the Netherlands. Many operations of the large container port at Felixstowe and of Trinity House, the lighthouse authority, are managed from Harwich.\n\nThe port is famous for the phrase \"Harwich for the Continent\", seen on road signs and in London & North Eastern Railway (LNER) advertisements.\n\nAt least three pairs of lighthouses have been built over recent centuries as leading lights, to help guide vessels into Harwich. The earliest pair were wooden structures: the High Light stood on top of the old Town Gate, whilst the Low Light (featured in a painting by Constable) stood on the foreshore. Both were coal-fired.\n\nIn 1818 these were replaced by stone structures, designed by John Rennie Senior, which can still be seen today (they no longer function as lighthouses: one houses the town's maritime museum, the other is (in 2015) also being converted into a museum). They were owned by General Rebow of Wivenhoe Park, who was able to charge 1d per ton on all cargo entering the port, for upkeep of the lights. In 1836 Rebow's lease on the lights was purchased by Trinity House, but in 1863 they were declared redundant due to a change the position of the channel used by ships entering and leaving the port, caused by shifting sands.\n\nThey were in turn replaced by the pair of cast iron lights at nearby Dovercourt; these too remain in situ, but were decommissioned (again due to shifting of the channel) in 1917.\n\nDespite, or perhaps because of, its small size Harwich is highly regarded in terms of architectural heritage, and the whole of the older part of the town, excluding Navyard Wharf, is a conservation area.\n\nThe regular street plan with principal thoroughfares connected by numerous small alleys indicates the town’s medieval origins, although many buildings of this period are hidden behind 18th century facades.\nThe extant medieval structures are largely private homes. The house featured in the image of Kings Head St to the left is unique in the town and is an example of a sailmaker's house, thought to have been built circa 1600. Notable public buildings include the parish church of St. Nicholas (1821) in a restrained Gothic style, with many original furnishings, including a somewhat altered organ in the west end gallery. There is also the Guildhall of 1769, the only Grade I listed building in Harwich.\nThe Pier Hotel of 1860 and the building that was the Great Eastern Hotel of 1864 can both been seen on the quayside, both reflecting the town's new importance to travellers following the arrival of the railway line from Colchester in 1854. In 1923, The Great Eastern Hotel was closed by the newly formed LNER, as the Great Eastern Railway had opened a new hotel with the same name at the new passenger port at Parkeston Quay, causing a decline in numbers.\nThe hotel became the Harwich Town Hall, which included the Magistrates Court and, following changes in local government, was sold and divided into apartments.\n\nAlso of interest are the High Lighthouse (1818), the unusual Treadwheel Crane (late 17th century), the Old Custom Houses on West Street, a number of Victorian shopfronts and the Electric Palace Cinema (1911), one of the oldest purpose-built cinemas to survive complete with its ornamental frontage and original projection room still intact and operational.\nThere is little notable building from the later parts of the 20th century, but major recent additions include the lifeboat station and two new structures for Trinity House. The Trinity House office building, next door to the Old Custom Houses, was completed in 2005. All three additions are influenced by the high-tech style.\n\nHarwich was the home town of Christopher Jones, the master and quarter-owner of the Mayflower. The famous diarist Samuel Pepys was the Member of Parliament for Harwich. Christopher Newport, captain of the expedition that founded Jamestown, Virginia, also hailed from Harwich. Captain Charles Fryatt lived in Harwich, and his body was brought back from Belgium in 1919 and he was buried at Dovercourt. More recently, the reclusive but well-known award-winning Western Australian writer Randolph Stow made his home in Harwich, in the knowledge that his ancestors lived in the area. Stow's 1984 novel \"The Suburbs of Hell\" draws on Old Harwich for its setting. Stow was a notable and determined supporter of the project to conserve the historic character of the town.\n\nHarwich has also historically hosted a number of notable visitors, including many members of British and European royalty; in many instances these visits are linked with Harwich's rich maritime past.\n\nHarwich is home to Harwich & Parkeston F.C.; Harwich and Dovercourt RFC; Harwich & Dovercourt Sailing Club; Harwich, Dovercourt & Parkeston Swimming Club; Harwich & Dovercourt Rugby Union Football Club; Harwich & Dovercourt Cricket Club; and Harwich Runners who with support from Harwich Swimming Club host the annual Harwich Triathlons.\n\n\n", "id": "14065", "title": "Harwich"}
