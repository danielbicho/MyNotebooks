{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "    \n",
    "    if hasattr(t, 'node') and t.node:\n",
    "        if t.node == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "                \n",
    "    return entity_names\n",
    "\n",
    "\n",
    "with open('text/shipman5.txt', mode='r') as f:\n",
    "    sample = f.read().decode('utf8')  \n",
    "    sentences = nltk.sent_tokenize(sample)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "entity_names = []\n",
    "for tree in chunked_sentences:\n",
    "    # Print results per sentence\n",
    "    # print extract_entity_names(tree)    \n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "#print entity_names\n",
    "\n",
    "# Print unique entity names\n",
    "print(set(entity_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shipman4.out.txt',\n",
       " 'shipman1.out.txt',\n",
       " 'shipman3.out.txt',\n",
       " 'shipman2.out.txt',\n",
       " 'shipman5.out.txt',\n",
       " 'shipman6.out.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# textblob usage example\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "\n",
    "blobs = []\n",
    "for textfile in os.listdir('./tagged_text'):    \n",
    "    with open('./tagged_text/' + textfile) as f:\n",
    "        text = f.read().decode('utf8')           \n",
    "        t = TextBlob(text)\n",
    "        blobs.append(t)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "r = re.compile(r\"<PERSON>.*?<\\/PERSON>\")\n",
    "\n",
    "correlations = []\n",
    "# get named entities correlations\n",
    "with open('./tagged_text/' + textfile) as f:\n",
    "    for line in iter(f):        \n",
    "        correlation = r.findall(line)        \n",
    "        #print(len(correlation))        \n",
    "        if len(correlation) > 1:                                      \n",
    "            correlations.append(correlation)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_persontag(text):\n",
    "    import re\n",
    "    r = re.compile(r\"<PERSON>(.*)<\\/PERSON>\")\n",
    "    m = r.match(text)\n",
    "    return m.group(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a set of tuple\n",
    "unique_correlations = set()\n",
    "nodes = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for correlation in correlations:\n",
    "    for i in range(len(correlation)-1):  \n",
    "        nodes.add(correlation[i])\n",
    "        nodes.add(correlation[i+1])\n",
    "        pair = (correlation[i],correlation[i+1])\n",
    "        unique_correlations.add(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "cw = csv.writer(open(\"edge.csv\",'w'))\n",
    "for pair in unique_correlations:\n",
    "    cw.writerow([pair[0],pair[1],\"Edge from \" + strip_persontag(pair[0]) + \" from \" + strip_persontag(pair[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cw = csv.writer(open(\"nodes.csv\", 'w'))\n",
    "for node in nodes:\n",
    "    cw.writerow([node,strip_persontag(node)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def tokenize(text):\n",
    "    result_words = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = re.sub(r'[\\W_\\d]', ' ', text).lower().split()\n",
    "    for word in words:\n",
    "        result_words.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return result_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents_text = []\n",
    "for textname in os.listdir('./text/'):\n",
    "    with open('./text/' + textname) as f:\n",
    "        document_text = f.read().decode('utf8')\n",
    "        documents_text.append(document_text)        \n",
    "        \n",
    "#vectorizer = TfidfVectorizer(stop_words='english', tokenizer = tokenize)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "termsDocumentsMatrix = vectorizer.fit_transform(documents_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16022\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svdMatrix = svd.fit_transform(termsDocumentsMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16022)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept:\n",
      "zeebrugge\n",
      "leniency\n",
      "oversaw\n",
      "aides\n",
      "lesion\n",
      "epidemic\n",
      "envisaging\n",
      "environments\n",
      "proponent\n",
      "uptake\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Concept:\n",
      "death\n",
      "di\n",
      "deaths\n",
      "shipman\n",
      "coroner\n",
      "dr\n",
      "smith\n",
      "mrs\n",
      "reynolds\n",
      "cause\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Concept:\n",
      "drugs\n",
      "shipman\n",
      "controlled\n",
      "death\n",
      "patient\n",
      "wards\n",
      "pgi\n",
      "drug\n",
      "ward\n",
      "home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Concept:\n",
      "death\n",
      "doctor\n",
      "gmc\n",
      "medical\n",
      "wards\n",
      "pgi\n",
      "case\n",
      "shipman\n",
      "cases\n",
      "ward\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Concept:\n",
      "shipman\n",
      "di\n",
      "wards\n",
      "pgi\n",
      "patients\n",
      "ward\n",
      "smith\n",
      "gmc\n",
      "patient\n",
      "mrs\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find top terms for each concept\n",
    "import numpy as np\n",
    "\n",
    "for i in range(0, svd.components_.shape[0]):    \n",
    "    sortedTerms = np.argsort(svd.components_[i])\n",
    "    print \"Concept:\"\n",
    "    for k in range(10):\n",
    "        print terms[sortedTerms[k]]\n",
    "    print \"\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
